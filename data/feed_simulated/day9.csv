job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
IDB Invest Data Engineer Consultant,IDB Invest,"Washington, DC",https://www.linkedin.com/jobs/view/idb-invest-data-engineer-consultant-at-idb-invest-3780117228,2023-12-17,Sterling,United States,Associate,Onsite,"Analytics, Knowledge, and Information Division
Would you like to use your skills and experience to contribute to the economic and social development of Latin America and the Caribbean?
Location:
The IDB INVEST, a member of the Inter-American Development Bank (IDB) Group, is a multilateral organization based in Washington, D.C., that is committed to the development of Latin America and the Caribbean through the private sector.
IDB Invest is looking for a data engineer with SQL proficiency and experience structuring information for flexible querying and fast updates to provide business insight., preferably with experience in observational analyses, trend analyses, modeling, and new measurement strategies.
About This Position
Data Management and Analytics is the team under AKI Division that is responsible to define and implement the data strategy for IDB Invest.
What You’ll Do
Designs and develops data pipelines that extract data from various sources, transform it into the desired format, and load it into the appropriate data storage systems
Collaborates with data scientists and analysts to optimize models and algorithms for data quality, security, and governance.
Integrates data from different sources, including databases, data warehouses, APIs, and external systems.
Ensures data consistency and integrity during the integration process, performing data validation and cleaning as needed.
Transforms raw data into a usable format by applying data cleansing, aggregation, filtering, and enrichment techniques.
Optimizes data pipelines and data processing workflows for performance, scalability, and efficiency.
Implement large-scale data ecosystems including data management, governance, and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
What You´ll Need
Education
: A bachelor’s degree in computer science, data science, software engineering, information systems, or related quantitative field; master’s degree preferred
Experience: At least 4 years of experience working as a data engineer. Proven project experience developing and maintaining data warehouses in big data solutions. Ability to design, build, and deploy data solutions that capture, explore, transform, and utilize data to support AI, ML, and BI. Great communication and collaboration skill. Loves thinking out of the box to solve challenges at an unprecedented scale. Knowledge of data mining, machine learning tools, such as Databricks will be considered a plus. Knowledge of data governance solutions such as Informatica, collibra, allation and etc. will be considered a plus. Functional expertise: [i] You are able to work in a high-pressure environment while juggling multiple tasks, and effectively prioritize tasks, [ii] you are able to understand and translate clients’ (business units) needs into solutions, providing services that meet clients’ expectations for quality and performance; [iii] you listen actively, obtain necessary input, share ideas, speak persuasively, and convey information in a clear, objective, and concise manner.
Languages: You are proficient in written and verbal communication in English and in a second official language of the IDB Group (Spanish, Portuguese or French).
Requirements
Consanguinity
: You have no family members (up to the fourth degree of consanguinity and second degree of affinity, including spouse) working at the IDB, IDB Invest, or IDB Lab.
Type Of Contract And Duration
Type of contract:
Consultant Full-Time.
Length of contract: 18 months.
Location: Washington DC.
Responsible person: Head of Data Management & Analytics
What We Offer
The IDB group provides benefits that respond to the different needs and moments of an employee’s life. These benefits include a
competitive compensation
package comprised of:
Leaves and vacations: 2 days per month of contract + gender- neutral parental leave.
Health Insurance: the IDB Group provides a monthly allowance for the purchase of health insurance.
Savings plan: The IDB Group cares about your future, depending on the length of the contract, you will receive a monthly savings plan allowance.
Assistance with relocation and visa applications for you and your family, if applicable.
Work Schedules: Hybrid and flexible work schedules.
Development support: We offer learning opportunities to boost your professional profile such as seminars, one-to-one professional counseling, and much more.
Health and wellbeing: Access to our Health Services Center which provides preventive care and health education for all employees.
Other perks: Lactation Room, Daycare Center, Gym, Bike Racks, Parking, and others.
Our culture
At the IDB Group we work so everyone brings their best and authentic selves to work, willing to try new approaches without fear, and where they are accountable and rewarded for their actions.
Diversity, Equity, Inclusion and Belonging (DEIB) are at the center of our organization. We celebrate all dimensions of diversity and encourage women, LGBTQ+ people, persons with disabilities, Afro-descendants, and Indigenous people to apply.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job interview process. If you are a qualified candidate with a disability, please e-mail us at diversity@iadb.org to request reasonable accommodation to complete this application.
Our Human Resources Team reviews carefully every application.
About IDB Invest
IDB Invest, a member of the IDB Group, is a multilateral development bank committed to promoting the economic development of its member countries in Latin America and the Caribbean through the private sector. IDB Invest finances sustainable companies and projects to achieve financial results and maximize economic, social, and environmental development in the region. With a portfolio of $14.1 billion in asset management and 325 clients in 25 countries, IDB Invest provides innovative financial solutions and advisory services that meet the needs of its clients in a variety of industries.
Follow Us
https://www.linkedin.com/company/idbinvest/
https://www.facebook.com/IDBInvest
https://twitter.com/BIDInvest
Additional Information
External Opening Date: Dec 5, 2023
External Closing Date: Dec 21, 2023
External Contact Email: hrsc@iadb.org
External Contact Name: HR Service Center
Job Field: Technical Support
Show more
Show less","Data Engineering, Data Mining, Machine Learning, Business Intelligence, SQL, Data Warehousing, Data Pipelining, Data Integration, Data Cleansing, Data Transformation, Data Enrichment, Cloud Computing, Databricks, Informatica, Collibra, Allation, Data Governance, English, Spanish, Portuguese, French","data engineering, data mining, machine learning, business intelligence, sql, data warehousing, data pipelining, data integration, data cleansing, data transformation, data enrichment, cloud computing, databricks, informatica, collibra, allation, data governance, english, spanish, portuguese, french","allation, business intelligence, cloud computing, collibra, data engineering, data enrichment, data governance, data integration, data mining, data transformation, databricks, datacleaning, datapipeline, datawarehouse, english, french, informatica, machine learning, portuguese, spanish, sql"
Data Analyst,"ALTA IT Services, LLC","Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-at-alta-it-services-llc-3768084873,2023-12-17,Sterling,United States,Associate,Onsite,"IF INTERESTED REACH OUT DIRECTLY TO KSPICER@ALTAITS.COM OR CALL/TEXT 301-252-8762
DEPT OF STATE /HYBRID
Data/Business Intelligence Analyst
Required: SECRET can hold higher
Position Description
Seeking a Data Analyst to support a large federal government agency.
Description Of Duties
Utilize enterprise IT management systems and tools (e.g. PowerBI and ServiceNow) to collect and analyze data, develop, organize, and present dashboards, ad-hoc reports and other data visualizations for internal and external customers.
Apply statistical methodologies to identify trends and patterns in the data. Interpret findings and make recommendations based on industry standard IT management processes.
Design, develop, create and maintain visually intuitive, appealing and interactive dashboards, reports and visualizations using tools such as Microsoft Power BI, Tableau or similar platforms.
Prepare and brief senior level management, effectively communicating complex information to technical and non-technical stakeholders.
Collaborate with cross-functional teams and other external organizations in the performance of responsibilities and duties.
Required Experience/Skills/Attributes
Bachelor’s in computer science, data science, related field, or equivalent work experience
3-5+ years of work experience in data management and analysis
Desired Experience/Skills/Attributes
Prior work experience in government
ITIL v4 Foundations
ISO 20K
Show more
Show less","Data Analysis, PowerBI, ServiceNow, Dashboards, Statistical methodologies, Data visualizations, Microsoft Power BI, Tableau, Data management, ITIL v4 Foundations, ISO 20K","data analysis, powerbi, servicenow, dashboards, statistical methodologies, data visualizations, microsoft power bi, tableau, data management, itil v4 foundations, iso 20k","dashboard, data management, data visualizations, dataanalytics, iso 20k, itil v4 foundations, microsoft power bi, powerbi, servicenow, statistical methodologies, tableau"
Data Security Specialist / Security Engineer,ECS,"Washington, DC",https://www.linkedin.com/jobs/view/data-security-specialist-security-engineer-at-ecs-3737595664,2023-12-17,Sterling,United States,Associate,Onsite,"ECS is seeking a
Data Security Specialist / Security Engineer
to work in our
Washington, DC
office / Remote .
Job Description:
ECS is seeking an experienced Data Security Specialist.In this role you will work directly with administrators to architect secure systems. Independently assess systems for secure configuration and compliance to Federal, NIST 800-53, and Treasury directives.Thoroughly and accurately write security documentation including System Security Plans and Security Assessment Reports.Brief management and administrators on findings.Expect to work directly with administrators to identify and remedy findings and self-direct work to meet OFR deadlines.Design, develop, engineer, and implement solutions to MLS requirements. Perform complex risk analyses which also include risk assessment. Establish and satisfy information assurance and security requirements based upon the analysis of user, policy, regulatory, and resource demands. Support customers at the highest levels in the development and implementation of doctrine and policies. Apply know-how to government and commercial common user systems, as well as to dedicated special purpose systems requiring specialized security features and procedures. Perform analysis, design, and development of security features for system architectures.
Engage directly with administrators and advise them on how to securely configure and administer their applications and operating systems
Perform risk assessments on major applications and technologies and advise management of risks involved in system operation
Perform the work to complete and write from scratch all of the following documentation in a thorough, accurate, and grammatically correct manner:
Security Assessment Reports
Security Impact Assessments
System Security Plans
Risk Assessments
Security Risk Compliance Matrix
Certification Memos
Accreditation Memos
Risk Acceptance Memos
POAMs
Architect secure systems by direct engagement with system and application administrators
Assess system compliance with federal information security mandates, Treasury Directives, and NIST guidance—and advise management when compliance is deficient
Read results of vulnerability scans, identify false positives, and work with administrators to resolve vulnerabilities
Brief management and administrators on findings and recommendations
Be proactive, self-directed, and align schedule to meet OFR deadlines
Required Skills:
10+ years experience in Information Security
5+ years writing security documentation such as System Security Plans (SSP) and System Assessment Reports
3+ years performing risk assessments.
3+ years as security system architect—advising administrators/developers on how to create and configure secure applications.
3+ years as a system or network administrator
Expert knowledge of NIST 800 Special Publications, Federal Information Processing Standards, and OMB memos on Information Security
Desired Skills:
CISSP or like certification desired
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Information Security, NIST 800, System Security Plans (SSP), Security Assessment Reports, Risk Assessments, Security System Architect, System Administrator, Network Administrator, Vulnerability Scans, CISSP","information security, nist 800, system security plans ssp, security assessment reports, risk assessments, security system architect, system administrator, network administrator, vulnerability scans, cissp","cissp, information security, network administrator, nist 800, risk assessments, security assessment reports, security system architect, system administrator, system security plans ssp, vulnerability scans"
DoD Cleared - Data Analyst,ISN Corp,"Bethesda, MD",https://www.linkedin.com/jobs/view/dod-cleared-data-analyst-at-isn-corp-3763926887,2023-12-17,Sterling,United States,Associate,Onsite,"Company Description
Headquartered in Bethesda, Maryland, ISN Corporation is a nationwide provider of specialized professional services to Federal government agencies. ISN's portfolio includes work with over 100 Federal agencies.
ISN Corporation has been awarded a place in the
Washington Business Journal's list of the 50 Fastest Growing Government Contractors.
ISN is was also on the
Business Inc. Magazine's 5000 List of Fastest Growing Private Companies
for two years.
ISN Corporation offers many competitive benefits to its employees, including:
Major medical insurance with prescription coverage
Dental plan
Flexible spending
Short-term and long-term disability benefits at no cost to employees
Basic life insurance at no cost to employees
Retirement Plan
Paid leave with accrual beginning at Date of Hire
We are looking for an energetic, self-starting, organized, detail-oriented candidate with the ability to multitask in a fast-paced environment to join our team.
Headquartered in Bethesda, Maryland, ISN Corporation is a nationwide provider of specialized professional services to Federal government agencies. ISN's portfolio includes work with over 100 Federal agencies.
ISN Corporation has been awarded a place in the
Washington Business Journal's list of the 50 Fastest Growing Government Contractors.
ISN is was also on the
Business Inc. Magazine's 5000 List of Fastest Growing Private Companies
for two years.
ISN Corporation offers many competitive benefits to its employees, including:
Major medical insurance with prescription coverage
Dental plan
Flexible spending
Short-term and long-term disability benefits at no cost to employees
Basic life insurance at no cost to employees
Retirement Plan
Paid leave with accrual beginning at Date of Hire
We are looking for an energetic, self-starting, organized, detail-oriented candidate with the ability to multitask in a fast-paced environment to join our team.
Job Description
The Department of Defense (DOD), Office of Inspector General (OIG), established in 1982 by the Inspector General Act of 1978, as amended, DOD OIG is an independent, agency that conducts oversight and identifies, deters, and detects fraud, waste, and abuse in DOD operations and programs. The investigative arm of the DOD OIG is the Defense Criminal Investigative Service (DCIS) that works closely with law enforcement agencies to conduct criminal investigations of matters critical to DOD property, programs, and operations with an emphasis on life, safety, and readiness. The importance of the DOD OIG oversight work is signified by the enormity of the Department’s mission, the numerous assets that DOD utilizes to accomplish its mission, the magnitude of the $600 billion budget, and the over 3 million personnel who are part of the DOD family. The DOD OIG’s oversight work balances the needs and requests of both the DOD and Congress. The purpose of this contract effort is intended to acquire various levels of support to assist each of the components accomplish prospective activities. The ISN DoD OIG IT Contract Support Services Data Analyst, will provide analytical data support to various components as needed (floater), with minimal government assistance. The contractor shall serves as the subject matter expert in the creation and maintenance of the external (public) and internal OIG websites. This position serves as the subject matter expert in the configuration and maintenance of the Issuetrak application. This position researches and recommends web analytical tools to support internal and external OIG websites. The position recommends strategies for search engine optimization to ensure the OIG content is searchable across all Internet search engines for greater visibility of OIG website and content. Works with OIG's Office of the Chief Information Officer to assess and address any technical challenges in displaying the content. The contractor shall assist with data queries, cleanup, and database maintenance to ensure data integrity and reliability consistent with Government Accountability Office guidelines, and OIG standards. The following tasks serve to provide the contactor with a better understanding of the Objective. The tasks are not all inclusive as this is a performance based work statement designed to allow the contractor to develop the total solution for meeting the needs of the objective: Task 1: Respond to user questions and concerns regarding access, data entry, data quality and application issues. Task 2: Write new requirements for development, modification to forms within the system. Task 3: Engage with OCIO to recommend system changes or enhancements for the development and modification of pages and forms within the application. Task 4: Interpret complex data results and communicate orally and in writing to non-technical users such as headquarters managers and other personnel. Task 5: Develop, modify, and manage complex queries using a customized version of Microsoft Dynamics CRM to support user requirements. Task 6: Review results of queries and resolve data discrepancies. Task 7: Develop searchable reports, views and dashboards that meet the needs of stakeholders. Task 8: Create, modify, configure, and maintain Microsoft Dynamics CRM entities, attributes, data views, and forms using functional requirements and process maps to satisfy the business case.
Qualifications
Data analysts/technical support should have the following skills in order to perform the above functions: 1. Experience with application development using Oracle Developer Suite (form and report), ASP .NET, ColdFusion, or C#. 2. Technical proficiency in Microsoft products including but not limited to: SharePoint, Office (especially Excel), Visio, and Project. As well as other software products including Web Site Design and functionality, Adobe Captivate and Lexis CaseSoft Suite. 3. Proficiency in basic database structure, parent-child entity relationships, or permissions-based systems roles. 4. Experience with Microsoft Dynamics CRM. 5. Certified in Microsoft Dynamics CRM.
Must pass and maintain Active DoD Secret clearance throughout the life of the contract.
Bachelors in technical field. Masters preferred.
Additional Information
ISN Corporation is an Equal Opportunity Employer
ISN Corporation is an Equal Employment Opportunity/Affirmative Action employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, protected veteran status, disability status, marital status, genetic information, or any other characteristic protected by law.
ISN Corporation is a Drug-Free Workplace. Candidates are required to pass a pre-employment background investigation before beginning employment.
Show more
Show less","ASP.NET, ColdFusion, C#, Oracle Developer Suite, SharePoint, Microsoft Office, Visio, Project, Web Site Design, Adobe Captivate, Lexis CaseSoft Suite, Microsoft Dynamics CRM, Database Structure, Oracle","aspnet, coldfusion, c, oracle developer suite, sharepoint, microsoft office, visio, project, web site design, adobe captivate, lexis casesoft suite, microsoft dynamics crm, database structure, oracle","adobe captivate, aspnet, c, coldfusion, database structure, lexis casesoft suite, microsoft dynamics crm, microsoft office, oracle, oracle developer suite, project, sharepoint, visio, web site design"
Data Analyst,ECS,"Arlington, VA",https://www.linkedin.com/jobs/view/data-analyst-at-ecs-3787116972,2023-12-17,Sterling,United States,Associate,Hybrid,"ECS is seeking a
Data Analyst
to work in our
Arlington, VA
office .
Job Description:
Job Description: ECS is looking to hire a Data Analyst to join our Federal Practice in the DC area. This role requires you to interface with customers and other stakeholders to develop quantitative measurements and data insights at a large organization. Responsibilities: Assist with the development of concrete and demonstrable metrics and measures of program outputs and outcomes Mine quantifiable metrics from interviews and written narratives from program stakeholders Develop and review program and project documentation Support efforts to improve access to data and reporting tools through improved and standardized customer experience (CX) design Provide support for agile practices and ceremonies Support project management planning and task execution Provide analysis and solution recommendations for continual process improvement, data organization and integrity, security compliance and assessments for new technologies Develop processes for data analysis inclusion in new project planning
Required Skills:
US Citizenship is required.
Ability to obtain government security clearance (Public Trust/ Sensitive)
Must hold a Bachelor’s degree 3+ years of experience in a relevant field.
Experience with data analysis/reporting/visualization.
Desired Skills:
Strong requirement elicitation, analysis, and documentation skills.
Ability to manage multiple responsibilities across multiple workstreams with tight deadlines in a fast-paced environment.
Ability to effectively communicate and share knowledge with contacts at all levels. Skilled in developing collaborative relationships.
Ability to operate under minimum supervision where appropriate and understand when to escalate issues leadership Proficient at consolidating data from numerous sources to create presentations for all levels of management and other stakeholders.
Ability to build dashboards and visualize data using tools such as PowerBI or Tableau.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Data Analysis, Data Visualization, PowerBI, Tableau, Data Mining, Metrics Development, Reporting, Project Management, Process Improvement, Data Integrity, Security Compliance, Agile Development, Customer Experience Design, Requirement Elicitation, Data Consolidation, Presentation Skills, Leadership, Communication, Collaboration","data analysis, data visualization, powerbi, tableau, data mining, metrics development, reporting, project management, process improvement, data integrity, security compliance, agile development, customer experience design, requirement elicitation, data consolidation, presentation skills, leadership, communication, collaboration","agile development, collaboration, communication, customer experience design, data consolidation, data integrity, data mining, dataanalytics, leadership, metrics development, powerbi, presentation skills, process improvement, project management, reporting, requirement elicitation, security compliance, tableau, visualization"
Senior Data Analyst,"The Clearing, Inc.","District of Columbia, United States",https://www.linkedin.com/jobs/view/senior-data-analyst-at-the-clearing-inc-3780469062,2023-12-17,Sterling,United States,Associate,Hybrid,"Description
COMPANY DESCRIPTION
The Clearing is a management consulting firm that helps leaders identify underlying causes of organizational obstacles, resolve highly complex challenges, prioritize the fewest, most important initiatives to tackle regardless of conflicting needs, and make informed decisions in the context of an agreed upon mission, vision, and strategy.
We help create peak performance organizations through strategy, organizational development, leadership training, and change management. Our experienced consultants bring a powerful blend of analytic and creative skills from diverse fields, including change management, finance, engineering, communications, education, policy, design, and corporate leadership.
POSITION DESCRIPTION
In this position, data meets consulting. Senior Data Analysts infuse scientific insight into TC’s change strategy consulting work by viewing things with a scientific, data-centric lens.
This position requires an experienced data analyst who can lead client relations with a customer-centric focus. He/she/they leads opportunities for complex data to be leveraged to put analytics into the hands and brains of multiple clients. The Senior Data Analyst improves client decision-making and processes through complex data insights, accuracy, and integrity.
WHAT YOU’LL GET TO DO
Use complex data analysis to drive change strategies and process improvement, infusing efficiency and automation into many analytical tasks for customers
Think creatively about customer data sources, proxy measures, and novel means for extracting data
Develop and deliver data-focused presentations and reports, including a variety of data visualizations (dashboards, charts, graphs, and plots) and partner with visual consulting team as needed
Develop and write code for both analysis and visualization purposes. May oversee or review code development of more junior colleagues.
Proactively identify opportunities for and provide support to drive data-based decision-making with customers
Reevaluate and update processes and clean datasets when needed
Ensure data quality and integrity by performing extensive peer Quality Assurance (QA) reviews
Translate client objectives into clear data-centered change and other strategies, using non-technical language
Keep a pulse on where data tools are headed
Develop and maintain client relationships
Identify and support opportunities to expand project scope and actively share information about emerging customer support needs and trends with team members and management
Design, facilitate, and manage client meetings to successfully achieve identified and desired outcomes
Work on a full-time project or juggle multiple (2-4) part-time projects
Validate requirements and act as a supporting writer on proposals with a data-centric lens
Oversee work of others, which may include leading medium-sized project teams
Develop short- and long-term project plans
Deliver work on time, within budget, and to requested specifications; deliver error proof files at the completion of a project
Requirements
WHAT YOU BRING
Minimum of 6 years experience, at least 4 of which involve working as a data analyst and/or data scientist, in a consultative capacity, preferably with a background in management consulting, client relations, data analytics, data science, and/or coding
Master’s Degree in a highly analytical field required for the position (e.g., STEM or social sciences, data analytics or data science).
Strong curiosity, initiative, and willingness to learn, including an interest in exploring, learning, and upskilling data science capabilities
Incredible detail-orientation in all data and consulting tasks
Strong analytical and critical thinking abilities, including creative, out of the box thinking
Excellent ability to communicate complex data into digestible, simple ways to non-technical customers
Strong ability to engage and communicate with senior leaders and/or multiple stakeholders
A passionate commitment to quality
Ability to work with autonomy, but knowledge of when to reach to senior leadership as needed
Ability to review the work of peers and others and provide constructive, detailed feedback
Proven ability to adapt quickly to emerging requirements and prioritize as appropriate
Comfort working through times of ambiguity and in an environment of rapid growth
Excellent verbal and written communication skills; ability to strategically and effectively present data work with supporting reasoning
Ability to understand issues quickly and recommend and apply problem solving skills to data-centered solutions
Strong time management and organization skills and willingness to be accountable for deadlines and project outcomes
Willingness and ability to obtain and maintain a U.S. security clearance
Willingness and ability to travel to client sites in the surrounding DC and Baltimore metro areas; occasional travel outside of these regions could be expected (5%)
WHAT TOOLS/LANGUAGES YOU HAVE DEMONSTRATED EXPERIENCE APPLYING:
Advanced In
Intermediate In
Bonus Points For
Excel, Google Sheets
Tableau, SQL
Python, R
Statistics
Predictive Analytics
PHYSICAL AND MENTAL REQUIREMENTS
While performing duties of this job, an employee may be required to perform any, or all of the following: Attend meetings in and out of the office, travel (sometimes extensively); Occasional evening and weekend work may be required as job duties demand; Communicate effectively (both orally and in writing); Ability to effectively use computers and other electronic and standard office equipment; Occasionally exerting up to 10 pounds to lift, carry, push, pull or otherwise move objects, including the human body. Work is primarily sedentary and involves sitting for several hours at a time. Occasionally walking, climbing stairs, and standing occur in this role. Additionally, this job requires certain mental demands, including the ability to use judgment, withstand moderate amounts of stress, and maintain attention to detail. The Clearing is committed to partnering with all candidates and employees to ensure reasonable accommodations are made to meet these requirements.
EEO
The Clearing is committed to providing equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, religion, color, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, family medical history or genetic information, political affiliation, military service, or other non-merit based factors. In addition to federal legal requirements, The Clearing complies with applicable state and local laws governing nondiscrimination in employment. These protections extend to all terms and conditions of employment, including recruiting and hiring practices, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training and career development programs.
Show more
Show less","Data analysis, Consulting, Data scientist, Data analytics, Data science, Coding, Data visualization, SQL, Tableau, Python, Excel, Statistics, Predictive analytics, Machine learning, R","data analysis, consulting, data scientist, data analytics, data science, coding, data visualization, sql, tableau, python, excel, statistics, predictive analytics, machine learning, r","coding, consulting, data science, data scientist, dataanalytics, excel, machine learning, predictive analytics, python, r, sql, statistics, tableau, visualization"
Sr. Data Analyst (Hybrid),Fannie Mae,"Reston, VA",https://www.linkedin.com/jobs/view/sr-data-analyst-hybrid-at-fannie-mae-3783190153,2023-12-17,Sterling,United States,Associate,Hybrid,"Company Description
At Fannie Mae, futures are made. The inspiring work we do helps make a home a possibility for millions of homeowners and renters. Every day offers compelling opportunities to use tech to tackle housing’s biggest challenges and impact the future of the industry. You’ll be a part of an expert team thriving in an energizing, flexible environment. Here, you will grow your career and help create access to fair, affordable housing finance.
Job Description
As a valued colleague on our team, you will enhance and/or refine existing applications and systems to meet new technical requirements with support from leadership, and perform data analysis at various stages of projects to ensure development of a quality application.
THE IMPACT YOU WILL MAKE
Responsibilities
The Sr Data Analyst role will offer you the flexibility to make each day your own, while working alongside people who care so that you can deliver on the following responsibilities:
Collaborate with application owners to review defined business processes.
Review and define technical and/or data requirements for moderately complex application enhancements.
Independently establish and communicate standards for project teams.
Contribute to application test plan and evaluate test results.
Qualifications
THE EXPERIENCE YOU BRING TO THE TEAM
Minimum Required Experiences
At least 2 years of relevant professional work experience
Experience in querying databases with the use of relevant software such as Toad and DBeaver
Communication skills including communicating in writing or verbally, copywriting, planning and distributing communication, etc.
Desired Experiences
Bachelor's degree or equivalent
4+ years of relevant professional work experience
Experience working as a Product Owner
Ability to work independently as well as part of a bigger team.
Working knowledge of the mortgage banking industry and secondary mortgage market processes and systems is desired but not essential.
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Product Development skills including designing products, developing product roadmaps, translating design requirements, prototyping, etc.
Skilled in documentation and database reporting for the purposes of analysis, data discovery, and decision-making with the use of relevant software such as Crystal Reports, Excel, or SSRS
Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand
Experience in the process of analyzing data to identify trends or relationships to inform conclusions about the data
Operational Excellence skills including improving and overseeing operations
Determining causes of operating errors and taking corrective action
Experience gathering accurate information to explain concepts and answer critical questions
Working with people with different functional expertise respectfully and cooperatively to work toward a common goal
Skilled in the graphical representation of information in the form of a charts, diagrams, pictures, and dashboards with programs and tools such as Excel, Tableau, or Power BI
Experience using JIRA
Experience using SharePoint
Additional Information
The future is what you make it to be. Discover compelling opportunities at careers.fanniemae.com.
Fannie Mae is an Equal Opportunity Employer, which means we are committed to fostering a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, religion, national origin, gender, gender identity, sexual orientation, personal appearance, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation in the application process, email us at careers_mailbox@fanniemae.com.
The hiring range for this role is set forth on each of our job postings located on Fannie Mae's Career Site. Final salaries will generally vary within that range based on factors that include but are not limited to, skill set, depth of experience, certifications, and other relevant qualifications. This position is eligible to participate in a Fannie Mae incentive program (subject to the terms of the program). As part of our comprehensive benefits package, Fannie Mae offers a broad range of Health, Life, Voluntary Lifestyle, and other benefits and perks that enhance an employee’s physical, mental, emotional, and financial well-being. See more here.
Show more
Show less","Data Analysis, Software Development, Technical Writing, Product Development, Database Reporting, Data Visualization, Project Management, Communication, AWS, Excel, Tableau, Power BI, JIRA, SharePoint, Toad, DBeaver, Crystal Reports, SSRS","data analysis, software development, technical writing, product development, database reporting, data visualization, project management, communication, aws, excel, tableau, power bi, jira, sharepoint, toad, dbeaver, crystal reports, ssrs","aws, communication, crystal reports, dataanalytics, database reporting, dbeaver, excel, jira, powerbi, product development, project management, sharepoint, software development, ssrs, tableau, technical writing, toad, visualization"
Data Scientist,Burtch Works,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-scientist-at-burtch-works-3778877258,2023-12-17,Sterling,United States,Associate,Hybrid,"New Data Science opening in Washington, D.C.! Our client, an exciting, established startup, is looking to bring on a Data Scientist to join their growing analytics team. This highly-visible role will give you the chance to learn from a team of great mentors, working on analytics and data science projects while also liaising with team members across the business all the way up to the top floor! Ideal candidates will have solid Python or R skills and experience utilizing these tools to analyze large datasets to build models from the ground up.
Requirements:
• Master’s degree in a quantitative field
• Strong proficiency in Python (preferred) and/or R
• Statistical modeling experience – time series, regression, forecasting, etc.
• The ability to draw insights and tell stories from data
• Strong communication required – presenting to C-Suite and outside organizations
• Knowledge of Artificial Intelligence a plus
Base salary up to around $90K. Unfortunately, only US Citizens and Green Card holders can be considered at this time. This HYBRID position is based in Washington, D.C. and is in office 2 days a week.
Keywords:
data science, R, python, SQL, Tableau, analytics, time series, forecasting, optimization, machine learning, ML, AI, artificial intelligence, modeling, statistical, data mining, AWS, reporting, Power BI
Show more
Show less","Data Science, Python, R, Statistical Modeling, Time Series, Regression, Forecasting, Data Analysis, Data Mining, Artificial Intelligence, Machine Learning, Cloud Computing, AWS, Reporting, Data Visualization","data science, python, r, statistical modeling, time series, regression, forecasting, data analysis, data mining, artificial intelligence, machine learning, cloud computing, aws, reporting, data visualization","artificial intelligence, aws, cloud computing, data mining, data science, dataanalytics, forecasting, machine learning, python, r, regression, reporting, statistical modeling, time series, visualization"
Data Visualization Analyst,ECS,"Arlington, VA",https://www.linkedin.com/jobs/view/data-visualization-analyst-at-ecs-3764263419,2023-12-17,Sterling,United States,Associate,Hybrid,"ECS is seeking a
Data Visualization Analyst
to work in our
Arlington, VA
office.
ECS is seeking a passionate and driven visualization-focused data analyst to help our public sector customer secure the federal civilian IT enterprise, specifically focusing on the cybersecurity health of High Value Assets. Strong candidates will take advantage of modern approaches to business analysis, user centered design (UCD), information presentation, and the use of data analytics to improve client mission outcomes. This front line, operationally focused, client facing consultant develops deep understanding of the client’s mission and priorities, anticipates client needs, and translates them into mission-impactful analytics questions that can be addressed by data analytics using data available in the operational environment. Collaborating with other data ECS colleagues and co-contractor peers, the ideal candidate builds out polished information (data) presentations — dashboards, excel files, other media as appropriate — that are intuitively comprehensible for “regular people”, those without the word “data” in their job titles.
Job Responsibilities:
Assemble and maintain a holistic point of view about the client’s mission and operations. Document this perspective using industry standard business analysis methods.
Decompose this holistic perspective into a portfolio of distinct, separable use cases addressable by analytics. Work with client stakeholders to prioritize portfolio elements in terms of mission benefit. Consult data technology peers to estimate technical complexity for each.
Sequentially and incrementally build out information visualization use cases according to an Agile-like delivery methodology. Apply user centered design methods to achieve information visualization solutions that are fit to the way that client stakeholders perform their missions. Make optimum use of data analytics and visualization tools available in client’s tightly security-constrained IT environment.
Respond to ad hoc client needs arising from a dynamic, continually changing operating tempo. Reprioritize work activities on the fly and clearly and proactively communicate the resulting impacts to stakeholders awaiting other deliverables in backlog.
Required Skills:
Must be a US Citizen
Must be able to obtain Public Trust Suitability
Bachelor’s degree (or equivalent experience) in computer science, information systems management, mathematics, engineering, or other relevant discipline.
5+ years front-line technology consulting experience with a proven track record of delivering data analytics work products that address questions and decisions of central importance to clients’ missions.
Fluency in leading approaches to user-centered design (UCD). Implementation and execution of repeatable UCD methods that capture analytics consumers operations and inject relevant information into them.
Experience generating data products and reports that organize data analysis in a consumable format for a broad range of stakeholders, including interactive data visualizations and infographics.
Proven ability in using tools like Tableau, PowerBI, kibana, and Grafana to present mission information in intuitive, useful interfaces. Realization of user journeys and workflows through mission information (data) that allow analytics consumers to quickly navigate through a sequence of questions.
Proficiency with analytics information assembly via relational databases and other structured-information technologies. Fluency in SQL with the ability to quickly construct queries from denormalized and normalized data structures.
Keen understanding of the value of data quality and its importance to effective data analytics. Ability to articulate essential quality aspects of available data to clients and limitations of what can be learned from them.
Desired Skills:
Business and logical data modeling. Ability to recognize mission-essential information in operational activities and procedures. Experience in systematically deconstructing information into distinct domains in the form of conceptual data models.
Conversance with business analytics techniques, particularly descriptive analytics. Familiarity with basic statistical methods. Ability to clearly discern which statistical methods are best suited to a given question, and to articulate limitations in certainty from statistical results.
Project management leadership in an Agile development context. Ability to assemble team member inputs in terms of task scope and duration as well as interdependencies, then incorporate them into sprint and epic plans. Confidence to track work performance progress and communicate risk and issues to client stakeholders.
Business process modeling and mining. Experience with methods and techniques to document operational workflows comprised of interactions between humans and systems. Knowledge in methods to identify mission information exchanges within those workflows and associate them with technical data elements in client’s mission information environment.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3000+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Business Analysis, User Centered Design (UCD), Data Analytics, Dashboard Creation, Data Visualization, Agile Delivery, SQL, Relational Databases, Tableau, PowerBI, Kibana, Grafana, Data Quality, Business Intelligence, Business Process Modeling, Business Process Mining, Agile Development, Project Management","business analysis, user centered design ucd, data analytics, dashboard creation, data visualization, agile delivery, sql, relational databases, tableau, powerbi, kibana, grafana, data quality, business intelligence, business process modeling, business process mining, agile development, project management","agile delivery, agile development, business analysis, business intelligence, business process mining, business process modeling, dashboard creation, data quality, dataanalytics, grafana, kibana, powerbi, project management, relational databases, sql, tableau, user centered design ucd, visualization"
Unstructured Data Analyst,ITAC Solutions,"Alexandria, VA",https://www.linkedin.com/jobs/view/unstructured-data-analyst-at-itac-solutions-3359693007,2023-12-17,Sterling,United States,Associate,Hybrid,"ITAC Solutions is assisting one of our clients in Alexandria, VA, in their search for an
Unstructured Data Analyst
.
What you’ll be doing (duties of this position):
Follow detailed instructions on all work
Complete assigned metadata creation related tasks that are routine in nature
Review documents
Identify metadata information from the documents
Key in the metadata information
Perform other duties as assigned.
What you’ll need to be considered (requirements):
Trained and skilled in overall computer systems and applications (i.e., Microsoft Word, PowerPoint, and Excel)
Meet certification level requirements to the degree defined in the GSA contract
FS08, the individual should have at a minimum, a Bachelor’s Degree with two years of experience
FS09, the individual should have at a minimum, an Associate’s Degree with three years of related experience
FS10, the individual should have at a minimum, an Associate’s Degree with five years of experience.
Hold current United States of America citizenship
Minimum Top Secret security clearance is required to perform this task.
What could set you apart from the rest (preferred skills / experience / knowledge):
Previous military or DOD experience is a plus
Show more
Show less","Microsoft Word, Microsoft PowerPoint, Microsoft Excel, Metadata Creation, Document Review, Data Entry","microsoft word, microsoft powerpoint, microsoft excel, metadata creation, document review, data entry","data entry, document review, metadata creation, microsoft excel, microsoft powerpoint, microsoft word"
Data Scientist - Advana,Zencon Group,"Arlington, VA",https://www.linkedin.com/jobs/view/data-scientist-advana-at-zencon-group-3758132346,2023-12-17,Sterling,United States,Associate,Hybrid,"Currently looking for Secret cleared and higher Data Scientists local to the DC Metro.  On-site 2-3 days/week, some flexibility, but preference given to candidates willing to go on-site more.
Booz Allen is seeking a skilled and highly motivated Data Scientist to join our Advana team. As a Data Scientist, you will play a crucial role in building data pipelines to efficiently collect and process data from various external sources. Your expertise in developing validation and analytics processes to ensure our team is fully developed to provide data accuracy and reliability for executing mission-critical work.
Responsibilities:
Collaborate with analysts and product leads to identify and implement data solutions.
Optimize data pipelines to ensure efficient performance with minimal human intervention.
Build data pipelines for collecting and processing data from various external sources.
Work with data engineers to manage data pipelines and troubleshoot issues to maintain data quality and reliability.
Develop validation and analytics processes that are consistent with business needs and strategic goals.
Assist with machine learning and data visualization initiatives as needed.
Continuously improve data solutions to effectively address customer needs.
Communicate and coordinate with teams to accomplish objectives and deliver excellence.
Requirements:
Secret clearance or higher is required.
Minimum of 4 years with
SQL and modern Big Data ETL technologies like NiFi or StreamSets.
Bachelor's degree in a related field.
Strong background in a modern programming language, such as Python or Java, with at least 4 years of experience working in a big data and cloud environment.
Ability to quickly grasp technical concepts and collaborate with multiple functional groups
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Detail-oriented mindset with a commitment to delivering high-quality results.
Must be in the DC Metro area and available to work onsite (Crystal City, VA and Alexandria, VA) 2-3 days per week.
Nice to Have:
Recent DoD or IC-related experience.
Experience in working in an agile development environment.
Previous experience with Advana is a plus.
Show more
Show less","Data Scientist, Secret Clearance, SQL, Apache NiFi, StreamSets, ETL, Python, Java, Big Data, Cloud, Data Pipelines, Machine Learning, Data Visualization, Bachelor's degree, Agile Development","data scientist, secret clearance, sql, apache nifi, streamsets, etl, python, java, big data, cloud, data pipelines, machine learning, data visualization, bachelors degree, agile development","agile development, apache nifi, bachelors degree, big data, cloud, data scientist, datapipeline, etl, java, machine learning, python, secret clearance, sql, streamsets, visualization"
Privacy Data Analyst,Venable LLP,"Washington, DC",https://www.linkedin.com/jobs/view/privacy-data-analyst-at-venable-llp-3725943397,2023-12-17,Sterling,United States,Associate,Hybrid,"Venable LLP’s Technology & Innovation Group seeks a Privacy Data Analyst to join the Venable Blue team in the Washington, D.C., New York, Los Angeles, or San Francisco office. The Privacy Data Analyst, works on and supports all aspects of client-based projects related to privacy program management, privacy operations, and regulatory response. This is a non-attorney position.
This role will help bring efficiency to how clients respond to regulatory compliance requests, including data gathering, automation of regulatory responses, and measuring operational effectiveness of privacy safeguards. This will include gap analysis and actionable recommendations for improvements in quality and accuracy.
The Privacy Data Analyst will work on and support the delivery of various workstreams such as Safeguard Maintenance, Safeguard Development, Program Management, and Audit Support.
Key responsibilities include working with clients to:
Formulate responses to privacy compliance requests within SLA, perform data quality reviews, and conduct operational effectiveness testing against privacy safeguards.
Develop and communicate best practices for regulatory response.
Develop automation for compliance responses to increase efficiency and to provide consistency and quality in responses.
Articulate data and technology gaps and their compliance impact to a variety of technical and non-technical stakeholders, including product and engineering teams, risk and compliance partners, assessors and regulators.
Recommend process improvements and strategic initiatives as related to privacy compliance response.
Coordinate and drive client privacy response activities for both inbound and outbound relationships.
Support business relationships with internal and external auditors and regulators.
Qualifications:
B.A. or B.S. degree; Master’s degree (or equivalent) preferred
3 years of minimum work experience in high profile settings, such as presenting to leadership and driving cross-functional teams
3 years of minimum experience in a quantitative role related to data, reporting, and analytical problem solving
3 years of minimum experience writing complex SQL queries to drive analysis and insights
Experience coding in Python
Preferred Qualifications:
Experience coding in R, PHP and/or similar programming language
Experience with developing security/privacy reporting and recommendations that are meaningful, defensible and actionable for a variety of audiences
Experience developing and submitting audit and compliance reports to governing bodies, legal entities, and/or external authorities
Experienced in processes for assessing and designing internal controls for large scale organizations
Experience performing risk assessments or safeguard/control operational effectiveness testing
Venable offers full-service solutions to everything from routine to novel privacy and cybersecurity challenges. Our team brings to bear significant experience and industry knowledge to help clients satisfy data privacy and security laws and maximize their business potential. Fully immersed in all aspects of data privacy, cybersecurity, and information governance, Venable is unique among privacy and cybersecurity practices.
We participate in legislative advocacy, rulemakings, and development of new legal standards. Our team advises organizations with regard to industry best practices and drafting codes of conduct and standards, helping them stay compliant with federal, state, international, and self-regulatory requirements.
We strengthen the integrity of our clients’ data, ecommerce security, and customer or user records; develop internal data collection and use practices; and ensure the creation of sound privacy policies and procedures.
Venable Blue helps organizations and individuals manage and mitigate risk in the online space. Whether it’s an issue of data access, account takeover, cyber harassment, child safety, or a government or regulatory investigation, we build, operationalize, and deploy integrated programs and systems designed with people and products in mind.
For additional information about the Technology & Innovation Group, Venable Blue team see: www.venableblue.com
Venable LLP is an
American Lawyer
Global 100 law firm headquartered in Washington, D.C., with offices in California, Delaware, Florida, Illinois, Maryland, New York, and Virginia. Our lawyers and legislative advisors serve domestic and international clients in all areas of corporate and business law, complex litigation, intellectual property, regulatory matters, and government affairs. Additional information can be found at Venable.com.
The salary range for Privacy Data Analyst positions is $85,000 - $130,000 per year. This is the minimum and maximum salary that Venable in good faith believes at the time of this posting that it is willing to pay for the advertised position. Exact compensation will be determined based on individual candidate qualifications and location.
We comply with the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance. Conviction of a crime will not necessarily be a bar to employment at the Firm. Factors such as age at the time of the offense, type of the offense, seriousness of the offense, remoteness of the offense in time, position applied for, rehabilitation, overall record, and other relevant factors will be taken into account in determining effect on suitability for employment.
V
enable LLP is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age, protected Veteran status and any other protected basis under applicable law.
Show more
Show less","Privacy Data Analyst, SQL, Python, R, PHP, Data Analysis, Reporting, Analytical Problem Solving, Regulatory Response, Privacy Compliance, Data Gathering, Automation, Data Quality Reviews, Operational Effectiveness Testing, Process Improvement, Strategic Initiatives, Risk Assessment, Safeguard Testing, Internal Controls, Legislative Advocacy, Rulemakings, Code of Conduct, Corporate Law, Complex Litigation, Intellectual Property, Regulatory Matters, Government Affairs","privacy data analyst, sql, python, r, php, data analysis, reporting, analytical problem solving, regulatory response, privacy compliance, data gathering, automation, data quality reviews, operational effectiveness testing, process improvement, strategic initiatives, risk assessment, safeguard testing, internal controls, legislative advocacy, rulemakings, code of conduct, corporate law, complex litigation, intellectual property, regulatory matters, government affairs","analytical problem solving, automation, code of conduct, complex litigation, corporate law, data gathering, data quality reviews, dataanalytics, government affairs, intellectual property, internal controls, legislative advocacy, operational effectiveness testing, php, privacy compliance, privacy data analyst, process improvement, python, r, regulatory matters, regulatory response, reporting, risk assessment, rulemakings, safeguard testing, sql, strategic initiatives"
Data Analytics Engineer,Eco Financial,"McLean, VA",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-eco-financial-3787934572,2023-12-17,Sterling,United States,Mid senior,Onsite,"Eco Financial is looking for an analytical and detail-oriented research analyst to assist in improving the operations and decision-making of our business. The research analyst's responsibilities include researching, collecting, analyzing, and interpreting data, and using the data to guide the business's decision-making. A research analyst should be prepared to work in various sectors, such as marketing, business operations, and finance.
To be a successful research analyst, you should have excellent mathematical, critical thinking, and communication skills. You should be flexible and able to work independently or in a team.
Responsibilities:
Analyzing past operations' results and performing variance analyses.
Identifying and analyzing trends and forecasts and recommending improvements to the business processes.
Researching market trends, conducting surveys, analyzing data from competitors, and analyzing the business's operations, expenditures, and customer retention to identify patterns of potential issues or improvements.
Using data analysis and interpretations to guide the decision-making of the business.
Using operations data to develop pricing models and identify areas for improvement.
Using statistical, economic, and data modeling techniques and tools.
Organizing and analyzing data, creating charts and graphs, and presenting your findings to the leadership team.
Providing recommendations to improve future business operations.
Organizing and storing data for future research projects.
Testing processes, policies, and protocols for efficiency and improvements.
Requirements:
A bachelor's or associate's degree in economics, finance, statistics, computer science, or related field.
Experience in applied research or data management may be advantageous.
Strong mathematical, analytical, and data modeling skills.
The ability to manipulate large, complex data sets into manageable, understandable reports.
Excellent problem-solving, communication, and team-working skills.
Familiarity with data modeling software and Excel software.
Attention to detail and organizational skills.
Powered by JazzHR
iI30NJICox
Show more
Show less","Data analysis, Data management, Research, Data modeling, Statistics, Economics, Computer science, Excel, Data visualization, Problemsolving, Communication, Teamwork","data analysis, data management, research, data modeling, statistics, economics, computer science, excel, data visualization, problemsolving, communication, teamwork","communication, computer science, data management, dataanalytics, datamodeling, economics, excel, problemsolving, research, statistics, teamwork, visualization"
Data Engineer,Jobs for Humanity,"McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3785372771,2023-12-17,Sterling,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Job Title: Data Engineer** **Location: McLean, Virginia, United States** Are you passionate about using technology to solve complex problems and thrive in a collaborative, inclusive, and fast-paced environment? Join the Capital One team and be part of a group of innovative problem solvers who prioritize meeting customer needs. We are currently looking for a Senior Associate, Data Engineer to drive a major transformation within Capital One's Finance Tech team. **What You'll Do:** - Seek out opportunities to address customer needs and build solutions that solve important problems. - Support the design and development of scalable data architectures and systems for extracting, storing, and processing large amounts of data. - Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity. - Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling. - Implement testing, validation, and pipeline observability to ensure data pipelines meet customer SLAs. **Basic Qualifications:** - Bachelor's Degree. - At least 2 years of application development experience. - At least 1 year of experience in big data technologies. **Preferred Qualifications:** - 3+ years of experience developing data pipelines using Python or Scala. - 2+ years of experience with distributed computing tools like Spark, EMR, and Hadoop. - 2+ years of experience with UNIX/Linux and basic commands and shell scripting. - 1+ years of experience with a public cloud platform like AWS, Microsoft Azure, or Google Cloud. - 1+ years of data warehousing experience with technologies like Redshift or Snowflake. - 1+ years of experience with Agile engineering practices. Please note that at this time, Capital One will not sponsor a new applicant for employment authorization for this position. **Why Join Capital One:** Capital One offers a comprehensive set of health, financial, and other benefits that support your total well-being. Learn more about our benefits on the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. **How to Apply:** To apply for this position, please [click here](link to application page). **Equal Opportunity Employer:** Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We welcome applicants from all backgrounds and do not discriminate based on sex, race, age, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state, or local law. As a company, we promote a drug-free workplace and will consider applicants with a criminal history in accordance with applicable laws. **Accessibility Accommodations:** If you require an accommodation to access our website or to apply for a position, please contact Capital One Recruiting at 1-800-304-9102 or email RecruitingAccommodation@capitalone.com. We will ensure that any accommodations needed are provided in a confidential manner and used only as required to provide needed reasonable accommodations. **Technical Support:** For technical support or questions regarding our recruiting process, please email Careers@capitalone.com. **Disclaimer:** Capital One does not provide, endorse, or guarantee third-party products, services, educational tools, or other information available through this site. Please note that positions posted in Canada are for Capital One Canada, positions posted in the United Kingdom are for Capital One Europe, and positions posted in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Python, Scala, Spark, EMR, Hadoop, UNIX/Linux, AWS, Microsoft Azure, Google Cloud, Redshift, Snowflake, Agile","python, scala, spark, emr, hadoop, unixlinux, aws, microsoft azure, google cloud, redshift, snowflake, agile","agile, aws, emr, google cloud, hadoop, microsoft azure, python, redshift, scala, snowflake, spark, unixlinux"
Sr Engineer - Data Science,Sonitalent Corp,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-at-sonitalent-corp-3718978162,2023-12-17,Sterling,United States,Mid senior,Onsite,"Role: Sr Engineer - Data Science
Location: Gaithersburg , MD Onsite 5x weekly (NEED TO LIVE IN A 15-20M RADIUS OF Gaithersburg MD)
Highlights in Yellow candidate must have
Required
Resume can be no longer than 2 pages
Needs to have degree/name of university/and year of graduation
Place of Birth
Conversion salary after 6m
Do not send any candidate that are not living in Fredrick or Montgomery County Maryland- or 15/20m radius of Gaithersburg Md
Needs to be a Senior candidate.
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Experience working across varying business and technical functional units
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Python, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Git, AWS, Google Cloud, MySQL, BigQuery, Tableau, Shiny","data science, machine learning, artificial intelligence, python, sql, pandas, scikitlearn, tensorflow, keras, git, aws, google cloud, mysql, bigquery, tableau, shiny","artificial intelligence, aws, bigquery, data science, git, google cloud, keras, machine learning, mysql, pandas, python, scikitlearn, shiny, sql, tableau, tensorflow"
"Looking for AWS Data Engineer - Reston, VA( Once in a Month) - Contract",Extend Information Systems Inc.,"Reston, VA",https://www.linkedin.com/jobs/view/looking-for-aws-data-engineer-reston-va-once-in-a-month-contract-at-extend-information-systems-inc-3755516291,2023-12-17,Sterling,United States,Mid senior,Onsite,"Hi,
I hope you are doing well!
We have an opportunity for
AWS Data Engineer
with one of our clients for
Reston, VA( Once in a Month)
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
AWS Data Engineer
Location: Reston, VA( Once in a Month)
Terms:
Contract
Job Description
And try for AWS Certified Profile(Good to Have).
Essential Skills
Strong Python development experience with Python, Pandas , NumPy, Pyspark
Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
Having prior working experience in Fannie Mae will be added advantage
Good Knowledge on Java and Database (Oracle Postgres)
Essential Qualification
Bachelors is a must
Thanks & Regards
Priyanka tiwari
Extend Information System Inc
Phone: (703) 956-1120
Email: priyanka1@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Show more
Show less","Python, Pandas, NumPy, PySpark, AWS, S3, RDS, EC2, Lambda, SQS, SNS, Redshift, Java, Oracle, Postgres, AWS Certified Profile","python, pandas, numpy, pyspark, aws, s3, rds, ec2, lambda, sqs, sns, redshift, java, oracle, postgres, aws certified profile","aws, aws certified profile, ec2, java, lambda, numpy, oracle, pandas, postgres, python, rds, redshift, s3, sns, spark, sqs"
AWS Data Engineer with Splunk,Futran Solutions,"Reston, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-with-splunk-at-futran-solutions-3687059599,2023-12-17,Sterling,United States,Mid senior,Onsite,"Role : AWS Data Engineer with Splunk
Location: Reston, VA( Once in a month)
Experience : 10+yrs
Job Description
Please don't Submit any Profile without Splunk exp.
AWS Data Engineer with splunk
Strong Python, AWS skills and expereince with Splunk development.
Advanced knowledge of AWS Services/Architecture
Experience in AWS Compute such as EC2, Lambda, Beanstalk, Batch or ECS
Experience with AWS Storage services such as: S3, EFS, Glacier.
Experience in AWS Management and Governance suite of products such as CloudTrail, CloudWatch
Experience in AWS Analytics such as Athena, EMR, Glue, Redshift, Kinesis
Strong knowledge in Python object-oriented programming
Strong experience with AWS Database services such as: RDS, DynamoDB
Experience using APIs for developing or programming software
Experience using AWS Application Integration Services such as: Simple Notification Service (SNS), Simple Queue Service (SQS), Step Functions.
Experience with AWS Developer tools such as: CodeDeploy, CodePipeline
Experience with JSON
Strong experience with SQL
Experience with enterprise data lakes, data warehouses, data marts, and big data.
Strong experience with data migration, cloud migration, and ETL processes.
Experience determining causes of operating errors and taking corrective action
Show more
Show less","Splunk, Python, AWS, EC2, Lambda, Beanstalk, Batch, ECS, S3, EFS, Glacier, CloudTrail, CloudWatch, Athena, EMR, Glue, Redshift, Kinesis, Objectoriented programming, RDS, DynamoDB, APIs, SNS, SQS, Step Functions, CodeDeploy, CodePipeline, JSON, SQL, Data lakes, Data warehouses, Data marts, Big data, Data migration, Cloud migration, ETL processes","splunk, python, aws, ec2, lambda, beanstalk, batch, ecs, s3, efs, glacier, cloudtrail, cloudwatch, athena, emr, glue, redshift, kinesis, objectoriented programming, rds, dynamodb, apis, sns, sqs, step functions, codedeploy, codepipeline, json, sql, data lakes, data warehouses, data marts, big data, data migration, cloud migration, etl processes","apis, athena, aws, batch, beanstalk, big data, cloud migration, cloudtrail, cloudwatch, codedeploy, codepipeline, data lakes, data marts, data migration, data warehouses, dynamodb, ec2, ecs, efs, emr, etl, glacier, glue, json, kinesis, lambda, objectoriented programming, python, rds, redshift, s3, sns, splunk, sql, sqs, step functions"
Database Engineer,Red Arch Solutions,"Chantilly, VA",https://www.linkedin.com/jobs/view/database-engineer-at-red-arch-solutions-3787773126,2023-12-17,Sterling,United States,Mid senior,Onsite,"Red Arch Solutions (RAS)
is building a DIA Enterprise Data Platform (EDP) that will facilitate the transition and retirement of duplicative and legacy data services and provide an automated, modernized, enterprise data hub within a secure environment. EDP will enable DIA to expose its data to a wider audience through a zero-trust based, data centric architecture. Based on a robust data governance policy approach, EDP will free data from siloes and allow more users to gain valuable insights from DIA data assets. RAS will maintain and improve the EDP to ensure DIA has high data availability, accessibility, veracity, integration, and discoverability while enabling data analytics, visualization, security, governance, and management across the enterprise.
Basic Job Responsibilities:
Data Onboarding Architecture: Data Engineers design and create the overall data onboarding architecture for DDH. They work closely with stakeholders to understand their data requirements and translate them into scalable data models and structures.
Data Analysis: Inspect data with the goal of discovering useful information needed to create data schemas.
Data Pipeline Development: Data Engineers build data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses. They develop efficient and reliable ETL processes to ensure data quality, integrity, and timeliness.
Data Integration: Data Engineers integrate disparate data sources and systems, both internal and external, to enable seamless data flow across different platforms. They may work with APIs, databases, cloud services, and third-party tools to establish data connections and ensure smooth data integration.
Data Transformation and Processing: Data Engineers transform raw data into a format suitable for analysis or consumption. This involves cleaning, filtering, aggregating, and structuring the data using tools like SQL, Python.
Data Quality and Governance: Ensuring data quality and integrity is a crucial responsibility of Data Engineers. They implement data quality checks, data validation rules, and data governance practices to maintain accurate and consistent data throughout the data lifecycle.
Scalability and Performance: Data Engineers optimize data pipelines and infrastructure for scalability and performance.
Data Security: Data Engineers play a role in securing data assets by validating security policies are implemented for the data.
Monitoring and Troubleshooting: Data Engineers monitor data pipelines and systems to identify and resolve issues proactively.
Collaboration: Data Engineers collaborate with internal DDH teams and cross-functional teams (CDO, MARs).
Customer Engagement: Engage in regular meetings and communication channels with Data Stewards and Data Steward Representatives to gather information needed to appropriately understand and secure their data in preparation for data onboarding.
Provide “white glove” service to Data Stewards for data onboarding questionnaire assistance.
Training and User Support: Provide training to Data Stewards and end users on DDH UI.
Documentation: Author or update documentation such as the Data Onboarding Execution Plan, data onboarding Standard Operating Procedure, and any associated DDH documentation (CONOPS, CDD).
Data Engineering Skills:
Data Modeling and Database Design: Proficiency in designing efficient and scalable data models, understanding relational and non-relational databases, and knowledge of SQL
ETL and Data Integration: Experience with Extract, Transform, Load (ETL) processes, data integration techniques, and tools like Apache NiFi.
Programming and Scripting: Strong programming skills in languages such as Python.
Data Warehousing: Understanding of data warehousing concepts and experience with data warehouse technologies.
Data Pipeline and Workflow Tools: Proficiency in tools for building and managing data pipelines and workflows.
Data Streaming: Knowledge of real-time data streaming technologies such as Apache Kafka for processing and analyzing streaming data.
Data Quality and Governance: Understanding of data quality frameworks, data validation techniques, and experience implementing data governance practices.
Cloud Platforms: Familiarity with cloud platforms like Amazon Web Services (AWS and their relevant data services.
Data Security: Knowledge of data security best practices, data encryption techniques, and understanding of privacy regulations.
Data Visualization: Ability to work with data visualization tools like Tableau, Power BI, or QlikView to create meaningful visualizations and reports.
Problem Solving and Troubleshooting: Strong analytical and problem-solving skills to identify and resolve issues in data pipelines, databases, or data infrastructure.
Collaboration and Communication: Effective communication skills to collaborate with cross-functional teams, understand requirements, and present technical solutions to non-technical individuals.
Continuous Learning: A mindset of continuous learning and keeping up-to-date with evolving technologies, tools, and industry trends in the field of data engineering.
Qualifications:
Bachelor Degree + 12 years or Master Degree + 10 years experience or 16 years of experience.
Experience developing requirements based on needs obtained from users.
Experience building scripts to manipulate data for ingestion into databases.
TS/SCI with Poly is required for this position**
All Red Arch Solutions openings require US Citizenship.
Red Arch Solutions is a proven and effective small business integrator and consultant, recognized as a leading provider of IT development to the Federal Government. We offer excellent benefits, including 20 days PTO, 10 holidays, up to 10% 401k contribution, and reimbursement for tuition/certifications. Top of the line PPO Medical, Dental, Vision, and Short and Long-Term disability are also offered.
Red Arch Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, national origin, age, marital status, disability, or protected veteran status. Red Arch Solutions takes affirmative action in support of its policy to advance in employment individuals who are minorities, women, protected veterans and individuals with disabilities.
Powered by JazzHR
09kC53NTiZ
Show more
Show less","Data Engineering, Data Modeling, Database Design, SQL, ETL, Data Integration, Apache NiFi, Python, Data Warehousing, Data Pipeline Tools, Workflow Tools, Data Streaming, Apache Kafka, Data Quality, Data Governance, Data Validation, Data Security, Data Encryption, Privacy Regulations, Data Visualization, Tableau, Power BI, QlikView, Problem Solving, Troubleshooting, Collaboration, Communication, Continuous Learning","data engineering, data modeling, database design, sql, etl, data integration, apache nifi, python, data warehousing, data pipeline tools, workflow tools, data streaming, apache kafka, data quality, data governance, data validation, data security, data encryption, privacy regulations, data visualization, tableau, power bi, qlikview, problem solving, troubleshooting, collaboration, communication, continuous learning","apache kafka, apache nifi, collaboration, communication, continuous learning, data encryption, data engineering, data governance, data integration, data pipeline tools, data quality, data security, data streaming, data validation, database design, datamodeling, datawarehouse, etl, powerbi, privacy regulations, problem solving, python, qlikview, sql, tableau, troubleshooting, visualization, workflow tools"
Big Data Engineer BDE21-1,"DataSync Technologies, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/big-data-engineer-bde21-1-at-datasync-technologies-inc-3361817635,2023-12-17,Sterling,United States,Mid senior,Onsite,"We are in search of a Big Data Engineer who can seamlessly mesh tech know-how with business acumen to help us navigate our clients big data computing needs. Not only are you experienced in cloud-based tech, but you have a firm grasp of platforms and applications, and have knowledge of how best to customize these attributes to our customers business demands. Not only can you help us connect the dots and evolve our capabilities over time, but you are also evolving your own capabilities as an engineer and keeping an eye on developments.
Big Data Engineer Responsibilities
Gather and process raw data at scale.
Design and develop data applications using selected tools and frameworks as required and requested.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.
Process unstructured data into a form suitable for analysis.
Monitoring data performance and modifying infrastructure as needed.
Define data retention policies.
Big Data Engineer Requirements
Bachelors Degree or more in Computer Science or a related field.
A solid track record of data management showing your flawless execution and attention to detail.
Programming experience, ideally in Python, Spark, Kafka or Java , and a willingness to learn new programming languages to meet goals and objectives.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
A willingness to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.
Experience with Data Lakes a plus.
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.
_______________________
Interested in Joining Our Team?
- Check out this YouTube video!
Be a part of an award-winning, employee friendly company in Northern VA and have the satisfaction of helping keep America safe. DataSync Technologies, Inc is a veteran owned small business providing consulting excellence and real time solutions for customers with complex information technology needs within Intelligence Community. Our cleared consultants bring real world experience with a common sense approach to their jobs whether they are creating complex analytic dashboards, architecting new cloud technology infrastructures, securing sensitive data or streamlining business processes for efficiency.
Equal Employment Opportunity
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at the Department of Labor's Website .
DataSync is committed to providing veteran employment opportunities to our service men and women.
Show more
Show less","Big Data, Cloud Computing, Data Mining, Data Lake, Python, Spark, Kafka, Java, SQL, Data Retention Policies, Data Management, Data Processing, Data Innovations, Data Analysis, Web Scraping","big data, cloud computing, data mining, data lake, python, spark, kafka, java, sql, data retention policies, data management, data processing, data innovations, data analysis, web scraping","big data, cloud computing, data innovations, data lake, data management, data mining, data processing, data retention policies, dataanalytics, java, kafka, python, spark, sql, web scraping"
Data Engineer ETL Expert (2021-0140),Acclaim Technical Services,"Chantilly, VA",https://www.linkedin.com/jobs/view/data-engineer-etl-expert-2021-0140-at-acclaim-technical-services-3787773691,2023-12-17,Sterling,United States,Mid senior,Onsite,"Acclaim Technical Services, founded in 2000, is a leading language and intelligence services company supporting a wide range of U.S. Federal agencies. We are an Employee Stock Ownership Plan (ESOP) company, which is uncommon within our business sector. We see this as a significant strength, and it shows: ATS is consistently ranked as a top workplace among DC area firms and continues to grow.
We are actively hiring a
Data Engineer (ETL) Expert with TS/SCI clearance and polygraph
to join our team working in Chantilly, Virginia. In this position, you will manipulate data and data flows for both existing and new systems. You will also provide support in data extraction, transformation and load (ETL), data mapping, analytical support, operational support, and maintenance support of data and associated systems. As a member of this team, you will work in a fast-paced, dynamic team-based environment.
Responsibilities
Research, design, develop and/or modifies enterprise-wide systems and/or application software.
Develop complex data flows, or makes significant enhancements to existing pipelines.
Resolves complex hardware/software compatibility and interface design considerations.
Conducts investigations and tests of considerable complexity.
Researches emerging technologies to determine impact on application execution.
Provides input to staff involved in writing and updating technical documentation.
Troubleshoots complex problems and provides customer support for the ETL process
Advises hardware engineers on machine characteristics that affect software systems, such as storage capacity, processing speed, and input/output requirements.
Required Education & Experience
Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
8-10 years of related software engineering and ETL experience.
Experience building and maintaining data flows in NiFi or Pentaho.
Excellent organizational, coordination, interpersonal and team building skills.
Familiarization with NoSQL datastores
Familiarization executing jobs in Big Data Technologies (i.e., Hadoop or Spark)
Excellent organizational, coordination, interpersonal and team building skills.
Familiarization with NoSQL datastores
Familiarization executing jobs in Big Data Technologies (i.e., Hadoop or Spark)
Desired Experience
Experience with the following languages: Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML/XHTML, CSS, Python, Shell Scripting, JSON
Knowledge of servers operating systems; Windows, Linux, Distributed Computing, Blade Centers, and cloud infrastructure
Strong problem solving skills
Ability to comprehend database methodologies
Focus on continual process improvement with a proactive approach to problem solving
Ability to follow directions and finish task
Equal Employment Opportunity / Affirmative Action
ATS is committed to a program of equal employment opportunity without regard to race, color, ethnicity, national origin, ancestry, citizenship, sex, pregnancy, marital status, sexual orientation, gender identity, age, religion/creed, hairstyles and hair textures, handicap/disability, genetic information/history, military/veteran status, or any other characteristic or condition protected by federal, state or local law. It is the policy of ATS not merely to refrain from employment discrimination as required by the various federal, state, and local enactments, but to take positive affirmative action to realize for women, people of color, individuals with disabilities and protected veterans full equal employment opportunity. We support the employment and advancement in employment of individuals with disabilities and of protected veterans, and we treat qualified individuals without discrimination on the basis of their physical or mental disability or veteran status.
Powered by JazzHR
yto09kmPBt
Show more
Show less","Data Engineering, ETL, Data Extraction, Data Transformation, Data Loading, Hadoop, Spark, Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML, XHTML, CSS, Python, Shell Scripting, JSON, Windows, Linux, Distributed Computing, Blade Centers, Cloud Infrastructure, NoSQL","data engineering, etl, data extraction, data transformation, data loading, hadoop, spark, javaj2ee, c, c, sql, xml, xquery, xpath, ruby on rails, html, xhtml, css, python, shell scripting, json, windows, linux, distributed computing, blade centers, cloud infrastructure, nosql","blade centers, c, cloud infrastructure, css, data engineering, data extraction, data loading, data transformation, distributed computing, etl, hadoop, html, javaj2ee, json, linux, nosql, python, ruby on rails, shell scripting, spark, sql, windows, xhtml, xml, xpath, xquery"
Senior Data Engineer,"IT Concepts, Inc","Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-it-concepts-inc-3767063622,2023-12-17,Sterling,United States,Mid senior,Onsite,"Description
Founded in 2003, IT Concepts’ core values – customer-centricity, teamwork, driven to deliver, innovation, and integrity – ensure we work together to be the best, realize objectives, and make a positive impact in our communities. We intentionally created and sustain our ITC culture that embraces change, experimentation, continuous learning, and improvement. We bring our design thinking problem solving approach that challenges assumptions, prioritizes curiosity, and invites complexity to deliver innovative, efficient, and effective solutions. As we continue to grow in the support of our government customers, we are looking for driven and innovative individuals to join our team.
IT Concepts is looking for an experienced Senior-Level Data Engineer to support our intelligence customer. The Data Engineer and team will also provide the following functions:
Design and develop end-to-end data pipelines in SQL Server database; oversee build, implementation, and optimization of these pipelines.
Monitor and ensure data pipelines meet performance, availability, and quality standards.
Implement data orchestration solutions for data pipelines.
Manipulate large volumes of data into formats suitable for analytics and reporting.
Adhere to data governance policies and best practices for protecting data.
Research, analyze, and document various technical approaches (including implementation, benefits, constraints, outcomes, and associated risks).
Collaborate with data architect/database administrator to address and resolve problems in the case of failed jobs.
Develop metadata documentation and architecture documents.
Engage with technical and functional program subject matter experts and stakeholders to understand business processes, data requirements, critical data elements, data mappings, data protection guidelines, etc.
Provide recommendations for improvements in data strategies, including governance, integration, and standardization changes.
Assist in developing briefing packages to communicate complex ideas to non-technical customers.
Location: Work location is JBAB, DC or Reston, VA
Requirements
Bachelor’s degree in computer science, mathematics, engineering, information systems, or related.
7+ years of experience as a data engineer (e.g., building data pipelines, coding best practices, maintaining data quality)
Demonstrated experience with ETL processes and concepts in relational (SQL) database management systems.
Expertise in Python and/or SQL
Proficient in Agile development, git operations
Experience translating requirements into technical tasks and developing data structures to support the generation of business insights and strategy.
Innovative problem-solving and root cause identification skills.
Knowledge of data modeling
Experience with data orchestration tools (e.g., Dagster, Prefect, Apache Airflow)
Experience communicating and presenting data to non-technical stakeholders, written and verbally.
Must be self-directed, detail-oriented, and able to coordinate and work with a dynamic team and multiple stakeholders.
Clearance requirements:
Must have an active TS/SCI at the time of hire, and willing/able to get CI Polygraph
Benefits
The Company
We believe in generating success collaboratively, enabling long-term mission success, and building trust for the next challenge. With you as our partner, let’s solve challenges, think innovatively, and maximize impact. As a valued member of our ITC community, you have the unique opportunity to work in a diverse range of technology and business career paths, all while supporting our nation and delivering innovative technology solutions. We are a close community of experts that pride ourselves on creating an environment defined by teamwork, dedication, and excellence.
We hold three ISO certifications (27001:2013, 20000-1:2011, 9001:2015) and two CMMI ML 3 ratings (DEV and SVC).
Industry Recognition
Growth | Inc 5000’s Fastest Growing Private Companies, DC Metro List Fastest Growing; Washington Business Journal: Fastest Growing Companies, Top Performing Small Technology Companies in Greater D.C.
Culture | Northern Virginia Technology Council Tech 100 Honoree; Virginia Best Place to Work; Washington Business Journal: Best Places to Work, Corporate Diversity Index Winner – Mid-Size Companies, Companies Owned by People of Color; Department of Labor’s HireVets for our work helping veterans transition; SECAF Award of Excellence finalist; Victory Military Friendly Brand; Virginia Values Veterans (V3); Cystic Fibrosis Foundation Corporate Breath Award
Benefits
We offer great benefits – Competitive Paid Time Off, Medical, Dental and Vision Insurance, Identity Protection, Pet Insurance, 401(k) with company matching.
We invest in our employees – Every employee is provided with a stipend to invest in certifications, a master’s degree, or even a doctorate. We want you to grow as an expert and a leader and offer flexibility for you to take a course, a certification, or attend a conference. We are committed to supporting your curiosity and sustaining a culture that prioritizes commitment to continuous professional development.
We work hard, we play hard. ITC is committed to injecting fun into every day. We dedicate funds for activities – virtual and in-person – e.g., we have four season tickets to Nationals games that are available every month, we host happy hours, holiday events, fitness events, and annual celebrations. In alignment with our commitment to our communities, we host and attend charity galas/events. We believe in appreciating your commitment and building a positive workspace for you to be creative, innovative, and happy.
AAEO & VEVRAA
IT Concepts is an Affirmative Action/Equal Opportunity employer and a VEVRAA (Vietnam Era Veterans' Readjustment Assistance Act) Federal Contractor. As such, any personnel decisions (hire, promotion, job status, etc.) on applicants and/or employees are based on merit, qualifications, competence and business needs, not on race, color, citizenship status, national origin, ancestry, , sexual orientation, gender identity, age, religion, creed, physical or mental disability, pregnancy, childbirth or related medical condition, genetic information of the employee or family member of the employee, marital status, veteran status, political affiliation, or any other factor protected by federal, state or local law.
IT Concepts maintains a strong commitment to compliance with VEVRAA and other applicable federal, state, and local laws governing equal employment opportunity. We have developed comprehensive policies and procedures to ensure that our hiring practices align with these requirements.
As a part of our VEVRAA compliance efforts, [Company Name] has established an affirmative action plan that outlines our commitment to the recruitment, hiring, and advancement of protected veterans. This plan is regularly reviewed and updated to ensure its effectiveness.
We encourage protected veterans to self-identify during the application process. This information is strictly confidential and will only be used for reporting and compliance purposes as required by law. Providing this information is voluntary, and it will not impact your eligibility for employment.
Our commitment to equal employment opportunity extends beyond legal compliance. We are dedicated to fostering an inclusive workplace where all employees, including protected veterans, are treated with dignity, respect, and fairness.
How To Apply
To apply to IT Concept Position- Please click on the: “Apply for this Job” button at the bottom of this Job Description or the button at the top: “Application.” You can upload your resume and complete all the application steps. You must submit the application for IT Concepts to receive. If you need alternative application methods, please email careers@useitc.com and request assistance.
Accommodations
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.
Show more
Show less","SQL, Python, Data pipelines, Data governance, Data orchestration, Data modeling, ETL processes, Data analysis, Data strategizing, Agile development, Git, Cloud computing, Presentation skills","sql, python, data pipelines, data governance, data orchestration, data modeling, etl processes, data analysis, data strategizing, agile development, git, cloud computing, presentation skills","agile development, cloud computing, data governance, data orchestration, data strategizing, dataanalytics, datamodeling, datapipeline, etl, git, presentation skills, python, sql"
Expert Data Engineer,ANSER,"Washington, DC",https://www.linkedin.com/jobs/view/expert-data-engineer-at-anser-3693335224,2023-12-17,Sterling,United States,Mid senior,Onsite,"Company Overview
ANSER enhances national and homeland security by strengthening public institutions. We provide thought leadership for complex issues through independent analysis, and we deliver practical, useful solutions. ANSER values collaboration, integrity, and initiative and we are client focused in all that we do. Because we were established for the purpose of public service and not for profit, we measure our success in the impact of our service.
Job Description
ANSER is seeking an Expert Data Engineer (Operational Suitability Specialist) to support programs at (Joint Base Anacostia-Bolling JBAB).
Responsibilities
Designs, implements, and operates data management systems for intelligence needs. Designs how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems. Works with data users to determine, create, and populate optimal data architectures, structures, and systems. Plans, designs, and optimizes data throughput and query performance. Participates in the selection of backend database technologies (e.g., SQL, NoSQL, HPC, etc.), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.
Experience providing operational suitability test support to test programs or support DIA in testing procedure development for externally funded developments.
Possesses extensive logistics and suitability experience, particularly in operations analysis and support, logistics, and maintenance.
Possesses a minimum 4 years of academic course work w/ current logistics/suitability experience with space systems or 6 years practical knowledge of space systems ground systems, ground segments, manufacturing and maintenance, and mission planning.
Experience serving as the suitability expert for issues relating to reliability, availability, compatibility, transportability, interoperability, maintainability, safety, human factors, manpower supportability, logistics supportability, environmental effects, system documentation, and training requirements; contribute to all facets of test design and planning using in-depth knowledge of system under test, test policies, and test range capabilities.
Experience assisting the Test Director in timely preparation of high-quality suitability analysis across the varied test venues during all test phases to inform system development and operational evaluation; developing space system suitability specific data collection, data processing, data evaluation and data reporting as well as proactively identify suitability gaps, risks, or other issues across several test programs.
Qualifications
Current TS/SCI with ability to pass a polygraph.
Minimum 20 years of experience conducting analysis relevant to the specific labor category with at least a portion of the experience within the last 2 years.
Master’s degree in an area related to the labor category from a college or university accredited by an agency recognized by the U.S. Department of Education.
Disclaimer
In compliance with the Americans with Disabilities Act Amendment Act (ADA), if you have a disability and would like to request an accommodation in order to apply for a position with ANSER, please call 703-416-2000 or e-mail Recruiting@anser.org.ANSER is proud to be an Equal Opportunity Employer. We seek individuals from a broad variety of backgrounds with varying levels of experience who have a desire to do meaningful work. We recruit, employ, train, compensate, and promote regardless of race, color, gender, religion, national origin, ancestry, disability, age, veteran status, sexual orientation, or any other characteristic protected by law.
Show more
Show less","SQL, NoSQL, HPC, Data management systems, Data architectures, Data structures, Data systems, Data throughput, Query performance, Backend database technologies, ETL, Test design, Test planning, Test policies, Test range capabilities, Data collection, Data processing, Data evaluation, Data reporting, Suitability analysis, System development, Operational evaluation, Reliability, Availability, Compatibility, Transportability, Interoperability, Maintainability, Safety, Human factors, Manpower supportability, Logistics supportability, Environmental effects, System documentation, Training requirements","sql, nosql, hpc, data management systems, data architectures, data structures, data systems, data throughput, query performance, backend database technologies, etl, test design, test planning, test policies, test range capabilities, data collection, data processing, data evaluation, data reporting, suitability analysis, system development, operational evaluation, reliability, availability, compatibility, transportability, interoperability, maintainability, safety, human factors, manpower supportability, logistics supportability, environmental effects, system documentation, training requirements","availability, backend database technologies, compatibility, data architectures, data collection, data evaluation, data management systems, data processing, data reporting, data structures, data systems, data throughput, environmental effects, etl, hpc, human factors, interoperability, logistics supportability, maintainability, manpower supportability, nosql, operational evaluation, query performance, reliability, safety, sql, suitability analysis, system development, system documentation, test design, test planning, test policies, test range capabilities, training requirements, transportability"
Sr. Software/Systems/Data Engineer - SixMap (DC Area),DataTribe,"Washington, DC",https://www.linkedin.com/jobs/view/sr-software-systems-data-engineer-sixmap-dc-area-at-datatribe-3787755630,2023-12-17,Sterling,United States,Mid senior,Onsite,"Do you want to help build the next generation network security solution?
SixMap is working on leading edge network intrusion detection technology that enables enterprises and network operators to gain insights into their complete network attack surface and identify network vulnerabilities at unheard of speed and comprehensiveness. SixMap’s platform can complete IPv4 scans with deep and configurable service interrogation that is orders of magnitude faster than anything currently available. The team is building the world’s first platform to perform comprehensive IPv6 scans, previously thought to be impossible. Position Summary: We are looking for a systems-oriented senior software engineer to help build the core network mapping and interrogation engine. Candidates should have deep hands-on experience working on complex systems, data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking. The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development.
Come join us, if you are ready to change the world of network security while having some fun along the way.
Position Requirements:
To be considered for this position, you must:
Have at least 5 years’ experience and have a passion in understanding users’ needs and system requirements and turning them into working software
Have a BS degree or higher in computer science, electrical/computer engineering, or related technical field or equivalent work experience
Be fully fluent in Python and common data analysis Python libraries, C++, SQL, Airflow or other ETL / data pipeline tools, and preferably be a polyglot comfortable in many additional programming languages.
Be experienced in using relational databases and NoSQL data stores - PostgreSQL experience is a must.
Be experienced with Linux environments.
Have experience working on container-based cloud infrastructure frameworks such as Docker or Kubernetes within common cloud service providers such as AWS, GCP, or Azure.
Be experienced using Agile methodologies, operating cloud dev-ops, and coordinating with product development teams
Have the ability to thrive when presented a complex challenge in a fast-paced, performance-oriented culture with intelligent people
Have exceptional level of integrity, raw intelligence, creativity, energy, and passion
Operate efficiently with individual responsibility in a highly collaborative environment
Powered by JazzHR
UjAcy3uGZM
Show more
Show less","TCP/IP, Python, C++, SQL, Data Analysis, ETL, Airflow, PostgreSQL, NoSQL, Linux, Docker, Kubernetes, AWS, GCP, Azure, Agile Methodologies, Cloud DevOps","tcpip, python, c, sql, data analysis, etl, airflow, postgresql, nosql, linux, docker, kubernetes, aws, gcp, azure, agile methodologies, cloud devops","agile methodologies, airflow, aws, azure, c, cloud devops, dataanalytics, docker, etl, gcp, kubernetes, linux, nosql, postgresql, python, sql, tcpip"
Data Engineer/DBA,Cohere Technology Group,"Bethesda, MD",https://www.linkedin.com/jobs/view/data-engineer-dba-at-cohere-technology-group-3787903680,2023-12-17,Sterling,United States,Mid senior,Onsite,"Cohere is seeking fully cleared, dynamic technical professionals to join our team as a Member of an agile team you will work with the government customer to help shape a new phase of applications into the cloud environment. You will work closely with multiple contractor teams and the customer to transform approved requirements into features during each sprint. You will help to develop the new suite of applications one sprint at a time. As a Database Administrator/ Engineer, you will deliver services within a cloud environment, while working within an Agile project team. You will also be responsible for participating in collaborative discussions and helping to establish DB administration norms and practices to ensure consistency across the multiple development and integration efforts.
As a Database Administrator/ Engineer you will be responsible for the following:
Providing technical expertise and support in the use of PostgreSQL DBMS
Creating and maintaining database objects such as tables, views, index, constraints for data platform and custom applications
Developing, implementing, and maintaining database backup and recovery procedures for the processing environments, and ensures that data integrity, security, and recoverability are built into the DBMS applications.
Creating views to drive front-end data analytics and reporting.
Driving end-to-end availability, performance monitoring, and capacity planning for PostgreSQL, using native and 3rd party tools
Implementing automated methods and industry best practices for consistent installation and configuration of PostgreSQL for production, pre-production, and non-production environments
Administering PostgreSQL databases throughout the non-production (Sandbox, development, test, UAT, Training), pre-production (Staging) and production lifecycles in an AWS environment
Enhancing and maintaining logical and physical database designs from application requirements
Designing and implementing automation using SQL scripts and database functions
Responding to user reported errors in a timely manner.
Monitoring systems performance and identifying problems that may arise.
Liaison with developers and IT project managers
Recommending changes and enhancements for database performance and protection
Maintaining an awareness of trends and developments in database development and administration
Leveraging experience to provide support in the areas of data mapping, data extraction, transformation, and loading (ETL)
The ideal Database Administrator/ Engineer candidate should demonstrate the following skills:
Active TS/SCI w/FSP
Bachelor’s degree in computer science, software engineering, or related field of study
6+ years of experience in administering on-prem/cloud based multiuser environment with expertise in planning, designing, building, and implementing complex database systems.
Experience with AWS RDS or Aurora for PostgreSQL
Experience with optimizing database performance through exhaustive proactive testing and ongoing real-time monitoring
Experience with SQL scripting and database functions
Experience testing database security and enterprise policy guidelines according to established best practices.
Monitored and maintained the backups for database recoverability for all production databases.
Experience working in the database domain managing all operations of RDBMS databases such as Oracle/SQL Server/DB2 or PostgreSQL DB.
Familiar with open-source DBMS tools such as pgAdmin, pgModeler, or DBeaver.
Experience with data modeling and Entity-Relationship Diagrams (ERDs)
Experience with Structured Query Language (SQL)
Possess analytic problem-solving skills.
Knowledge of computer software applications, including database technologies and programming languages
Excellent verbal and written communication skills
Additional desired skills for the Database Administrator/ Engineer candidate include the following:
Experience implementing and deploying databases in Amazon Web Services (AWS)
Experience with AGILE / DEVOPS a plus
Working knowledge of COTS tools supporting data governance.
Experience with Financial Management Systems.
Powered by JazzHR
hsD9u2YHCB
Show more
Show less","PostgreSQL, SQL, AWS RDS, Aurora, pgAdmin, pgModeler, DBeaver, EntityRelationship Diagrams (ERDs), Structured Query Language (SQL), Amazon Web Services (AWS), AGILE, DEVOPS, COTS tools, Financial Management Systems","postgresql, sql, aws rds, aurora, pgadmin, pgmodeler, dbeaver, entityrelationship diagrams erds, structured query language sql, amazon web services aws, agile, devops, cots tools, financial management systems","agile, amazon web services aws, aurora, aws rds, cots tools, dbeaver, devops, entityrelationship diagrams erds, financial management systems, pgadmin, pgmodeler, postgresql, sql, structured query language sql"
Azure Cloud Data Engineer,"Harmonia Holdings Group, LLC","McLean, VA",https://www.linkedin.com/jobs/view/azure-cloud-data-engineer-at-harmonia-holdings-group-llc-3706226281,2023-12-17,Sterling,United States,Mid senior,Remote,"We have an exciting opportunity for a
Azure
Cloud Data Engineer
to join our team.
Design and implement cloud data solutions on Microsoft Azure, migrating on-premises data to cloud data warehouses like Azure Synapse Analytics, and Azure Databricks.
Build data pipelines to move data between a variety of sources and sinks such as SQL Server, Azure Data Lake Storage, and Cosmos DB using tools like Azure Data Factory.
Develop Python scripts and notebooks for ETL processes, data transformations, and visualization using services like Databricks and Azure HDInsight.
Implement data analytics solutions on the Azure platform leveraging services like Stream Analytics, Power BI, and Time Series Insights.
Engineer and deploy end-to-end big data solutions on Azure using technologies like Spark, Logic Apps, Function Apps, Databricks, and Azure Kubernetes Containers.
Automate deployment and infrastructure-as-code using ARM templates, Terraform, Ansible, and PowerShell.
Monitor and optimize performance of cloud data systems using tools like Azure Monitor, Log Analytics, and Application Insights.
Design schemas and implement ETL processes in data warehouses and data lakes for analytical workloads.
Design solutions using emerging technologies to meet business requirements.
Translate the requirements into functional or technical specifications, prototype, configuration of the specific emerging technology solution.
Support data scientists by provisioning and managing infrastructure like GPU VMs for machine learning and Azure Databricks
Work in an Agile environment collaborating with stakeholders and teams to deliver data projects.
Qualifications:
7+ years of experience in cloud data engineering with a focus on Microsoft Azure.
5+ years of experience as a Solution Architect.
Analysis level UML, activity diagrams, business use cases, business rules, and system level features.
Experience in enterprise architecture frameworks (TOGAF is preferred).
Experience with data warehouse and big data technologies such as Azure Synapse Analytics, Azure Data Lake Storage, Spark, Kafka, Databricks, Azure Logic Apps, Azure Function apps, Azure Cognitive Services, and Containers.
Hand-on experience with ETL tools such as Azure Data Factory and Databricks.
Experience with data analytics tools such as Stream Analytics, Power BI, and Time Series Insights.
Extensive Experience with cloud infrastructure automation tools such as ARM templates,
Terraform
, Ansible, and PowerShell.
Experience with monitoring and optimizing Azure cloud data systems.
Experience with designing and implementing data schemas.
Experience with working in an Agile environment.
Master’s or bachelor’s degree in computer science, Information Technology, or a related field.
As per the Executive Order on Ensuring Adequate Covid Safety Protocols for Federal Contractors and regulations as detailed by
www.saferfederalworkforce.gov
, it is recommended that all federal government contractors be vaccinated against Covid-19, unless approved for an exemption/ accommodation on the basis of a sincerely held religious belief or medical circumstance.
Show more
Show less","Azure, Cloud Data Engineer, Microsoft Azure, Data Pipeline, Python, ETL Processes, Data Transformations, Databricks, Azure HDInsight, Azure Monitor, Log Analytics, Application Insights, Data Warehouses, Data Lakes, SQL Server, Azure Data Lake Storage, Cosmos DB, Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Stream Analytics, Power BI, Time Series Insights, Apache Spark, Logic Apps, Function Apps, Azure Kubernetes Containers, ARM Templates, Terraform, Ansible, PowerShell, UML, Activity Diagrams, Business Use Cases, Business Rules, Enterprise Architecture Frameworks, TOGAF, Experience with ETL tools, Experience with data analytics tools, Experience with cloud infrastructure automation tools, Designing and implementing data schemas, Working in an Agile environment","azure, cloud data engineer, microsoft azure, data pipeline, python, etl processes, data transformations, databricks, azure hdinsight, azure monitor, log analytics, application insights, data warehouses, data lakes, sql server, azure data lake storage, cosmos db, azure data factory, azure synapse analytics, azure databricks, stream analytics, power bi, time series insights, apache spark, logic apps, function apps, azure kubernetes containers, arm templates, terraform, ansible, powershell, uml, activity diagrams, business use cases, business rules, enterprise architecture frameworks, togaf, experience with etl tools, experience with data analytics tools, experience with cloud infrastructure automation tools, designing and implementing data schemas, working in an agile environment","activity diagrams, ansible, apache spark, application insights, arm templates, azure, azure data factory, azure data lake storage, azure databricks, azure hdinsight, azure kubernetes containers, azure monitor, azure synapse analytics, business rules, business use cases, cloud data engineer, cosmos db, data lakes, data pipeline, data transformations, data warehouses, databricks, designing and implementing data schemas, enterprise architecture frameworks, etl, experience with cloud infrastructure automation tools, experience with data analytics tools, experience with etl tools, function apps, log analytics, logic apps, microsoft azure, powerbi, powershell, python, sql server, stream analytics, terraform, time series insights, togaf, uml, working in an agile environment"
Senior Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Herndon, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-wfh-at-get-it-recruit-information-technology-3778807374,2023-12-17,Sterling,United States,Mid senior,Remote,"Since our founding in 2008, our company has been fueled by a simple yet powerful idea: understanding people. We empower leading brands and agencies with deep consumer intelligence, fostering unbreakable relationships through better connections, more meaningful engagement, and compelling customer experiences. At the core of our mission is a ""people-centric"" approach, revealing the Human Element—a holistic understanding of individuals, starting with their values and motivations.
As we continue to grow, we are seeking a talented individual to join us as a Senior Data Engineer. This role involves mentoring our Data Integration Engineering team, collaboratively designing and implementing product features. If you're an engineer who thrives in a highly skilled team, values ownership of your work, and has a passion for coding, this opportunity is ideal for you.
Key Responsibilities
Design, implement, and enhance data integration and data streaming systems.
Develop data pipelines and Spring-based microservices as part of an agile/scrum team.
Support and maintain existing features, debugging throughout the stack, and applying fixes in a timely manner.
Apply best practices in continuous integration and delivery.
Design and develop APIs using RESTful principles.
Write unit, integration, and full end-to-end tests for new features.
Participate in the release of new features to production.
Collaborate with product managers and other engineers to implement and document complex and evolving requirements.
Act as on-call high-availability support for triage/bug fixing periodically throughout the year.
Required Qualifications & Experience
Experience with languages like Scala, Java, or similar.
Proficiency in cloud technologies (AWS) such as EMR/EC2, Lambda, and CloudWatch.
Experience with Big Data technologies like Hive/Spark.
Master's degree or equivalent relevant work experience.
5+ years of Scala and data processing development experience.
Experience in developing RESTful web services and JSON.
Proficiency in relational DB table design, implementation, and tuning (pl/SQL, ETL, etc).
Experience in database integration, especially ORM tools (Hibernate, JPA, etc).
Familiarity with build and deployment tools (Maven, Gradle, or SBT).
Practical knowledge of OOP/JS design patterns.
Understanding of ""12-Factor App"" concepts.
Experience with distributed system development for large-scale applications.
Familiarity with continuous integration and testing.
Experience with agile methodologies and short release cycles.
Strong communication skills, attention to detail, good work ethic, and ability to manage multiple projects simultaneously.
Knowledge of Continuous Integration & Continuous Deployment tools and processes.
Experience with large-scale SQL databases is a strong plus.
Desired Qualifications & Experience
Experience working on a SaaS product in a commercial environment.
Worked with Postgres DB.
Real-time data processing experience, such as Kafka.
Knowledge of Splunk, Grafana, and other AWS technologies.
Experience with Athena and AWS cost optimization.
Background in digital media, online advertising, or reporting/analytical applications.
Benefits
In addition to the opportunity to work with smart, fun, hard-working team members, we offer uncapped growth potential, a work/life balance, and a competitive suite of benefits.
Location:
Our flexible work environment combines the best of both worlds, with team members seamlessly collaborating across physical locations. Whether you prefer working from home or from our state-of-the-art offices, you'll have access to the tools and resources you need to succeed.
We are headquartered in Reston, VA, with offices in New York City and Washington, D.C.
Our EEO Statement
We are an equal opportunity employer committed to diversity and inclusion in the workplace, prohibiting discrimination and harassment based on various characteristics outlined by federal, state, or local laws.
Learn more about our story at our website.
Employment Type: Full-Time
Show more
Show less","Data Integration, Data Streaming, Spring Microservices, Agile/Scrum, Continuous Integration, Continuous Delivery, RESTful APIs, Unit Testing, Integration Testing, EndtoEnd Testing, Production Release, Product Management, Complex Requirements, Oncall Support, Scala, Java, AWS, EMR/EC2, Lambda, CloudWatch, Hive/Spark, Big Data, Master's Degree, Relational DB, Table Design, PL/SQL, ETL, ORM tools, Hibernate, JPA, Maven, Gradle, SBT, OOP/JS Design Patterns, 12Factor App Concepts, Distributed Systems, Continuous Integration, Continuous Testing, Agile Methodologies, Communication Skills, Attention to Detail, Work Ethic, Project Management, Continuous Integration & Continuous Deployment tools, Largescale SQL databases, SaaS, Postgres DB, Kafka, Splunk, Grafana, Athena, AWS cost optimization, Digital Media, Online Advertising, Reporting/Analytical Applications","data integration, data streaming, spring microservices, agilescrum, continuous integration, continuous delivery, restful apis, unit testing, integration testing, endtoend testing, production release, product management, complex requirements, oncall support, scala, java, aws, emrec2, lambda, cloudwatch, hivespark, big data, masters degree, relational db, table design, plsql, etl, orm tools, hibernate, jpa, maven, gradle, sbt, oopjs design patterns, 12factor app concepts, distributed systems, continuous integration, continuous testing, agile methodologies, communication skills, attention to detail, work ethic, project management, continuous integration continuous deployment tools, largescale sql databases, saas, postgres db, kafka, splunk, grafana, athena, aws cost optimization, digital media, online advertising, reportinganalytical applications","12factor app concepts, agile methodologies, agilescrum, athena, attention to detail, aws, aws cost optimization, big data, cloudwatch, communication skills, complex requirements, continuous delivery, continuous integration, continuous integration continuous deployment tools, continuous testing, data integration, data streaming, digital media, distributed systems, emrec2, endtoend testing, etl, gradle, grafana, hibernate, hivespark, integration testing, java, jpa, kafka, lambda, largescale sql databases, masters degree, maven, oncall support, online advertising, oopjs design patterns, orm tools, plsql, postgres db, product management, production release, project management, relational db, reportinganalytical applications, restful apis, saas, sbt, scala, splunk, spring microservices, table design, unit testing, work ethic"
Senior Data Engineer,"Changeis, Inc.","Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-changeis-inc-3778344143,2023-12-17,Sterling,United States,Mid senior,Remote,"Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.
The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.
Roles And Responsibilities
Identifying new datasets and integrate them, focusing on enhancing product capabilities.
Conducting experiments with analytical techniques to solve complex problems across various domains.
Recognizing relevant data sources and gather structured and unstructured data to meet client needs.
Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.
Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.
Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.
Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.
Collaborating with software developers and machine-learning engineers to implement models in production environments.
Requirements
15+ years of experience in data analysis, data management, data science, or operations research.
7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.
5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).
5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.
Experience with data ETL (Extract, Transform, and Load).
Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).
Ability to formulate business needs and translate them into technical functional and non-functional requirements.
Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.
Experience in data modeling, including transactional and data warehouse design.
Prefer candidates holding active AWS certifications
Show more
Show less","Data analysis, Data management, Data science, Operations research, Data engineering, ETL, Data analytics, Big data, Machine learning, Software engineering, AWS, Python, JavaScript, Node.js, React, Vue.js, C#, ELK Stack, Elastic Search, Logstash, Kibana, Docker, Kubernetes, Data modeling, Transactional design, Data warehouse design","data analysis, data management, data science, operations research, data engineering, etl, data analytics, big data, machine learning, software engineering, aws, python, javascript, nodejs, react, vuejs, c, elk stack, elastic search, logstash, kibana, docker, kubernetes, data modeling, transactional design, data warehouse design","aws, big data, c, data engineering, data management, data science, data warehouse design, dataanalytics, datamodeling, docker, elastic search, elk stack, etl, javascript, kibana, kubernetes, logstash, machine learning, nodejs, operations research, python, react, software engineering, transactional design, vuejs"
AWS Data Engineer,IVY TECH SOLUTIONS INC,"Washington, DC",https://www.linkedin.com/jobs/view/aws-data-engineer-at-ivy-tech-solutions-inc-3787771995,2023-12-17,Sterling,United States,Mid senior,Remote,"CLIENT : AMTRAK
Location : DC
AWS, AWS GLUE, ETL, Informatica, KINESIS, LAMBDA
This position is for a Mid level AWS/Data Engineer, responsible for designing and implementing data processing pipelines using AWS Glue and other AWS services. The qualified individual will work closely with clients/onsite counterparts as needed.
Must-haves:
Familiarity with AWS data services and modules.
5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline)
3+ years of experience with data migration, data analysis, and SQLs
3+ years of experience with informatica
Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets
Ability to work in globally distributed teams
Knowledge of IT processes, including quality assurance, release management, and production support
Excellent analytical, troubleshooting, and problem-solving skills
Excellent communicator (written and verbal, formal, and informal).
Flexible and proactive/self-motivated working style with strong personal ownership.
Ability to multi-task and prioritize under pressure.
Ability to work independently with minimal supervision as well as in a team environment.
Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required
Please feel free to share resumes to sangeetha.sirivala@ivytechsol.us ,Adarsh@ivytechsol.com
Powered by JazzHR
vINzbmxXts
Show more
Show less","AWS, AWS GLUE, ETL, Informatica, KINESIS, LAMBDA, AWS Data Services, AWS Modules, AWS Services (Lambda S3 RDS Aurora DynamoDB Kinesis AWS Glue AWS Data Pipeline), Data Migration, Data Analysis, SQL, SQLs, IT Processes, Quality Assurance, Release Management, Production Support, Analytical Skills, Troubleshooting Skills, ProblemSolving Skills, Communication Skills (Written Verbal Formal Informal), Flexible Work Style, Proactive Work Style, SelfMotivated Work Style, Personal Ownership, MultiTasking Skills, Prioritization Skills, Independent Work Style, Team Environment, Computer Science Degree, Data Science Degree, Equivalent Education/Professional Experience","aws, aws glue, etl, informatica, kinesis, lambda, aws data services, aws modules, aws services lambda s3 rds aurora dynamodb kinesis aws glue aws data pipeline, data migration, data analysis, sql, sqls, it processes, quality assurance, release management, production support, analytical skills, troubleshooting skills, problemsolving skills, communication skills written verbal formal informal, flexible work style, proactive work style, selfmotivated work style, personal ownership, multitasking skills, prioritization skills, independent work style, team environment, computer science degree, data science degree, equivalent educationprofessional experience","analytical skills, aws, aws data services, aws glue, aws modules, aws services lambda s3 rds aurora dynamodb kinesis aws glue aws data pipeline, communication skills written verbal formal informal, computer science degree, data migration, data science degree, dataanalytics, equivalent educationprofessional experience, etl, flexible work style, independent work style, informatica, it processes, kinesis, lambda, multitasking skills, personal ownership, prioritization skills, proactive work style, problemsolving skills, production support, quality assurance, release management, selfmotivated work style, sql, sqls, team environment, troubleshooting skills"
Senior Data Engineer,"Changeis, Inc.","Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-changeis-inc-3759558676,2023-12-17,Sterling,United States,Mid senior,Remote,"Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.
The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.
Roles And Responsibilities
Identifying new datasets and integrate them, focusing on enhancing product capabilities.
Conducting experiments with analytical techniques to solve complex problems across various domains.
Recognizing relevant data sources and gather structured and unstructured data to meet client needs.
Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.
Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.
Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.
Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.
Collaborating with software developers and machine-learning engineers to implement models in production environments.
Requirements
15+ years of experience in data analysis, data management, data science, or operations research.
7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.
5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).
5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.
Experience with data ETL (Extract, Transform, and Load).
Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).
Ability to formulate business needs and translate them into technical functional and non-functional requirements.
Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.
Experience in data modeling, including transactional and data warehouse design.
Prefer candidates holding active AWS certifications
Show more
Show less","Data Engineering, Python, JavaScript, C#, Node.js, React, Vue.js, AWS, ETL, ELK Stack, Docker, Kubernetes, Data Analytics, Machine Learning, Big Data, Algorithms, Statistical Models, Cloud Computing, Database Services, Data Modeling, AWS Certifications","data engineering, python, javascript, c, nodejs, react, vuejs, aws, etl, elk stack, docker, kubernetes, data analytics, machine learning, big data, algorithms, statistical models, cloud computing, database services, data modeling, aws certifications","algorithms, aws, aws certifications, big data, c, cloud computing, data engineering, dataanalytics, database services, datamodeling, docker, elk stack, etl, javascript, kubernetes, machine learning, nodejs, python, react, statistical models, vuejs"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751643391,2023-12-17,Sterling,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Data Pipelines, Cloud Computing, AWS, Azure, GCP, ETL, Data Orchestration, Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion, Snowflake, Redshift, Databricks, SQL, NoSQL, PostgreSQL, MySQL, SQL Server, Oracle, Aurora, Presto, BigQuery, Star Schema, Snowflake Schema, De/Normalized Designs, Python, Java, R, C, C#, C++, Shell, Git, Jenkins, CI/CD, Jira, DevOps, Big Data, Open Source, Data Streaming, SQL, Database Design, Data Structure","data engineering, data pipelines, cloud computing, aws, azure, gcp, etl, data orchestration, azure data factory, ssis, informatica, alteryx, ab initio, pentaho, talend, matillion, snowflake, redshift, databricks, sql, nosql, postgresql, mysql, sql server, oracle, aurora, presto, bigquery, star schema, snowflake schema, denormalized designs, python, java, r, c, c, c, shell, git, jenkins, cicd, jira, devops, big data, open source, data streaming, sql, database design, data structure","ab initio, alteryx, aurora, aws, azure, azure data factory, big data, bigquery, c, cicd, cloud computing, data engineering, data orchestration, data streaming, data structure, database design, databricks, datapipeline, denormalized designs, devops, etl, gcp, git, informatica, java, jenkins, jira, matillion, mysql, nosql, open source, oracle, pentaho, postgresql, presto, python, r, redshift, shell, snowflake, snowflake schema, sql, sql server, ssis, star schema, talend"
Senior Data Engineer,Resonate,"Reston, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-resonate-3773575605,2023-12-17,Sterling,United States,Mid senior,Remote,"Senior Data Engineer
Since Resonate’s founding in 2008, the company has been driven by a simple but powerful idea: understanding people. Resonate empowers leading brands and agencies with deep consumer intelligence that ignites unbreakable relationships through better connections, more meaningful engagement and compelling customer experiences.
We are “people-centric.” It is at the heart of what we do and drives how we operate. We reveal the Human Element—a holistic understanding of a person that starts with what makes us the most human—our values and motivations. In other words, we help you understand the “what” that drives the “why” people decide to choose, buy, endorse or abandon a brand or cause. By combining this human, person-based ""why"" with relevant data about your industry, brand or product, we help you create powerful marketing engagement that drives results.
As a senior data engineer, you will be working as a mentor of our Data Integration Engineering team to jointly design and implement product features. This is an ideal job if you are an engineer who wants to be part of an intensely skilled team, values total ownership of your work, and can’t imagine a day without coding.
If you are a skilled developer, with professional experience with data processing technologies, and distributed systems we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both designs as well as working hands-on with the code.
Key Responsibilities
Design, implement, and improve the data integration, data streaming systems
Design and develop Data pipeline, Spring-based microservices, as part of an agile/scrum team
Support and maintain existing features, debugging throughout the stack, and applying fixes in a timely manner
Apply best practices in continuous integration and delivery
Design and develop API’s using RESTful principles
Write unit, integration, and full end-to-end tests for new features
Participate in the release of new features to production
Work with product managers and other engineers to implement and document complex and evolving requirements
Act as on-call high-availability support for triage/bug fixing periodically throughout the year
Required Qualifications & Experience Requirements
Experience with languages like Scala, Java, or similar language
Experience with cloud technologies (AWS) like EMR/EC2, Lambda and Cloud watch
Experience with Big Data technologies such as Hive/Spark.
Masters' degree or equivalent relevant work experience
5+ years of Scala and data processing development experience
Experience developing RESTful web services Java restful web services and JSON
Experience in relational DB table design, implementation, and tuning (pl/SQL, ETL, etc)
Experience in database integration, especially ORM tools (Hibernate, JPA, etc)
Experience in build and deployment tools such as Maven or Gradle or SBT (Simple Build Tool)
Practical knowledge of OOP/JS design patterns
Understanding of “12-Factor App” concepts
Distributed System Development for large-scale applications
Experience with continuous integration and testing
Experience with agile methodologies and short release cycles
Demonstrate strong communication skills, strong attention to detail, good work ethic, and ability to work on multiple projects simultaneously
Strong knowledge of Continuous Integration & Continuous Deployment tools and processes
Experience with large scale SQL databases is a strong plus
Ability to work with shifting deadlines in a fast paced environment
Desired Qualifications & Experience Requirements
Experience working on a SAAS Product in a commercial environment
Worked with Postgres DB
Realtime data processing like Kafka
Knowledge of Splunk, Grafana and other AWS technologies
Experience with Athena and AWS cost optimization
Experience in digital media, online advertising, or reporting/analytical applications
Benefits
Besides the opportunity to work with smart, fun, hard-working Resonate employees, you will have uncapped growth potential, a work/life balance, and a competitive suite of benefits.
Location
At Resonate, we're proud to offer a flexible work environment that combines the best of both worlds. Our team is made up of talented individuals who collaborate seamlessly across physical locations, thanks to our innovative hybrid and remote work policies. Whether you're working from home or from one of our state-of-the-art offices, you'll have access to the tools and resources you need to succeed.
Resonate is headquartered in Reston, VA with offices in New York City, and Washington, D.C. Be a part of the team that changes the industry!
Our EEO Statement:
Resonate is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outline by federal, state, or local laws.
Find out more about our story at www.resonate.com.
Show more
Show less","Scala, Java, Spring, Kafka, Database integration, Hive, Spark, Big Data, AWS, Data streaming, Data engineering, Agile, Continuous integration, Unit testing, Integration testing, Endtoend testing, Continuous deployment, SQL, PostgreSQL, Splunk, Grafana, Athena","scala, java, spring, kafka, database integration, hive, spark, big data, aws, data streaming, data engineering, agile, continuous integration, unit testing, integration testing, endtoend testing, continuous deployment, sql, postgresql, splunk, grafana, athena","agile, athena, aws, big data, continuous deployment, continuous integration, data engineering, data streaming, database integration, endtoend testing, grafana, hive, integration testing, java, kafka, postgresql, scala, spark, splunk, spring, sql, unit testing"
IT ENGINEERING DATA ENGINEER,IVY TECH SOLUTIONS INC,"Vienna, VA",https://www.linkedin.com/jobs/view/it-engineering-data-engineer-at-ivy-tech-solutions-inc-3787781068,2023-12-17,Sterling,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
IT ENGINEERING ( DATA ENGINEER)
Location: VA
Duration: 12+Months
Please send the resume to
or 847- 350-1008
DATAWAREHOUSE, DB2, ETL, ORACLE, POWER BI, SQL
We will only accept the following:
US Citizens, Green Card Holders, TN Visa
Description:
Below are the primary skills required for the role:
SQL, Oracle, DB2 server
Hands on experience of building and maintaining artifacts in Datawarehouses, Data lakes etc for both on-prem (Teradata) and on-cloud (preferably on Azure)
Hands on experience of using ETL tools such as Informatica
Hands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOps
Business analytics and Intelligence tools such as Tableau, Power BI etc
Experience with Agile frameworks
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
5dhv59Zytw
Show more
Show less","SQL, Oracle, DB2, Datawarehouses, Data lakes, Teradata, Azure, Informatica, Azure Data Factory, Azure Data Bricks (PySpark), Synapse, Azure DevOps, Tableau, Power BI, Agile","sql, oracle, db2, datawarehouses, data lakes, teradata, azure, informatica, azure data factory, azure data bricks pyspark, synapse, azure devops, tableau, power bi, agile","agile, azure, azure data bricks pyspark, azure data factory, azure devops, data lakes, datawarehouses, db2, informatica, oracle, powerbi, sql, synapse, tableau, teradata"
"Senior Data Engineer, Mortgage Backed Securities (REMOTE) - (CS2023-2153)","3 Key Consulting, Inc.","Bethesda, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-mortgage-backed-securities-remote-cs2023-2153-at-3-key-consulting-inc-3594799930,2023-12-17,Sterling,United States,Mid senior,Remote,"Job Title:
Senior Data Engineer, Mortgage Backed Securities - (CS2023-2153)
Location:
Bethesda, MD. 20814 (100% remote)
Employment Type:
Permanent
Business Unit:
Data Management Group
Pay Range:
Range $101,575 to $173,341
Posting Date:
05//01/23
Notes:
Only qualified candidates need apply.
3 Key Consulting is hiring a
Senior Data Engineer
for a permanent position with our client, a mortgage securitization solution company based in Bethesda, MD.
Job Description
Our client built and operates the largest and most advanced mortgage securitization platform in the world, supporting the Uniform Mortgage-Backed Security (UMBS) of Fannie Mae and Freddie Mac. Supporting 70% of the mortgage-backed securities in the market, they provide best-in-class single-family issuance, bond administration, disclosure, and tax services. The support a broad portfolio of products for our clients with full lifecycle management.
Their market-leading, cloud-based, end-to-end platform executes transactions on an extraordinary scale which has bolstered liquidity in the secondary mortgage market, one of the largest and most important financial markets in the world. Their unique approach to securitization combines the best minds in financial services with the know-how, flexibility, and innovation of leading technologists.
The Data Engineer is responsible for solution engineering of enterprise scale data management best practices. This includes modern data integration frameworks, building of scalable distributed systems using emerging cloud-based data design patterns. This role will be responsible for developing data integration tasks in data and analytics space. This position will report to director of data management group under Data Operations organization. This is an individual performer role.
Top Must Have Skills
Python -AWS (Build data pipeline in AWS environment, Foundational AWS services (s3, VPC, IAM).
Programming in AWS ).
Snowflake or Redshift.
Data Integration tool – PySpark or Glue
Day To Day Responsibilities
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Basic Qualifications
Bachelor’s degree in Computer Science or a related field
Minimum of 4 years of experience in building data driven solutions.
Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future.
Client does not offer H-1B sponsorship for this position.
Employee Value Proposition
You will be joining a great team and company. Client is a fully virtual company, meaning this is a remote role.
Interview Process
Video Skype Panel Interview
We invite qualified candidates to send your resume to resumes@3keyconsulting.com. If you decide that you’re not interested in pursuing this particular position, please feel free to take a look at the other positions on our website www.3keyconsulting.com/careers. You are also welcome to share this opportunity with anyone you think might be interested in applying for this role.
Regards,
3KC Talent Acquisition Team
Show more
Show less","Python, AWS, Snowflake, Redshift, PySpark, Glue, Tableau, Data Warehouse, Data Integration, Data Enrichment, Machine Learning, AI, Predictive Analytics, Data Products, Computer Science, Data Analytics","python, aws, snowflake, redshift, pyspark, glue, tableau, data warehouse, data integration, data enrichment, machine learning, ai, predictive analytics, data products, computer science, data analytics","ai, aws, computer science, data enrichment, data integration, data products, dataanalytics, datawarehouse, glue, machine learning, predictive analytics, python, redshift, snowflake, spark, tableau"
"Senior Software Engineer, FHIR Data",For People,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-software-engineer-fhir-data-at-for-people-3787934429,2023-12-17,Sterling,United States,Mid senior,Remote,"For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.
Your Impact
We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare & Medicaid Services (CMS) to serve Medicare beneficiaries' demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will
directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive
.
Our Culture
For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!
This position is
100% remote
. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!
This position's published base salary range is between
$125,000 and $160,000
annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.
Your Opportunities
Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.
Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.
Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.
Design and maintain the data warehouse's FHIR-based data model to meet the needs of downstream API systems.
Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.
Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.
Stay informed about industry best practices and evolving FHIR standards.
You Bring
A humble and caring attitude
In-depth knowledge and experience with FHIR standards and resource types.
Expert-level Java programming abilities, alongside some familiarity with Python and Bash scripts.
Proficiency in designing and implementing data ingestion and transformation processes.
Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.
A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.
Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.
If you're passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.
Some fine print.
You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.
For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status.
Powered by JazzHR
UWyjSwTOgS
Show more
Show less","Healthcare technology, FHIR (Fast Healthcare Interoperability Resources), Java, Python, Bash scripts, Data ingestion, Data transformation, Database design, Data modeling, ETL (Extract Transform Load), HIPAA, Security measures, Access controls, Data mapping","healthcare technology, fhir fast healthcare interoperability resources, java, python, bash scripts, data ingestion, data transformation, database design, data modeling, etl extract transform load, hipaa, security measures, access controls, data mapping","access controls, bash scripts, data ingestion, data mapping, data transformation, database design, datamodeling, etl extract transform load, fhir fast healthcare interoperability resources, healthcare technology, hipaa, java, python, security measures"
"Principal, Data Engineer Architect",Lovelytics,"Arlington, VA",https://www.linkedin.com/jobs/view/principal-data-engineer-architect-at-lovelytics-3787784770,2023-12-17,Sterling,United States,Mid senior,Remote,"Lovelytics is seeking a skilled consultant with experience delivering and solutionging strategic Databricks and data engineering client engagements.
This Principal Consultant will play the role of a data and analytics solution architect, primarily focusing on leading the technical sales process related to data warehousing, ETL development, data integrations, and data modeling. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data through thought leadership and execution.
Role Location: Arlington, VA, or Remote in the US (MD, DC, CA, IA, ID, IN, NC, SC, TX, TN, GA, CO, NY, NJ, VA, FL, PA)
Primary Job Responsibilities:
Fill the role of data engineer solutions architect for our largest and most complex client engagements focused on developing data warehouses, optimizing back-end performance, and integrating data sources to systems using Databricks
Partner with our Technical Directors to identify emerging trends in the market, design and implement internal initiatives, etc.
Provide thought leadership in the form of blogs, technical ideation for Lovelytics' new offerings, and solutions for client's problems across various verticals.
Work hand-in-hand with our other capability teams to successfully sell, plan, and deliver engagements
Gather requirements from clients and develop creative and effective technical solutions
Partner with our sales team to play an extensive role in the presales and pursuit process, applying knowledge to potential client problems, enabling Lovelytics to expand the business
Apply your skills with Databricks, using Python, and big data streaming to pioneer client technologies and data
Manage projects to ensure project milestones are reached within the given timeline and budget allocated
Support other team members on projects, which can oftentimes mean wearing many different hats
Integrate Databricks with 3rd-party applications to support customers' architectures
Troubleshooting data issues on the fly with prospects and clients
Our Ideal Candidate's Skills and Experiences:
B.S. in Computer Science or equivalent
6+ years in data engineering working with cloud-based data analytics architectures and 4+ yrs' experience working directly with clients.
2+ years of involvement in technology pre-sales
Extensive knowledge of data warehousing and data lake concepts and hands-on experience deploying pipelines using Databricks
Experience developing Machine Learning models or ML Ops processes a plus
Excellent communication skills are a MUST, all our employees are client-facing, and this role requires both written and verbal client management skills.
Proven success in client development and partnering with Sales to develop solutions in the presales and discovery phase
Databricks Data Engineer Professional and Databricks Machine Learning Professional certifications a plus
Experience designing architectures within a public cloud (AWS or Azure)
Hands-on experience with Big Data technologies, including Spark, Hadoop, Cassandra, and others
Ability to extract and transform data via Python, deep exposure and understanding of data warehousing, ETL pipelines, etc.
Overall understanding of analytics from analytic engineering to visualization tools
What We Promise You:
Exciting projects with great clients in varying departments and verticals across the world
The ability to work closely with experienced data engineers and quickly grow and expand your skillset
The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. The salary range for candidates in the US for this position is $148,000-$202,000 depending on level, experience, and skills. Note, that a realistic starting salary is the mid point of the provided range.
Powered by JazzHR
MDKwLSZjWa
Show more
Show less","Databricks, Data Warehousing, ETL Development, Data Integration, Data Modeling, Python, Big Data Streaming, Machine Learning, ML Ops, Public Cloud (AWS or Azure), Spark, Hadoop, Cassandra, Data Extraction, Data Transformation, Analytic Engineering, Visualization Tools","databricks, data warehousing, etl development, data integration, data modeling, python, big data streaming, machine learning, ml ops, public cloud aws or azure, spark, hadoop, cassandra, data extraction, data transformation, analytic engineering, visualization tools","analytic engineering, big data streaming, cassandra, data extraction, data integration, data transformation, databricks, datamodeling, datawarehouse, etl development, hadoop, machine learning, ml ops, public cloud aws or azure, python, spark, visualization tools"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/data-engineer-aws-azure-gcp-at-captech-3774195945,2023-12-17,Sterling,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
Specific responsibilities for the Data Engineer – Cloud position include:
Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly
Utilizing your skills in engineering best practices to solve complex data problems
Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization.
Articulating architectural differences between solution methods and the advantages/disadvantages of each
Qualifications
Typical experience for successful candidates includes:
Experience delivering solutions on a major cloud platform
Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture
Experience in the design and implementation of data architecture solutions
A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines
Ability to assess and utilize traditional and modern architectural components required based on business needs.
A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning.
Skills
Successful candidates usually have demonstrable experience with technologies in some of these categories:
Languages: SQL, Python, Java, R, C# / C++ / C
Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle
DevOps: git, docker, subversion, Kubernetes, Jenkins
Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR
Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
#LM-Remote
Show more
Show less","Python, Java, SQL, R, C#, C++, C, SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle, Git, Docker, Subversion, Kubernetes, Jenkins, Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR, AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer","python, java, sql, r, c, c, c, sql server, postgresql, snowflake, redshift, aurora, presto, bigquery, oracle, git, docker, subversion, kubernetes, jenkins, spark, databricks, kafka, kinesis, hadoop, lambda, emr, aws cloud practitioner, microsoft azure data fundamentals, google associate cloud engineer","aurora, aws cloud practitioner, bigquery, c, databricks, docker, emr, git, google associate cloud engineer, hadoop, java, jenkins, kafka, kinesis, kubernetes, lambda, microsoft azure data fundamentals, oracle, postgresql, presto, python, r, redshift, snowflake, spark, sql, sql server, subversion"
Azure Data Engineer,CC Pace,"Vienna, VA",https://www.linkedin.com/jobs/view/azure-data-engineer-at-cc-pace-3779239091,2023-12-17,Sterling,United States,Mid senior,Remote,"​​​​
Job Description:
You will be working with all levels of technology from backend data processing technologies (Databricks/Apache Spark) to other Cloud computing technologies / Azure Data Platform. You should be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development. Must be a hand-on technologist passionate about learning new technologies and help improve the ways we can better leverage Advanced Analytics and Machine Learning.
Responsibilities
• Build end-to-end direct capabilities.
• Create and maintain optimal data pipeline architecture.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
• Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business.
• Derive insights to differentiate member and team member experiences.
• Collaborate with cross-functional teams.
• Analyze and define with product teams the data migration and data integration strategies.
• Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems.
• Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights.
• Integrate source systems with information management solutions and target systems for automated migration processes.
• Create proof-of-concepts to demonstrate viability of solutions under consideration.
Qualifications
1. Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions.
2. Bachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience.
3. Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming)
4. Significant hands-on experience guiding technical teams as a mentor while leading collaboration with multiple teams across the organization.
5. Significant hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources.
6. Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala)
7. Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills.
8. Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics
Desired
• Proficiency in collaborative coding practices, such as pair programming, and ability to thrive in a team-oriented environment
• The following certifications:
o Microsoft Certified Azure Data Engineer
o Microsoft Certified Azure Solutions Architect
o Databricks Certified Associate Developer for Apache 2.4/3.0
Show more
Show less","Data Engineering, Data Analytics, Machine Learning, Data Visualization, Data Modeling, Data Integration, Data Migration, Data Processing, Big Data, Cloud Computing, Azure, Azure Data Factory, Azure Databricks, Azure Data Lake Storage, Azure SQL, Apache Spark, PySpark, Java, Scala, Python, Data Structures, Algorithms, Design Patterns, Problem Solving, Collaborative Coding, Pair Programming","data engineering, data analytics, machine learning, data visualization, data modeling, data integration, data migration, data processing, big data, cloud computing, azure, azure data factory, azure databricks, azure data lake storage, azure sql, apache spark, pyspark, java, scala, python, data structures, algorithms, design patterns, problem solving, collaborative coding, pair programming","algorithms, apache spark, azure, azure data factory, azure data lake storage, azure databricks, azure sql, big data, cloud computing, collaborative coding, data engineering, data integration, data migration, data processing, data structures, dataanalytics, datamodeling, design patterns, java, machine learning, pair programming, problem solving, python, scala, spark, visualization"
Sr. Azure Data Engineer (100% Remote),Eliassen Group,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/sr-azure-data-engineer-100%25-remote-at-eliassen-group-3768087153,2023-12-17,Sterling,United States,Mid senior,Remote,"Eliassen Group is looking for a Sr. Azure Data Engineer to support a brand new data migration project as the lead SME that will be working with a highly prestigious logistics company located in the DMV area. You will work as the first Data Engineer and play a pioneering role in establishing and shaping our data engineering function, leveraging Microsoft Azure and its associated services. You will be responsible for designing, developing, and maintaining our data infrastructure, building robust data pipelines, ensuring data quality and availability, and supporting the analytics and data science teams in extracting actionable insights from our data. Your expertise in data engineering and architecture, specifically within the Microsoft Azure ecosystem, will be instrumental in charting the course of our data journey.
Eliassen Group offers a wide range of benefits from 401k match to medical, dental and vision insurance for those who qualify
I cannot do C2C on this position
Key Responsibilities:
Data Engineering Leadership:
Spearhead the establishment of data engineering practices, standards, and processes at Peak Technologies, with a primary focus on the Microsoft Azure platform.
Data Pipeline Development:
Design, develop, and maintain scalable and reliable data pipelines utilizing Microsoft Azure services to collect, transform, and load (ETL) data from various sources into our data warehouse.
Data Modeling:
Create and maintain data models, ensuring data accuracy, consistency, and efficiency within the Microsoft Azure environment. Optimize data structures for performance and ease of access.
Data Integration:
Collaborate with cross-functional teams to integrate data from various systems and sources, including APIs, databases, and third-party services, leveraging Microsoft Azure tools.
Data Quality:
Establish data quality controls and monitoring processes using Microsoft Azure services to ensure the integrity and accuracy of our data.
Performance Optimization:
Continuously improve data pipeline and query performance within the Microsoft Azure framework for optimal speed and efficiency in a global context.
Data Security:
Implement and maintain data security and privacy best practices, ensuring data compliance with relevant regulations and standards across diverse geographical regions with Microsoft Azure security features.
Documentation:
Create and maintain clear and comprehensive documentation of data pipelines, data models, and processes, emphasizing the use of Microsoft Azure services.
Mentorship:
Provide guidance and mentorship to junior data engineers, fostering a culture of knowledge sharing and skill development within the team, with a focus on Microsoft Azure expertise.
Stay Current:
Keep up-to-date with industry trends and emerging technologies in data engineering, particularly within the Microsoft Azure ecosystem, and apply them where relevant on a global scale.
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
10+ years of experience in data engineering, with a strong focus on ETL processes, data modeling, and data warehousing within Microsoft Azure.
Proficiency in one or more programming languages commonly used in data engineering (e.g., Python, Java, Scala).
Hands-on experience with Microsoft Azure and its associated data services (e.g., Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse, Azure Synapse Analytics).
Expertise in SQL and database management systems within the Microsoft Azure environment.
Strong knowledge of data integration methods, data quality, and data governance best practices in a global supply chain context with a Microsoft Azure focus.
Excellent problem-solving skills and the ability to work independently or in a team.
Strong communication and collaboration skills, with the ability to convey technical concepts to non-technical stakeholders.
Certifications in relevant data engineering and Microsoft Azure technologies are a plus.
US-based remote position with the willingness for occasional travel.
Show more
Show less","Data Engineering, Azure Architecture, Microservices, Data Warehousing, Data Modeling, Data Integration, Data Quality, Performance Optimization, Data Security, Documentation, Mentorship, Industry Trends, Emerging Technologies, SQL, Python, Java, Scala, Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse, Azure Synapse Analytics, Data Governance, Communication, Collaboration","data engineering, azure architecture, microservices, data warehousing, data modeling, data integration, data quality, performance optimization, data security, documentation, mentorship, industry trends, emerging technologies, sql, python, java, scala, azure data factory, azure databricks, azure sql data warehouse, azure synapse analytics, data governance, communication, collaboration","azure architecture, azure data factory, azure databricks, azure sql data warehouse, azure synapse analytics, collaboration, communication, data engineering, data governance, data integration, data quality, data security, datamodeling, datawarehouse, documentation, emerging technologies, industry trends, java, mentorship, microservices, performance optimization, python, scala, sql"
Data Warehouse Engineer,Venturi,"McLean, VA",https://www.linkedin.com/jobs/view/data-warehouse-engineer-at-venturi-3784574672,2023-12-17,Sterling,United States,Mid senior,Hybrid,"A large hotel & hospitality client of mine is looking to fill a Sr Product/Project Manager position. Please see below for the job description.
Description:
Experience in analyzing data integration requirements, sources, targets, business rules and transformation logic.
Very strong experience designing and architecting data warehouses.
Experience in designing and developing data extraction, cleansing, transformation, and loading processes.
More than 10 years of experience in creating advanced mappings, transformations, and workflows using Informatica PowerCenter with warehouse implementations.
Exposure in building on-premise to cloud-based ETLs.
Well versed in SQL, dimensional modeling and relational databases.
Strong communication, analytical and problem solving skills.
If this sounds like an opportunity you would be interested in, please apply with an up to date resume.
Venturi is an equal opportunity employer, committed to supporting and creating a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve.All qualified applicants will receive consideration for employment without regard to sex, race, color, national origin, sexual orientation, gender, gender identity, genetic information, religion, disability, age, veteran status, or any other legally protected status under national, federal, state, or local law.
Show more
Show less","Informatica PowerCenter, SQL, Dimensional modeling, Relational databases, Data warehouses, Data integration, Data extraction, Data cleansing, Data transformation, Data loading, ETLs, Cloudbased ETLs","informatica powercenter, sql, dimensional modeling, relational databases, data warehouses, data integration, data extraction, data cleansing, data transformation, data loading, etls, cloudbased etls","cloudbased etls, data extraction, data integration, data loading, data transformation, data warehouses, datacleaning, dimensional modeling, etls, informatica powercenter, relational databases, sql"
Data Engineer,Robert Half,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-engineer-at-robert-half-3780029888,2023-12-17,Sterling,United States,Mid senior,Hybrid,"This is a Full time Direct Hire positions. This is a Manager, Data Engineering role that will manage two people.
This will be a hybrid role and will only need to come to the office for all hands meetings and other reasons once every couple of months.
Top Must Haves:
Azure is a Must for the backend data and getting it in the right format to Power Bi. Power Bi is a Must. Create and maintain Power BI apps, reports, dashboards that assist various departments and communities to use analytics to increase organizational efficiencies. Snowflake is a very nice to have.
Team lead or someone who has managed people. This will be 75% hands on 25% management. They will have two people under them. One which is strong with Power Bi and one that is strong with the ETL Development.
Improve governance of our data assets including our PowerBi Datasets, Dataflows. Experience with ETL / ELT tools such as: SSIS, Azure Data Factory.
The Data Engineering Manager is a key role responsible for leveraging expertise in data warehousing, analytical reporting, and team leadership to ensure the effectiveness of business intelligence systems. This multifaceted position involves collaboration across teams, departments, and with business stakeholders. The Manager oversees end-to-end solutions to democratize data, ensuring reliability, accuracy, and timeliness of data sources. The role demands collaboration with both technical and non-technical team members to support organizational operations.
Key Responsibilities:
Contribute significantly to establishing the ETL/ELT framework for the Data Analytics Team.
Design and develop pipelines for data movement between the Data Warehouse and other analytical solutions.
Provide support for existing pipelines and troubleshoot issues as they arise.
Develop and maintain PowerBI apps, reports, and dashboards to enhance organizational efficiency and senior care.
Enhance governance of data assets, including PowerBI Datasets and Dataflows.
Collaborate with other teams within the organization on data engineering initiatives.
Job Description:
Identify new areas of data, research, and data technology to address business problems.
Utilize effective project planning techniques to manage project scope, break down complex projects into tasks, and ensure timely completion.
Apply data best practices and lessons learned to develop technical solutions for descriptive analytics, ETL, predictive modeling, and prescriptive ""real-time decisions"" analytics.
Develop technical solutions using data techniques in data and analytics processes.
Build frameworks/prototypes integrating data and advanced analytics for business decision-making.
Implement new data technologies (ingestion, processing, distribution) and research delivery methods to solve business problems.
Understand data-related problems and requirements to identify optimal technical approaches.
Collaborate with peers to ensure efforts within assigned tracks meet their needs.
Identify and develop data sources and techniques to address business problems.
Co-mingle data sources to lead work on data and problems across departments, driving improved business and technical results through design.
Maintain compliance with required training applicable to the role.
Experience and Qualifications:
Successful candidates should meet the following criteria:
Bachelor's Degree in Computer Science, Computer Engineering, or a related field.
6+ years of IT experience.
4+ years of experience as a Data Engineer or 1+ years as a Manager of Data Engineering (or equivalent).
Strong communication skills.
Project management experience.
Technical mastery of systems core to the Data Platform.
Hands-on experience with databases like SqlServer, Oracle, Snowflake.
Ability to create reports using PowerBI or other Business Intelligence Software.
Experience with ETL/ELT tools such as SSIS, Azure Data Factory, Pentaho, Talend, etc.
DevOps experience using GIT, developing, deploying code to production.
Proficiency in working with Unix/Linux.
Show more
Show less","Azure, Power Bi, Snowflake, SSIS, Azure Data Factory, ETL / ELT tools, Data Engineering Manager, Governance, PowerBi Datasets, Dataflows, Technical solutions, Frameworks / Prototypes, Data techniques, Advanced analytics, Datarelated problems, Data sources, Comingle data sources, Compliance, SQL Server, Oracle, Business Intelligence Software, Pentaho, Talend, GIT, Unix/Linux, DevOps, RDBMS, ETL, ELT, Data Warehouse, Analytical solutions, Data pipelines, Data movement, Reporting, Dashboards, Organizational Efficiency, Senior Care, Project Planning, Project Management, Data Best Practices, Descriptive Analytics, Predictive Modeling, Prescriptive Analytics, Data and Analytics Processes, Data Ingestion, Data Processing, Data Distribution","azure, power bi, snowflake, ssis, azure data factory, etl elt tools, data engineering manager, governance, powerbi datasets, dataflows, technical solutions, frameworks prototypes, data techniques, advanced analytics, datarelated problems, data sources, comingle data sources, compliance, sql server, oracle, business intelligence software, pentaho, talend, git, unixlinux, devops, rdbms, etl, elt, data warehouse, analytical solutions, data pipelines, data movement, reporting, dashboards, organizational efficiency, senior care, project planning, project management, data best practices, descriptive analytics, predictive modeling, prescriptive analytics, data and analytics processes, data ingestion, data processing, data distribution","advanced analytics, analytical solutions, azure, azure data factory, business intelligence software, comingle data sources, compliance, dashboard, data and analytics processes, data best practices, data distribution, data engineering manager, data ingestion, data movement, data processing, data sources, data techniques, dataflows, datapipeline, datarelated problems, datawarehouse, descriptive analytics, devops, elt, etl, etl elt tools, frameworks prototypes, git, governance, oracle, organizational efficiency, pentaho, powerbi, powerbi datasets, predictive modeling, prescriptive analytics, project management, project planning, rdbms, reporting, senior care, snowflake, sql server, ssis, talend, technical solutions, unixlinux"
"Data Engineer, Data Platform",Grammarly,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656898066,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Data Lakehouse, AWS, Azure, GCE, SQL, APIs, Microservices, System Design, Internal Tools, Data Lakes, Admin Sites","python, scala, java, data lakehouse, aws, azure, gce, sql, apis, microservices, system design, internal tools, data lakes, admin sites","admin sites, apis, aws, azure, data lakehouse, data lakes, gce, internal tools, java, microservices, python, scala, sql, system design"
Data Engineer (Python/SQL),Motion Recruitment,"Chevy Chase, MD",https://www.linkedin.com/jobs/view/data-engineer-python-sql-at-motion-recruitment-3783218809,2023-12-17,Sterling,United States,Mid senior,Hybrid,"An Applied AI company is looking for a Data Engineer to help manage and clean data from various databases. Ideal candidates for this position will have 3-4+ years of professional experience, preferably in a start-up or small company, and be proficient with Python and SQL. Candidates will also be working hybrid in the office. Required Skills & Experience
3-4+ years of professional experience
Python, SQL
The Offer
Competitive Salary
Full-Health Benefits
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k)
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Sean Thompson
Show more
Show less","Data Engineering, Python, SQL, Databases","data engineering, python, sql, databases","data engineering, databases, python, sql"
"Digital, Data Engineer",Madison Energy Infrastructure,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/digital-data-engineer-at-madison-energy-infrastructure-3762695053,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Madison Energy Infrastructure (“MEI”) believes in the power of clean energy infrastructure and has quickly emerged as a preeminent developer, investor, asset owner, and operator of distributed generation. The Digital Data Engineer will actively support the Digital Team and will assist with development and implementation of data security policies and classifications according to best practices. This position presents significant growth potential within our dynamic organization, offering exposure to Distributed Generations (DG) solar and Battery Energy Storage Systems (BESS) nationwide.
What You’ll Be Doing:
Designing, building, maintaining, and optimizing infrastructure for data collection, storage, management, transformation, and delivery to MEI teams.
Identifying and implementing internal process improvements to ensure greater scalability, optimize data delivery, and automate manual processes.
Building required infrastructure for optimally extracting, transforming, and loading raw data from a variety of sources using AWS and SQL technologies.
Building pipelines to collect, store, transform, and deliver raw data in usable formats for visualization, analysis, business intelligence, and reporting.
Building analytical tools that leverage data pipelines to provide actionable insight into key business performance metrics including financial planning and analysis as well as operational efficiency.
Working with the Digital Director to develop, implement, and improve internal data security policies and classifications.
Working with MEI teams to support data infrastructure needs while assisting with data related technical issues.
What We Are Looking For
:
Demonstrates a steady, even pace to ensure precision and high-quality work outcomes.
Possess a technical and analytical focus, working within established systems, standards, and procedures, with job-related communication based on knowledge and expertise.
Engages in decision-making within a well-defined job scope, adhering to established policies and procedures, with managerial support.
Emphasizes job-related knowledge and expertise, maintaining helpful and supportive communication with both management and peers in a structured work environment.
Demonstrates leadership focused on consistent, accurate, and high-quality work output, utilizing a supportive and non-threatening leadership style. Delegates tasks to others appropriately, providing training, coaching, and on-the-job experience.
Location: Vienna, VA or Charlottesville, VA
Madison Energy Investments is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
Show more
Show less","AWS, SQL, Data security, Data classification, Data infrastructure, Data transformation, Data visualization, Data analysis, Business intelligence, Financial planning, Operational efficiency, Data pipelines","aws, sql, data security, data classification, data infrastructure, data transformation, data visualization, data analysis, business intelligence, financial planning, operational efficiency, data pipelines","aws, business intelligence, data classification, data infrastructure, data security, data transformation, dataanalytics, datapipeline, financial planning, operational efficiency, sql, visualization"
Azure Data Engineer,Seneca Resources,"Vienna, VA",https://www.linkedin.com/jobs/view/azure-data-engineer-at-seneca-resources-3782029952,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Position Title:
Azure Data Engineer
Location:
Hybrid in Vienna, VA (1 day onsite each week)
Position Status:
Contract to Hire
Position Description:
Our client is looking for
Azure Data Engineer
candidates for a hybrid position located in
Vienna, VA
.
Responsibilities:
Build end-to-end direct capabilities.
Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business.
Derive insights to differentiate member and team member experiences.
Collaborate with cross-functional teams.
Analyze and define with product teams the data migration and data integration strategies.
Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems.
Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights.
Integrate source systems with information management solutions and target systems for automated migration processes.
Create proof-of-concepts to demonstrate viability of solutions under consideration.
Required Education:
Bachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience.
Required Skills:
Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions.
Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming)
Significant hands-on experience guiding technical teams as a mentor while leading collaboration with multiple teams across the organization.
Significant hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources.
Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala)
Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills.
Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics
Desired Skills:
The following certifications:
Microsoft Certified Azure Data Engineer
Microsoft Certified Azure Solutions Architect
Databricks Certified Associate Developer for Apache 2.4/3.0
About Seneca Resources:
Seneca Resources is a client driven provider of strategic Information Technology consulting services and Workforce Solutions to government and industry. Seneca Resources is a leading IT services provider with offices in Virginia, Alabama, Georgia, Florida, and North Carolina that service clients throughout the United States.
We are an Equal Opportunity Employer and value the benefits of diversity in our workplace.
Show more
Show less","Azure, Azure Data Factory, Azure Databricks, Azure Data Lake Storage, Azure SQL, Data Management, Information Architecture, Performance Management, Information Delivery, Advanced Analytics, Apache Spark, PySpark, Java, Scala, Data Structures, Algorithms, Design Patterns, Analytics, Data Visualization, Modeling, Python","azure, azure data factory, azure databricks, azure data lake storage, azure sql, data management, information architecture, performance management, information delivery, advanced analytics, apache spark, pyspark, java, scala, data structures, algorithms, design patterns, analytics, data visualization, modeling, python","advanced analytics, algorithms, analytics, apache spark, azure, azure data factory, azure data lake storage, azure databricks, azure sql, data management, data structures, design patterns, information architecture, information delivery, java, modeling, performance management, python, scala, spark, visualization"
Data Engineer/Sr Data Engineer,DNV,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-sr-data-engineer-at-dnv-3779262392,2023-12-17,Sterling,United States,Mid senior,Hybrid,"About Us
We are the independent expert in assurance and risk management. Driven by our purpose, to safeguard life, property, and the environment, we empower our customers and their stakeholders with facts and reliable insights so that critical decisions can be made with confidence.
As a trusted voice for many of the world’s most successful organizations, we use our knowledge to advance safety and performance, set industry benchmarks, and inspire and invent solutions to tackle global transformations.
About The Role
EVOLVE Intelligence accelerates the transition towards a carbon-free future through software and analytics.
We are looking for a
Data Engineer/Sr. Data Engineer
to help us accomplish this mission.
The Analytics & Data Science team in DNV – Energy Management’s Technology group is a remote-first team. We offer more than just a job; we provide a community where you can learn, grow, and thrive your way. Join a dynamic and diverse technology team that values relationships and the environment as much as results. Help us create software that empowers utility clean energy customers to combat climate change!
This is a remote position open to any location in the continental United States.
What You’ll Do
As a Data Engineer/Sr. Data Engineer, you will design, develop, and maintain data architecture, pipelines, and systems that play a vital role in how our utility partners steward clean energy programs. Your impact will be immediate and will directly enable pathways to decarbonization through energy efficiency, demand response, storage, electric vehicles, and renewable energy technologies. You will solve a variety of problems that leverage your deep understanding of data engineering principles.
How You’ll Succeed
Collaborate with cross-functional teams, including machine learning engineers, software developers, analytics engineers, and product managers to translate business requirements into highly available data solutions
Leverage your creative problem-solving skills to architect, develop, and maintain scalable and efficient data processing pipelines using PySpark and other distributed computing technologies
Create and optimize data models that support reporting, analytics, artificial intelligence, and software
Optimize data storage and retrieval by designing and implementing efficient storage systems that use technologies such as Timescale and Apache Spark
Apply data validation, data profiling, and data cleansing to ensure data quality and integrity
Write clean, efficient, maintainable code, and actively engage with team members in code reviews
Create and maintain technical documentation covering data models, flows, views, dictionaries, and mapping schemes
Serve as a mentor and resource to other members of the team
What we offer
Generous paid time off (vacation, sick days, company holidays, personal days)
Multiple Medical and Dental benefit plans to choose from, Vision benefits
Spending accounts – FSA, Dependent Care, Commuter Benefits, company-seeded HSA
Special programs – Employee Assistance Program, ID theft protection, and accident and critical illness options for you and your family
Employer-paid, therapist-led, virtual care services through Talkspace
Company provided life insurance, short-term, and long-term disability benefits
Tuition assistance
Flexible work schedule with hybrid/remote opportunities
Advancement opportunities
Benefits may vary based on position, tenure, location, and employee election
Immigration-related employment benefits, for example visa sponsorship, are not available for this position**
How We Do It
We Care, We Dare, We Share
DNV is a proud equal opportunity employer committed to building an inclusive and diverse workforce. All employment is decided on the basis of qualifications, merit or business need, without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
DNV is committed to ensuring equal employment opportunity, including providing reasonable accommodations to individuals with a disability. US applicants with a physical or mental disability who require a reasonable accommodation for any part of the application or hiring process may contact the North America Recruitment department (hrrecruitment.northamerica@dnv.com). Information received relating to accommodations will be addressed confidentially.
For more information
https://www.eeoc.gov/know-your-rights-workplace-discrimination-illegal
Read more here
Diversity at DNV
Meet our Employees
About DNV
Careers in DNV
Please visit our website at www.dnv.com
As required by the Pay Transparency laws in Colorado, New York City, California, Washington, and Connecticut, DNV provides a reasonable compensation range for roles that may be hired in those locations. Actual compensation is influenced by a wide array of factors, including but not limited to skill set, level of experience, and specific office location. For the states of Colorado, New York City, NY, California, Washington, and Connecticut only, the range of starting pay for this role is
$105,000 - $150,000.
About You
What is Required
An undergraduate or advanced degree in a quantitative field
Proven professional experience as a Data Engineer
Strong proficiency in SQL, Python, and Spark/PySpark
Hands on experience with distributed computing frameworks such as Apache Spark
Familiarity with working in Azure – experience with Data Factory is strongly desired
Experience with version control systems (e.g., Git) and familiarity with agile development practices
Excellent communication and teamwork skills, with the ability to effectively collaborate with cross-functional teams
Experience working on an agile product team is a bonus
Someone who is eager to learn new things and is coachable
Most importantly, this team member should display a positive, team-oriented attitude to match our friendly and enthusiastic work environment
Willingness and the ability to undergo a background investigation and drug screening
Excellent written and verbal English communication skills
We conduct pre-employment drug and background screening
Show more
Show less","Data engineering, Apache Spark, ETL, Data quality, Data validation, Data profiling, Data cleansing, Data storage, Data retrieval, Data pipelines, Data models, Data architecture, Analytics, Reporting, Python, SQL, TimeScale, Azure, Git, Agile, English, Teamoriented","data engineering, apache spark, etl, data quality, data validation, data profiling, data cleansing, data storage, data retrieval, data pipelines, data models, data architecture, analytics, reporting, python, sql, timescale, azure, git, agile, english, teamoriented","agile, analytics, apache spark, azure, data architecture, data engineering, data models, data profiling, data quality, data retrieval, data storage, data validation, datacleaning, datapipeline, english, etl, git, python, reporting, sql, teamoriented, timescale"
"Job Opportunity :: AWS Data Platform Engineer :: 6 months Contract :: Stamford, CT, Charlotte, CT (Hybrid)",Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/job-opportunity-aws-data-platform-engineer-6-months-contract-stamford-ct-charlotte-ct-hybrid-at-steneral-consulting-3762482758,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Title- AWS Data Platform Engineer
Location: Hybrid,
NYC,
Lawrenceville, NJ or Washington DC
– 3 Days a week onsite :: Local must
Linkedin must
JD-
Notes
This is an AWS Platform Engineer who is building the underlying platform that other Data Engineers will use. So, they need experience building out the Platform and a focus on Infrastructure as code.
Here is some specific details from the hiring manager. First 2 are mandatory and 3rd can be optional:
Infrastructure as Code(IaC) is required. AWS CDK with Typescript as language for CDK development
Working Experience/Expertise creating AWS infrastructure for AWS services
Some experience in Databricks environment
Must have a heavy background in AWS Platform Engineering with a focus on setting up AWS environment for other engineers to use. Strong background with Infrastructure as Code and building out ETL Engines.
I’m not looking for a Data Application Engineer.
Please don’t send someone who’s last job was not AWS specific or is a Big Data Engineer.
Our client, a leading provider of audio entertainment products, is seeking an AWS Data Platform Engineer to join their data team.
In this role you will be a member of a team responsible for designing, developing and supporting the data platform which will be used across data organization and other groups.
What You’ll Do
Build on the AWS data platform which supports Datalake, Job Orchestration, ETL template, ETL Compute, integration with third party tools like fivetran, Monte Carlo
Design, code and maintain infrastructure as a code (IaC) using CDK, typescript, CDKTF.
Build and improve workflow orchestration tooling to support efficient data pipelines E.g., airflow plugins, systems integration, deployments.
What You’ll Need
7+ years’ experience developing infrastructure as Code working with AWS CDK, typescript/or Python.
3+ years of experience working on AWS building out the cloud platform
AWS CDK with Typescript as language for CDK development
Working Experience for Job Orchestration tool – Airflow/MWAA
Working Experience/Expertise creating AWS infrastructure for AWS services including but not limited to: S3 Datalake, Kms keys, IAM Role/Policy, MWAA, RDS, Lambda function
Knowledge/Expertise on tools viz (Fivetran, Monte-Carlo, Datadog, Tableau/Looker)
Plus Skills
Experience/Expertise on Databricks and Monte Carlo is a plus.
Show more
Show less","AWS, AWS CDK, Typescript, Infrastructure as Code (IaC), AWS Cloud Platform, ETL Engines, Data Lake, Job Orchestration, ETL Templates, ETL Compute, Third Party Tools (Fivetran Monte Carlo), Airflow Plugins, Systems Integration, Deployments, Airflow, MWAA, S3 Data Lake, KMS Keys, IAM Role/Policy, RDS, Lambda Function, Fivetran, Monte Carlo, Datadog, Tableau/Looker, Databricks","aws, aws cdk, typescript, infrastructure as code iac, aws cloud platform, etl engines, data lake, job orchestration, etl templates, etl compute, third party tools fivetran monte carlo, airflow plugins, systems integration, deployments, airflow, mwaa, s3 data lake, kms keys, iam rolepolicy, rds, lambda function, fivetran, monte carlo, datadog, tableaulooker, databricks","airflow, airflow plugins, aws, aws cdk, aws cloud platform, data lake, databricks, datadog, deployments, etl compute, etl engines, etl templates, fivetran, iam rolepolicy, infrastructure as code iac, job orchestration, kms keys, lambda function, monte carlo, mwaa, rds, s3 data lake, systems integration, tableaulooker, third party tools fivetran monte carlo, typescript"
Sr. Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-steampunk-inc-3770815197,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around.
We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
5-7 years industry experience coding commercial software and a passion for solving complex problems.
5-7 years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, SQL, Big Data, Agile, DevOps, Cloud Computing, Data Warehousing, Data Modelling, Data Streaming, Machine Learning, Data Analytics","python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, sql, big data, agile, devops, cloud computing, data warehousing, data modelling, data streaming, machine learning, data analytics","agile, airflow, aws, azkaban, big data, cassandra, cloud computing, data modelling, data streaming, dataanalytics, datawarehouse, devops, ec2, elasticsearch, emr, hadoop, kafka, lucene, luigi, machine learning, postgres, python, rds, redshift, solr, spark, sparkstreaming, sql, storm"
Data Engineer Manager,Robert Half,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-engineer-manager-at-robert-half-3782267010,2023-12-17,Sterling,United States,Mid senior,Hybrid,"The Manager of Data Engineering plays a pivotal role in overseeing the operational efficiency of business intelligence systems through a collaborative, end-to-end approach. This multifaceted position involves team leadership and effective collaboration with various stakeholders to democratize data access while maintaining stringent governance for reliability and accuracy.
Key Responsibilities:
ETL/ELT Framework Development: Spearhead the establishment of robust ETL/ELT frameworks for the Data Analytics Team, making significant contributions to set a solid foundation.
Pipeline Design and Maintenance: Design and develop data pipelines, ensuring seamless data flow between the Data Warehouse and other analytical solutions. Provide support for existing pipelines, promptly troubleshooting issues as they arise.
PowerBI Expertise: Create and maintain PowerBI apps, reports, and dashboards to facilitate departments and communities in leveraging analytics for organizational efficiency and enhancing senior care.
Data Governance Enhancement: Improve governance of data assets, including PowerBi Datasets and Dataflows, to ensure data integrity and compliance with standards.
Collaborative Data Engineering: Collaborate with other Sunrise teams on data engineering efforts, fostering a unified approach to data-related challenges and opportunities.
Additional Responsibilities:
Innovative Data Solutions: Identify new areas of data, research, and data technology to address business problems, implementing effective project planning techniques for complex projects.
Technical Expertise: Leverage and contribute to data best practices, applying lessons learned to develop technical solutions for descriptive analytics, ETL, predictive modeling, and real-time decision analytics.
Data Technology Integration: Develop frameworks and prototypes that integrate data and advanced analytics, supporting informed business decisions.
Continuous Improvement: Implement new data technologies, research delivery methods, and understand data-related problems and requirements to identify optimal technical approaches.
Qualifications:
Bachelor's Degree in Computer Science, Computer Engineering, or a related field.
6+ years of IT experience, with 4+ years as a Data Engineer or 1+ years as a Manager of Data Engineering.
Strong communication skills and proven project management experience.
Technical mastery of core systems in the Sunrise Data Platform.
Hands-on experience with databases (e.g., SqlServer, Oracle, Snowflake) and BI software (especially PowerBI).
Expertise in ETL/ELT tools (e.g., SSIS, Azure Data Factory, Pentaho, Talend) and DevOps using GIT.
Proficiency in Unix/Linux environments.
Show more
Show less","ETL/ELT Frameworks, Pipelines, PowerBI, Data Governance, Collaborative Data Engineering, Data Solutions, Data Best Practices, Data Technology Integration, Continuous Improvement, SQL, Oracle, Snowflake, PowerBI, SSIS, Azure Data Factory, Pentaho, Talend, GIT, Unix/Linux","etlelt frameworks, pipelines, powerbi, data governance, collaborative data engineering, data solutions, data best practices, data technology integration, continuous improvement, sql, oracle, snowflake, powerbi, ssis, azure data factory, pentaho, talend, git, unixlinux","azure data factory, collaborative data engineering, continuous improvement, data best practices, data governance, data solutions, data technology integration, etlelt frameworks, git, oracle, pentaho, pipelines, powerbi, snowflake, sql, ssis, talend, unixlinux"
Data Mining and Analytics Engineer (Junior),ICF,"Arlington, VA",https://www.linkedin.com/jobs/view/data-mining-and-analytics-engineer-junior-at-icf-3726721043,2023-12-17,Sterling,United States,Mid senior,Hybrid,"ICF International seeks a Junior Data Mining and Analytics Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate. Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale.
As the Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.
The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!
This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the National Capital Region.
What You Will Be Doing
Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions
Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners
Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training
Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment
Develop custom data modeling procedures to assist with data mining, modeling, and production
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Develop processes and tools to monitor and analyze model performance and data accuracy
Interpret and communicate results to non-technical customers
What You Must Have
Active high-level security clearance required as part of client contract requirements
Bachelor’s degree in Computer Science, Mathematics, Engineering, or related field
US Citizenship required as part of client contract requirements
Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.
Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
Preferred Skills/Experience
Master’s degree in Computer Science, Mathematics, Engineering, or related field
Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details
Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise
Experience with computational notebook software such as Zeppelin or Jupyter
Experience with the application of visual analytics to computational analytic results
Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)
Experience with database querying like SQL
Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products
Scaled Agile Framework (SAFe) experience
Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired
CompTIA Security+ or higher cybersecurity certification preferred
#cybsr1
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.
Pay Range
- There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$64,372.00 - $109,432.00
Arlington, VA (VA31)
Show more
Show less","Data Mining, Analytics, Security Clearance, Computer Science, Mathematics, Engineering, Data Modeling, Data Analysis, Programming Languages, SQL, Scaled Agile Framework (SAFe), Amazon Web Services (AWS), CompTIA Security+, Zeppelin, Jupyter, Cybersecurity","data mining, analytics, security clearance, computer science, mathematics, engineering, data modeling, data analysis, programming languages, sql, scaled agile framework safe, amazon web services aws, comptia security, zeppelin, jupyter, cybersecurity","amazon web services aws, analytics, comptia security, computer science, cybersecurity, data mining, dataanalytics, datamodeling, engineering, jupyter, mathematics, programming languages, scaled agile framework safe, security clearance, sql, zeppelin"
Senior Data Engineer (AWS),Harnham,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/senior-data-engineer-aws-at-harnham-3756540852,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Senior Data Engineer (AWS)
Hybrid (D.C. area)
$100,000 -$150,000
You will be working with a rapidly growing government contracting technical solutions firm in the US. They work on extremely impactful projects with multiple government agencies. They are looking to bring on a full stack software engineer who will hit the ground running and have major growth opportunities in their organization.
Responsibilities
Design, code, test, and debug software applications and integrations.
Collaborate with cross-functional teams to define software requirements and system specifications.
Work closely with team members, stakeholders, and clients to understand requirements, provide updates, and address concerns, fostering a collaborative work environment.
Create and maintain technical documentation for efficient collaboration and knowledge transfer.
Write and execute software tests to ensure reliability, quality, functionality, and performance.
Utilize software development tools, version control systems, and SDLC methodologies to deliver high-quality software within timelines.
Write documentation to support DOD mission system compliance and approval.
Requirements
Proven experience in software engineering, with a strong understanding of software development principles, methodologies, and best practices.
Proficiency in programming languages such as Pyspark, databricks, Python, and familiarity with modern development frameworks and tools.
Excellent problem-solving and analytical thinking abilities, with a focus on delivering innovative and scalable solutions.
Experience collaborating with cross-functional teams and key stakeholders.
Knowledge of cloud-native technologies and experience with cloud-based development and deployment platforms (e.g., AWS)
Understanding of DOD CC SRG, impact levels such as IL-5, NIST 800-53 and Fedramp.
Active Secret Clearance.
Security+ certification.
Preferred Qualifications
Active CAC card or have previously obtained one.
Desired Traits
Takes initiative, proactively manages multiple priorities, and works well under pressure.
Analytical, creative thinker, and strategic problem-solver.
Delivers fast but carefully thought-out, high-quality results.
Structured, organized, and efficient work style.
Factual, direct communication style.
Aligned with our core values: honesty, humility, hard work, commitment, innovation, and exceptionalism.
Seniority Level
Mid-Senior level
Industry
TelecommunicationsComputer & Network SecurityInformation Technology and Services
Employment Type
Full-time
Job Functions
AnalystMarke
Show more
Show less","Software Engineering, Pyspark, Databricks, Python, Modern Development Frameworks, Development Tools, Version Control Systems, SDLC Methodologies, DOD Mission System Compliance, Programming Languages, Analytical Thinking, Problem Solving, Cloudnative Technologies, Cloudbased Development, Cloudbased Deployment Platforms, AWS, Security+ Certification, DOD CC SRG, NIST 80053, FedRamp","software engineering, pyspark, databricks, python, modern development frameworks, development tools, version control systems, sdlc methodologies, dod mission system compliance, programming languages, analytical thinking, problem solving, cloudnative technologies, cloudbased development, cloudbased deployment platforms, aws, security certification, dod cc srg, nist 80053, fedramp","analytical thinking, aws, cloudbased deployment platforms, cloudbased development, cloudnative technologies, databricks, development tools, dod cc srg, dod mission system compliance, fedramp, modern development frameworks, nist 80053, problem solving, programming languages, python, sdlc methodologies, security certification, software engineering, spark, version control systems"
Graph Data Developer #: 23-07299,HireTalent - Diversity Staffing & Recruiting Firm,"Rockville, MD",https://www.linkedin.com/jobs/view/graph-data-developer-%23-23-07299-at-hiretalent-diversity-staffing-recruiting-firm-3784903682,2023-12-17,Sterling,United States,Mid senior,Hybrid,"The our client Knowledge Graph team is looking for an experienced Graph Data Developer to join our team developing the next generation of graph-driven data solutions at our client. Our products connect the people, places, and things enabling pattern detection, self-exploration, and machine learning applications. The right candidate thrives in a team based environment focused on value and outcomes
Required Skills
5 or more years experience with Python or Scala development (OOP and scripting)
3 or more years of experience performing data analysis tasks in big data, graph, business intelligence, or similar environments
2 or more years of experience in AWS
Demonstrated expert level skills in SQL
1 or more years of experience in Neo4j
Experience with large scale data processing engines (Spark, Presto, or equivalent)
Naturally inquisitive and enjoys exploring data and business outcomes
Nice to Have
Financial industry experience
Experience in machine learning environments
Typical Job Activities
Data analytics and processing in Spark
Data profiling new data sources
Design and support of graph architecture
Identification of patterns in data
Engage with upstream and downstream engineering partners in value pipeline improvements
Agile ceremonies and team engagement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, identity, national origin, disability, or protected veteran status.
Show more
Show less","Python, Scala, OOP, SQL, Neo4j, Spark, Presto, AWS, Data analytics, Data profiling, Graph architecture, Pattern identification, Agile, Machine learning","python, scala, oop, sql, neo4j, spark, presto, aws, data analytics, data profiling, graph architecture, pattern identification, agile, machine learning","agile, aws, data profiling, dataanalytics, graph architecture, machine learning, neo4j, oop, pattern identification, presto, python, scala, spark, sql"
Senior Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-steampunk-inc-3734165558,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years experience with a Bachelor's Degree or 5+ years of experience with a Master's Degree
5-7 years industry experience coding commercial software and a passion for solving complex problems.
5-7 years direct experience in Data Engineering with experience in tools such as
Big data tools DataBricks, Confluent Kafka, Collibra, Spark, etc.
Relational SQL and NoSQL databases, including SQL, Postgres and Oracle.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, S3 (or Azure equivalents)
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a Change Agent in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our Human-Centered delivery methodology, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an employee owned company, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, Java, C++, Scala, SQL, PostgreSQL, Oracle, Airflow, Spark, DataBricks, Confluent Kafka, Collibra, EC2, EMR, RDS, S3, Data Processing, Data Modeling, Data Engineering, Agile Development, Big Data, Databases, Data Warehouses, Data Mining, Cloud Computing, Enterprise Architecture, Machine Learning","python, java, c, scala, sql, postgresql, oracle, airflow, spark, databricks, confluent kafka, collibra, ec2, emr, rds, s3, data processing, data modeling, data engineering, agile development, big data, databases, data warehouses, data mining, cloud computing, enterprise architecture, machine learning","agile development, airflow, big data, c, cloud computing, collibra, confluent kafka, data engineering, data mining, data processing, data warehouses, databases, databricks, datamodeling, ec2, emr, enterprise architecture, java, machine learning, oracle, postgresql, python, rds, s3, scala, spark, sql"
Senior Database Engineer,Mastercard,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785140981,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","MySQL, Aurora, MongoDB, AWS, Sharding, Clustering, HA, Scaling, Monitoring, Deploying, Tuning, DBE, DBA, DevSecOps, Cloud Native","mysql, aurora, mongodb, aws, sharding, clustering, ha, scaling, monitoring, deploying, tuning, dbe, dba, devsecops, cloud native","aurora, aws, cloud native, clustering, dba, dbe, deploying, devsecops, ha, mongodb, monitoring, mysql, scaling, sharding, tuning"
Senior Data Engineer,Motion Recruitment,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-motion-recruitment-3765926117,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Our client is seeking a Senior Data Engineer to collaborate with their internal and external requirements. You would be assisting with architecting, implementing, and improving end-to-end pipelines and automation for database processes. Their products are powered by extensive database to enable their clients with insights to make an impact at the local, state, and national level.
Requirements
5+ years with tech
SQL and NoSQL databases
Elasticseach
Spark, Java, or Python
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
LI#-DP1
Posted By:
Derek Progin
Show more
Show less","SQL, NoSQL, Elasticsearch, Spark, Java, Python","sql, nosql, elasticsearch, spark, java, python","elasticsearch, java, nosql, python, spark, sql"
Data Engineer - Python/Java/NoSQL/Big Data/ELK/Hybrid,Motion Recruitment,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-python-java-nosql-big-data-elk-hybrid-at-motion-recruitment-3764709687,2023-12-17,Sterling,United States,Mid senior,Hybrid,"A metro-DC-based company is searching for a Mid-Level Data Engineer for a full-time opportunity working in the government/info systems realm. This company sits on the cutting edge of technology, delivering innovative products and services through the strategic use of data, software and analytics. Their dual customer base of political organizations and commercial clients presents a unique variety of business requirements that drive faster innovation and encourage cross-application of practices between verticals.
This is a position for a data engineer who wants to be involved in every step of the process. You will help design, architect, and build end-to-end database solutions. The role itself is less of a traditional Data Engineering position focused around ETL work, but instead includes a mix of Data Engineering, Big Data, and Software Engineering responsibilities.
This opportunity is 4 days a week on-site in Clarendon, VA and offers excellent pay and benefits.
Required Skills & Experience
BA/BS in Computer Science, Computer Engineering, Statistics, or similar
3+ years working with Python and Java
Proficiency in NoSQL technologies
Proficiency with messaging/streaming technologies (Kafka/RabbitMQ)
Proficiency in version control technologies (Git/SVN/TFS)
Familiarity with Linux/Unix
Familiarity with Elasticsearch, Logstash, and Kibana (ELK stack)
Desired Skills & Experience
Masters or higher in Computer Science, Computer Engineering, Statistics, or similar
Experience with Spring framework, AOP, JPA and REST
Experience with building RESTful web services
Experience with Cloud system architecture and design, large scale streaming data processing
Experience in automation area for database technologies using Chef or Puppet
Experience coding in C#, R, GO, Rust etc.
The Offer You Will Receive The Following Benefits
Medical Insurance, including optional dental and vision benefits
HSA and flexible spending account
3 weeks PTO
401(k) w/ match
Paid parental leave
Infertility/adoption assistance
Bonus eligibility
Posted By:
Lindsay Troyer
Show more
Show less","Data Engineer, Data Engineering, Big Data, Software Engineering, ETL, Python, Java, NoSQL, Kafka, RabbitMQ, Git, SVN, TFS, Linux, Unix, Elasticsearch, Logstash, Kibana, Spring framework, AOP, JPA, REST, Cloud system architecture, SQL, Chef, Puppet, C#, R, GO, Rust","data engineer, data engineering, big data, software engineering, etl, python, java, nosql, kafka, rabbitmq, git, svn, tfs, linux, unix, elasticsearch, logstash, kibana, spring framework, aop, jpa, rest, cloud system architecture, sql, chef, puppet, c, r, go, rust","aop, big data, c, chef, cloud system architecture, data engineering, dataengineering, elasticsearch, etl, git, go, java, jpa, kafka, kibana, linux, logstash, nosql, puppet, python, r, rabbitmq, rest, rust, software engineering, spring framework, sql, svn, tfs, unix"
Data Analyst,BryceTech,"Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-at-brycetech-3753018072,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Company Description
BryceTech has partnered with technology and R&D clients to deliver mission and business success since 2017. Bryce combines core competencies in analytics and engineering with domain expertise. Our teams help government agencies, Fortune 500 firms, and investors manage complex programs, develop IT tools, and forecast critical outcomes. We offer clients proprietary, research-based models that enable evidence-based decision-making. Bryce cultivates a culture of engagement and partnership with our clients. BryceTech is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Job Description
We are looking for an experienced data analyst with strong experience in Python, SQL, OLAP concepts to join our team and support our Federal Government Customer. Responsibilities include:
Develop new data storage schema for workforce data records and migrate existing database records, including developing data extraction, transform, and load routines from systems of record to the newly designed tool
Ensure transition and preservation of historical records from existing database.
Develop quality assurance and data validation routines, and support data collection, integration, cleaning, and processing
Develop data presentations and visualizations based on existing database capabilities, conduct data analysis, develop analytic tools using Tableau and/or Power BI
Develop data access and sharing guidelines for internal and external communities, to include options for public facing data site
Ensure a seamless transition for users to a new system, including documenting usage and maintenance guidelines
Candidate must be proficient in the functionality of Python, SQL, OLAP concepts. Tableau and PowerBI is highly desired to create dashboards for analysis. Knowledge of the R programming language, Shiny, Dash or Streamlit are helpful.
Qualifications
Relevant Bachelor’s degree or higher in data analysis, computer science, or related analytic, engineering, math, science, or technology field.
4+ years of experience in data analysis
Additional Information
BryceTech offers a full range of benefits, including competitive salary, a comprehensive health plan including dental and vision coverage, company-paid life & disability insurance policies, 401(k) plan with company match, and an educational reimbursement program.
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Python, SQL, OLAP, Tableau, Power BI, R, Shiny, Dash, Streamlit, Data visualization, Data analysis, Data storage schema, Data extraction, Data transformation, Data loading, Data validation, Data cleaning, Data processing, Data presentations, Data access guidelines, Data sharing guidelines, User documentation","python, sql, olap, tableau, power bi, r, shiny, dash, streamlit, data visualization, data analysis, data storage schema, data extraction, data transformation, data loading, data validation, data cleaning, data processing, data presentations, data access guidelines, data sharing guidelines, user documentation","dash, data access guidelines, data cleaning, data extraction, data loading, data presentations, data processing, data sharing guidelines, data storage schema, data transformation, data validation, dataanalytics, olap, powerbi, python, r, shiny, sql, streamlit, tableau, user documentation, visualization"
Senior Database Administrator / Database Engineer,CoStar Group,"Washington, DC",https://www.linkedin.com/jobs/view/senior-database-administrator-database-engineer-at-costar-group-3762966267,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Company Overview
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives.
We have been living and breathing the world of real estate information and online marketplaces for over 35 years, giving us the perspective to create truly unique and valuable offerings to our customers. We’ve continually refined, transformed and perfected our approach to our business, creating a language that has become standard in our industry, for our customers, and even our competitors. We continue that effort today and are always working to improve and drive innovation. This is how we deliver for our customers, our employees, and investors. By equipping the brightest minds with the best resources available, we provide an invaluable edge in real estate.
Position Overview
CoStar Group serves up over 1.8PB of data from more than 800 SQL Servers that span four On-Premise data centers, two continents, and six different time zones and we are expanding to AWS Cloud solution rapidly and start to support Aurora MySQL, PostgreSQL, DynamoDB, Databricks, Snowflake and MongoDB Atlas and other DB engines on the cloud. Our highly motivated, extremely talented DBA group is looking for an exceptionally bright
Senior Database Administrators / Database Engineers
both On-prem and AWS Cloud that will help the company continue to grow to the next level. If you are passionate about automating and programming to support databases, this is your job! Our goal is to provide an exceptional experience for our customers through continued development, refinement, and improvement of our industry-leading applications.
We are a data-centric; Microsoft focused shop that uses the latest SQL Server, AWS RDS, DynamoDB and .NET technologies for major applications and also supports MongoDB, and more. As a member of the team, you will be relied on for your problem-solving, communication, collaboration, project management, and process adherence skills.
This position is located in Washington, DC and offers a hybrid schedule of 3 days onsite and 2 days remote per week.
Responsibilities
Demonstrate extensive knowledge of all aspects of Microsoft SQL Server, including administration, replication, backup/restore procedures and related components such as Reporting Services, Analysis Services, and Integration Services.
Implement and Maintain Automated Processes using TSQL/PowerShell/Python scripts
Understand AWS-based architecture and closely work with DevOps, SecOps and develop scripts to deploy and maintain the environment.
Install, configure, monitor, and maintain databases in the production, development, and quality assurance environments.
Recommend and implement solutions for SQL performance monitoring, tuning, and automation.
Understand SAN-based storage and related technologies, specifically as applicable and useful to MS SQL Server installations.
Work with the Operations team planning server hardware, storage, and OS configuration upgrades to help get the most out of SQL Server for specific application requirements and performance metrics.
Process database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policies.
Analyze problems, anticipate future problem areas, and implement solutions.
Work outside of normal business hours as required to complete projects, work on deployments, and resolving system outages.
To be placed on on-call rotation for a week every 3 months and respond to system alerts
Basic Qualifications
Bachelor's degree required from an accredited, not for profit university or college
5+ years of experience with Microsoft SQL Server 2016 / 2017 / 2019 supporting high-traffic websites.
Experience with the implementation and support of database replication.
Experience working with very large databases, upgrades, troubleshooting performance, etc.
Experience with the implementation and support of SQL database replication via script
Experience with SQL Server High Availability Architecture (AlwaysOn) and troubleshooting.
Understanding of Windows Server configuration and management, specifically as it relates to MS SQL Server installations and mission-critical systems.
Excellent understanding of coding with Microsoft T-SQL and stored procedures.
Desire to learn and administrate new cloud technologies in order to support a variety of database engines such as AWS Database Engines (RDS, DynamoDB, Atlas MongoDB)
Experience with adhering to and implementing best practices within an enterprise environment that includes using source control for all database-related artifacts (including jobs, SSIS packages, and replication configuration) and following a change request/notification policy.
Experience with either Python scripting and/or PowerShell is required
Experience with TFS/Visual Studio Database Projects, Azure DevOps, Terraform, CloudFormation, or any other CI/CD with a Source Control system is required.
Experience with SANs (3PAR/EMC/Pure), specifically with aspects related to MS SQL Server.
Understands and is able to properly implement backup and disaster recovery procedures.
Excellent work ethic and willingness to put in significant extra time with large team members are required.
Preferred Qualifications
Strong AWS experience
Deep Knowledge of Relational Database Management experience such as MS SQL, Aurora / RDS PostgreSQL, MySQL, and Redshift is preferred.
Deep knowledge of Azure DevOps / Terraform / GitHub is a big plus
NoSQL database experience (MongoDB/Hadoop/Redis/DynamoDB/Elastic Search)
Experienced with automation and deployment tools & Scripts: MS Team Foundation Service, ELK, PRTG, Lambda, CloudFormation, etc.
AWS solution architect certification
Any other programming skills are valuable, such as C#
Having knowledge of all aspects of Azure Database Engines (Managed Instances, Data Warehouse), including administration
Azure solutions architect associate certification
Excellent technical expertise in relevant state-of-the-art data technologies, database warehouse data security, data systems, and its design. Able to formulate and present various solutions for business initiatives and discuss pros/cons for each option.
Knowledge of monitoring tools such as Kibana, CloudWatch, DataDog, or any other tools, etc.
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar is not able to provide visa sponsorship for this position.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","SQL Server, SQL Services, Reporting Services, Analysis Services, Integration Services, TSQL, PowerShell, Python, AWS, RDS, DynamoDB, .NET, MongoDB, SANbased storage, Server hardware, Storage, OS configuration, TSQL, Stored procedures, Cloud technologies, Source control, Change request/notification policy, Python scripting, PowerShell, TFS/Visual Studio Database Projects, Azure DevOps, Terraform, CloudFormation, CI/CD, Source Control, NoSQL databases, Automation and deployment tools, Scripts, MS Team Foundation Service, ELK, PRTG, Lambda","sql server, sql services, reporting services, analysis services, integration services, tsql, powershell, python, aws, rds, dynamodb, net, mongodb, sanbased storage, server hardware, storage, os configuration, tsql, stored procedures, cloud technologies, source control, change requestnotification policy, python scripting, powershell, tfsvisual studio database projects, azure devops, terraform, cloudformation, cicd, source control, nosql databases, automation and deployment tools, scripts, ms team foundation service, elk, prtg, lambda","analysis services, automation and deployment tools, aws, azure devops, change requestnotification policy, cicd, cloud technologies, cloudformation, dynamodb, elk, integration services, lambda, mongodb, ms team foundation service, net, nosql databases, os configuration, powershell, prtg, python, python scripting, rds, reporting services, sanbased storage, scripts, server hardware, source control, sql server, sql services, storage, stored procedures, terraform, tfsvisual studio database projects, tsql"
Senior Big Data Engineer,Homes.com,"Washington, DC",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-homes-com-3785820520,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
Job Description
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives.
Homes.com is already one of the fastest growing real estate portals in the industry, we are driven to be #1. Just ask Brad Bellflower, Chief Change Officer at Apartments.com. After its acquisition in 2014, Apartments.com quickly turned into the most popular place to find a place. Proven success at the highest level – and we’re doing it again with the new Homes.com. Homes.com is a CoStar Group company with 20+ years' experience in leading and growing digital marketplaces. We pride ourselves on continually improving, innovating, and setting the standard for property search and marketing experiences. With Homes.com we’re building a brand on the cusp of defining the industry.
We’re looking for big thinkers, brave leaders, and creative advertising wizards ready to influence a new age of homebuying within a tried-and-true, award-winning company.
Learn more about Homes.com.
We are hiring a talented Senior Big Data Engineer to build cloud-based data pipelines for machine learning, data processing with Apache Spark, and database development.
This position is located in Washington, DC and offers a hybrid schedule of 3 days onsite, 2 days remote.
Responsibilities
Designing, building, testing and deploying scalable, reusable and maintainable applications that handle substantial amounts of data.
Taking full ownership of your work from development and testing to eventual deployment and support in production.
Collaborating with other engineers, product owners, designers, and leadership.
Becoming a trusted team member in matters of technical architecture, design and code.
Advocating for evolution and improvement - both technical and non-technical - within our teams. Includes new tech, tools and best practices.
Basic Qualifications
Bachelor’s Degree required from an accredited, not for profit university or college, with degree preferably in Computer Science, Data Science, or related field. MSc or PhD is a plus.
A track record of commitment to prior employers
A demonstrable record of accomplishment of building and launching successful products that use terabytes of data
5+ years of data pipeline engineering experience, and/or deep database engineering experience
Ability to analyze technical requirements and design new architectures, data models and ETL strategies
Hands-on experience with cloud-based relational and non-relational databases and proficiency in SQL.
Deliver work products that meet specifications, are free of defects, with excellent performance
Define Architecture and Development best practices
Preferred Skills
Performance tuning of database queries (SQL Server or PostgreSQL), database design, monitoring, and analysis
Experience with No-SQL databases (e.g. DynamoDB)
Experience with data pipeline tools (e.g. Glue, Step Functions, Lambda)
Experience using Confluent Kafka
Knowledge and/or experience working with Apache Spark/Databricks
Monitoring & dashboard metric management (e.g. CloudWatch, Kibana)
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
On-site fitness center and/or reimbursed fitness center membership costs (location dependent), with yoga studio, Pelotons, personal training, group exercise classes
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar Group is not able to provide visa sponsorship for this position.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","Cloudbased data pipelines, Machine learning, Data processing, Apache Spark, Database development, SQL, Cloudbased relational databases, Nonrelational databases, NoSQL databases, Data pipeline tools, Confluent Kafka, Apache Spark/Databricks, Monitoring & dashboard metric management, CloudWatch, Kibana, Performance tuning, Database design, Monitoring, Analysis, ETL strategies","cloudbased data pipelines, machine learning, data processing, apache spark, database development, sql, cloudbased relational databases, nonrelational databases, nosql databases, data pipeline tools, confluent kafka, apache sparkdatabricks, monitoring dashboard metric management, cloudwatch, kibana, performance tuning, database design, monitoring, analysis, etl strategies","analysis, apache spark, apache sparkdatabricks, cloudbased data pipelines, cloudbased relational databases, cloudwatch, confluent kafka, data pipeline tools, data processing, database design, database development, etl strategies, kibana, machine learning, monitoring, monitoring dashboard metric management, nonrelational databases, nosql databases, performance tuning, sql"
Data Integration Engineer/ Developer,Nine Mind Solutions,"Dulles, VA",https://www.linkedin.com/jobs/view/data-integration-engineer-developer-at-nine-mind-solutions-3768021546,2023-12-17,Sterling,United States,Mid senior,Hybrid,"We are seeking a qualified Cyber Security Data Integration Engineer/ Developer to support the design, development, and deployment of advanced cybersecurity capabilities.
Eligibility
Must be a US Citizen
Must have an active TS/SCI clearance.
Must be able to obtain Client Suitability prior to starting employment
6+ years of directly relevant experience
4+ years of experience with administration of enterprise SIEM technologies ( Splunk primarily)
Responsibilities
Responsibilities :
The Security Engineer is to play a key role in supporting a statewide program providing cyber assessment services and management that will protect 20+ affiliates from growing and evolving cyber threats. The engineering effort will focus on cloud security, SIEM and log management, and endpoint detection/response protecting customers from the ever growing and evolving cyber threats. This person will also work with customers to ensure the organization's compliance standards are met and maintained while also driving solid customer relationships to the next level.
This position requires a thorough understanding of network architecture fundamentals, protocols, routing, firewalls, cloud, and DevOps. This position is part of a larger team; however, the candidate is expected to work well on his or her own under general supervision, be self-directed, able to multi-task, and prioritize work.
Required Skills
Splunk Cloud experience: Architect, design, engineer, support, configure, administer content and maintain infrastructure for a highly available and disaster recovery configuration
Splunk experience: Administer Splunk and Splunk Application for Enterprise Security log or event management
Expertise with EDR toolsets administration, analysis, and integrations preferably CrowdStrike
Familiarity with SOAR Products include Phantom and ThreatConnect
Experience with scripting (e.g., PowerShell, bash/ksh/sh,python)
Ability to assist team with Incident response and handling
Excellent demonstrated experience in communicating technical information to non-technical and technical audiences.
Experience working directly with senior leadership and management.
Desired Skills
Automation: Experience related to Ansible for performing administration using code and Git/Gitlab for workflow management
Familiarity with Windows and Linux integration, SQL database technologies, troubleshooting, deployment, patching, and administration
Experience with Logstash and ability to collect, parse, and transform logs
Experience with the standards compliance process (e.g., NIST) and writing network security documentation
Desired Certifications : Splunk IT Service Intelligence Certified Admin, Splunk Enterprise Security Certified Admin, Splunk Cloud Certified Admin, CCNA, CCNP), NCSP 800-53 Practitioner
Required Education : Bachelor's degree in Information Security, Cyber Engineering or a related discipline is required. Two years of related work experience may be substituted for each year of degree-level education.
Show more
Show less","Cybersecurity, Data Integration, Splunk Cloud, SIEM, EDR, CrowdStrike, Phantom, ThreatConnect, PowerShell, Bash, Ksh, Sh, Python, Incident Response, Ansible, Git, Gitlab, SQL, Logstash, NIST, CCNA, CCNP, NCSP 80053 Practitioner, Information Security, Cyber Engineering","cybersecurity, data integration, splunk cloud, siem, edr, crowdstrike, phantom, threatconnect, powershell, bash, ksh, sh, python, incident response, ansible, git, gitlab, sql, logstash, nist, ccna, ccnp, ncsp 80053 practitioner, information security, cyber engineering","ansible, bash, ccna, ccnp, crowdstrike, cyber engineering, cybersecurity, data integration, edr, git, gitlab, incident response, information security, ksh, logstash, ncsp 80053 practitioner, nist, phantom, powershell, python, sh, siem, splunk cloud, sql, threatconnect"
Senior Cloud Data Engineer,BDO USA,"Potomac, MD",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470287,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, SQL, C#, Python, Java, Scala, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Data Definition Language, Data Manipulation Language, Views, Functions, Stored Procedures, Performance Tuning, Semantic Model Definition, Star Schema Construction, Tabular Modeling, Microsoft Fabric, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, data warehousing, data modeling, sql, c, python, java, scala, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, data definition language, data manipulation language, views, functions, stored procedures, performance tuning, semantic model definition, star schema construction, tabular modeling, microsoft fabric, azure data factory, redshift, uipath, cloud, rpa, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, automation tools, aws, aws lake formation, azure analysis services, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud, computer vision, data definition language, data lake medallion architecture, data manipulation language, data ops, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, functions, git, java, kinesis, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, quicksight, redshift, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, streaming data ingestion, tabular modeling, terraform, uipath, views"
Senior Cloud Data Engineer,BDO USA,"Washington, DC",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467839,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data analytics, Business intelligence, AI, Machine learning, Data modeling, Data ingestion, Semantic modeling, Automation, RPA, DevOps, Cloud computing, Azure, AWS, SQL, C#, Python, Java, Scala, Git, Linux, Data lake, Star schema, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, ai, machine learning, data modeling, data ingestion, semantic modeling, automation, rpa, devops, cloud computing, azure, aws, sql, c, python, java, scala, git, linux, data lake, star schema, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai, automation, aws, azure, bicep, business intelligence, c, cloud computing, data ingestion, data lake, data ops, dataanalytics, datamodeling, dbt, delta, devops, git, java, linux, machine learning, microsoft fabric, pandas, purview, python, rpa, scala, semantic modeling, spark, spark sql, sql, ssas, ssis, ssrs, star schema, terraform"
Senior Data Engineer - Data Quality/Governance,SiriusXM,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-quality-governance-at-siriusxm-3752310020,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role, you will be a member of SiriusXM’s Data Quality (DQ) Team within the larger Data Organization, and will be responsible for designing, developing and supporting data quality tools, applications and data marts for use by the DQ Team and other groups.
What You’ll Do
Define technical requirements for DQ tools and applications that support profiling, controls, alerts, KPIs and dashboards for business-critical partner and internal data utilized across SXM’s systems.
Design, develop and improve applications and tools to monitor data quality using data science algorithms and methodologies. This includes cloud-based data pipeline frameworks and workflow orchestration tooling.
Design and develop a data quality analytics framework to standardize the detection of data issues, identify root cause, and drive improvements in data processing procedures.
Develop subject matter expertise in SXM’s data domains (e.g., car/radio lifecycle, customer, marketing, streaming, contact center) to develop advanced quality controls and more easily navigate anomalies affecting a broad cross-functional audience.
Become proficient in using in-house and off-the-shelf DQ tools and applications.
Write documentation to encourage adoption of these tools and support users in their use; support analysts as needed with query/ETL optimization.
Strengthen corporate best practices around data engineering software development processes.
What You’ll Need
BS/MS or above in Computer Science or related field, or relevant experience
5+ years of overall work experience, including demonstrated understanding of the software development life cycle
5+ years of experience in a subscription services or data-driven industry or environment
5+ years of experience developing data ETL pipelines and data tools in Scala and/or Python
Experience with data technologies: e.g., Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies: e.g., Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing: e.g., Google Cloud Platform, Amazon Web Services
Experience with Data Visualization or Data Notebook tools: e.g., Jupyter, Zeppelin, Tableau, etc.
Experience developing and deploying machine learning algorithms
Experience with workflow management systems: e.g., Airflow, Composer, Luigi
Experience with unit and integration testing frameworks
Experience with API design/development: e.g., RPC, REST
Experience with DevOps tools and practices: e.g., Version control, CI/CD, Infrastructure as Code, build/deployment systems, performance monitoring
Experience with data serialization systems: e.g., Avro, Protobuf
Good public speaking and presentation skills
Interpersonal skills and ability to interact and work with staff at all levels
Excellent written and verbal communication skills
Ability to work independently and in a team environment
Attention to detail and organizational skills
Ability to project professionalism over the phone and in person
Ability to handle multiple tasks in a fast-paced environment
Commitment to “internal client” and customer service principles
Willingness to take initiative and to follow through on projects
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $195,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-05-126
Show more
Show less","Data Quality, Data Tools, Data Marts, Data Pipelines, Data Analytics Framework, Machine Learning Algorithms, SQL Applications, Cloud Computing, Data Visualization Tools, Data Notebook Tools, Workflow Management Systems, Unit and Integration Testing, API Design/Development, DevOps Tools, Data Serialization Systems, Scala, Python, Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Jupyter, Zeppelin, Tableau, Airflow, Composer, Luigi, Version Control, CI/CD, Avro, Protobuf","data quality, data tools, data marts, data pipelines, data analytics framework, machine learning algorithms, sql applications, cloud computing, data visualization tools, data notebook tools, workflow management systems, unit and integration testing, api designdevelopment, devops tools, data serialization systems, scala, python, databricks, mapreduce, hdfs, hive, tez, spark, sqoop, kafka, kafka connect, kstreams, ksql, kinesis, beam, flink, jupyter, zeppelin, tableau, airflow, composer, luigi, version control, cicd, avro, protobuf","airflow, api designdevelopment, avro, beam, cicd, cloud computing, composer, data analytics framework, data marts, data notebook tools, data quality, data serialization systems, data tools, data visualization tools, databricks, datapipeline, devops tools, flink, hdfs, hive, jupyter, kafka, kafka connect, kinesis, ksql, kstreams, luigi, machine learning algorithms, mapreduce, protobuf, python, scala, spark, sql applications, sqoop, tableau, tez, unit and integration testing, version control, workflow management systems, zeppelin"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3688219277,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Preferred
Experience with Informix and Data Stage
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Scala, SQL, Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, SQL Server, Big Data, Agile, Informix, Data Stage","data engineering, python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, scala, sql, amazon s3, athena, redshift spectrum, aws glue, aws glue catalog, aws functions, sql server, big data, agile, informix, data stage","agile, airflow, amazon s3, athena, aws, aws functions, aws glue, aws glue catalog, azkaban, big data, cassandra, data engineering, data stage, ec2, elasticsearch, emr, hadoop, informix, kafka, lucene, luigi, postgres, python, rds, redshift, redshift spectrum, scala, solr, spark, sparkstreaming, sql, sql server, storm"
Data Engineer,IntraFi,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-at-intrafi-3757234803,2023-12-17,Sterling,United States,Mid senior,Hybrid,"What is the role?
You will be a key member of our IT Data Engineering team responsible for providing the company with accurate and timely data gathered from our business processes and external sources. Your primary responsibility will be to assist in establishing the security, architecture, engineering, operations, and governance of the company’s data pipelines, data stores, and associated cloud-based infrastructure.
Your Responsibilities Will Include
Designing cloud infrastructure architecture options for the company’s data pipelines and stores
Researching and implementing best practices derived from industry experiences and standards
Facilitating and leading cross-functional teams to define, expand, document, and implement the company's data pipelines and data lake resources
Further developing subject matter expertise in data and cloud architecture and technologies to support initiatives across the company
Managing identity and access management solutions for the cloud infrastructure
Refining data governance and compliance standards to ensure the security and auditability of the data platform
Learning, understanding, and assisting with the operational management of the existing on-premises data applications and infrastructure
You Should Possess The Following Experience, Skills, And Qualifications
3+ years of progressive systems infrastructure and cloud experience
5+ years of infrastructure as code or software development experience; Python experience is preferred
3+ years of ETL and data pipeline experience using tools such as Talend, Airflow, SSIS, and AWS Glue
3+ years of database experience, including database administration, queries, and data modelling
Bachelor’s degree in computer science or equivalent experience; a Master’s degree is preferred
Experience designing, deploying, and supporting cloud technologies using AWS with expertise in AWS storage, security, data lake, database, and serverless compute tools
An infectious sense of exploration and experimentation, allowing you to learn new technologies and functional areas quickly
Ability to create standards and procedures for the production use of sufficiently mature technologies
Exceptional presentation, written, and verbal communication skills directed to both technical and nontechnical audiences
Outstanding interpersonal skills for listening to the input and needs of clients and building consensus among the team
Excellent organizational, planning, and project management skills with the ability to demonstrate mature, timely, and professional problem-solving abilities
Must demonstrate initiative and be customer-focused, self-directed, and results/goal-oriented
Show more
Show less","Cloud Infrastructure Architecture, Data Pipelines, Data Stores, Data Governance, Compliance Standards, Identity and Access Management, Data Applications, Infrastructure as Code, Software Development, Python, ETL, Data Pipeline, Talend, Airflow, SSIS, AWS Glue, Database Administration, Queries, Data Modelling, AWS Storage, AWS Security, AWS Data Lake, AWS Database, Serverless Compute Tools, DevOps, Agile, Scrum","cloud infrastructure architecture, data pipelines, data stores, data governance, compliance standards, identity and access management, data applications, infrastructure as code, software development, python, etl, data pipeline, talend, airflow, ssis, aws glue, database administration, queries, data modelling, aws storage, aws security, aws data lake, aws database, serverless compute tools, devops, agile, scrum","agile, airflow, aws data lake, aws database, aws glue, aws security, aws storage, cloud infrastructure architecture, compliance standards, data applications, data governance, data modelling, data pipeline, data stores, database administration, datapipeline, devops, etl, identity and access management, infrastructure as code, python, queries, scrum, serverless compute tools, software development, ssis, talend"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3757463845,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned
Data Engineer
to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
10+ years industry experience coding commercial software and a passion for solving complex problems.
10+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Java, C++, Scala, SQL, Agile","python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, java, c, scala, sql, agile","agile, airflow, aws, azkaban, c, cassandra, ec2, elasticsearch, emr, hadoop, java, kafka, lucene, luigi, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3734162947,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
2-4 years industry experience coding commercial software and a passion for solving complex problems.
2-4 years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Python, AWS, Data Exploitation, HumanCentered Design, DevSecOps, ETL, BI tools, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Hadoop, Spark, Kafka, SQL, NoSQL, Java, C++, Scala, Data modeling, Agile, Data warehousing","data engineering, python, aws, data exploitation, humancentered design, devsecops, etl, bi tools, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, hadoop, spark, kafka, sql, nosql, java, c, scala, data modeling, agile, data warehousing","agile, airflow, aws, azkaban, bi tools, c, cassandra, data engineering, data exploitation, datamodeling, datawarehouse, devsecops, ec2, elasticsearch, emr, etl, hadoop, humancentered design, java, kafka, lucene, luigi, nosql, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3757460949,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned
Data Engineer
to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
5+ years industry experience coding commercial software and a passion for solving complex problems.
5+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Data Platforms, Data Pipelines, ETL Jobs, BI Tools, Data Products, AWS, Azure, GCP, Python, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, SQL, NoSQL, Java, C++, Scala, Data Modeling, Agile, DevOps, Machine Learning","data engineering, data platforms, data pipelines, etl jobs, bi tools, data products, aws, azure, gcp, python, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, sql, nosql, java, c, scala, data modeling, agile, devops, machine learning","agile, airflow, aws, azkaban, azure, bi tools, c, cassandra, data engineering, data platforms, data products, datamodeling, datapipeline, devops, ec2, elasticsearch, emr, etl jobs, gcp, hadoop, java, kafka, lucene, luigi, machine learning, nosql, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Lead Data Analyst,Texas Tech University,"Lubbock, TX",https://www.linkedin.com/jobs/view/lead-data-analyst-at-texas-tech-university-3733827590,2023-12-17,Lubbock,United States,Mid senior,Onsite,"Lubbock
Lead Data Analyst
35349BR
Data Management Division
Position Description
Responsibilities
Performs specialized analytical duties in the operation and maintenance of assigned area. Responsibilities include collecting, analyzing and developing data relative to area, making recommendations and assisting in implementation of projects. Work is performed under general supervision with latitude for independent judgment in accordance with established policies and procedures. While some management oversight may be expected for specific projects, this position is expected to exercise discretion and independent judgment in the performance of the following duties:
Performs data analysis and develops database actions defined in project plans to meet customer requirement and/or marketing objectives.
About The Department And/or College
Through collaboration and coordination with campus partners, the Data Management Division will support all areas of Texas Tech University with the development, execution, and supervision of plans, policies, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their lifecycles.
Major/Essential Functions
Oversight for Data Governance Projects (Data Quality and Audits).
Evaluate/understand high level data flow and data exchange between systems.
Evaluate technical standards for critical systems.
Coordinate with technical staff on metadata, data dictionaries, data protection, and data quality management.
Understands data across systems and alignment with business processes.
Supports change controls related to critical data.
Manages initial triage work for data related issues and projects before it is escalated to the Data Stewardship Council.
Identify and track critical dependencies between business requirements and data.
Required Qualifications
Bachelor's degree plus four years progressively responsible related experience; OR a combination of education and/or experience to equal eight years
Preferred Qualifications
Experience developing data quality measures that align with business processes. Understanding of Master Data, Metadata, Reference Data, Data Warehousing, and BI principles and processes including technical architecture. Familiarly with enterprise information tools like SQL Server, Power BI, Oracle, etc. Excellent soft skills, including the ability to communicate well with various levels. Experience with Data Management Principles or Data Governance and Technical Writing.
Safety Information
Adherence to robust safety practices and compliance with all applicable health and safety regulations are responsibilities of all TTU employees.
Occasional Duties
Occasional work outside of normal hours required to meet project deadlines and travel to attend conferences or training.
Does this position work in a research laboratory?
No
Required Attachments
Cover Letter, Professional/Personal References, Resume / CV
Optional Attachments
Transcript
Job Type
Full Time
Pay Basis
Hourly
Minimum Hire Rate
20.76
Pay Statement
Compensation is commensurate upon the qualifications of the individual selected and budgetary guidelines of the hiring department, as well as the institutional pay plan. For additional information, please reference the institutional pay plan by visiting www.depts.ttu.edu/hr/payplan.
Travel Required
Up to 25%
Shift
Day
Schedule Details
M-F, 8AM-5PM
Grant Funded?
No
Job Group
Information and Records Clerks
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age, disability, genetic information or status as a protected veteran.
Show more
Show less","Data Analysis, Data Governance, Data Quality, Data Warehousing, Business Intelligence, SQL Server, Power BI, Oracle, Master Data, Metadata, Reference Data, Technical Writing, Data Management Principles, Data Stewardship","data analysis, data governance, data quality, data warehousing, business intelligence, sql server, power bi, oracle, master data, metadata, reference data, technical writing, data management principles, data stewardship","business intelligence, data governance, data management principles, data quality, data stewardship, dataanalytics, datawarehouse, master data, metadata, oracle, powerbi, reference data, sql server, technical writing"
data engineer iv,Skiltrek,"Rockledge, FL",https://www.linkedin.com/jobs/view/data-engineer-iv-at-skiltrek-3785509484,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"Job Summary:
Are you a process driven IT professional? Are you looking to work for a company where you can grow and learn and be a part of an outstanding health and IT organization? Our client in Rockledge, FL is looking for a highly motivated Data Engineer with Salesforce Development experience as well as as well as ETL and data integration experience to add to their quickly growing IT organization. This is a 13 week contract role with the possibility of extension. 100% remote. Visa sponsorship is not available for this opportunity.
location: ROCKLEDGE, Florida
job type: Contract
salary: $50 - 75 per hour
work hours: 8am to 5pm
education: Bachelors
Responsibilities:
POSITION SUMMARY
The Data Engineer IV serves as a senior technical member of an engineering team. The position will work across multiple disciplines to satisfy key business initiatives by delivering data and analytics solutions that scale efficiently while improving performance and minimizing costs. The Data Engineer IV works independently, with minimal supervision and is the lead for their technical domain or discipline. This role provides technical leadership to the engineering team, is able to work across multiple disciplines, and is looked to by others for guidance.
The Data Engineer IV is a lead contributor in developing strategies, standards, new capabilities, and design to enable faster and easier data access, automated workflows, and technical proofs of concept. The Data Engineer IV will have experience in design, implementation and support of data integration and ETL/ELT solutions within healthcare or a comparable industry. The position will require experience and working knowledge of Agile and/or Lean methodologies to deliver information services in an efficient and repeatable manner.
The Data Engineer IV proactively works with customers, stakeholders, analysts, architects, and other engineers to troubleshoot and find root cause of solution defects. They need to be excellent team players and have demonstrated experience across multiple systems.
Primary Accountabilities
Lead contributor to the development and design of end-to-end enterprise data delivery and analytic solutions using varying technologies (ETL, API, SQL, cloud), conducts reviews and validation of systems, code, and security.
Develops data solution and security standards for the organization.
Drives resolution of complex issues and implements corrective actions to prevent future occurrences.
Leads automation initiatives for testing and deployment.
Collaborates with business stakeholders to institute and advance data governance and quality frameworks.
Leads and coordinates cross functional teams and vendors to ensure the health of client's IT systems.
Reviews and jointly develops specifications and requirements for complex data integration and delivery products with users, vendors, and other engineers.
Will manage/lead complex technical projects. Proactively communicates with users and management the status of projects and requests.
Supports, maintains, and troubleshoots enterprise data delivery solutions with the ability to perform root cause analysis (RCA).
Shares expertise with a variety of business data domains, source systems, and business processes to enable delivery of solutions applicable across the business' clinical and business functions.
Minimum Qualifications
Education: Bachelor's Degree in Computer Science or equivalent STEM field
Licensure: None Required
Certification: Advanced data delivery-specific technical certification within one year of hire.
For Example:
Certified Data Vault 2.0 Practitioner (CDVP2)
TDWI Certified Business Intelligence Professional (CBIP)
DAMA Certified Data Management Professional (CDMP)
Work Experience: Six years of experience in delivery of data engineering solutions on large, heterogeneous datasets, to include data modeling, database/data mart development, ETL/ELT/data integration, and data security.
Work Experience In Lieu Of Education:
Associates degree plus six years of applicable technical experience in areas as noted above.
Or, in lieu of no degree, ten years of applicable technical experience in areas as noted above.
Knowledge/Skills/Abilities:
strong understanding of current developments and trends in Information Technology in one or more of the following domains:
Data Warehousing
ETL/ELT/Data integration
Data modeling / Database design
Ability to work in SQL, Oracle SQL and DB2
Data integration technologies such as data replication/CDC, message-oriented data movement, API, data stream integration, or CEP.
Dimensional modeling / Star Schema
Data governance framework (DMBOK, TDWI, etc.)
strong soft skills, verbal and written communication, including the ability to translate complex technical topics into understandable terminology, is a necessity.
This position must have the proven ability to lead large projects and initiatives from kick off to completion.
This position builds trust among colleagues, must be a continuous learner, and focus on delivering capabilities for the customer.
Preferred Qualifications
Education: None
Licensure: None
Certification: Existing advanced data delivery-specific technical certification. For example:
Certified Data Vault 2.0 Practitioner (CDVP2)
TDWI Certified Business Intelligence Professional (CBIP)
DAMA Certified Data Management Professional (CDMP)
Work Experience: 6+ years of experience preferred.
Knowledge/Skills/Abilities:
Cloud services, especially Azure
Hadoop/Mongo DB experience
R, Python, C#, Java or other OO development
Knowledge of MUMPS
Formal training in Lean or Agile
Physical Requirements
Sedentary
Majority of time involves sitting or standing; occasional walking, bending, stooping
Long periods of computer time or at workstation
Light work that may include lifting or moving objects up to 20 pounds with or without assistance.
May be exposed to inside environments with varied temperatures, air quality, lighting and/or low to moderate noise
Communicating with others to exchange information.
Visual acuity and hand-eye coordination to perform tasks
Workspace may vary from open to confined, on site or remote
May require travel to various facilities within and beyond county perimeter; may require use of personal vehicle
Qualifications:
Experience level: Experienced
Minimum 6 Years Of Experience
Education: Bachelors (required)
Skills:
Data Warehouse
ETL
About Us
Skiltrek is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US.
At Skiltrek, we promise you the perfect opportunity of building technical excellence, understand business performance and nuances,
be abreast with the latest happenings in technology world and enjoy a satisfying work life balance.
Skiltrek is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender,
race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law.
Skiltrek is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Show more
Show less","Data Engineering, Salesforce Development, ETL, Data Integration, Agile, Lean, SQL, Oracle SQL, DB2, Data Replication, CDC, MessageOriented Data Movement, API, Data Stream Integration, CEP, Dimensional Modeling, Star Schema, Data Governance Framework, DAMA, Hadoop, Mongo DB, R, Python, C#, Java, MUMPS","data engineering, salesforce development, etl, data integration, agile, lean, sql, oracle sql, db2, data replication, cdc, messageoriented data movement, api, data stream integration, cep, dimensional modeling, star schema, data governance framework, dama, hadoop, mongo db, r, python, c, java, mumps","agile, api, c, cdc, cep, dama, data engineering, data governance framework, data integration, data replication, data stream integration, db2, dimensional modeling, etl, hadoop, java, lean, messageoriented data movement, mongo db, mumps, oracle sql, python, r, salesforce development, sql, star schema"
Senior Principal Data Engineer (Melbourne FL),Northrop Grumman,"Melbourne, FL",https://www.linkedin.com/jobs/view/senior-principal-data-engineer-melbourne-fl-at-northrop-grumman-3768861310,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Are you motivated to work in an environment that will challenge you, force you to continuously innovate, and work on solutions that make a difference for your customers?
Northrop Grumman Aerospace Systems is looking for a passionate
Senior Principal Data Engineer
in
Melbourne Florida
to design and develop automated, end-to-end, ETL pipelines and Data Analytics/Visualization solutions from disparate data sources. You will share in the ownership of the technical vision and direction for advanced analytics systems that change the way we see and use data. We are looking for people who are self-motivated, hardworking, and have demonstrated the ability to find innovative solutions to complex technical problems.
Job Responsibilities
Design, develop, and maintain a scalable ETL pipelines.
Enable storing, searching, processing, and securing of extremely large structured or unstructured data sets.
Data preparation/cleaning, integration, and automation from heterogeneous sources
Ensure data integrity and system availability
Identify, evaluate, and recommend core technologies and strategies
Monitor and optimize system performance
Decomposition of user requirements into logical functions/components
Basic Qualifications for Senior Principal Data Engineer:
Bachelor's in a STEM related field with 9 years of experience; Master's in a STEM related field with 7 years of experience. PhD with 4 years of experience.
5+ years of experience working with ETL techniques and frameworks
Understanding of elastic data storage and archive storage lifecycle management
Experience with integration of data from multiple sources
Experience with Python, SQL (structure query language), and relational databases.
Experience with data visualization tools (e.g. Tableau)
Experience with Agile Software Development
US Citizenship with the ability to obtain/maintain an active DoD Secret Clearance.
Must be able to obtain Program Access (PAR) within a reasonable amount of time
Preferred Qualifications:
Master's degree with 7 years of relevant experience.
5+ years of experience working with ETL techniques and frameworks.
Expert in Python, SQL, and data visualization tools.
Development experience utilizing Hadoop, Spark, PowerShell, and automation scripts
Familiarity with Data Virtualization and Data Cataloging tools (e.g. Denodo, Collibra).
Experience with NoSQL Databases (e.g., MongoDB, Neo4J, etc.)
CompTIA Security+ Certification.
Current DOD Top Secret clearance.
Salary Range:
$112,000 - $168,000
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Hadoop, Spark, NoSQL Databases, SQL, Python, Databases, ETL Pipelines, Data Analytics, Visualization, Relational Databases, Data Visualization Tools, Tableau, Agile Software Development, Denodo, Collibra, Automation Scripts, PowerShell, CompTIA Security+ Certification, Data Virtualization, Data Cataloging","hadoop, spark, nosql databases, sql, python, databases, etl pipelines, data analytics, visualization, relational databases, data visualization tools, tableau, agile software development, denodo, collibra, automation scripts, powershell, comptia security certification, data virtualization, data cataloging","agile software development, automation scripts, collibra, comptia security certification, data cataloging, data virtualization, data visualization tools, dataanalytics, databases, denodo, etl pipelines, hadoop, nosql databases, powershell, python, relational databases, spark, sql, tableau, visualization"
Sr Data Operations Engineer,Wabtec Corporation,"West Melbourne, FL",https://www.linkedin.com/jobs/view/sr-data-operations-engineer-at-wabtec-corporation-3748730547,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"Wabtec Corporation is a leading global provider of equipment, systems, digital solutions and value-added services for freight and transit rail. Drawing on nearly four centuries of collective experience across Wabtec, GE Transportation and Faiveley Transport, the company has unmatched digital expertise, technological innovation, and world-class manufacturing and services, enabling the digital-rail-and-transit ecosystems. Wabtec is focused on performance that drives progress, creating transportation solutions that move and improve the world. Wabtec has approximately 27,000 employees in facilities throughout the world. Visit the company’s new website at: http://www.WabtecCorp.com.
It’s not just about your career… or your job title…it’s about who you are and the impact you are going to make on the world. Do you want to go into uncharted waters…do things that haven’t been done to make yours and someone else's life better? Wabtec has been doing that for decades and we will continue to do so! Through our people, leadership development, services, technology and scale, Wabtec delivers better outcomes for global customers by speaking the language of industry.
Who will you be working with?
Our system architecture team combines deep knowledge of Linux, Windows, Networking, and Database Technologies. We deliver excellence and automation in deploying and maintaining many customer environments and projects. Here you will grow in your specialized field and partner with other key stakeholders to support our customers and their operations.
How will you make a difference?
As a member of the Digital Operations group, The Database Administrator will have deep, focused, and varied experience specifically with supporting production Oracle Database environments as well as aptitude and desire for growth into other database arenas such as Cockroach or PostgreSQL.
The Oracle expert will plan, coordinate, and administer Oracle database(s) in an environment that requires deep hands-on design, implementation, and maintenances of a mature database infrastructure in a fully automated environment using Ansible.
What do we want to know about you?
BS/BA in Computer Science or related field, or equivalent work experience
3 to 5 years of deep experience in Cockroach Database Administration and design.
Working knowledge of Oracle Database Administration.
Experience with migration of data cross database platforms.
Working knowledge of incident and service management. ITIL certified (preferred).
Hands on, production experience with troubleshooting of database issues.
The ability to seamlessly work with application developers, other strong technical leaders, and non-technical staff.
Extremely strong written and verbal skills. Documentation will be a large part of the job.
Desire and flexibility to be on-call to support customers at varying contractual agreement levels.
What will your typical day look like?
As a part of a cross-functional IT team, the Oracle DBA will create and maintain all databases required for development, testing, and production usage.
Performs capacity planning required to create and maintain Oracle databases. The DBA works closely with System Administration Engineers to build capacity plans.
Performs ongoing tuning of all Oracle databases.
Installs new versions of Oracle and its tools.
Patches Oracle on a consistent cycle.
Plans and implements backup and recovery for Oracle databases.
Implements and enforces security for all Cockroach databases.
Use knowledge of Oracle internals to help develop and implement application design and coding use of the database.
Provide technical support to the application development team.
Automation of database related activities using Ansible.
Maintain Oracle Goldengate infrastructure and processes.
Oracle Dataguard monitoring, trouble-shooting, and creation.
Oracle RMAN backup experience.
Other duties as assigned
LI-TD1
Wabtec Corporation is committed to taking on the world’s toughest challenges. In order to fulfill that commitment we rely on a culture of leadership, diversity and inclusiveness. We aim to employ the world’s brightest minds to help us create a limitless source of ideas and opportunities. We believe in hiring talented people of varied backgrounds, experiences and styles…people like you! Wabtec Corporation is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, or protected Veteran status. If you have a disability or special need that requires accommodation, please let us know.
Show more
Show less","Linux, Windows, Networking, Database Technologies, Oracle Database, Cockroach, PostgreSQL, Ansible, ITIL, Incident and service management, Troubleshooting, Database administration, Capacity planning, Tuning, Patching, Backup and recovery, Security, Application design and coding, Technical support, Automation, Oracle Goldengate, Oracle Dataguard, Oracle RMAN","linux, windows, networking, database technologies, oracle database, cockroach, postgresql, ansible, itil, incident and service management, troubleshooting, database administration, capacity planning, tuning, patching, backup and recovery, security, application design and coding, technical support, automation, oracle goldengate, oracle dataguard, oracle rman","ansible, application design and coding, automation, backup and recovery, capacity planning, cockroach, database administration, database technologies, incident and service management, itil, linux, networking, oracle database, oracle dataguard, oracle goldengate, oracle rman, patching, postgresql, security, technical support, troubleshooting, tuning, windows"
Senior Principal Data Engineer (Melbourne FL) with Security Clearance,ClearanceJobs,"Melbourne, FL",https://www.linkedin.com/jobs/view/senior-principal-data-engineer-melbourne-fl-with-security-clearance-at-clearancejobs-3770046062,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Responsibilities
Are you motivated to work in an environment that will challenge you, force you to continuously innovate, and work on solutions that make a difference for your customers? Northrop Grumman Aerospace Systems is looking for a passionate Senior Principal Data Engineer in Melbourne Florida to design and develop automated, end-to-end, ETL pipelines and Data Analytics/Visualization solutions from disparate data sources. You will share in the ownership of the technical vision and direction for advanced analytics systems that change the way we see and use data. We are looking for people who are self-motivated, hardworking, and have demonstrated the ability to find innovative solutions to complex technical problems. Job Responsibilities:
Design, develop, and maintain a scalable ETL pipelines.
Enable storing, searching, processing, and securing of extremely large structured or unstructured data sets.
Data preparation/cleaning, integration, and automation from heterogeneous sources
Ensure data integrity and system availability
Identify, evaluate, and recommend core technologies and strategies
Monitor and optimize system performance
Decomposition of user requirements into logical functions/componentsBasic Qualifications for Senior Principal Data Engineer:
Bachelor's in a STEM related field with 9 years of experience; Master's in a STEM related field with 7 years of experience. PhD with 4 years of experience.
5+ years of experience working with ETL techniques and frameworks
Understanding of elastic data storage and archive storage lifecycle management
Experience with integration of data from multiple sources
Experience with Python, SQL (structure query language), and relational databases.
Experience with data visualization tools (e.g. Tableau)
Experience with Agile Software Development
US Citizenship with the ability to obtain/maintain an active DoD Secret Clearance. * Must be able to obtain Program Access (PAR) within a reasonable amount of timePreferred Qualifications:
Master's degree with 7 years of relevant experience.
5+ years of experience working with ETL techniques and frameworks.
Expert in Python, SQL, and data visualization tools.
Development experience utilizing Hadoop, Spark, PowerShell, and automation scripts
Familiarity with Data Virtualization and Data Cataloging tools (e.g. Denodo, Collibra).
Experience with NoSQL Databases (e.g., MongoDB, Neo4J, etc.)
CompTIA Security+ Certification.
Current DOD Top Secret clearance.Salary Range: $112,000 - $168,000 The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions. Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","ETL, Elastic data storage, Data visualization, Agile software development, Python, SQL, Hadoop, Spark, PowerShell, Data virtualization, Data cataloging, NoSQL databases, MongoDB, Neo4J","etl, elastic data storage, data visualization, agile software development, python, sql, hadoop, spark, powershell, data virtualization, data cataloging, nosql databases, mongodb, neo4j","agile software development, data cataloging, data virtualization, elastic data storage, etl, hadoop, mongodb, neo4j, nosql databases, powershell, python, spark, sql, visualization"
Sr. Data Analyst,Eversource Solutions LLC,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-analyst-at-eversource-solutions-llc-3787922706,2023-12-17,West Springfield,United States,Mid senior,Onsite,"RESPONSIBILITIES:
Improve use case implementation and reporting through data modeling and design.
Ability to grasp, document, and articulate revenue assurance and fraud analysis pertaining to customers. Additionally, possess the skills to create reports for both Directors and field personnel, which hold potential for legal action against criminals.
Proficiency in SAS, Oracle, Teradata, Excel, and database analytics is a prerequisite.
Capability to write intricate SQL queries for report generation.
The aptitude to craft visually captivating presentations using elements such as (graphics, graphs, clipart, MS Visio diagrams, and MS PowerPoint).
Successfully engages in multiple initiatives simultaneously.
Collaborates autonomously with users to formulate concepts while operating under the guidance of project managers.
Act as the intermediary linking the customer community (both internal and external customers) with the software development team, facilitating the seamless flow of requirements.
Formulates requirement specifications in accordance with established templates, employing natural language as and when required.
Collaborates with developers and subject matter experts to establish the data sources, and the technical vision and analyze tradeoffs between usability and performance needs.
Be the liaison between the business units, technology teams, and support teams.
Assigned responsibilities will encompass assisting in the QA process and contributing to the development of training materials.
Applicants should either hold U.S. citizenship or possess the authorization to work full-time in the United States for any employer.
Should be capable of obtaining a public trust clearance from the US government.
Strong proficiency in verbal, written, and interpersonal communication skills is a prerequisite for this role.
Requires on-site presence in Washington DC with the client, at least 2-3 days a week.
QUALIFICATIONS:
Minimum Qualifications:
Must have a bachelor’s degree in computer science or information systems or science-related field or related experience.
The Sr. Data Analyst must have minimum of 6+ years of professional experience analyzing data needs, business needs, performing data analysis and performing ETL against a multitude of data sources and platforms, business analytics, proposing business solutions, and generating reports.
Must have 5+ years’ business and technical analysis experience in a client-facing role with the ability to use PowerPoint, excel, and other graphical tools such as Qlik, PowerBI, Tableau or Microstrategy to report to upper management the results of the analysis.
Must be proficient and at an advanced level in SAS, Teradata, and Oracle ETL processes and reporting.
Knowledge of Internet technologies and web applications is useful including Jason, XML and web services.
Must understand data modeling data entities and data communication and processing.
Complete ad hoc report requests from business stakeholders leveraging necessary Business Intelligence (Bl) tools.
Must have strong skills with Excel, SAS, SQL queries, functions, procedures, views.
Must understand relational databases; past database development and data analysis experience is needed.
Must have served as a data analyst in 3-5 projects in the past as a team member and in a lead capacity on at least one project.
Must have prior customer support experience to be able to work with customers and explain the reports and decisions and trends to them.
Must be able to solve problems and help modelers with the right type of cleaned data.
Good interpersonal skills are also required.
Must have strong written and verbal communication skills.
Desired Qualifications:
Master’s Degree in the related field is a plus.
Prior experience of working with Teradata or other data warehouse databases is a plus.
Big data experience is a huge plus.
Understanding supply chain management communication and business transactions is a big plus.
WHY YOU SHOULD JOIN:
Collaborative & Inclusive Culture:
Our client’s colleagues work in small, self-organized and cross-functional teams that determine the best tactics to support the vision and strategy set by leadership. They encourage and welcome all ideas, taking an agile approach to creating an amazing product.
Design Thinking for Innovation:
With focus on their colleague’s growth, they have developed a Learning Management System (LMS) that trains all colleagues on Design Thinking for Innovation, Security, BI & Analytics, and the business they are about to support. More trainings are added every year.
Their Colleagues are recognized for their innovative problem solving, goodwill, candor with respect and excellence. We also give out spot bonuses besides the yearly performance bonus.
Workplace:
While their headquarters is in Sterling, Virginia, and our largest client in Washington DC (on-site), their team lives across the globe. They partner with their team to help them achieve personal and professional alignment, helping their colleagues to define their own blend of work-life balance.
Office Perks
: Colleagues that work out of one of their main offices enjoy beverages, snacks, lunches, and happy hour benefits.
Vacation & Holiday Schedule: They believe this is a partnership and they trust their colleagues to build their own time-off schedule and encourage their teams to take time for their well-being while minimizing impacts to Client’s deliverables.
Growth Opportunity: Through hands-on learning and development, the opportunities are endless.
Focus on Well-Being:
They pride themselves on offering numerous benefit options to best fit the needs of our colleagues and their families, including domestic partners. Benefits include medical (PPO & HDHP-HSA), dental, vision, flexible spending account, commuter benefit, discount programs, life & disability insurance, and accident insurance. They also offer a 401k program with company match and paid parental leave.
Live your Passion:
The client encourages and promotes monetary and personnel involvement in the causes around the globe. They believe in their Value of Goodwill, and work to find new ways to give back to their communities and make a positive impact. They focus on initiatives in the following areas - technology, charity, volunteerism, and honorary contributions.
The duties and responsibilities listed in this job description generally cover the nature and level of work being performed by individuals assigned to this position. This is not intended to be a complete list of all duties, responsibilities, and skills required. Subject to the terms of an applicable collective bargaining agreement, the company management reserves the right to modify, add, or remove duties and to assign other duties as may be necessary. We wish to thank all applicants for their interest and effort in applying for the position; however, only candidates selected for interviews will be contacted.
EverSource Solutions LLC. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Powered by JazzHR
XJq2wmUi2L
Show more
Show less","Data modeling, Data design, Revenue assurance, Fraud analysis, SAS, Oracle, Teradata, Excel, Database analytics, SQL, PowerPoint, Visio, MS Visio diagrams, Natural language, ETL, Business analytics, Data sources, Technical vision, Usability, Performance needs, Business Intelligence (BI) tools, Relational databases, Database development, Data analysis, Data analyst, Customer support, Data communication, Data processing, Jason, XML, Web services, Data entities, Ad hoc report requests, Business stakeholders, Data modeling, Data cleaning, Interpersonal skills, Written communication, Verbal communication","data modeling, data design, revenue assurance, fraud analysis, sas, oracle, teradata, excel, database analytics, sql, powerpoint, visio, ms visio diagrams, natural language, etl, business analytics, data sources, technical vision, usability, performance needs, business intelligence bi tools, relational databases, database development, data analysis, data analyst, customer support, data communication, data processing, jason, xml, web services, data entities, ad hoc report requests, business stakeholders, data modeling, data cleaning, interpersonal skills, written communication, verbal communication","ad hoc report requests, business analytics, business intelligence bi tools, business stakeholders, customer support, data cleaning, data communication, data design, data entities, data processing, data sources, dataanalytics, database analytics, database development, datamodeling, etl, excel, fraud analysis, interpersonal skills, jason, ms visio diagrams, natural language, oracle, performance needs, powerpoint, relational databases, revenue assurance, sas, sql, technical vision, teradata, usability, verbal communication, visio, web services, written communication, xml"
Data Analyst I,SimIS Inc.,"Quantico, VA",https://www.linkedin.com/jobs/view/data-analyst-i-at-simis-inc-3787741409,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Position
Data Analyst I
Security Clearance
Secret
Work Location
Hybrid, contractor/remote site and government on-site, Quantico, VA
Education
BS
Position Description
Serve as the Data Analyst supporting a team of Enterprise Architects and Business Analysts developing, updating, and archiving Department of Defense Architecture Framework (DoDAF) products and services that inform customer enterprise-level system integration and capability requirements and documentation.
Experience
2 or more years of experience, within the past 5 years providing data analysis/data analytical procedures for a DOD agency.
Current experience in conducting data collection activities including data analysis methodology, data validation techniques, and quality assurance for IT support systems.
Current experience in the application of Agile procedures to support development, analysis, and refinement of system capabilities to support client requirements.
Desired – USMC military or civilian Service.
Desired - Prior TECOM experience.
Desired - Current experience in and understanding of military operations and in conducting in-depth research, analysis, and documentation of analytical findings and conclusions.
Desired - Experience in Joint and Combined operations, inter-agency operations, or combat development/defense acquisition
Knowledge
Current knowledge and experience in data modeling techniques, data aggregation methodologies, and statistical analysis procedures.
Skills
Analyzes data in various formats to support assessment of enterprise trends and developing requirements.
Uses a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining.
Uses Defense Collaborative System, WebEx and other collaborative planning tools.
Uses MS Office/365 office, MS SharePoint, MS Windows server and desktop operating environments.
Abilities
Maintain excellent writing, analytic, statistical, and communications skills in order to coordinate effectively, document analytical procedures and products, and communicate with customer leadership and stakeholders.
Credentials
No specific requirements
SimIS Offers:
Flexible Spending Account (FSA)
Medical, Dental, and Vision
Short Term Disability (SimIS provides Short-Term Disability benefits at no cost to you)
LTD
Life Insurance
401(k) Savings Plan
Tuition Assistance Program
Paid Time Off (PTO)
10 Holidays each year
SimIS, Inc. is an AA / EOE / M / F / Disability / Vet / V3 certified / Drug Free Employer
Powered by JazzHR
y4QwInQNDW
Show more
Show less","Data Analysis, Data Analytics, Data Aggregation, Data Mining, Statistical Analysis, Agile, Data Modeling, Defense Collaborative System, WebEx, MS Office/365, MS SharePoint, MS Windows, DoD Architecture Framework (DoDAF)","data analysis, data analytics, data aggregation, data mining, statistical analysis, agile, data modeling, defense collaborative system, webex, ms office365, ms sharepoint, ms windows, dod architecture framework dodaf","agile, data aggregation, data mining, dataanalytics, datamodeling, defense collaborative system, dod architecture framework dodaf, ms office365, ms sharepoint, ms windows, statistical analysis, webex"
Data Analyst - Secret,"KBR, Inc.","Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-secret-at-kbr-inc-3770440580,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Title:
Data Analyst - Secret
The Data Analyst supports operations and conducts data analytics projects to provide information and insights to stakeholders for decision-making or solution development; implements data standards and deploys automation tools to extract, synthesize, and validate data from different sources and transform data into usable metrics; constructs data sets, monitors data quality, troubleshoots and resolves database issues to ensure data integrity; utilizes scripting and querying tools like Python, R or SQL, data visualization/BI tools, statistical methods, and data modeling to produce reports, data files, and dashboards; collaborates with stakeholders to understand their needs, objectives, and requirements.
The Data Analyst shall:
Work independently or under general direction to perform analysis, preparation, and entry of data into prescribed data formats.
Directly support the Senior Data Analyst as well as Subject Matter Experts (SMEs) with analytical support services and related studies.
Submit recommendations for solutions.
Prepare written reports to management.
Required skills:
Have at least 2 years but no more than 6 years of experience in support of government acquisition and knowledge of requirements to collect, store, organize, and assess data related to acquisition programs.
Demonstrated capability to work both independently and under the direction of experienced analysts.
Demonstrated skill in defining, developing, and maintaining databases and working with spreadsheets and complex data models.
Demonstrated ability to work in a dynamic work environment and ability to coordinate and perform multiple assignments.
Possession of a government or industry standard data science certification is preferred but not required.
Experience utilizing scripting and querying tools like Python, R or SQL
Experience with data visualization/BI tools such as Power BI and Tableau
Bachelors Degree or higher
Experience with the production of reports, data files, and dashboards
Clearance Requirement:
Secret Clearance Required, Top Secret Preferred.
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Data analytics, Data visualization, Python, R, SQL, Power BI, Tableau, Statistics, Data modeling, Data mining, Data warehousing, Data governance, Machine learning, Artificial intelligence, Databases, Spreadsheets, Data science, Report writing, Project management, Team work, Communication","data analytics, data visualization, python, r, sql, power bi, tableau, statistics, data modeling, data mining, data warehousing, data governance, machine learning, artificial intelligence, databases, spreadsheets, data science, report writing, project management, team work, communication","artificial intelligence, communication, data governance, data mining, data science, dataanalytics, databases, datamodeling, datawarehouse, machine learning, powerbi, project management, python, r, report writing, spreadsheets, sql, statistics, tableau, team work, visualization"
Data Engineer,NR Consulting,"Herndon, VA",https://www.linkedin.com/jobs/view/data-engineer-at-nr-consulting-3768016667,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Job Requirements:
Database engineering and management activities associated with high volume, multi-format data management services
Designs large-scale data management solutions including developing efficient storage, indexing, and access approaches
Updates data enterprise architecture models (if needed) and data abstraction layer to enable access to new data sources
Designs and implements indexing strategies for ingested data.
Designs and implements storage and indexing strategies across the multiple repositories to provide efficient storage and retrieval
Supports design and development of data access APIs
Basic Qualifications
Bachelor's degree with 9+ years of relevant experience OR a master's degree with 7 years of relevant experience OR A PhD; an additional 4 years of experience maybe considered in lieu of a degree requirement
Active TS/SCI clearance
Experience with Postgres, SQL, PL/pgSQL
Experience installing application software in an operational environment
Experience with Agile, AWS, RDBMS technology, Elasticsearch, Linux
Security+
Active TS/SCI CI poly clearance (can start unclass before indoc)
Preferred
Knowledge of data modeling, Cameo
Familiarity with NoSQL technology (Hadoop and/or EMR)
CentOS System Administration
Linux
Comments for Suppliers: TS/SCI CI poly required; however selected candidate can start unclass before indoc
Show more
Show less","Database engineering, Data management, Data architecture, Data abstraction, Indexing, Storage, Retrieval, Data access APIs, Postgres, SQL, PL/pgSQL, Agile, AWS, RDBMS, Elasticsearch, Linux, Security+, Data modeling, Cameo, NoSQL, Hadoop, EMR, CentOS System Administration","database engineering, data management, data architecture, data abstraction, indexing, storage, retrieval, data access apis, postgres, sql, plpgsql, agile, aws, rdbms, elasticsearch, linux, security, data modeling, cameo, nosql, hadoop, emr, centos system administration","agile, aws, cameo, centos system administration, data abstraction, data access apis, data architecture, data management, database engineering, datamodeling, elasticsearch, emr, hadoop, indexing, linux, nosql, plpgsql, postgres, rdbms, retrieval, security, sql, storage"
Senior Data Engineer - Principal Associate,Jobs for Humanity,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-principal-associate-at-jobs-for-humanity-3768928383,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Job Title: Senior Data Engineer - Principal Associate Are you passionate about technology and solving complex business problems? Do you enjoy working in a collaborative and inclusive environment? At Capital One, we are a diverse group of makers, breakers, doers, and disruptors who strive to meet the needs of our customers. We are currently looking for Data Engineers who are excited about combining data with emerging technologies. As a Data Engineer at Capital One, you will play a vital role in driving a major transformation within our organization. What You'll Do: - Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies. - Work with a team of experienced developers specializing in data pipelines, distributed microservices, and full stack systems. - Utilize programming languages like Java, Scala, Python, and open-source databases to develop cloud-based data stores such as DynamoDB, Elasticache, and Snowflake. - Stay up-to-date with tech trends, experiment with new technologies, participate in technology communities, and mentor other members of the engineering community. - Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans to achieve financial well-being. - Design and implement effective data models optimized for search, query, reporting, or analytics. - Identify and select high-quality data sources for target data models. - Develop data pipelines, including ETL programs, to extract data from various sources and transform it to fit the target model. - Test and deploy data pipelines to ensure compliance with data governance and security policies. - Implement audits and checks to ensure data pipelines are functioning correctly. - Perform unit tests and code reviews to ensure high-quality and well-performing code. Basic Qualifications: - Bachelor's Degree. - At least 4 years of experience in application development (Internship experience does not apply). - At least 1 year of experience in big data technologies. Preferred Qualifications: - 5+ years of experience in application development, including Java, Python, NoSQL, and SQL. - 2+ years of experience with a public cloud platform (AWS, Microsoft Azure, Google Cloud). - 3+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL). - 2+ years of experience with real-time data and streaming applications. - 2+ years of experience with NoSQL implementations (DynamoDB, Redis, Elasticache). - 2+ years of experience with data warehousing (Redshift or Snowflake). - 3+ years of experience with UNIX/Linux including basic commands and shell scripting. - 2+ years of experience with Agile engineering practices. At Capital One, we believe in creating an inclusive and diverse workplace. We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to sex, race, age, color, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other prohibited basis under applicable law. We promote a drug-free workplace and consider qualified applicants with a criminal history in accordance with applicable laws. To apply for this position or request accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. For technical support or questions about our recruiting process, please email Careers@capitalone.com. Learn more about our comprehensive benefits package at the Capital One Careers website (www.capitalonecareers.com/benefits). Note: This salary information is specific to the location mentioned in the job posting. Salaries for part-time roles will be prorated based on hours worked regularly. Candidates hired in different locations will receive compensation based on the associated pay range for that location. Please note that Capital One Financial is comprised of multiple entities. Positions posted in Canada are for Capital One Canada, in the United Kingdom are for Capital One Europe, and in the Philippines are for Capital One Philippines Service Corp. (COPSSC). Thank you for considering a career at Capital One!
Show more
Show less","Agile, AWS, cloud computing, data analytics, data modeling, data pipelines, data warehousing, DynamoDB, Elasticache, ELK Stack, ETL, Google Cloud, Hadoop, Hive, Java, Kafka, MapReduce, Microsoft Azure, MySQL, NoSQL, Python, Redshift, Redis, Scala, Snowflake, Spark, SQL, UNIX/Linux","agile, aws, cloud computing, data analytics, data modeling, data pipelines, data warehousing, dynamodb, elasticache, elk stack, etl, google cloud, hadoop, hive, java, kafka, mapreduce, microsoft azure, mysql, nosql, python, redshift, redis, scala, snowflake, spark, sql, unixlinux","agile, aws, cloud computing, dataanalytics, datamodeling, datapipeline, datawarehouse, dynamodb, elasticache, elk stack, etl, google cloud, hadoop, hive, java, kafka, mapreduce, microsoft azure, mysql, nosql, python, redis, redshift, scala, snowflake, spark, sql, unixlinux"
Data Engineer Senior / Sr. ETL developer (min 12 Yrs exp) (LOCAL Candidates preferred) (Webcam interviews),Prohires,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-senior-sr-etl-developer-min-12-yrs-exp-local-candidates-preferred-webcam-interviews-at-prohires-3617892415,2023-12-17,West Springfield,United States,Mid senior,Onsite,"We are looking for
Data Engineer Senior / Sr.
ETL developer
(min 12 Yrs exp) (LOCAL Candidates preferred) (Webcam interviews)
POSITION DESCRIPTION
DIRECT CLIENT Position
Number of positions: 1
Length: 15 Months +
Location: Washington, DC 20019
Immediate interviews either Webcam or In-person interviews
Please note that most of the resources are currently working remote. However, depending on the project situation, this position may require 1 -2 days onsite and rest REMOTE.
12+ years of experience needed to serve as the subject matter expert on data architecture for the client's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse.
We are looking for the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by client to ensure that the client has accurate and reliable data maintained by high quality systems, all client data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
Specific Duties:
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist
for the client's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of data standards.
Participate in the development and maintenance of data security, privacy, policies, procedures, and best practices.
Bachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Candidates must have ALL the ""Required"" skills in order to be considered for the position. ""Desired"" or ""Highly Desired"" skills are a PLUS but may NOT be required.
Skill Matrix
Experience with Business workflow processes
Required / Desired
Amount
of Experience
Experience in developing, testing, maintaining the process of transaction, transformation, and load (ETL) data processes
Required
12
Years
Experience with command-line scripting (Ex: Linux-Bash, GREP, SED, AWK, Kerberos, LDAP, CRON, Windows PowerShell, Windows Active Directory, Windows
Required
12
Years
Experience with programing scripting languages (Ex: Python, R/RStudio, JavaScript, JSON/CVS, Perl, Scala, C/C++)
Required
12
Years
Experience leveraging SQL relational databases to manage complex datasets and analytical reporting
Required
12
Years
Bachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred. .
Required
BS in CS or IS or overall exp of 16 yrs
Years
Experience with Linux-based Operating Systems and Servers
Required
8
Years
Experience with Windows-based Operating Systems and Servers
Required
8
Years
Experience with System Administration of Microsoft SQLServer
Required
8
Years
Experience with System Administration of PostGresSQL
Required
8
Years
Experience in the maintenance and monitoring of system & network related activities associated with data security and controls.
Required
8
Years
Understanding of the Software Development Lifecycle (SDLC) in Agile and Waterfall environments
Required
5
Years
Proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing
Required
5
Years
Communication skills, both written and spoken, business level English mandatory
Required
5
Years
Writing System Guides, Manuals, and User Guides for complex systems
Required
5
Years
Strong communication skills - ability to articulate technical terms and complex data clearly to non-technical audience both verbally and in writing
Required
5
Years
Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions.
Required
5
Years
Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives.
Desired
5
Years
Familiarity with CKAN, DKAN, and/or ArcGIS
Desired
2
Years
Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy)
Desired
2
Years
Familiarity with CKAN, DKAN, and/or ArcGIS
Required
2
Years
Experience writing SQL queries for checking expected results and setting up test data;
Required
12
Years
Show more
Show less","Data Engineer, ETL Developer, Data Architecture, Data Warehouse, Data Governance, Data Gap Analysis, Data Catalog, Data Quality, Data Analysis, Data Reporting, Data Security, Data Privacy, Agile Development, Waterfall Development, Agile Testing, Automation Testing, Blackbox Testing, Unit Testing, CrossBrowser Testing, Communication, Writing, Consultative Skills, CKAN, DKAN, ArcGIS, Tableau, MicroStrategy, SQL","data engineer, etl developer, data architecture, data warehouse, data governance, data gap analysis, data catalog, data quality, data analysis, data reporting, data security, data privacy, agile development, waterfall development, agile testing, automation testing, blackbox testing, unit testing, crossbrowser testing, communication, writing, consultative skills, ckan, dkan, arcgis, tableau, microstrategy, sql","agile development, agile testing, arcgis, automation testing, blackbox testing, ckan, communication, consultative skills, crossbrowser testing, data architecture, data catalog, data gap analysis, data governance, data privacy, data quality, data reporting, data security, dataanalytics, dataengineering, datawarehouse, dkan, etl developer, microstrategy, sql, tableau, unit testing, waterfall development, writing"
Data Engineer,"KBR, Inc.","Reston, VA",https://www.linkedin.com/jobs/view/data-engineer-at-kbr-inc-3770445008,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Title:
Data Engineer
KBR is seeking a Data Engineer to support one of our government customers in Reston, Virginia. The Data Engineer utilizes a diverse range of data-focused skills, experience, deep technical and analytical problem solving, and team-centered solutioning for complex problems to satisfy priority information enabling operations. This position requires strong attributes for problem solving, data analysis, troubleshooting, analytical thinking, and experimentation, often under time sensitive decisions.
Required Qualifications:
Develop and implement methods of automation and optimization of data and products to present to upper management
Demonstrate understanding of statistical analysis such as regression, anomaly detection and clustering
Create data packages in the form of databases (DBs), reports, and interactive visualizations
Demonstrate effective communication skills to relay data science activities, technical findings, and data products for both technical and non-technical customers
Ability to combine a diverse set of data sources containing multiple forms of information including, but not limited to, Open Source, Publicly Available Information (PAI), Commercially Available Information (CAI), and intelligence records to provide technical information to produce data packages
Demonstrated ability to use technical and analytic skills to solve complex problem
Bachelor of Science Degree in Science, Technology, Engineering, or Mathematics
5+ years of progressive, relevant work experience.
Preferred Qualifications:
Hands-on experience using commercially available data exploitation and visualization tools for analysis (e.g., MS Excel, Tableau, Power BI, Wireshark)
Knowledge of National Intelligence Agencies and Department of Defense elements
Advanced Statistical knowledge and analysis methods (e.g., Linear models, multivariate analysis)
Experience in analytical tool development, identification, and integration
Master's degree in a science related field
Travel: CONUS and OCONUS travel may be required. (10%)
Security Requirements:
Active TS/SCI security clearance w/CI Poly
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Data engineering, Automation, Optimization, Data analysis, Statistical analysis, Regression, Anomaly detection, Clustering, Data visualization, Databases, Reports, Communication, Open Source, Publicly Available Information (PAI), Commercially Available Information (CAI), MS Excel, Tableau, Power BI, Wireshark, National Intelligence Agencies, Department of Defense, Linear models, Multivariate analysis, Analytical tool development, Identification, Integration, CONUS, OCONUS, TS/SCI security clearance, CI Poly","data engineering, automation, optimization, data analysis, statistical analysis, regression, anomaly detection, clustering, data visualization, databases, reports, communication, open source, publicly available information pai, commercially available information cai, ms excel, tableau, power bi, wireshark, national intelligence agencies, department of defense, linear models, multivariate analysis, analytical tool development, identification, integration, conus, oconus, tssci security clearance, ci poly","analytical tool development, anomaly detection, automation, ci poly, clustering, commercially available information cai, communication, conus, data engineering, dataanalytics, databases, department of defense, identification, integration, linear models, ms excel, multivariate analysis, national intelligence agencies, oconus, open source, optimization, powerbi, publicly available information pai, regression, reports, statistical analysis, tableau, tssci security clearance, visualization, wireshark"
Data Analyst Senior,Prometheus Federal Services (PFS),"Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-senior-at-prometheus-federal-services-pfs-3747078112,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Data Analyst Senior
Prometheus Federal Services (PFS), a trusted partner to federal health and social services agencies,has anopening for a Data Analyst Senior with Federal Consulting experience. You will work closely with and support Federal health and social services agency clients. You will collaborate with client leadership to shape work product development and ensure quality delivery.
The Data Analyst Senior will be responsible for driving defined PowerBI dashboards, data visualization reports, data models, automation processes, and general integration. As the PowerBI expert, this role will collaborate with stakeholders to define requirements, develop powerful data driven solutions, and support PowerBI training as needed (i.e., Dashboard Training).All applicants must reside in the U.S.
Essential Duties and Responsibilities
Conceptualize and develop integrated data solutions including task trackers, dashboards, customized application, and automation scripts to aid in data capture through products that support a variety of workstreams.
Design and deploy a continuum of integrated data management solutions within NDS authorized datasets and platforms including CDW, Pyramid Analytics, TMS, PowerBI, PowerApps, Power Automate.
Minimum Qualifications
Bachelor’s degree from an accredited institution
Minimum 8 years of working with large, complex health-related data sets
Ability to run scenarios
Ability to conduct data analysis
Experience with Data visualization (e.g., PowerSuite)
Hands-on experience building visually appealing Power BI dashboards
Experience using SQL and RDBMS
Strong analytical skills with ability to identify alternatives
Excellent oral and written communication skills
Proven ability to work independently and as part of a team
Proficiency with MS Office
Ability to work in the US indefinitely without sponsorship
Ability to obtain a US security clearance if needed
Preferred Qualifications
Knowledge of Department of Veterans Affairs databases and datasets
Ambition to bring creative ideas to the table, pursue new learning opportunities and delight clients
Microsoft Certifications within the Power Platform, AZURE, or other closely related Microsoft Products
Six Sigma and/or continuous improvement background
Experience with SQL query design and database architecture through Microsoft SQL Server Management Studio
Experience with PowerBI Paginated Reporting or similar SSRS reporting capabilities
Show more
Show less","Data Analysis, Data Visualization, PowerBI, CDW, Pyramid Analytics, TMS, PowerApps, Power Automate, SQL, RDBMS, Microsoft Office, MS SQL Server Management Studio, PowerBI Paginated Reporting, SSRS","data analysis, data visualization, powerbi, cdw, pyramid analytics, tms, powerapps, power automate, sql, rdbms, microsoft office, ms sql server management studio, powerbi paginated reporting, ssrs","cdw, dataanalytics, microsoft office, ms sql server management studio, power automate, powerapps, powerbi, powerbi paginated reporting, pyramid analytics, rdbms, sql, ssrs, tms, visualization"
Healthcare Data Analyst,"Northwest Human Services, Inc.","Salem, OR",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-northwest-human-services-inc-3755785764,2023-12-17,Silverton,United States,Associate,Onsite,"Northwest Human Services is a non-profit leader in providing advocacy, quality healthcare and social services in Marion and Polk counties since 1970. We are a mission focused organization providing compassionate and professional medical, dental, psychiatry, mental health, and wraparound services for those in our community who need it most - uninsured individuals, families, the homeless, and migrant workers. As a Community Health Center we value a culture of equity, diverse perspectives, and life experiences. Our organization embraces innovation, collaboration, and work-life harmony.
HEALTHCARE DATA ANALYST
Location: West Salem Clinic, 1233 Edgewater Street NW, Salem OR 97304 (Not a remote position)
Job Status: Full-time, Monday - Friday
YOUR ROLE:
This position is responsible for producing clinical and operational business intelligence (BI) from many complex data sources, using various analytic methods. The Healthcare Data Analyst provides guidance and advice regarding the availability and validity of data to answer questions regarding organizational and provider performance. The Healthcare Data Analyst provides interpretation of trends and drivers of performance and evaluates the effect of improvement projects.
SPECIFIC DUTIES
Conduct routine and exploratory analysis to describe performance, evaluate programs/projects. Serves as an internal consultant for identifying improvement opportunities and potential strategic opportunities.
Apply advanced analysis techniques to generate insights about drivers, correlations, historical trends, and predictions.
Generate BI reports to assist with decision-making, help leaders interpret and use reports and dashboards effectively, and provide innovative data visualizations that promote understanding and decision-making around key issues.
Collaborate with IT, Quality Management, and Finance teams to ensure NWHS’ raw information assets contain high-quality data that can be used for effective reporting and decision-making.
Remain informed of best practices, national and state trends, tools, data sources, and techniques in the field of data, analytics, and business intelligence in the healthcare industry. Maintains thorough understanding of current and future performance measurement requirements from regulatory agencies.
Qualifications
Bachelor’s degree in Business Information Systems, Computer Science, Data Analytics, or related field with coursework in quantitative analysis.
3-5 years of analysis and reporting experience in an ambulatory, primary care medical group, or community health center environment.
Knowledge and experience with SQL and PowerBI, Tableau, or an analytics automation platform such as Alteryx.
Knowledge and experience using statistical analysis.
Knowledge of healthcare data sources, concepts, and metrics.
Experience working within a health center or clinical setting.
SUMMARY OF BENEFITS: Our Agency strives to provides a benefits program that is comprehensive and competitive within our industry.
Competitive Salary
Comprehensive Health Plans: Dental and Vision
Flex Spending Account
Group Life: Short-Term & Long-Term Disability 100% paid by employer
403(b) retirement plan with 3% employer match
10 hours of monthly Paid Time Off based on FTE
7½ paid holidays each year + 2 paid floating holiday
Continuing Education Plan
Employee Healthy Living Program
TO APPLY:
If you are interested in joining a team that makes a difference in the lives of many, apply online at: Employment (northwesthumanservices.org)
For more information, contact the HR/Recruiting Department at: HR@nwhumanservices.org | 503.588.5828
All candidates who receive a written offer of employment will be required to undergo a criminal records check.
Equal Opportunity Employer | We celebrate diversity and are committed to creating an inclusive environment for all employees.
Show more
Show less","Healthcare Data Analytics, SQL, PowerBI, Tableau, Alteryx, Statistical Analysis, Healthcare Data Sources, Healthcare Concepts, Healthcare Metrics, Business Intelligence","healthcare data analytics, sql, powerbi, tableau, alteryx, statistical analysis, healthcare data sources, healthcare concepts, healthcare metrics, business intelligence","alteryx, business intelligence, healthcare concepts, healthcare data analytics, healthcare data sources, healthcare metrics, powerbi, sql, statistical analysis, tableau"
Database Administrator,Clayton Services,"Houston, TX",https://www.linkedin.com/jobs/view/database-administrator-at-clayton-services-3783989397,2023-12-17,Houston,United States,Associate,Onsite,"Clayton Services is searching for a
Database Administrator
to join a thriving company in Houston. The
Database Administrator
will administer, maintain, and optimize company databases, utilizing Python, SQL, and Power BI expertise to support data management initiatives and collaborate with the data engineering team.
Pay Rate:
$75,000 - $80,000/annually
Benefits:
Fully covered medical and dental benefits with excellent PTO available!
Location:
Spring Branch Central
Work Setting:
onsite, 5 days per week
Database Administrator Responsibilities:
Assist in installing, configuring, and maintaining database software (e.g., MySQL, PostgreSQL, SQL Server).
Aid in data migration and transformation tasks using Python and SQL.
Collaborates with the data engineer to ensure smooth data integration and reporting within Power BI.
Monitor database systems, optimize SQL queries, and troubleshoot performance issues.
Perform routine database backups, implement disaster recovery plans, and uphold data security.
Create and maintain comprehensive documentation for database configurations, processes, and procedures.
Stay updated on emerging trends in database technologies, Power BI advancements, and developments in data analytics.
Database Administrator Skills and Abilities:
Excellent problem-solving and troubleshooting skills.
Excellent communication and teamwork abilities.
Familiarity with database management tools and utilities.
Basic understanding of database management systems (DBMS).
Knowledge of database security best practices.
Certification in database administration (e.g., Oracle Certified Associate, Microsoft Certified: Azure Database Administrator Associate).
Strong proficiency in Python and SQL for data manipulation and analysis.
Ability to take on new challenges and adapt to evolving technologies.
Ability to take and pass a technical assessment based on SQL and coding skills.
Database Administrator Education and Experience:
Bachelor's degree in Computer Science, Information Technology, or related field.
2+ years of hands-on experience.
Experience with cloud-based databases (e.g., AWS RDS, Azure SQL Database).
Experience with the SDLC (front-end and/or back-end development).
Experience with data visualization and reporting.
Show more
Show less","Python, SQL, Power BI, MySQL, PostgreSQL, SQL Server, Data migration, Data transformation, Data integration, Data reporting, Database backups, Disaster recovery, Data security, Database documentation, Database management tools, Database management systems (DBMS), Database security best practices, Oracle Certified Associate, Microsoft Certified: Azure Database Administrator Associate, Cloudbased databases (AWS RDS Azure SQL Database), SDLC (frontend and/or backend development), Data visualization, Data reporting","python, sql, power bi, mysql, postgresql, sql server, data migration, data transformation, data integration, data reporting, database backups, disaster recovery, data security, database documentation, database management tools, database management systems dbms, database security best practices, oracle certified associate, microsoft certified azure database administrator associate, cloudbased databases aws rds azure sql database, sdlc frontend andor backend development, data visualization, data reporting","cloudbased databases aws rds azure sql database, data integration, data migration, data reporting, data security, data transformation, database backups, database documentation, database management systems dbms, database management tools, database security best practices, disaster recovery, microsoft certified azure database administrator associate, mysql, oracle certified associate, postgresql, powerbi, python, sdlc frontend andor backend development, sql, sql server, visualization"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX",https://www.linkedin.com/jobs/view/data-qa-engineer-locals-at-steneral-consulting-3762496226,2023-12-17,Houston,United States,Mid senior,Hybrid,"Spring TX
USC or GC Holder
12 month contract
Hybrid (onsite Tuesday – Thursday)
Top 3 Skills
Experience extracting from user stories
SQL (needs to be a 3-5 out of 10)
Experience working in a continuous integration team – must know the principles
Must have tested data warehouses
Data QA Engineer
The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.
Responsibilities
Utilize Azure DevOps for test case management and issue/defect tracking.
Write clear and concise defect details describing actual versus expected behaviors.
Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines.
Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies.
Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues.
Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration.
Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications
Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration.
5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications
Show more
Show less","SQL, Azure DevOps, ETL/ELT, Data Warehouses, Data Pipelines, Data Quality, Data Validation, Analytics Testing, Regression Testing, Continuous Integration, Test Case Management, Defect Tracking, Software Testing Methodologies, Data Analytics, Data Warehousing","sql, azure devops, etlelt, data warehouses, data pipelines, data quality, data validation, analytics testing, regression testing, continuous integration, test case management, defect tracking, software testing methodologies, data analytics, data warehousing","analytics testing, azure devops, continuous integration, data quality, data validation, data warehouses, dataanalytics, datapipeline, datawarehouse, defect tracking, etlelt, regression testing, software testing methodologies, sql, test case management"
Data Governance Manager,Harnham,"Houston, TX",https://www.linkedin.com/jobs/view/data-governance-manager-at-harnham-3786550637,2023-12-17,Houston,United States,Mid senior,Hybrid,"DATA GOVERNANCE MANAGER
$135,000 – $145,000 + BONUS + BENEFITS
HYBRID – OFFICE LOCATED IN HOUSTON, TX
This automotive/retail company is seeking a Data Governance Manager to implement, execute, and manage the data governance strategy for the organization. This is a pivotal role for the company and the strategy that is created by the Data Governance Manager will have high visibility across the company.
ROLE OVERVIEW:
Partner closely with senior leadership within the Data and Analytics group to implement and manage the overall Data Governance strategy.
Lead critical initiatives, including intake and requirement processes, data and KPI standardization, Governance committee implementation, Master Data Management, Data Security, Data Quality, Data Retention, Data Glossary, and Self-Service reporting.
RESPONSIBILITIES:
Collaborate with stakeholders, IT teams, and various business units.
Understand data quality and BI tools, particularly their interaction with data virtualization systems.
Discover and connect with data across different business units, focusing on KPI metrics and business acumen.
Develop and execute a comprehensive data governance strategy based on business unit needs.
Lay out a plan and roadmap for the implementation of the data governance strategy.
SKILLS AND EXPERIENCE:
Proven experience building a data governance model and executing it from the ground up.
5+ years of industry experience
Background in data (data analyst, engineer, or product manager) before transitioning to implementing data governance.
Strong communication skills, and ability to influence and command a room.
Familiarity with master data, domains, transactions, data warehouses, and quality standards.
BENEFITS
$135,000 - 145,000 Base Salary + Bonus + Benefits
HOW TO APPLY
Please register your interest by sending your Resume to Emma Spagnola via the Apply link on this page or at emmaspagnola@harnham.com.
Show more
Show less","Data Governance, Data Analyst, Data Engineer, Data Product Manager, Data Quality, Data Retention, Data Security, Data Virtualization, Data Warehouse, Data Standardization, SelfService Reporting, Business Acumen, KPI Metrics, Master Data Management, Master Data, Domains, Transactions","data governance, data analyst, data engineer, data product manager, data quality, data retention, data security, data virtualization, data warehouse, data standardization, selfservice reporting, business acumen, kpi metrics, master data management, master data, domains, transactions","business acumen, data governance, data product manager, data quality, data retention, data security, data standardization, data virtualization, dataanalytics, dataengineering, datawarehouse, domains, kpi metrics, master data, master data management, selfservice reporting, transactions"
Data Governance Manager,Harnham,"Houston, TX",https://www.linkedin.com/jobs/view/data-governance-manager-at-harnham-3787367814,2023-12-17,Houston,United States,Mid senior,Hybrid,"DATA GOVERNANCE MANAGER
$135,000 - $145,000 + BONUS + BENEFITS
HYBRID - OFFICE LOCATED IN HOUSTON, TX
This automotive/retail company is seeking a Data Governance Manager to implement, execute, and manage the data governance strategy for the organization. This is a pivotal role for the company and the strategy that is created by the Data Governance Manager will have high visibility across the company.
ROLE OVERVIEW:
Partner closely with senior leadership within the Data and Analytics group to implement and manage the overall Data Governance strategy.
Lead critical initiatives, including intake and requirement processes, data and KPI standardization, Governance committee implementation, Master Data Management, Data Security, Data Quality, Data Retention, Data Glossary, and Self-Service reporting.
RESPONSIBILITIES:
Collaborate with stakeholders, IT teams, and various business units.
Understand data quality and BI tools, particularly their interaction with data virtualization systems.
Discover and connect with data across different business units, focusing on KPI metrics and business acumen.
Develop and execute a comprehensive data governance strategy based on business unit needs.
Lay out a plan and roadmap for the implementation of the data governance strategy.
SKILLS AND EXPERIENCE:
Proven experience building a data governance model and executing it from the ground up.
5+ years of industry experience
Background in data (data analyst, engineer, or product manager) before transitioning to implementing data governance.
Strong communication skills, and ability to influence and command a room.
Familiarity with master data, domains, transactions, data warehouses, and quality standards.
BENEFITS
$135,000 - 145,000 Base Salary + Bonus + Benefits
Show more
Show less","Data Governance, Data Analyst, Data Quality, Data Retention, Master Data Management, Data Security, Data Virtualization, BI tools, KPI Metrics, Business Acumen, Data Warehouse, Quality Standards, Data Visualization, Data Engineering, Data Product Management, Data Strategy, Data Governance Model, Business Intelligence, Data Governance Committee, Data Glossary, SelfService Reporting","data governance, data analyst, data quality, data retention, master data management, data security, data virtualization, bi tools, kpi metrics, business acumen, data warehouse, quality standards, data visualization, data engineering, data product management, data strategy, data governance model, business intelligence, data governance committee, data glossary, selfservice reporting","bi tools, business acumen, business intelligence, data engineering, data glossary, data governance, data governance committee, data governance model, data product management, data quality, data retention, data security, data strategy, data virtualization, dataanalytics, datawarehouse, kpi metrics, master data management, quality standards, selfservice reporting, visualization"
Data Science Specialist,Hydrogen Group,"Houston, TX",https://www.linkedin.com/jobs/view/data-science-specialist-at-hydrogen-group-3780021087,2023-12-17,Houston,United States,Mid senior,Hybrid,"Ob Description
We are currently seeking an experienced data scientist to join the Big Data and Advanced Analytics department. As part of the Data Analytics team, the Lead Data Scientist will work closely with the Data Engineering team and business functions to solve real-world oil and gas midstream problems using machine learning, data science algorithms and artificial intelligence.
Responsibilities Include
Work independently on optimization projects for multiple business functions
Identify and frame the optimization opportunity from understanding the business problem / opportunity
Gather, cleanse, and transform internal and external data
Analyze data and deliver insights via visualizations and dashboards
Create, productionize, and maintain models / solutions that address business problems
Present, explain and defend results from analysis and modeling, and approach taken
Participate in strategic planning discussions around optimizations, data science and big data analytics
The Successful Candidate Will Meet The Following Qualifications
5+ years of practical experience framing and solving optimization problems in supply chain, logistics, or operations
5+ years of hands on experience with applied statistics / math and optimization techniques
Professional experience with optimization tools such as linear programming, integer programming, or heuristic methods
Professional experience programming in Python
Experience with AWS is a plus
Educational background in Operations Research, Applied Mathematics, or Industrial Engineering is a plus
Ability to adapt in a rapidly changing environment
Ability to communicate insights and approaches in a simple, actionable manner
Ability to work independently and with team members from different backgrounds
Excellent attention to detail and problem-solving skills
Show more
Show less","Data Science, Optimization, Machine Learning, Data Analytics, Python, AWS, Operations Research, Applied Mathematics, Industrial Engineering, Linear Programming, Integer Programming, Heuristic Methods, Data Engineering, Business Intelligence, Data Visualization, Dashboarding, Presentation Skills, Communication Skills, Teamwork Skills, Problem Solving Skills","data science, optimization, machine learning, data analytics, python, aws, operations research, applied mathematics, industrial engineering, linear programming, integer programming, heuristic methods, data engineering, business intelligence, data visualization, dashboarding, presentation skills, communication skills, teamwork skills, problem solving skills","applied mathematics, aws, business intelligence, communication skills, dashboard, data engineering, data science, dataanalytics, heuristic methods, industrial engineering, integer programming, linear programming, machine learning, operations research, optimization, presentation skills, problem solving skills, python, teamwork skills, visualization"
Expert Data Engineer,HP,"Spring, TX",https://www.linkedin.com/jobs/view/expert-data-engineer-at-hp-3772976776,2023-12-17,Houston,United States,Mid senior,Hybrid,"We are looking for an
Expert Data Engineer
to join our team and help us build and maintain scalable data pipelines and systems. You will be responsible for designing, developing, testing, and deploying data solutions that meet the needs of our clients and stakeholders. You will also collaborate with data analysts, data scientists, and other data engineers to ensure data quality, reliability, and performance.
If you are interested in working on petabytes of data from millions of devices. If solving complex data issues excites you this is the opportunity for you.
Responsibilities
Leads the team to write, deploy, and maintain software to build, integrate, manage, maintain, and quality-assure data.
Architects, designs, implements, and maintains reliable and scalable data solutions in the AWS cloud environment using Scrum/Agile methodology.
Implement data ingestion, transformation, and processing workflows using ETL tools and frameworks.
Researches and promotes new tools and techniques to shape the future of the data engineering environment.
Ensure data security, privacy, and compliance with relevant regulations and policies.
Monitor, troubleshoot, and debug data issues and performance bottlenecks. Guides team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI / CD pipeline.
Document and communicate data engineering processes and solutions to stakeholders and users.
Represents the data engineering team for all phases of larger and more-complex development projects.
Works with following site-reliability engineering standard methodologies: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Knowledge & Skills
Demonstrable coding expertise in one or more object-oriented programming languages (e.g., Python, Scala, Java, etc.)
Deep and hands-on experience (7+ years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.
Hands on experience with:
Expert in AWS tools and services such as S3, Glue, Lambda, EMR, Redshift, Athena, etc.
Experience with other cloud platforms and services such as Azure, GCP, etc. is a plus.
Experience with data quality, testing, and validation tools and techniques
Experience with data visualization and reporting tools such as QuickSight, Tableau, Power BI, etc.
Strong analytical and problem-solving skills
Excellent communication and collaboration skill
Understanding Data Structures & Algorithms & their performance
Experience designing and implementing large-scale distributed systems.
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Internal client management and ability to lead large organizations via influence .
Ability to effectively communicate product architectures, design proposals and negotiate options at senior management levels.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Python, Scala, Java, AWS, S3, Glue, Lambda, EMR, Redshift, Athena, Azure, GCP, QuickSight, Tableau, Power BI, Data Structures, Algorithms, Distributed Systems, Data Lifecycle, Data Visualization, Communication, Collaboration","python, scala, java, aws, s3, glue, lambda, emr, redshift, athena, azure, gcp, quicksight, tableau, power bi, data structures, algorithms, distributed systems, data lifecycle, data visualization, communication, collaboration","algorithms, athena, aws, azure, collaboration, communication, data lifecycle, data structures, distributed systems, emr, gcp, glue, java, lambda, powerbi, python, quicksight, redshift, s3, scala, tableau, visualization"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX",https://www.linkedin.com/jobs/view/data-qa-engineer-locals-at-steneral-consulting-3759316531,2023-12-17,Houston,United States,Mid senior,Hybrid,"Spring TX
USC or GC Holder
12 month contract
Hybrid (onsite Tuesday – Thursday)
Data QA Engineer
The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.
Responsibilities
Utilize Azure DevOps for test case management and issue/defect tracking.
Write clear and concise defect details describing actual versus expected behaviors.
Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines.
Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies.
Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues.
Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration.
Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications
Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration.
5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications
Show more
Show less","Data QA, ETL/ELT, Azure DevOps, Manual testing, Automated testing, Data validation, Analytics testing, Data lakehouse, Regression testing, Software testing methodologies, Test case management, Defect tracking, Test case automation, Data quality","data qa, etlelt, azure devops, manual testing, automated testing, data validation, analytics testing, data lakehouse, regression testing, software testing methodologies, test case management, defect tracking, test case automation, data quality","analytics testing, automated testing, azure devops, data lakehouse, data qa, data quality, data validation, defect tracking, etlelt, manual testing, regression testing, software testing methodologies, test case automation, test case management"
"Data Analyst-SQL, Python",Zortech Solutions,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-analyst-sql-python-at-zortech-solutions-3763554032,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Analyst-SQL, Python
Location: Los Angeles, CA (Hybrid)
Duration: 6+ Months
Job Description
As a Data Analyst you will:
Partner closely with Business, Product Management, Product Development, and Design teams to leverage the key steps of data analytics: requirement gathering, data acquisition (ETL), presenting findings, driving to impact
Develop clear articulation of business-relevant analysis from data for all levels – executive levels to granular reports
Present to senior leaders within and outside the organization, telling a story with data
Skills
5-7 years relevant experience
Requires a Bachelors degree
Proficient with SQL to perform data acquisition, segmentation, and aggregation from scratch from existing data sources
Knowledge of programming languages (e.g. Python)
Knowledge of DWH concepts
Apply analytics and systems thinking to analyze data, identify emerging trends and turn the data into actionable information/insights.
Strong analytical and problem-solving skills, including hypothesis-driven testing and root-cause analysis to understand drivers of process variations
Able to effectively interact and communicate with all levels of management and with users at all levels of technical/analytical expertise
Drive results through influence, partnership and cross functional teams
Experienced and comfortable applying appropriate statistical mathematical techniques (academic or practical)
Outstanding communication skills with the ability to present the most relevant outcomes
Familiar with agile methodology
Payments domain knowledge – good to have
Show more
Show less","SQL, Python, ETL, DWH, Business Analytics, Data Acquisition, Data Segmentation, Data Aggregation, HypothesisDriven Testing, RootCause Analysis, Statistical Mathematical Techniques, Communication, Agile Methodology, Payments Domain Knowledge","sql, python, etl, dwh, business analytics, data acquisition, data segmentation, data aggregation, hypothesisdriven testing, rootcause analysis, statistical mathematical techniques, communication, agile methodology, payments domain knowledge","agile methodology, business analytics, communication, data acquisition, data aggregation, data segmentation, dwh, etl, hypothesisdriven testing, payments domain knowledge, python, rootcause analysis, sql, statistical mathematical techniques"
Sr. Data Analyst/ETL Developer,Stellar Professionals,"Lansing, MI",https://www.linkedin.com/jobs/view/sr-data-analyst-etl-developer-at-stellar-professionals-3750160030,2023-12-17,Belleville, Canada,Associate,Hybrid,"Skills Required
Bachelor's degree in a related field (or equivalent work experience).
Proven experience as a Data Analyst or ETL Developer.
Expertise in SQL, especially with Microsoft SQL Server.
Proficiency in data integration and migration, with cloud migration experience as a plus.
Understanding of industry data management best practices and standards.
Strong problem-solving and analytical skills.
Ability to mentor and guide junior team members.
Knowledge of data management concepts, including MDM, Data Quality, Metadata Management, Data Security, ILM, and Data Governance.
Excellent stakeholder engagement, negotiation, and leadership skills.
Strong organizational skills with the ability to work independently and collaborate effectively within a team.
Show more
Show less","SQL, Microsoft SQL Server, Data integration, Data migration, Cloud migration, Data management, MDM, Data Quality, Metadata Management, Data Security, ILM, Data Governance, Stakeholder engagement, Negotiation, Leadership, Problemsolving, Analytical","sql, microsoft sql server, data integration, data migration, cloud migration, data management, mdm, data quality, metadata management, data security, ilm, data governance, stakeholder engagement, negotiation, leadership, problemsolving, analytical","analytical, cloud migration, data governance, data integration, data management, data migration, data quality, data security, ilm, leadership, mdm, metadata management, microsoft sql server, negotiation, problemsolving, sql, stakeholder engagement"
Data Engineer,VeeAR Projects Inc.,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-veear-projects-inc-3777319182,2023-12-17,Belleville, Canada,Associate,Hybrid,"Responsibilities
Develop data pipeline to orchestrate the lifecycle of crowd grading tasks, build dashboard to monitoring and reporting the quality of tasks
Programming in one of the following languages: Python, Scala
Audit a wide variety of data for correctness
Required Skills
Programming language Python or Scala
Excellent experience with Spark
Familiarity with Git or similar version control system
Good understanding of data processing concepts and data engineering process.
Able to frequently collaborate closely with a diverse tight-knit team
Experience investigating and/or debugging problems
Able to track, analyze, and report issues in a timely manner
Attention to detail and a commitment to quality, while delivering work on schedule
Highly organized and efficient
Flexibility to respond and react to changing priorities quickly and efficiently
Excellent oral and written communication in English
Highly Desired Skills
Big data processing, e.g. Spark/Hadoop
Cloud based services, e.g. AWS
Workflow management tools, e.g. Apache Airflow
SQL
UI design fundamentals
Web development technologies, such as React for JS
QA/QE
Education
BA or BS degree or equivalent work experience.
Show more
Show less","Python, Scala, Spark, Git, Data processing, Data engineering, Collaboration, Problem solving, Reporting, Attention to detail, Quality assurance, Organization, Efficiency, Flexibility, Communication, Big data processing, Cloud services, Workflow management, SQL, UI design, Web development, React","python, scala, spark, git, data processing, data engineering, collaboration, problem solving, reporting, attention to detail, quality assurance, organization, efficiency, flexibility, communication, big data processing, cloud services, workflow management, sql, ui design, web development, react","attention to detail, big data processing, cloud services, collaboration, communication, data engineering, data processing, efficiency, flexibility, git, organization, problem solving, python, quality assurance, react, reporting, scala, spark, sql, ui design, web development, workflow management"
"Data Analyst-SQL, Tableau",Zortech Solutions,"Mountain View, CA",https://www.linkedin.com/jobs/view/data-analyst-sql-tableau-at-zortech-solutions-3782767161,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Analyst-SQL, Tableau
Location : Mountain View CA (2 days per week)
Duration: 6+ Months
Job Description
Must have:
Strong SQL
Tableau
Roles And Responsibilities
Describe and manipulate data sets, sources, and structures. Develop reports to make data more meaningful to business initiatives, analyze data quantitatively and qualitatively and communicate findings.
Develop data models to transform raw data into meaningful insights
Design, develop, and deploy dashboards using Tableau.
Owning the full BI lifecycle from requirements gathering through design & development, and through release and support/maintenance phases
Identify repeat processes and assist in process documentation and automation
Articulate key process flows, manage multiple inputs and priorities.
Mandatory Qualifications
Bachelor's/master’s degree in engineering, Economics, Finance, Mathematics, Statistics, Business Administration, or a related quantitative field
3-4 years of relevant analytics and BI experience and overall experience of 4 to 7 years.
Basic Knowledge about data warehouse design and data mining
Expert knowledge of SQL and Tableau
Strong experience in creating data-rich dashboards
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Good communication skills and ability to coordinate with multiple stakeholders across multiple functions
Prior work experience in with FinTech will be added advantage.
Show more
Show less","SQL, Tableau, Data visualization, Data analysis, Data modeling, Data manipulation, Business intelligence, Data mining, Data warehouse design, Dashboards, Data mining, Process automation, Data extraction, Data processing, Data communication","sql, tableau, data visualization, data analysis, data modeling, data manipulation, business intelligence, data mining, data warehouse design, dashboards, data mining, process automation, data extraction, data processing, data communication","business intelligence, dashboard, data communication, data extraction, data manipulation, data mining, data processing, data warehouse design, dataanalytics, datamodeling, process automation, sql, tableau, visualization"
Azure Data Engineer,VeeAR Projects Inc.,"Milwaukee, WI",https://www.linkedin.com/jobs/view/azure-data-engineer-at-veear-projects-inc-3767588471,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Descripation
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable to support in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Show more
Show less","Data Architecture, Data Engineering, Azure Data Analytics, Data Visualization, SQL, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake, Data Modelling, Data Design","data architecture, data engineering, azure data analytics, data visualization, sql, azure data factory, azure logic apps, azure functions, azure storage, azure sql data warehousesynapse, azure data lake, data modelling, data design","azure data analytics, azure data factory, azure data lake, azure functions, azure logic apps, azure sql data warehousesynapse, azure storage, data architecture, data design, data engineering, data modelling, sql, visualization"
Database Engineer V,doTERRA International LLC,"Pleasant Grove, UT",https://www.linkedin.com/jobs/view/database-engineer-v-at-doterra-international-llc-3721908821,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job description:
Seeking a highly self-motivated Oracle Database Administrator to join our growing IT Infrastructure Group. The Oracle Database Administrator is responsible for maintaining the integrity and performance of company databases and guarantee that data is stored securely and optimally. Assist with the evaluation of database software purchases and supervise the modification of any existing database software.
Job Responsibilities:
Identifies database requirements by analyzing department applications, programming, and operations; evaluating existing systems and designing proposed systems
Recommends solutions by defining database physical structure and functional capabilities, database security, data back-up, and recovery specifications
Maintains database performance by calculating optimum values for database parameters; implementing new releases; completing maintenance requirements
Evaluating computer operating systems and hardware products
Maintains quality service by establishing and enforcing organization standards
Performs data queries, coordinates testing, trains users and provides documentation
Works with other IT groups to ensure security procedures are maintained and adhered to
Participates in regular security audits
Completes all other tasks as assigned
Job Qualifications:
Oracle Database Administration experience including installing, patching, and upgrading Oracle databases
Experience working in RMAN backup and recovery
Knowledge of security procedures and technologies, and an understanding of audit processes
Strong hands-on experience in monitoring and tuning a database to provide a high availability service and optimum storage capability including: query optimization, initialization parameters, etc.Solid organizational and communication skills
Meticulous attention to detail
Works in an agile environment with the Application Development, Quality Assurance and other IT teams, along with the Business stakeholders to ensure that the necessary changes to the database meet the required deliverables
The DBA team bridges the gap between development and external clients and other IT functional groups
Core Competencies: Oracle; Oracle RAC; Oracle Enterprise Manager; Oracle RMAN, Oracle Enterprise Partitioning, Oracle Performance tuning
Prior experience in various scripting languages Preferred Qualifications:5+ years of Oracle Database Administration
UNIX/Linux system administration
Bachelor's degree in Computer Science/Information Systems or comparable experience
Experience with SAP HANA, MS SQL
Experience with Oracle CloudOracle 12c
Show more
Show less","Oracle Database Administration, Oracle RAC, Oracle Enterprise Manager, Oracle RMAN, Oracle Enterprise Partitioning, Oracle Performance tuning, UNIX/Linux system administration, SQL, SAP HANA, MS SQL, Oracle Cloud, Oracle 12c, RMAN backup and recovery, Query optimization, Initialization parameters, Scripting languages","oracle database administration, oracle rac, oracle enterprise manager, oracle rman, oracle enterprise partitioning, oracle performance tuning, unixlinux system administration, sql, sap hana, ms sql, oracle cloud, oracle 12c, rman backup and recovery, query optimization, initialization parameters, scripting languages","initialization parameters, ms sql, oracle 12c, oracle cloud, oracle database administration, oracle enterprise manager, oracle enterprise partitioning, oracle performance tuning, oracle rac, oracle rman, query optimization, rman backup and recovery, sap hana, scripting languages, sql, unixlinux system administration"
Data Analyst,"Anveta, Inc","Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-at-anveta-inc-3717200523,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: IN - DCS Data Analyst (720257)
Location: Indianapolis, IN 46204--- Webcam only--- Hybrid
Duration: Long term
Client: State of Indiana
Note: Only USC
Resource will be responsible for the activities of Data Analyst for the State of Indiana's child support system. Resource will work under direction of the Indiana Department of Child Services (DCS), Child Support Bureau (CSB) IT office.
Job Duties
Design, develop, and format polished excel and tableau data visualizations (reports and dashboards) to support business requirements. Focus on Visualization Creation using data to perform reporting and direct analysis. Perform analysis that may be descriptive, diagnostic, predictive, or prescriptive. Responsible for maintaining and developing excel and tableau dashboards and reports, preparing data visualizations, and using data to forecast or guide business activity. Present data in a fashion that is easy to understand with proper documentation and user testing for successful adoption. Publish tableau workbooks to appropriate QA, production, and public servers. Meet project deadlines and requirements. Participate in and contribute to the CSB-IT Data Services Team. Performs other related duties as assigned.
Job Requirements
Experience with Analyzing data, identify trends, interpret results, and prepare excel and tableau reports and data visualizations for bureau leadership, county partners, and other stakeholders.
Experience with developing, maintaining, and managing tableau reports and dashboards.
Experience with creating Analytical, Time Series, Metrics, Rankings, Statistical, Christmas Tree, Tracking, and Expenditure reports. Experience with using excel data analysis functions, scripts, reports, and charts.
Experience with writing SQL queries to get data from different databases like DB2 Z/OS; AWS Aurora and AWS RDS and Data Warehouse.
Experience in working with Cross Technical, Functional and Business Teams.
Capable to understand business requirements and develop reports based on requirements.
Capable to work individually and work in a team environment
Thanks & Regards
Charan
Anveta, Inc.
1333 Corporate Drive, Suite #108
Irving, TX 75038
charan@anveta.com
https://www.linkedin.com/in/charan-reddy-ba6450236/
Show more
Show less","SQL, Excel, Data Analysis, Tableau, Data Visualization, Reporting, Data Analytics, Data Warehousing, AWS, DB2 Z/OS","sql, excel, data analysis, tableau, data visualization, reporting, data analytics, data warehousing, aws, db2 zos","aws, dataanalytics, datawarehouse, db2 zos, excel, reporting, sql, tableau, visualization"
Associate Engineer - Database Services,"Susquehanna International Group, LLP (SIG)",Greater Philadelphia,https://www.linkedin.com/jobs/view/associate-engineer-database-services-at-susquehanna-international-group-llp-sig-3456696600,2023-12-17,Belleville, Canada,Associate,Hybrid,"Overview:
Our trading systems production engineering team is looking for a Database Engineer to join our team. You would join a diverse team of database administrators, software developers, and production engineers that are responsible for providing reliable platforms and services for our trading systems, including SQL and NoSQL databases, messaging middleware, reference data distribution, and observability and alerting solutions.
This person would join an existing group of Data Engineers to assist with the administration, configuration, maintenance, and deployment of an existing environment of 150+ servers. This role would allow you to diagnose full stack problems across multiple systems and collaborate with systems engineers, developers, database administrators, and business users to share ideas and recommendations for solutions that increase and expand our database environment.
Day to day responsibilities would include:
Supporting production operation of over 150 database servers used across a wide variety of systems and end-users
Supporting, operating, troubleshooting, and enhancing ETL processes to support both core business functionality and application specific needs
Design and implement highly available fault tolerant database platforms to support existing and new applications and use cases
Working with stakeholders to continuously improve our systems, including software and hardware upgrades, reliability improvements, monitoring and alerting, and performance tuning in order to meet ongoing business needs
Working with developers and end-users to improve the efficiency and performance of their database operations, including SQL tuning and write operations
Working with other teams to assist with diagnosing and remediating production incidents.
Remediating and repairing database systems and production data to ensure trading systems have access to required data and services
Assisting in the archiving, regeneration, and repair of historical data
What we're looking for
Bachelor’s degree in Computer Science, Computer Engineering, Information Technology or a related field
Minimum of 5 years of experience as a MySQL/MariaDB (operating on Linux) database administrator
Experience with large databases with tens of millions of rows and terabytes of data.
Experience with multiple database systems, including MongoDB, Oracle, Postgres, and Microsoft SQL Server
Experience with Linux and Windows operating systems
Ability to learn and maintain a legacy code base
Hands on experience with SQL development, shell scripting, and Python.
Experience with ETL tools such as Informatica a plus
Experience with DevOps and CI/CD tools such as Perforce, GitLab, TeamCity, Jenkins, or Ansible a plus
About SIG
SIG is a global quantitative trading firm founded by a group of friends who share a passion for game theory and probabilistic thinking. We have incorporated this approach into our culture, where you will find relentless problem solvers within each of our core disciplines: Trading, Technology, and Quantitative Research. From offices around the world, our employees collaborate to make optimal decisions and are driven by the desire to achieve winning results together.
‌
What we do
We are experts in trading essentially all listed financial products and asset classes, with a focus on derivatives trading. Through market making and market taking, we handle millions of trading transactions around the world every day, providing liquidity and ensuring competitive prices for buyers and sellers. While our presence in the market is broad, our trading desks are highly specialized, allowing for a deep understanding of unique drivers of each asset class.
SIG does not accept unsolicited resumes from recruiters or search firms. Any resume or referral submitted in the absence of a signed agreement will become the property of SIG and no fee will be paid.
Show more
Show less","MySQL, MariaDB, Linux, MongoDB, Oracle, Postgres, Microsoft SQL Server, Windows, SQL, Shell Scripting, Python, Informatica, DevOps, CI/CD, Perforce, GitLab, TeamCity, Jenkins, Ansible","mysql, mariadb, linux, mongodb, oracle, postgres, microsoft sql server, windows, sql, shell scripting, python, informatica, devops, cicd, perforce, gitlab, teamcity, jenkins, ansible","ansible, cicd, devops, gitlab, informatica, jenkins, linux, mariadb, microsoft sql server, mongodb, mysql, oracle, perforce, postgres, python, shell scripting, sql, teamcity, windows"
Data Engineer with Cloudera,Diverse Lynx,"North Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-with-cloudera-at-diverse-lynx-3768054900,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Engineer with Cloudera
Experience: 5+ Years
Location: North Chicago, IL 60064- Onsite from day 1
Duration: 12-24 Months
Must-Have**
Demonstrate mastery across a wide variety of data engineering activities, including data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, and data ops.
Demonstrate knowledge of pharmaceutical R&D/Life Sciensssce centric datasets and utilize this knowledge to advance agile, impactful, and cost-effective solutions rapidly.
Bachelor's degree with 7 years of IT experience
Must have experience with data analysis programming languages (e.g., SQL, Python & Apache Spark, SAS & R)
Must have experience with database technologies (e.g., Oracle, Postgres, Hive, and HBase)
Must have experience with ETL/Orchestration tools (e.g., Informatica, Autosys, and Airflow, etc.)
Experience with AWS and Cloudera Public Cloud architecture is preferred.
Experience working with Pharmaceutical R&D industry-centric datasets is preferred.
Good-to-Have
Collaborate & contribute to the architecture, design, development, and maintenance of large-scale data & analytics platforms, system integrations, data pipelines, data models & API integrations to support evolving business strategy.Must have experience with software development life cycle; Experience with DevOps is preferred.
Contribute and maintain the team's methodology to conform and curate data, benchmarking against industry standards. Ensure that data are optimally standardized and analysis-ready.
Prototype emerging business use cases to validate technology approaches and propose potential solutions.
Research and recommend opportunities to adopt new technologies for continuous improvement.
Ensure compliance with applicable Client software development lifecycle policies and procedures.
Responsibility of / Expectations from the Role
To coordinate with Business and transform their needs into IT requirement
To understand Business Vision/goal and to deliver the same from IT side
Experience of working in Phase-0 projects
Knowlede of ETL & reporting tools
Knowldege of working in Data Lake projects from scratch
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Cloudera, Data warehousing, Master data management, Data cataloging, System integration, Data streaming, Data visualization, Data analysis, Data ops, Oracle, Postgres, Hive, HBase, Informatica, Autosys, Airflow, SQL, Python, Apache Spark, SAS, R, AWS, DevOps, ETL, Reporting tools, Data Lake projects","cloudera, data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, data ops, oracle, postgres, hive, hbase, informatica, autosys, airflow, sql, python, apache spark, sas, r, aws, devops, etl, reporting tools, data lake projects","airflow, apache spark, autosys, aws, cloudera, data cataloging, data lake projects, data ops, data streaming, dataanalytics, datawarehouse, devops, etl, hbase, hive, informatica, master data management, oracle, postgres, python, r, reporting tools, sas, sql, system integration, visualization"
Jr/Mid Opensource Database Engineer,"Liberty Personnel Services, Inc.","Philadelphia, PA",https://www.linkedin.com/jobs/view/jr-mid-opensource-database-engineer-at-liberty-personnel-services-inc-3602463706,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Details:
Jr/Mid Opensource Database Engineer
My client is looking for a Jr/Mid Opensource Database Engineer. They would like to see a STEM bachelors/MS degree along with 1-5 years of opensource data engineer experience. Python is required for scripting. Any ETL would be a plus. Opensource databases such as Maria, mysql, mongo, cassandra, nosql. This is a full time perm position that works a Hybrid schedule.
If you are interested please forward your resume in word format to kevin@libertyjobs.com Kevin Mccarthy
#associate
#mid-senior
Show more
Show less","Opensource, Python, ETL, Maria, MySQL, MongoDB, Cassandra, NoSQL","opensource, python, etl, maria, mysql, mongodb, cassandra, nosql","cassandra, etl, maria, mongodb, mysql, nosql, opensource, python"
Data Analyst,Steneral Consulting,"Richardson, TX",https://www.linkedin.com/jobs/view/data-analyst-at-steneral-consulting-3741122848,2023-12-17,Belleville, Canada,Associate,Hybrid,"Hybrid role in either Chicago or Richardson (Must be local and undrer 60 mins to either location)
Need valid LinkedIn
W2 candidates only
Description
This role requires the individual to have experience with collecting, organizing, and analyzing data from various resources. Primary tasks of this position are compilation and analysis of data definitions, meaning, usage, labelling standards and other source system information to produce IT consumable information. They may be required to take on other tasks as needed such as but not limited to: technical expertise with automatic data collection and reporting systems, including a capacity for program troubleshooting and system security measures
Required Qualification(s)
Show more
Show less","Data analysis, Data collection, Data organization, Data compilation, Data usage, Data labelling, System security, Program troubleshooting, Automatic data collection, Reporting","data analysis, data collection, data organization, data compilation, data usage, data labelling, system security, program troubleshooting, automatic data collection, reporting","automatic data collection, data collection, data compilation, data labelling, data organization, data usage, dataanalytics, program troubleshooting, reporting, system security"
Data Analyst,Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/data-analyst-at-stellar-professionals-3757507292,2023-12-17,Belleville, Canada,Associate,Hybrid,"Skills Required
Data retrieval (UI, SQL, API)
Data Engineering (cleaning, preparation, validation)
Data analytics, integration, and visualization
Advanced Excel skills
Visualization platforms: Tableau, PowerBI
Scripting/coding for data science: Python, R
GIS tools: ArcMap, ArcGIS Pro
Exposure to some transportation skills & concepts
Data fusion
Data science (model development, applying machine learning to transportation problems)
Map conflation
Attention to detail
Strong organizational skills
Ability to work independently
Ability to present data in a format understandable to diverse audience
Strong communicati on skills
Show more
Show less","Data retrieval, SQL, API, Data engineering, Data cleaning, Data preparation, Data validation, Data analytics, Data integration, Data visualization, Excel, Tableau, PowerBI, Python, R, GIS, ArcMap, ArcGIS Pro, Transportation skills, Transportation concepts, Data fusion, Data science, Model development, Machine learning, Map conflation","data retrieval, sql, api, data engineering, data cleaning, data preparation, data validation, data analytics, data integration, data visualization, excel, tableau, powerbi, python, r, gis, arcmap, arcgis pro, transportation skills, transportation concepts, data fusion, data science, model development, machine learning, map conflation","api, arcgis pro, arcmap, data cleaning, data engineering, data fusion, data integration, data preparation, data retrieval, data science, data validation, dataanalytics, excel, gis, machine learning, map conflation, model development, powerbi, python, r, sql, tableau, transportation concepts, transportation skills, visualization"
Data Engineer - DataStage,Brickhouse Resources,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-datastage-at-brickhouse-resources-3756662844,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title:                             Data Engineer, DataStage
Position Type:                   Contract
Contract Duration:           12 months (possibility of extension or conversion to FTE)
Workplace Type:              Hybrid (3 days in-office, 2 days remote)
Workplace Schedule:   8am to 5pm (Tues, Wed, Thursday are in-office)
We are seeking an experienced Data Engineer, specifically with IBM Infosphere DataStage/Quality Stage. The Data Engineer will design, develop, automate, and support complex applications to extract, transform, and load data; Implement and automate complex processes; Plan and conduct ETL unit and development tests; Monitor results and take corrective actions; Translate data access, transformation, and movement requirements into functional requirements and mapping designs. Use SQL to design database schemas, optimize data retrieval, and implement ETL processes. Investigate, analyze, and resolve complex technical problems related to system functions, environment, and procedures; and Manage service levels, standards, and configurations; Participate in Agile teams to meet goals; Development and support of Kafka integrations, including topics, producers, consumers, and streaming applications;
Design, develop and maintain Enterprise micro services  security, logging, common APIs and enforcing coding standards.
Required Skills:
Bachelor's degree in Computer Science, Computer Engineering, Electrical/Electronics Engineering, or related technical field and 7 years of software development experience.
Must have extensive hands-on experience in designing, developing and maintaining software solutions on ETL process utilizing IBM Infosphere DataStage/Quality Stage
Strong Skills in SQL, PL/SQL Programming, Query Optimization.
Must have experience with analyzing, tuning and optimizing queries in the RDBMS.
5+ years experience working in a Data Warehouse environment (or equivalent).
6 years of experience with the design, development, automation, and support of applications to extract, transform, and load data;
6 years of experience with databases, including Oracle, DB2, Teradata, SQL Server, and Access;
6 years of experience with IBM InfoSphere DataStage;
6 years of experience with Data warehouse;
6 years of experience with Unix Shell Scripting;
6 years of experience working within an Agile Scrum environment;
Preferred Skills:
2+ years of experience with Spring boot is preferred
2+ years of experience building applications using Java and experience with API integrations is preferred
Knowledge of Kafka and streaming solutions is a plus
Position Responsibilities and Required Knowledge:
Design, build and maintain batch or real-time data pipelines.
Automate data workflows such as data ingestion, aggregation, and ETL processing.
Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.
Monitor data systems performance and implement optimization strategies.
Analyzing, tuning and optimizing queries in the RDBMS.
Must be able to produce thorough technical design documents, test plans and documented results.
Will work in Agile methodologies and a fast-paced environment
Developing API Integrations with cloud and other applications.
Experience building applications using Java and Java based frameworks like Spring Boot is  strongly preferred.
Knowledge of Data Management Strategies (ETL Frameworks, Data Modeling concepts)
Strong understanding of relational databases, oracle functions, procedures, triggers and packages.
Willingness to learn new technologies.
Excellent verbal and written communication skill to communicate complex information clearly and simply.
Show more
Show less","DataStage, SQL, PL/SQL, Java, Spring Boot, Unix Shell Scripting, Kafka, Data Warehousing, ETL, Data Modeling, Data Management Strategies, Agile Development, Relational Databases, Oracle, DB2, Teradata, SQL Server, Access, API Integrations, Cloud Computing","datastage, sql, plsql, java, spring boot, unix shell scripting, kafka, data warehousing, etl, data modeling, data management strategies, agile development, relational databases, oracle, db2, teradata, sql server, access, api integrations, cloud computing","access, agile development, api integrations, cloud computing, data management strategies, datamodeling, datastage, datawarehouse, db2, etl, java, kafka, oracle, plsql, relational databases, spring boot, sql, sql server, teradata, unix shell scripting"
Senior Data Engineer,Brother USA,"Bridgewater, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-brother-usa-3743996500,2023-12-17,Belleville, Canada,Associate,Hybrid,"Let's Grow Together
Our mission is to live our “at your side” promise and simplify and enrich the lives of our customers, employees, and communities. ""At your side"" is more than a slogan to us; it’s the purpose we do our best to fulfill every day. With a legacy spanning over a century, this is a great place to launch or expand any career and push the boundaries of what comes next. We're committed to achieving shared success, and we provide opportunities for you to develop through experience, exposure and education. Our people have always leveraged their unique perspectives to keep us on the right track for a lasting future. If you want to innovate, learn, and grow with a global leader that builds products, services, and a company people love, then we’ll be “at your side” every step of the way.
The Senior Data Engineer is dedicated to advancing analytics capabilities to support our business. This role plays a crucial part in acquiring, cleaning, and transforming data from various sources to facilitate advanced analytics. The responsibilities include data preparation, ML pipeline development, data integration, modeling, performance tuning, data governance, and collaborative documentation. The role's expertise contributes to the success of data-driven initiatives and help drive optimal project outcomes for our stakeholders.
Duties and Responsibilities
Data Engineering - Data Enablement Support
Data Preparation and Cleaning: Acquire, clean, and transform data from various sources, ensuring data quality and suitability for advanced analytics. Handle missing data, outliers, and validate data
Machine Learning Pipeline Development: Collaborate closely with Data Scientists to design, build, and maintain machine learning pipelines. Develop data preprocessing and feature engineering workflows for accurate model training
Data Integration: Integrate data from diverse sources, including databases, APIs, and external data providers, using tools like Alteryx, Databricks, and Azure. Design and implement Extract, Transform, Load (ETL) processes to harmonize data for analytical purposes
Data Modeling and Optimization: Assist in creating data models supporting analytical needs, including data warehousing and data marts. Optimize data structures and databases for query performance.
Performance Tuning: Ensure efficient data processing systems and timely execution of analytical workloads through indexing, query optimization, and parallel processing
Data Governance and Compliance: Incorporate data governance policies and compliance into data engineering processes to maintain data integrity and security. Capture data lineage to meet governance requirements
Collaboration and Documentation: Collaborate with cross-functional teams, documenting data sources, transformations, and data lineage to ensure transparency and reproducibility in analytics projects
Business Knowledge Utilization: Utilize business acumen to contribute to project success and align data engineering efforts with stakeholder objectives. Understand the business problem and how data is used for decision-making
Data Quality Assurance: Develop and implement data quality checks and validation procedures to maintain data accuracy
Data Monitoring: Continuously monitor data pipelines and proactively address issues to ensure data availability and reliability
Technology Evaluation: Stay updated on emerging data engineering technologies and recommend their adoption where appropriate to enhance our capabilities.
Data Security: Implement and enforce data security measures to protect sensitive information throughout the data lifecycle
Business Consultation
Facilitate meetings and/or projects, determine audience and tactics appropriate for a particular discussion
Analyze raw data to find opportunities to implement artificial intelligence into the current business processes
Perform statistical analysis to determine gaps that critically affect the performance of the process and prioritize for review
Identify options to streamline and/or eliminate manual processes, where applicable, by developing innovative solutions
Education
Bachelor's Degree (or equivalent experience) Computer Science, Information Technology, Business Administration, or related field Required
Experience
Minimum 5 years
Experience in data management disciplines demonstrating knowledge of analytics delivery and data integration best practices Required
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative Required
Software/Technical Skills
Strong knowledge of database systems (SQL and NoSQL) and data warehousing Required
Experience with data integration/ETL tools like Alteryx, Databricks, etc. Required
Proven Experience In Data Engineering And ETL Processes Required
Proficiency in programming languages such as Python, Java, or Scala Required
Knowledge of data governance and compliance practices Required
Familiarity With Machine Learning Frameworks And Concepts Preferred
Experience with data security practices and tools Preferred
Other Skills/Knowledge/Abilities
Knowledge of Visualization Tools (Tableau, PowerBI, etc.) Preferred
Data-driven approach with the ability to aid business transformation Required
Strong Critical Thinking Skills Required
Strong communication skills with ability to adapt facilitation style in order to engage a variety of group settings Required
Business acumen and ability to collaborate effectively with cross-functional teams Required
Ability to effectively convey complex and detailed technical information in a timely manner Required
Ability to challenge the status quo and foster shared understanding, transparency, and mastery of the process and/or system Required
Ability to balance multiple priorities and act with resolve in an ambiguous Required
Excellent Problem-solving And Troubleshooting Skills Required
Strong communication and documentation abilities Required
This role will be a hybrid role. Subject to business needs, employees may work remotely up to two days per week. Assigned office days will be determined by managers.
The salary (or hiring) range for this position is $125,000-$145,000 per year
Starting salary to be determined by the education, experience, knowledge, skills and abilities of the applicant, internal equity, location, and alignment with market data
Benefits include, but are not limited to, healthcare and wellness coverage, life and disability insurance, 401K, tuition reimbursement, and Paid Time Off. Details are available at https://mybenefits.nfp.com/Brother/2024/guidebook/
Brother International Corporation has earned its reputation as a premier provider of home office and business products, home appliances for the sewing and crafting enthusiast as well as industrial solutions that revolutionize the way we live and work. Brother International Corporation is a wholly-owned subsidiary of Brother Industries Ltd. With worldwide sales exceeding $6 billion, this global manufacturer was started more than 100 years ago. Bridgewater, New Jersey is the corporate headquarters for Brother in the Americas. It has fully integrated sales, marketing services, manufacturing, research and development capabilities located in the U.S. In addition to its headquarters, Brother has facilities in California, Illinois and Tennessee, as well as subsidiaries in Canada, Brazil, Chile, Argentina, Peru and Mexico. For more information, visit www.brother.com.
Brother International Corporation (""Brother"") is an equal opportunity employer and does not discriminate or make employment decisions on the basis of race, color, religion, sex, disability, or any other characteristic protected by applicable state or federal laws. If you require any physical or other assistance in completing this application, a reasonable accommodation will be made upon request.
Show more
Show less","Data Engineering, Data Preparation, Machine Learning, Data Integration, Data Modeling, Data Governance, Data Quality Assurance, Data Monitoring, SQL, NoSQL, Data Warehousing, Alteryx, Databricks, Python, Java, Scala, Tableau, PowerBI, Visualization Tools, Critical Thinking, Communication, Business Acumen, Problemsolving, Troubleshooting, Documentation","data engineering, data preparation, machine learning, data integration, data modeling, data governance, data quality assurance, data monitoring, sql, nosql, data warehousing, alteryx, databricks, python, java, scala, tableau, powerbi, visualization tools, critical thinking, communication, business acumen, problemsolving, troubleshooting, documentation","alteryx, business acumen, communication, critical thinking, data engineering, data governance, data integration, data monitoring, data preparation, data quality assurance, databricks, datamodeling, datawarehouse, documentation, java, machine learning, nosql, powerbi, problemsolving, python, scala, sql, tableau, troubleshooting, visualization tools"
Java UI / Data Engineer,"The Dignify Solutions, LLC","Boston, MA",https://www.linkedin.com/jobs/view/java-ui-data-engineer-at-the-dignify-solutions-llc-3768013820,2023-12-17,Belleville, Canada,Associate,Hybrid,"PrISM, Landing Page, DA Framework UI
Java
SQL
Oracle or Postgres
React.js/JavaScript
Additional Skills - SSC specific Cloud (CDT)
Show more
Show less","PrISM, Landing Page, DA Framework UI, Java, SQL, Oracle, Postgres, React.js, JavaScript, Cloud, CDT","prism, landing page, da framework ui, java, sql, oracle, postgres, reactjs, javascript, cloud, cdt","cdt, cloud, da framework ui, java, javascript, landing page, oracle, postgres, prism, reactjs, sql"
"Senior Data Analyst________________westfield center,OH",Steneral Consulting,"Ohio, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-westfield-center-oh-at-steneral-consulting-3707345721,2023-12-17,Belleville, Canada,Associate,Hybrid,"Hello,
I am reaching out to you on an exciting job opportunity with one of our clients.
Title:- Senior Data Analyst -locals
Location:-
Westfield Center, OH(Hybrid)
Duration:- 6 Months
Visa:- GC/USC
Interview Mode:-Skype/Video
Description
Title- Senior Data Analyst
Location- Hybrid 3 days a week in office and for PI Planning :
One Park Circle, Westfield Center, OH 44251
Live within 50 mile radius.
Linkedin must.
Jd-
Must Haves
Insurance Claims and Policy
Data Design & Data Modeling
SQL
Description
As a member of the Enterprise Information Management and Analytics (EIMA) team within the IT department, a Data Analyst 4 will have direct impact on important initiatives that enable our business by:
Providing analysis and insights of our data
Driving clarity of business requirements
Assessing whether data exists within the organization to satisfy those requirements and the quality of it
Determining how individuals or systems should use the data
Working in an agile team environment, the Data Analyst 4 partners with other analysts, members of business units and IT to perform data analysis, data profiling, data sourcing and data requirements gathering. The Data Analyst 4 is expected to be an excellent communicator that challenges themselves and has a strong desire to continually improve their knowledge and skills. A Data Analyst 4 works independently and serves as a subject matter expert. They direct the work and serve as a peer mentor to less experienced data analysts.
Essential Functions (primary functions and/or reasons the job exists in order of importance)
Performs data analysis, data profiling and data sourcing utilizing knowledge of data definitions, domain values, data relationships, business rules, data sources and the enterprise integrated data environment.
Gathers and documents functional and non-functional data requirements.
Creates, updates, and validates sources to target mapping documents.
Analyzes impacts of planned new application development or enhancements to existing applications on legacy and other relational data stores and the enterprise integrated data environment.
Creates conceptual, logical, and physical data models in support of requested changes.
Loads and manages reference data in partnership with business customers.
Analyzes and identifies data quality concerns or cleansing opportunities to correct data quality anomalies and facilitate resolution between technical, business, and other stakeholders.
Ensures deliverables are compliant with Westfield's data standards.
Serves as a peer mentor to less experienced data analysts.
Desired Qualifications/Experience/Certification/Education (in Order Of Importance)
10+ years of experience as a Data Analyst.
10+ years of experience understanding business requirements, translating them into data requirements, analyzing and designing data processes and assisting in building a solution to support the business requirements.
Highly skilled at systematically applying logical reasoning techniques to query, inspect, cleanse, profile, and study data in structured, semi-structured, and unstructured formats for the purpose of deriving knowledge to support business decision-making and mapping data between source and target systems to support application integrations.
Highly skilled at SQL Programming with the ability to perform the most complex queries against relational databases in an efficient manner.
Knowledgeable at building conceptual, logical and physical data models which are visual representations of an enterprise's business data. Experience with ErWin or other modeling tools a plus. Able to build and modify simple data models.
Knowledgeable at transforming data into compelling, meaningful and appealing visualizations, such as graphs, dashboards and reports, using intuitive industry visualization tools (e.g. Tableau, PowerBI, Excel, etc.)
Bachelor's degree in Information Technology, Computer Science, Engineering, Mathematics or related field or commensurate experience.
Candidate must have some
Data Modeling
experience (Erwin or other)
Candidate, if selected will be asked during the first 2 weeks trial period to demonstrate and Model a problem successfully.
Insurance Industry Experience Is Required Policy And Claims
Om Prakash
Talent Acquisition - North America
Direct:+13026017357
om@steneral.com
Show more
Show less","Data Analysis, Data Profiling, Data Sourcing, Data Requirements Gathering, Data Modeling, SQL Programming, Data Visualization, Tableau, PowerBI, Excel, ErWin, Data Quality Management, Business Intelligence, Business Requirements Gathering, Data Integration, Data Governance, Data Standards, Data Cleansing, Data Warehousing, Data Mining, Data Analytics, Data Science, Machine Learning, Artificial Intelligence","data analysis, data profiling, data sourcing, data requirements gathering, data modeling, sql programming, data visualization, tableau, powerbi, excel, erwin, data quality management, business intelligence, business requirements gathering, data integration, data governance, data standards, data cleansing, data warehousing, data mining, data analytics, data science, machine learning, artificial intelligence","artificial intelligence, business intelligence, business requirements gathering, data governance, data integration, data mining, data profiling, data quality management, data requirements gathering, data science, data sourcing, data standards, dataanalytics, datacleaning, datamodeling, datawarehouse, erwin, excel, machine learning, powerbi, sql, tableau, visualization"
Senior Data Lake Engineer - Databricks,Sev1Tech LLC,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-lake-engineer-databricks-at-sev1tech-llc-3772665208,2023-12-17,Belleville, Canada,Associate,Hybrid,"We are seeking a highly experienced and skilled Senior Data Lake Engineer to join our team. As the Senior Data Lake Engineer, you will play a critical role in establishing and configuring an enterprise-level Databricks solution to support our federal customer organization's data lake initiatives. This position offers a unique opportunity to work with cutting-edge technologies and shape the future of our federal customer's data infrastructure.
This position requires onsite presence at the customer location (Arlington, VA) one day per week.
If you are a highly skilled and experienced Senior Data Lake Engineer with expertise in Databricks and passion for building scalable and secure data lake solutions, we would like to hear from you.
Responsibilities:
Lead the design, implementation, and configuration of an enterprise Data Lake solution utilizing Databricks, ensuring scalability, reliability, and optimal performance.
Collaborate with cross-functional teams to gather requirements, understand data integration needs, and define data lake architecture and governance policies.
Establish and configure Databricks workspaces, clusters, and storage components, optimizing the solution for efficient data processing, query performance, and data governance.
Design and implement data ingestion pipelines to efficiently extract, transform, and load data from various sources into the data lake using Databricks tools and services.
Develop and maintain data lake security frameworks, including access controls, encryption solutions, and data masking techniques to protect sensitive data.
Collaborate with data engineers and data scientists to optimize data pipelines, develop data transformations, and ensure data quality and integrity.
Monitor and tune Databricks clusters and workloads to ensure performance, reliability, and cost optimization, utilizing automated scaling and resource management techniques.
Implement best practices for data governance, data cataloging, metadata management, and data lineage within Databricks, adhering to regulatory and compliance requirements.
Collaborate with infrastructure teams to ensure data lake infrastructure meets scalability and availability requirements, leveraging Databricks cluster management and AWS/Azure services.
Develop and maintain documentation and guidelines related to the Databricks solution, including architecture diagrams, standards, and processes.
Stay up to date with the latest advancements in Databricks, big data technologies, and cloud platforms, continuously evaluating and implementing new features and capabilities.
Provide technical guidance and mentorship to junior data engineers, promoting best practices and fostering a culture of continuous learning and growth.
Collaborate with stakeholders to understand their data analytics and reporting needs and develop scalable data models and data transformation processes to support these requirements.
Support data lake-related incident resolutions, troubleshooting data quality issues, performance bottlenecks, and other data-related challenges.
Collaborate with data governance and compliance teams to ensure data privacy, security, and compliance guidelines are adhered to within the data lake solution.
Participate in the evaluation and selection of new tools, technologies, and services to enhance the data lake infrastructure.
Minimum Qualifications
Bachelor's degree in computer science, information technology, or a related field. Equivalent experience will also be considered.
Proven experience in building and configuring enterprise-level data lake solutions using Databricks in an AWS or Azure environment.
In-depth knowledge of Databricks architecture, including workspaces, clusters, storage, notebook development, and automation capabilities.
Strong expertise in designing and implementing data ingestion pipelines, data transformations, and data quality processes using Databricks.
Experience with big data technologies such as Apache Spark, Apache Hive, Delta Lake, and Hadoop.
Solid understanding of data governance principles, data modeling, data cataloging, and metadata management.
Hands-on experience with cloud platforms like AWS or Azure, including relevant services like S3, EMR, Glue, Data Factory, etc.
Proficiency in SQL and one or more programming languages (Python, Scala, or Java) for data manipulation and transformation.
Knowledge of data security and privacy best practices, including data access controls, encryption, and data masking techniques.
Strong problem-solving and analytical skills, with the ability to identify and resolve complex data-related issues.
Excellent interpersonal and communication skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
Experience in a senior or lead role, providing technical guidance and mentorship to junior team members.
Relevant certifications such as Databricks Certified Developer or Databricks Certified Professional are highly desirable.
Eligibility/Clearance Requirements
: Must be able to provide proof of U.S. Citizenship.
Desired Qualifications
Clearance Preference
:
Active DHS/CISA suitability - 1st priority
Any DHS badge + DoD Top Secret - 2nd choice
DoD Top Secret + willingness to obtain DHS/CISA suitability - 3rd choice (it can take 10-60 days to obtain suitability – work can only begin once suitability is fully adjudicated).
About Sev1Tech LLC
Founded in 2010, Sev1Tech provides IT, engineering, and program management solutions delivery. Sev1Tech focuses on providing program and IT support services to critical missions across Federal and Commercial Clients. Our Mission is to Build better companies. Enable better government. Protect our nation. Build better humans across the country.
Join the Sev1Tech family where you can achieve great accomplishments while fostering a satisfying and rewarding career progression. Please apply directly through the website at: https://careers-sev1tech.icims.com/#joinSev1tech
For any additional questions or to submit any referrals, please contact: Caitlin.maupin@sev1tech.com
Sev1Tech is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Show more
Show less","Databricks, Data Lake, Apache Spark, Apache Hive, Delta Lake, AWS, Azure, S3, EMR, Glue, Data Factory, Python, Scala, Java, SQL, Hadoop, Data governance, Data modeling, Data cataloging, Metadata management, Data security, Data privacy, Encryption, Data masking, Data integration, Data pipelines, Data transformations, Data quality, Data analytics, Reporting, Business intelligence, Big data, Cloud computing, Automation, DevOps, Software development, Systems engineering, Program management, Project management, Agile, Scrum, Waterfall, Communication, Teamwork, Collaboration, Problemsolving, Analytical thinking, Innovation, Adaptability, Continuous learning","databricks, data lake, apache spark, apache hive, delta lake, aws, azure, s3, emr, glue, data factory, python, scala, java, sql, hadoop, data governance, data modeling, data cataloging, metadata management, data security, data privacy, encryption, data masking, data integration, data pipelines, data transformations, data quality, data analytics, reporting, business intelligence, big data, cloud computing, automation, devops, software development, systems engineering, program management, project management, agile, scrum, waterfall, communication, teamwork, collaboration, problemsolving, analytical thinking, innovation, adaptability, continuous learning","adaptability, agile, analytical thinking, apache hive, apache spark, automation, aws, azure, big data, business intelligence, cloud computing, collaboration, communication, continuous learning, data cataloging, data factory, data governance, data integration, data lake, data masking, data privacy, data quality, data security, data transformations, dataanalytics, databricks, datamodeling, datapipeline, delta lake, devops, emr, encryption, glue, hadoop, innovation, java, metadata management, problemsolving, program management, project management, python, reporting, s3, scala, scrum, software development, sql, systems engineering, teamwork, waterfall"
Data Analyst,ARC Mechanical,"Wilmington, MA",https://www.linkedin.com/jobs/view/data-analyst-at-arc-mechanical-3774011457,2023-12-17,Belleville, Canada,Associate,Hybrid,"Department:
ARC Mechanical
Employment Type:
Full Time
Location:
Wilmington, MA
Description
Kelvin Group is a leader in the field of industrial and commercial mission-critical mechanical services, process safety management and environmental compliance. Our turnkey solutions provide Design/Installation and Ongoing Service and Preventative Maintenance of world-class mechanical systems for leaders in the Food and Beverage, Cold Storage, Pharmaceutical, Educational, Health Care, and Process industries.
We are seeking a Data Analyst to join our team and support our 150 field technicians in our HVAC/R service business as well as work with our corporate team to build a comprehensive automated reporting package. The Data Analyst will play a critical role in gathering, analyzing, and interpreting data to optimize our operations, improve field service efficiency, and enhance overall business processes.
Key Responsibilities
Data Collection and Analysis:
Gather and compile data from various sources, including service reports, work orders, and back-end ERP systems.
Analyze and interpret data to identify trends, performance indicators, and areas for improvement.
Develop data collection protocols and ensure data accuracy and completeness.
Performance Tracking:
Monitor field technician performance and productivity.
Develop and maintain KPIs to measure service quality, response times, and customer satisfaction.
Generate regular reports and dashboards for management.
Operational Efficiency:
Work with the field service team to optimize technician routes and schedules.
Identify opportunities to reduce operational costs and improve resource allocation.
Assist in inventory management to ensure timely availability of spare parts.
Customer Satisfaction:
Analyze billing team feedback and service quality data to enhance the days to bill process.
Identify areas for service improvement and recommend action plans.
Collaborate with customer service teams to address customer concerns.
Technology Integration:
Assist in the integration of new technologies and validation of the data.
Ensure seamless data flow between field technicians and central databases.
Continuous Improvement:
Stay up to date with industry best practices and data analysis tools.
Propose process improvements and innovations to enhance field service operations.
Skills, Knowledge And Expertise
Bachelor's degree in a relevant field (e.g., Data Science, Business, Engineering, Finance).
Proven experience in data analysis and data management.
Proficiency in data analysis tools, such as Microsoft Excel, SQL, and data visualization tools.
Strong analytical and problem-solving skills.
Excellent communication and teamwork skills.
Knowledge of HVAC/R systems and field service operations (preferred but not mandatory).
Power BI or Tableau experience a plus.
Benefits
Check out these industry leading benefits:
Healthcare (medical, dental, vision, prescription drugs)
Health Reimbursement Arrangement (shared cost deductible)
Flexible Spending Account
Dependent Care Account
Accident Insurance
Life Insurance
AD&D Insurance
Short/Long Term Disability
Employer matched 401(k) savings plan
Paid vacation time
Paid sick time
Generous paid holiday schedule
Show more
Show less","Data Analysis, Data Management, Microsoft Excel, SQL, Data Visualization Tools, Analytical Skills, ProblemSolving Skills, Communication Skills, Teamwork Skills, HVAC/R Systems, Field Service Operations, Power BI, Tableau","data analysis, data management, microsoft excel, sql, data visualization tools, analytical skills, problemsolving skills, communication skills, teamwork skills, hvacr systems, field service operations, power bi, tableau","analytical skills, communication skills, data management, data visualization tools, dataanalytics, field service operations, hvacr systems, microsoft excel, powerbi, problemsolving skills, sql, tableau, teamwork skills"
Sr. Data Analyst,Stellar Professionals,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-stellar-professionals-3769267034,2023-12-17,Belleville, Canada,Associate,Hybrid,"Applicant must have 5 years of relevant experience with the following:
Experience as a Data Analyst
Strong SQL skills
Proficiency in Micro Strategy (or similar reporting tools)
Experience using extremely large data sets
Expert in Microsoft Excel
Advanced Access skills
Experience in data mapping and translation on Financial vertical
Show more
Show less","Data Analysis, SQL, Micro Strategy, Large Data Sets, Microsoft Excel, Access, Data Mapping, Financial Vertical","data analysis, sql, micro strategy, large data sets, microsoft excel, access, data mapping, financial vertical","access, data mapping, dataanalytics, financial vertical, large data sets, micro strategy, microsoft excel, sql"
Data Analyst - Operational Assessment,National Grid Renewables,"Bloomington, MN",https://www.linkedin.com/jobs/view/data-analyst-operational-assessment-at-national-grid-renewables-3748053954,2023-12-17,Belleville, Canada,Associate,Hybrid,"National Grid Renewables is a leading North American renewable energy company based in Minneapolis, Minnesota, with satellite offices located in the regions where it develops, constructs, and operates renewable energy projects. As a farmer-friendly and community-focused company, National Grid Renewables develops projects for corporations and utilities that seek to repower America’s electricity grid by reigniting local economies and reinvesting in a sustainable future. National Grid Renewables is part of the competitive, unregulated Ventures division of National Grid. It has a portfolio of solar, wind, and energy storage projects throughout the United States in various stages of development, construction, and operation.
National Grid Renewables partners with communities, farmers, and landowners where we develop. This means it’s not just about projects but about the people we work with, both outside and inside our organization. National Grid Renewables Team Members embody our foundational culture of being entrepreneurial, creative, and nimble and take pride in supporting National Grid’s vision to be at the heart of a clean, fair, and affordable energy future for all.
Data Analyst - Operational Assessment will be responsible for analyzing data from operating wind, solar, and battery storage projects, and supporting performance tracking / optimization efforts.
Duties:
Analyze meteorological and equipment performance data collected at operating sites
Identify field data quality issues and work with operations / engineering teams to resolve
Collaborate with plant operations staff to troubleshoot underperformance issues
Maintain and enhance internal performance monitoring processes
Assist in development of baseline production and loss expectations
Assist in performing operational assessments / production re-forecasts
Review contractual performance testing results
· Travel approximately 10% of the time
Qualifications:
3+ years in renewable energy
Experience with relational databases and large data sets
Experience with scientific programming languages
Familiarity with wind and solar energy assessments and industry-standard modeling software
BS in meteorology, engineering or other scientific field; MS preferred
Attributes:
Self-directed with ability to manage multiple complex projects simultaneously
Natural curiosity and desire for continuous improvement
Detail-oriented
Ability to effectively communicate technical information to non-technical collaborators
Ability to work in a collaborative environment with technical and non-technical associates
Show more
Show less","Data Analytics, Meteorology, Engineering, Scientific Programming, Relational Databases, Performance Monitoring, Production Forecasting, Wind and Solar Energy Assessments, IndustryStandard Modeling Software","data analytics, meteorology, engineering, scientific programming, relational databases, performance monitoring, production forecasting, wind and solar energy assessments, industrystandard modeling software","dataanalytics, engineering, industrystandard modeling software, meteorology, performance monitoring, production forecasting, relational databases, scientific programming, wind and solar energy assessments"
(5602) Data Engineer,"Merit321, Launching Careers","Columbia, MD",https://www.linkedin.com/jobs/view/5602-data-engineer-at-merit321-launching-careers-3700017595,2023-12-17,Belleville, Canada,Associate,Hybrid,"Position:
(5602) Data Engineer
Location:
Columbia, MD (Hybrid)
Clearance:
Active Secret Clearance
Our client is seeking a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network-related missions within military space, as either a developer, analyst, or engineer.
Work is performed mostly on customer site in Columbia, MD with some flexibility to work from home.
Essential Job Responsibilities
The ideal candidate will have experience with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance -
Must have a current Secret level security clearance and be willing to get PLACEMENT MANAGER/SCI and CI Poly and therefore all candidates must be a U.S. Citizen.
5 years’ experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change).
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired.
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Engineering, Big Data Systems, Programming (Python Java), Network Data, Data Extraction, Data Translation, Data Loading, Kibana, Elasticsearch, Log Formats (JSON XML), Data Flow, Data Storage (AWS S3 SQS), Troubleshooting, Security+ Certification, NOSQL Databases (Accumulo), Cyber Security, Network Security","data engineering, big data systems, programming python java, network data, data extraction, data translation, data loading, kibana, elasticsearch, log formats json xml, data flow, data storage aws s3 sqs, troubleshooting, security certification, nosql databases accumulo, cyber security, network security","big data systems, cyber security, data engineering, data extraction, data flow, data loading, data storage aws s3 sqs, data translation, elasticsearch, kibana, log formats json xml, network data, network security, nosql databases accumulo, programming python java, security certification, troubleshooting"
Senior Data Analyst,Steneral Consulting,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-steneral-consulting-3759955188,2023-12-17,Belleville, Canada,Associate,Hybrid,"Candidate’s Linked In Profile:
2 Managerial References:
First Manager’s Name:
Company:
Title:
Email:
Phone Number:
Second Manager’s Name:
Company:
Title:
Email:
Phone Number:
Attached resume
Job Title:
Senior Data Analyst
Location:
Deerfield Beach Florida 33064 – hybrid work environment, must be onsite a minimum of 3 days per week
Duration:
12+ month contract
Other Requirements
: candidates must be willing to live in the Deerfield Beach Florida area and go into the office a minimum of 3 days/week
Number of Positions
: 1
Job Overview
Selected candidate must be onsite a minimum of 3 days per week in Deerfield Beach, FL.*
Your Future Duties And Responsibilities
Responsible for eliciting, understanding, interpreting and representing business requirements and act as the conduit between the customer and technical teams to ensure requirements are understood.
Provide subject matter expertise on the use of data as well as educate teams on business model, metadata and standards.
Responsible for understanding source systems and its data models.
Develop source to target mappings for data lineage.
Document source architecture to include data flows.
Responsible for analyzing data to validate business domains and requirements.
Responsible for data profiling and ensuring data quality requirements are accurate and complete.
Act in an advisory capacity in data model reviews, architecture approach and solution design to ensure high quality deliverables.
Responsible for partnering with management and business units on innovative ways to successfully utilize data and related tools to advance business objectives.
Works with governance council to establish data governance standards and guidelines.
Assist with business data lake testing / experimentation
Assist with coordinating data dictionary completions
Mentor Project DA resources
Required Qualifications To Be Successful In This Role
Validated experience on projects involving data analysis and profiling, data integration, data cleansing, data mapping, and data conversion activities
Proficient in data management concepts, data lifecycle and methodologies
Knowledge and experience with an ERP system highly preferred. Experience using Sales Force a plus.
Experience and hands-on involvement in operational system modernization and transformation. Knowledge of Microsoft Dynamics 360 a big plus.
Excellent analytical, problem-solving, and decision-making skills, demonstrating both logic and creativity
Excellent written and verbal communication, as well as, strong organizational and presentation skills
Highly motivated and a strong desire to understand the organization, its industry, and its strategies
Resourceful at applying business and technical skills to drive innovation and performance improvement
Demonstrated ability to balance multiple contending priorities in a dynamic environment
Demonstrated facilitations and meeting management skills
Proven ability to work with business representatives to understand and detail their business and functional requirements and document it in an organized ‘functional design’ format
Excellent interpersonal skills with the ability to build relationships within and between individuals and multi-functional teams
Must be a self-starter and show strong initiative
Must exhibit strong customer service orientation
Ability to influence and motivate individuals and teams to drive mutually beneficial outcomes
Solid grasp of agile methodology framework is a plus
4+ years experience working as a data analyst using SQL, BI and other data analysis tools
3+ years hands-on experience working with SQL and a solid understanding of different data structures (flat files, relational, etc.)
Understands data modeling concepts and techniques
Experience working BI/Analytics tools such as Power BI and Tableau is a plus
Experience with MICROSOFT DYNAMICS 360 A BIG PLUS
Education
Bachelor’s degree or equivalent plus 5+ years of related professional experience
Degree in Technology and/or Finance related area preferred
Show more
Show less","Data analysis, Data profiling, Data integration, Data cleansing, Data mapping, Data conversion, Data management, Data lifecycle, ERP systems, Sales Force, Microsoft Dynamics 360, Analytical skills, Problemsolving, Decisionmaking, Written communication, Verbal communication, Organizational skills, Presentation skills, Motivation, Innovation, Performance improvement, Agile methodology, SQL, BI tools, Power BI, Tableau, MICROSOFT DYNAMICS 360, Bachelor's degree, Technology, Finance","data analysis, data profiling, data integration, data cleansing, data mapping, data conversion, data management, data lifecycle, erp systems, sales force, microsoft dynamics 360, analytical skills, problemsolving, decisionmaking, written communication, verbal communication, organizational skills, presentation skills, motivation, innovation, performance improvement, agile methodology, sql, bi tools, power bi, tableau, microsoft dynamics 360, bachelors degree, technology, finance","agile methodology, analytical skills, bachelors degree, bi tools, data conversion, data integration, data lifecycle, data management, data mapping, data profiling, dataanalytics, datacleaning, decisionmaking, erp systems, finance, innovation, microsoft dynamics 360, motivation, organizational skills, performance improvement, powerbi, presentation skills, problemsolving, sales force, sql, tableau, technology, verbal communication, written communication"
Data Engineer - Linguistics,Babel Street,"Somerville, MA",https://www.linkedin.com/jobs/view/data-engineer-linguistics-at-babel-street-3750636728,2023-12-17,Belleville, Canada,Associate,Hybrid,"About Babel Street and the Role:
Babel Street illuminates identity and information for a safer, more productive world. Engineered for mission-critical applications, our proven AI-powered products transform data into knowledge, build a more complete picture around identity, and discover digital evidence. Our advanced data analytics and intelligence platform, combined with a robust text analytics engine, helps teams rapidly transform massive amounts of global, multilingual data into actionable and contextual insights so they can act with confidence. Every day, we work with our customers in highly-regulated, high-stakes industries such as financial services, healthcare, legal and law enforcement, and the global public sector. The actionable insights we deliver safeguard lives and protect critical assets around the world. Learn more at babelstreet.com.
In this role you will have the opportunity to work with multiple, discrete engineering teams providing annotated, reliable data to train, develop, and evaluate natural language processing systems as well as consult on the language specific aspects of multilingual text. Join us and help us create the next wave of software for Natural Language Processing and Text Analytics.
What you will do:
Manage large-scale text mining, data acquisition and annotation projects
Train and supervise contractors as they perform manual annotation tasks
Measure reliability of parallel, manual annotations
Survey and catalogue new data releases and best practices in data maintenance, conversion, and analytics
What you will bring:
Strong scripting abilities, especially Python
Data cleaning, conversion, organization
Parsing XML, JSON, tabular data sets
Scraping and collecting text from online resources including web sites and APIs
Ability to write and revise annotation guidelines
Ability to translate product requirements into annotation guidelines
Ability to synthesize clear instructions and instructive examples
Knowledge of Linguistics and NLP applications including
Language identification
Tokenization
Part of speech tagging
Morphological analysis
Entity extraction, disambiguation, and linking
Syntactic parsing
Sentiment analysis
Experience working with manual annotation tools and platforms such as brat, WebAnno, Prodigy, Mechanical Turk, etc.
Nice to have:
Experience with databases such as SQL and Mongo
Experience with SPARQL query language
Proficiency in at least one natural language in addition to English
Experience with conversion, storage, version control and maintenance tasks for large multilingual text collections
Familiarity with prominent linguistic annotation guidelines (e.g., Penn Treebank)
Familiarity with linguistic community resources and data providers such as
Universal Dependencies treebank project
ClueWeb
CommonCrawl
Linguistic Data Consortium
Benefits at Babel Street (just to name a few...)
Health Benefits: Babel Street covers 90-100% monthly premium costs for Medical, Dental, Vision, Life & Disability insurances – for you and your family!
Retirement Plans: Babel Street offers both a Traditional and Roth 401(K) with a very competitive match.
Unlimited Flexible Leave: We trust our employees to manage their own time and balance their personal and work lives. At a minimum, employees need to take a least 15 to 25 days per year.
Holidays: Babel Street provides employees with 12 paid Federal Holidays
Tuition Reimbursement: We are committed to investing in our employees. One way we do that is with our Tuition Reimbursement Program for continuing education.
Babel Street is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Further, Babel Street will not discriminate against applicants for inquiring about, discussing or disclosing their pay or, in certain circumstances, the pay of their co
‐
worker, Pay Transparency Nondiscrimination.
In addition, Babel Street's policy is to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations and ordinances where a particular employee works. Upon request, we will provide you with more information about such accommodations.
Show more
Show less","Python, Data cleaning, Data conversion, Data organization, XML parsing, JSON parsing, Tabular data sets parsing, Web scraping, API scraping, Annotation guidelines writing, Annotation guidelines revision, Linguistics, NLP applications, Language identification, Tokenization, Partofspeech tagging, Morphological analysis, Entity extraction, Entity disambiguation, Entity linking, Syntactic parsing, Sentiment analysis, Manual annotation tools, Manual annotation platforms, SQL, Mongo, SPARQL query language, Natural language proficiency, Multilingual text collection conversion, Multilingual text collection storage, Multilingual text collection version control, Multilingual text collection maintenance, Linguistic annotation guidelines, Linguistic community resources, Linguistic data providers, Universal Dependencies treebank project, ClueWeb, CommonCrawl, Linguistic data consortium","python, data cleaning, data conversion, data organization, xml parsing, json parsing, tabular data sets parsing, web scraping, api scraping, annotation guidelines writing, annotation guidelines revision, linguistics, nlp applications, language identification, tokenization, partofspeech tagging, morphological analysis, entity extraction, entity disambiguation, entity linking, syntactic parsing, sentiment analysis, manual annotation tools, manual annotation platforms, sql, mongo, sparql query language, natural language proficiency, multilingual text collection conversion, multilingual text collection storage, multilingual text collection version control, multilingual text collection maintenance, linguistic annotation guidelines, linguistic community resources, linguistic data providers, universal dependencies treebank project, clueweb, commoncrawl, linguistic data consortium","annotation guidelines revision, annotation guidelines writing, api scraping, clueweb, commoncrawl, data cleaning, data conversion, data organization, entity disambiguation, entity extraction, entity linking, json parsing, language identification, linguistic annotation guidelines, linguistic community resources, linguistic data consortium, linguistic data providers, linguistics, manual annotation platforms, manual annotation tools, mongo, morphological analysis, multilingual text collection conversion, multilingual text collection maintenance, multilingual text collection storage, multilingual text collection version control, natural language proficiency, nlp applications, partofspeech tagging, python, sentiment analysis, sparql query language, sql, syntactic parsing, tabular data sets parsing, tokenization, universal dependencies treebank project, web scraping, xml parsing"
Salesforce Data Analyst,Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-extend-information-systems-inc-3779828511,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title: Salesforce Data Analyst
Location: New York, NY(Hybrid)
Duration: C2C
Experience level : 10+ yrs
Skills
: Salesforce Analytics, reporting, data sceince
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes. This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM
2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: -
571 - 386 - 2431
Email: Anoop@extendinfosys.com
Show more
Show less","Salesforce, Analytics, Data Science, Reporting, SQL, Salesforce Reporting Tools, Tableau, Power BI, Data Visualization Tools, Data Management Best Practices, Data Governance, Data Accuracy, SQL Server, Mainframe Database, SharePoint, Dynamics CRM, SQL Queries, ETL, SSIS, Salesforce Platform, Salesforce API, Salesforce Data Model, Salesforce OutoftheBox Functionality, Data Modeling, Data Documentation","salesforce, analytics, data science, reporting, sql, salesforce reporting tools, tableau, power bi, data visualization tools, data management best practices, data governance, data accuracy, sql server, mainframe database, sharepoint, dynamics crm, sql queries, etl, ssis, salesforce platform, salesforce api, salesforce data model, salesforce outofthebox functionality, data modeling, data documentation","analytics, data accuracy, data documentation, data governance, data management best practices, data science, data visualization tools, datamodeling, dynamics crm, etl, mainframe database, powerbi, reporting, salesforce, salesforce api, salesforce data model, salesforce outofthebox functionality, salesforce platform, salesforce reporting tools, sharepoint, sql, sql queries, sql server, ssis, tableau"
Salesforce Data Analyst,Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-extend-information-systems-inc-3778539353,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title: Salesforce Data Analyst
Location: New York, NY(Hybrid)
Duration: C2C
Experience level : 10+ yrs
Skills
: Salesforce Analytics, reporting, data sceince
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes. This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM
2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: -
571 - 386 - 2431
Email: Anoop@extendinfosys.com
Show more
Show less","Salesforce Analytics, Reporting, Data Science, SQL, SharePoint, Power BI, Tableau, Data Visualization, ETL, SSIS, Salesforce API, Salesforce Data Model, Salesforce Platform","salesforce analytics, reporting, data science, sql, sharepoint, power bi, tableau, data visualization, etl, ssis, salesforce api, salesforce data model, salesforce platform","data science, etl, powerbi, reporting, salesforce analytics, salesforce api, salesforce data model, salesforce platform, sharepoint, sql, ssis, tableau, visualization"
837 Data Analyst-local,Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/837-data-analyst-local-at-steneral-consulting-3736643582,2023-12-17,Belleville, Canada,Associate,Hybrid,"Location : Currently remote – starting next year 2-3 days on site in DC
2 rounds of interviews – 1 will be ON SITE
MUST BE LOCAL TO DC
MUST BE COVID VACCINATED
This if for a 837 Data Analyst
This is mostly a help desk type role where they will be handling production issues for the 837 transactions (some 835) They will monitor applications and coordinate with teams to resolve issues. They will enter tickets into Dynatrace. They use Onestop for their ticketing tool They also use Jenkins
Responsibilities
5-6 years of experience type person
Perform initial triage and full incident life cycle monitoring in support of FEP Bridge Tenant Plans. This position requires technical and analytical skills with a focus on incident definition, isolation, and resolution. It requires deep knowledge of EDI 837 Claims analysis, 999 errors and SQL queries.
Work with various members of the FEP TOS team to provide Tier 1 technical support to tenant Bridge plans. Tier 1 support involves the initial triaging of reported incidents with the FEP Bridge applications with a focus on data collection and prior incident pattern analysis.
Work closely with FEP Bridge Plan Consultants to ensure smooth communication and prompt follow-up on logged incidents with the applications.
Provide some basic technical support for tenant plans, running queries, and making recommendations on changes to the tenant plans interfaces to improve the overall customer experience.
Create and run performance monitoring reports for end customers including SLA tracking reports.
Manipulation of 837/835 EDI /XML files to generate a wide variety of valid claims
Write complex SQL queries for retrieval of required from multi-database environment. (Strong knowledge of Oracle database required).
Accountable for specific deliverables as defined by the Plan Consultants, and FEP TOS leadership.
Document incident resolutions, escalations, and closures within the CareFirst Incident tracking tool.
Document system monitoring design, implementation, as well as SOPs for various support teams in FEP TOS.
Monitor the FEP Bridge Applications through tools like Dynatrace, Jenkins Batch process.
Skills/Requirement – Support Analyst
This position requires a BA/BS in computer science or equivalent experience, and more than 7 years professional software support experience with an emphasis on incident resolution. Specific requirements include, but not limited to:
Detailed knowledge of 837/835 EDI formats including all segments and loops. Ability to manipulate 837/835 EDI /XML files to generate a wide variety of valid claims
Experience and In-depth knowledge of Claim Adjudication process
Experience with Incident management applications like SharePoint and Salesforce.
Experience with SQL queries.
Experience with Blue Plans is a plus
Some exposure to Object Oriented languages a plus.
Excellent communication and teamwork skills.
Experience with Monitoring tools like Jenkins
Show more
Show less","SQL, EDI, Oracle, Jenkins, Dynatrace, Onestop, SharePoint, Salesforce, XML, 837, 835, Object Oriented Programming Languages","sql, edi, oracle, jenkins, dynatrace, onestop, sharepoint, salesforce, xml, 837, 835, object oriented programming languages","835, 837, dynatrace, edi, jenkins, object oriented programming languages, onestop, oracle, salesforce, sharepoint, sql, xml"
837 Data Analyst-local,Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/837-data-analyst-local-at-steneral-consulting-3736305169,2023-12-17,Belleville, Canada,Associate,Hybrid,"Location : Currently remote – starting next year 2-3 days on site in DC
2 rounds of interviews – 1 will be ON SITE
Contract to hire. Must convert at max salary of $99,500
MUST BE LOCAL TO DC
MUST BE COVID VACCINATED
This if for a 837 Data Analyst
This is mostly a help desk type role where they will be handling production issues for the 837 transactions (some 835) They will monitor applications and coordinate with teams to resolve issues. They will enter tickets into Dynatrace. They use Onestop for their ticketing tool They also use Jenkins
Responsibilities
5-6 years of experience type person
Perform initial triage and full incident life cycle monitoring in support of FEP Bridge Tenant Plans. This position requires technical and analytical skills with a focus on incident definition, isolation, and resolution. It requires deep knowledge of EDI 837 Claims analysis, 999 errors and SQL queries.
Work with various members of the FEP TOS team to provide Tier 1 technical support to tenant Bridge plans. Tier 1 support involves the initial triaging of reported incidents with the FEP Bridge applications with a focus on data collection and prior incident pattern analysis.
Work closely with FEP Bridge Plan Consultants to ensure smooth communication and prompt follow-up on logged incidents with the applications.
Provide some basic technical support for tenant plans, running queries, and making recommendations on changes to the tenant plans interfaces to improve the overall customer experience.
Create and run performance monitoring reports for end customers including SLA tracking reports.
Manipulation of 837/835 EDI /XML files to generate a wide variety of valid claims
Write complex SQL queries for retrieval of required from multi-database environment. (Strong knowledge of Oracle database required).
Accountable for specific deliverables as defined by the Plan Consultants, and FEP TOS leadership.
Document incident resolutions, escalations, and closures within the CareFirst Incident tracking tool.
Document system monitoring design, implementation, as well as SOPs for various support teams in FEP TOS.
Monitor the FEP Bridge Applications through tools like Dynatrace, Jenkins Batch process.
Skills/Requirement – Support Analyst
This position requires a BA/BS in computer science or equivalent experience, and more than 7 years professional software support experience with an emphasis on incident resolution. Specific requirements include, but not limited to:
Detailed knowledge of 837/835 EDI formats including all segments and loops. Ability to manipulate 837/835 EDI /XML files to generate a wide variety of valid claims
Experience and In-depth knowledge of Claim Adjudication process
Experience with Incident management applications like SharePoint and Salesforce.
Experience with SQL queries.
Experience with Blue Plans is a plus
Some exposure to Object Oriented languages a plus.
Excellent communication and teamwork skills.
Experience with Monitoring tools like Jenkins
Show more
Show less","SQL, EDI, 837 Claims, 999 Errors, XML, Oracle, SharePoint, Salesforce, Linux, Jenkins, Dynatrace","sql, edi, 837 claims, 999 errors, xml, oracle, sharepoint, salesforce, linux, jenkins, dynatrace","837 claims, 999 errors, dynatrace, edi, jenkins, linux, oracle, salesforce, sharepoint, sql, xml"
Data Analyst II-locals,Steneral Consulting,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-analyst-ii-locals-at-steneral-consulting-3728264331,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title Data Analyst II
Business Sector Data Engineering
Work Location Pittsburgh, PA F2F interview 1 day onsite
Skills:Issue Management and servicenow and Issue Management, business analysis, and tools integration (requirements and UAT)
Duration 12 Months
Job Description The primary responsibilities of the candidate will include:
Issue Management analysis and triage through self-identified issues and data quality issues across the enterprise
Liase between stakeholders to identify and agree upon remediation plans and timelines for open issues
Use of ServiceNow tool for issue management process
Document technical requirements for ServiceNow tool enhancements and perform UAT
Assist with ServiceNow end user training including live demos, user guides, best practices, FAQs, etc. Manager Release Notes:
Is the goal to convert the candidate to FTE? Potentially
Show more
Show less","Issue Management, Business Analysis, ServiceNow, Requirements Gathering, UAT, End User Training, Technical Writing, Data Quality, Data Analysis","issue management, business analysis, servicenow, requirements gathering, uat, end user training, technical writing, data quality, data analysis","business analysis, data quality, dataanalytics, end user training, issue management, requirements gathering, servicenow, technical writing, uat"
MS BI Data Analyst-locals,Steneral Consulting,"The Woodlands, TX",https://www.linkedin.com/jobs/view/ms-bi-data-analyst-locals-at-steneral-consulting-3736683055,2023-12-17,Belleville, Canada,Associate,Hybrid,"Salary- $115,000/yr - $125,000/yr
The BI Developer will be responsible for delivering enterprise analytical reporting, dashboard and solutions. This position will work closely with business users and executives to gather, define, and translate business requirements into BI solutions. The applicant must have excellent collaboration skills with the ability to communicate concepts and solutions effectively and clearly. This position will also play a key technical role requiring expert level skills in the design, development and implementation of BI solutions using Visual Studio, SSMS, SSIS, SSAS, Power BI and SSRS.
Required Experience, Knowledge Skills, And Abilities
Must have the ability to perform complex analysis and be able to identify and resolve probable root cause issues.
Exception communication skills are mandatory as this position is forward facing with our business partners.
Must have strong analytical skills to identity business needs and formulate viable solutions.
Expert in the design, development, automation, and support of complex processes to extract, transform and load data into SQL Data Store and Data Warehouse environments using Visual Studio and Microsoft’s SQL Stack tools (SSMS, SSIS).
Expert in developing complete SSIS packages including development of stored procedures, connections, tasks, control flows, data flows, parameters, event handlers and variables.
Expert in the development of data warehouses and BI solutions with practicing knowledge of the Kimball methodology for data modeling.
Experience in developing, updating, managing, and monitoring SQL Agent Jobs.
Experience in SSAS tabular modeling with a thorough understanding of defining relationships, security and the use of partitions and perspectives.
Experience with writing custom DAX measures with a thorough understanding of the DAX language required to create DAX expressions/statements and queries.
Knowledge in the development of SSRS and Power BI reporting
Knowledge in the configuration and management of Replication, Change Data Capture and Change Tracking processes.
Required use of Azure DevOps for collaboration and code management
Knowledge of JDE/E1 data and table relationships is desirable.
Degree or Certifications required:
Bachelor’s degree or 6 years’ experience in related field.
Show more
Show less","Visual Studio, SSMS, SSIS, SSAS, Power BI, SSRS, SQL Server, SQL Agent Jobs, DAX, JDE/E1, Data Modeling, Data Warehousing, Azure DevOps, Kimball Methodology","visual studio, ssms, ssis, ssas, power bi, ssrs, sql server, sql agent jobs, dax, jdee1, data modeling, data warehousing, azure devops, kimball methodology","azure devops, datamodeling, datawarehouse, dax, jdee1, kimball methodology, powerbi, sql agent jobs, sql server, ssas, ssis, ssms, ssrs, visual studio"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Newyorkuniversity,"Belleville, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-newyorkuniversity-3750809129,2023-12-17,Belleville, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistics, Machine Learning, SQL, R, Python, Data Visualization, Tableau, Power BI, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, statistics, machine learning, sql, r, python, data visualization, tableau, power bi, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistics, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Dukeduchessinternational,"Greater Napanee, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-dukeduchessinternational-3759037979,2023-12-17,Belleville, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hrteam@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization Tools, Data Collection, Data Cleansing, Data Manipulation, SQL, R, Python, Tableau, Power BI, Data Modeling, Hypothesis Testing, A/B Testing, Data Quality, Data Integrity, Data Accuracy, Data Completeness, ETL Processes","data analysis, statistical techniques, data visualization tools, data collection, data cleansing, data manipulation, sql, r, python, tableau, power bi, data modeling, hypothesis testing, ab testing, data quality, data integrity, data accuracy, data completeness, etl processes","ab testing, data accuracy, data collection, data completeness, data integrity, data manipulation, data quality, data visualization tools, dataanalytics, datacleaning, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau"
Data Scientist/AI/ML Engineer (Onsite at MD),VLink Inc,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/data-scientist-ai-ml-engineer-onsite-at-md-at-vlink-inc-3786287501,2023-12-17,Frederick,United States,Mid senior,Onsite,"Responsibilities:
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements:
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications:
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in
Python
/R (Shiny preferably) Or
Tableau
.
Familiarity with software version control with git.
Strong work ethic and intellectual curiosity.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as
My SQL
and
BigQuery
.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units
Show more
Show less","Technical leadership, Solution finding, ML model design, Business goal tracking, Proof of concept execution, Data wrangling, Predictive modeling, Computer science degree, Data science degree, Computer networking, Data science experience, Analysis leading, Problemsolving, Business problem analysis, Solution implementation, Data science application design, DS design knowledge, ML/Ops/DevOps experience, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Data visualization tools, Tableau, Git, AWS, Google Cloud, MySQL, BigQuery, Communication, Problemsolving, Analytical concepts","technical leadership, solution finding, ml model design, business goal tracking, proof of concept execution, data wrangling, predictive modeling, computer science degree, data science degree, computer networking, data science experience, analysis leading, problemsolving, business problem analysis, solution implementation, data science application design, ds design knowledge, mlopsdevops experience, python, r, sql, pandas, scikitlearn, tensorflow, keras, data visualization tools, tableau, git, aws, google cloud, mysql, bigquery, communication, problemsolving, analytical concepts","analysis leading, analytical concepts, aws, bigquery, business goal tracking, business problem analysis, communication, computer networking, computer science degree, data science application design, data science degree, data science experience, data visualization tools, data wrangling, ds design knowledge, git, google cloud, keras, ml model design, mlopsdevops experience, mysql, pandas, predictive modeling, problemsolving, proof of concept execution, python, r, scikitlearn, solution finding, solution implementation, sql, tableau, technical leadership, tensorflow"
Sr Engineer - Data Science - Gaithersburg MD (Onsite) Local only,TekIntegral,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-gaithersburg-md-onsite-local-only-at-tekintegral-3715850372,2023-12-17,Frederick,United States,Mid senior,Onsite,"Title: Sr Engineer - Data Science
Location: Gaithersburg MD (Onsite) Local only
Duration: 6 months C2H
IV: Phone and Video
Visa: USC, GC and GC-EAD; Let me know if you have someone local with other Visas
Rate: $70/hr C2C
Vendor's Note:
I just spoke with the candidate that interviewed and it seems that the team is looking for a candidate that is very heavy on the data engineering side of the house with some experience in AI/ML and data science
NEED TO LIVE IN A 15-20M RADIUS OF Gaithersburg MD (will move fast) will probably close the role today.
Highlights in Yellow candidate must have
Required
Resume can be no longer than 2 pages
Needs to have degree/name of university/and year of graduation
Place of Birth
Conversion salary after 6m
Do not send any candidate that are not living in Fredrick or Montgomery County Maryland- or 15/20m radius of Gaithersburg Md
Needs to be a Senior candidate.
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Experience working across varying business and technical functional units
Show more
Show less","Data Science, AI/ML, Machine Learning, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Data visualization tools, Tableau, Git, AWS, Google, My SQL, BigQuery","data science, aiml, machine learning, python, r, sql, pandas, scikitlearn, tensorflow, keras, data visualization tools, tableau, git, aws, google, my sql, bigquery","aiml, aws, bigquery, data science, data visualization tools, git, google, keras, machine learning, my sql, pandas, python, r, scikitlearn, sql, tableau, tensorflow"
Sr Engineer Data Science,Hughes,"Germantown, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-at-hughes-3750890853,2023-12-17,Frederick,United States,Mid senior,Onsite,"EchoStar Corporation (NASDAQ: SATS) is a premier global provider of satellite communication solutions. Headquartered in Englewood, Colo., and conducting business around the globe, EchoStar is a pioneer in communications technologies through its Hughes Network Systems and EchoStar Satellite Services business segments. For more information, visit echostar.com. Follow @EchoStar on Twitter.
EchoStar has an exciting opportunity for a
Sr
Engineer - Data Science
in our Hughes Network Systems division. This position will be located at our headquarters in Germantown, MD.
Responsibilities
Collaborate with cross-functional teams to design, build, and deploy cloud-based solutions and infrastructure on GCP.
Create interactive, meaningful and insightful visualizations, to communicate findings and insights effectively to internal teams and other stakeholders.
Responsible for extracting large quantities of data from On-premises systems, developing efficient ETL and data management processes, and building architectures for rapid ingestion and dissemination of key data.
Design and implement complete and complex data modeling and analytics to measure the performances of Aero and telecommunication systems.
Understanding the data security and privacy concerns when dealing with sensitive data in the Aero and Mobility sector.
Build and maintain data pipelines. Monitor and troubleshoot production systems.
Continuously improve infrastructure and processes to increase reliability, scalability, and security
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Apply the knowledge in statistics, AI/ML coding and development, cloud-based analytics platform to create an integrated data analytics platform for complex communications system.
Collaborate with leaders and managers to determine and address data and reporting needs for various company projects.
Basic Requirements
Bachelor’s degree in data science, Computer Science, Engineering, Statistics, or related technical field.
5+ years of experience as an MLOPS Engineer
5 to 7 years of Data Analytics work which includes data modelling and data visualization.
At least 2 years’ experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
Preferred Qualifications
Understanding of the Aero and Mobility industries relevant technologies.
Understanding of specific data sources and data types in this sector such as flight data, transport data and sensor data.
Strong programming skills commonly used for data analysis and machine learning such as Go, R and Python
Experience with GCP, IAM, DB, streaming, and build and deploy services.
Experience in designing, implementing, and fine-tuning machine learning models for various applications (e.g., classification, regression, clustering, recommendation systems).
Experience with machine learning techniques and algorithms (e.g., TensorFlow, sklearn, numpy, pytorch) and their applications.
Experience with deep learning and neural networks is beneficial.
Experience building data pipelines for batch and streaming applications.
Experience designing applications for observability using metrics, monitoring, logging, and alarming.
Strong understanding of machine learning algorithms, statistical modeling, and data manipulation techniques.
Experience with data visualization tools and libraries such as Power BI & Tableau.
Familiarity with big data technologies like Snowflake, Big query, or other distributed computing frameworks
Knowledge of SQL and databases for data retrieval and manipulation.
Strong problem-solving, analytical and troubleshooting skills.
Excellent communication skills, including the ability to explain complex technical concepts to non-technical stakeholders.
Experience presenting findings and insights to diverse audiences.
Experience with Confluence, Jira, and Bitbucket
Will be eligible for discretionary bonus, with funding based on company performance.
EchoStar is committed to offering a comprehensive and competitive benefits package. Our programs are designed to provide you with the ability to customize your benefits to best meet the needs of you and your family. Our philosophy for these programs is to support and encourage healthy living and wellness. Our benefits package covers it all–from healthcare savings plans to education assistance and more!
Financial: 401(k) retirement savings plan with company match; employee stock purchase plan; profit-sharing; company-paid life insurance, AD&D and disability
Work-Life Balance: Paid Time Off (PTO), company-paid holidays, health and wellness events, exercise and sports facilities (locations may vary)
Employee Incentives: Tuition reimbursement, employee referral program, year round employee events and community programs, discounts on Dish Network and HughesNet
Health: Medical, Dental, Vision, Employee Assistance Program (EAP), Health Savings Account (HSA) with opportunities to earn employer contributions; Health Care, Dependent Care and Transportation Flexible Spending Accounts (FSA)
EchoStar and its Affiliated Companies are committed to hiring and retaining a diverse workforce. We are an Equal Opportunity/Affirmative Action employer and will consider all qualified applicants for employment without regard to race, color, religion, gender, pregnancy, sex, sexual orientation, gender identity, national origin, age, genetic information, protected veteran status, disability, or any other basis protected by local, state, or federal law. U.S. Persons or those able to obtain and maintain U.S. government security clearances may be required for certain positions. EEO is the law.
Show more
Show less","GCP, Machine learning, Data modeling, Data visualization, Data engineering, Data analytics, Data science, Cloud computing, SQL, Python, R, Go, TensorFlow, Sklearn, Numpy, Pytorch, Snowflake, Big query, Power BI, Tableau, Confluence, Jira, Bitbucket","gcp, machine learning, data modeling, data visualization, data engineering, data analytics, data science, cloud computing, sql, python, r, go, tensorflow, sklearn, numpy, pytorch, snowflake, big query, power bi, tableau, confluence, jira, bitbucket","big query, bitbucket, cloud computing, confluence, data engineering, data science, dataanalytics, datamodeling, gcp, go, jira, machine learning, numpy, powerbi, python, pytorch, r, sklearn, snowflake, sql, tableau, tensorflow, visualization"
Senior Database Engineer - MS SQL Server,EMP Trust HR Solutions,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-ms-sql-server-at-emp-trust-hr-solutions-3787739750,2023-12-17,Frederick,United States,Mid senior,Onsite,"Profound knowledge as Database Developer/ Database Architect with experience in working with large databases.
Expert knowledge in T-SQL and Microsoft SQL-Server 2012 / 2014
Ideally you will have experience in object oriented software development with C#.NET and the design of software in the database environment.
Ability to understand the functionality of RDBMS, Capability to build real-time and highly secure DB, Implementation of logic (using Procedures, functions etc)
Good working experience with Visual Studio, SSDT, TFS and SQL Server Management Studio.
Database Cloud hosting
EMP Trust is an Equal Opportunity Employer
Powered by JazzHR
8Gm0U4dILP
Show more
Show less","Database Development, Database Architecture, TSQL, Microsoft SQLServer, C#.NET, ObjectOriented Software Development, Software Design, RDBMS, Realtime Database Development, Secure Database Development, Procedures, Functions, Visual Studio, SSDT, TFS, SQL Server Management Studio, Database Cloud Hosting","database development, database architecture, tsql, microsoft sqlserver, cnet, objectoriented software development, software design, rdbms, realtime database development, secure database development, procedures, functions, visual studio, ssdt, tfs, sql server management studio, database cloud hosting","cnet, database architecture, database cloud hosting, database development, functions, microsoft sqlserver, objectoriented software development, procedures, rdbms, realtime database development, secure database development, software design, sql server management studio, ssdt, tfs, tsql, visual studio"
"Urgent Role || Sr Engineer - Data Science  || Gaithersburg, MD(Onsite) || USC,GC only",Steneral Consulting,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/urgent-role-sr-engineer-data-science-gaithersburg-md-onsite-usc-gc-only-at-steneral-consulting-3697184097,2023-12-17,Frederick,United States,Mid senior,Onsite,"Onsite 100%
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units
Show more
Show less","Machine learning, Data science, Data analysis, Data wrangling, Data visualization, Problem solving, Analytical skills, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Git, AWS, Google Cloud, MySQL, BigQuery, Tableau, Shiny","machine learning, data science, data analysis, data wrangling, data visualization, problem solving, analytical skills, python, r, sql, pandas, scikitlearn, tensorflow, keras, git, aws, google cloud, mysql, bigquery, tableau, shiny","analytical skills, aws, bigquery, data science, data wrangling, dataanalytics, git, google cloud, keras, machine learning, mysql, pandas, problem solving, python, r, scikitlearn, shiny, sql, tableau, tensorflow, visualization"
"Data Engineer IV, Day Shift, Information Technology",Adventist HealthCare,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/data-engineer-iv-day-shift-information-technology-at-adventist-healthcare-3739286195,2023-12-17,Frederick,United States,Mid senior,Onsite,"Support Center
If you are a current Adventist HealthCare employee, please click this link to apply through your Workday account.
Adventist HealthCare seeks to hire an experienced Data Engineer IV who will embrace our mission to extend God’s care through the ministry of physical, mental, and spiritual healing.
As a Data Engineer IV you will:
Lead the creation and supports the ETL process to facilitate the on-boarding of data into the data warehouse and distribution of data across data stores from wide variety of sources using different ETL tools and technologies.
Lead the design and develop the physical and virtual Data warehouse table schemas, database, data marts for new and existent data sources for the business’s data warehouse.
Lead the development and maintenance of informative and actionable data visualizations and reports for analysis, problem solving, and business solutions that highlights relevant business trends and opportunities for improvement.
Lead all service operational tasks including but not limited to database setup, maintenance, configuration, programming, troubleshooting, debugging, testing, fulfilling support requests, resolving incidents, and managing problems.
Lead the creation and support of routine and ad hoc data load processes through database refreshes and updates.
Optimize data integration platform to provide optimal performance under increasing data volumes.
Lead the preparation and maintenance of comprehensive understanding of data warehousing and ETL application lifecycle, technical architecture, data, configuration, and operational support requirements.
Maintains detailed charts, diagrams, and flow charts outlining systems, hardware, interfaces, and configurations related to data warehousing, ETL jobs, Data visualization, Data analytics and other similar.
Provide expertise in the design and development of best practice for big data stacking, data warehousing, data lake, ETL architecture, data federation, virtualization, data modeling procedures, data visualization, and workflows including technical documents.
Lead the analytics support to generate insights in the form of dashboards, metrics, or reports for healthcare solutions including the collection of detail business analytics requirements from stakeholders and the development of standardized functional requirement documents.
Lead the preparation of presentations and demonstrates business intelligence solutions to end users.
Lead in the training of end users on new reports and dashboards
Assist in leading the preparation of documents related to data governance, definitions, key metrics, and sample reports, ensuring alignment and consistency in reporting across the organization and standardized data analytics, visualization, and business intelligence reports.
Lead the data Integration and data warehousing application portfolio support in the timely resolution of technology incidents while adhering to SLA prioritization and success measures including 24x7 on call support.
Qualifications include:
Bachelor’s degree in computer science, information technology, or a related field.
7+ years’ experience and knowledge of coding languages, including Java, XML, and SQL to extract data from various relational databases, application systems, flat files, XML documents, and load into data warehouses.
7+ years’ experience in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDW.
7+ years’ experience as an ETL developer using.
integration tools such as Talend and SSIS.
Experience working on cloud-based environment such as AWS or Azure.
Experience working with large scale.
Data warehouses and related technologies.
7+ years working on building meaningful data analytics, reports, and visualizations in health care environment.
Snowflake experience is preferred.
Experience in metadata management and related tools.
Advanced knowledge of database security, integrity, backup and recovery, and performance monitoring standards.
3 Years Supervisory Experience.
Preferred Cloud-Data Warehouse certification or ETL Certification or Data Visualization.
Experience with business intelligence software applications such as Tableau visualization, PowerBI, SQL servicer Management studio, IBM Cognos B.
Work Schedule:
Full Time Days
Monday - Friday
At Adventist HealthCare our job is to care for you.
We do this by offering:
Work life balance through nonrotating shifts
Recognition and rewards for professional expertise
403(b) retirement plan
Free Employee parking
Benefits Eligible Positions:
Competitive, comprehensive benefit plans [including health, employer-paid disability and life insurance, PTO]
Employer retirement contribution and match after 1-year of eligible employment with 3 year vesting
Ancillary benefits such as flexible spending, legal and pet insurance to meet the needs of employees and their eligible family members
Subsidized childcare at participating childcare centers
As a faith-based organization, with over a century of caring for the communities in the Maryland area, Adventist HealthCare has earned a reputation for high-quality, compassionate care. Adventist HealthCare was the first and is the largest healthcare provider in Montgomery County.
If you want to make a difference in someone’s life every day, consider a position with a team of professionals who are doing just that, making a difference.
Join the Adventist HealthCare team today, apply now to be considered!
COVID-19 Vaccination
Adventist HealthCare requires all applicants to be fully vaccinated for COVID-19 before commencing employment. Applicants may be required to furnish proof of vaccination and, if needed, may elect to be vaccinated at any community pharmacy or location offering COVID-19 vaccinations.
Tobacco and Drug Statement
Tobacco use is a well-recognized preventable cause of death in the United States and an important public health issue. In order to promote and maintain a healthy work environment, Adventist HealthCare will not hire applicants for employment who either state that they are nicotine users or who test positive for nicotine and drug use.
While some jurisdictions, including Maryland, permit the use of marijuana for medical purposes, marijuana continues to be classified as an illegal drug under the federal Controlled Substances Act. As a result, medical marijuana use will not be accepted as a valid explanation for a positive drug test result.
Adventist HealthCare will withdraw offers of employment to applicants who test positive for Cotinine (nicotine) and marijuana. Those testing positive are given the opportunity to re-apply in 90 days, if they can truthfully attest that they have not used any nicotine products in the past ninety (90) days and successfully pass follow-up testing. (""Nicotine products"" include, but are not limited to: cigarettes, cigars, pipes, chewing tobacco, e-cigarettes, vaping products, hookah, and nicotine replacement products (e.g., nicotine gum, nicotine patches, nicotine lozenges, etc.).
Equal Employment Opportunity
Adventist HealthCare is an Equal Opportunity/Affirmative Action Employer. We are committed to attracting, engaging, and developing the best people to cultivate our mission-centric culture. Our goal is to have a welcoming, equitable, and safe place to work and grow for all employees, no matter their background. AHC does not discriminate in employment opportunities or practices on the basis of race, ethnicity, color, religion, sex, national origin, age, disability, sexual orientation, gender identity, pregnancy and related medical conditions, protected veteran status, or any other characteristic protected by law.
Adventist HealthCare will make reasonable accommodations for applicants with disabilities, in accordance with applicable law. Adventist HealthCare is a religious organization as defined under applicable law; however, it will endeavor to provide reasonable accommodations for applicants’ religious beliefs.
Applicants who wish to request accommodations for disabilities or religious belief should contact the Support Center HR Office.
Show more
Show less","ETL, Data Warehousing, Data Visualization, Database, Data Analytics, Java, XML, SQL, Talend, SSIS, AWS, Azure, Snowflake, Tableau, PowerBI, SQL Servicer Management Studio, IBM Cognos B, CloudData Warehouse Certification, ETL Certification, Data Visualization Certification","etl, data warehousing, data visualization, database, data analytics, java, xml, sql, talend, ssis, aws, azure, snowflake, tableau, powerbi, sql servicer management studio, ibm cognos b, clouddata warehouse certification, etl certification, data visualization certification","aws, azure, clouddata warehouse certification, data visualization certification, dataanalytics, database, datawarehouse, etl, etl certification, ibm cognos b, java, powerbi, snowflake, sql, sql servicer management studio, ssis, tableau, talend, visualization, xml"
Databricks Sr. Systems Engineer -- Security Clearance REQUIRED,Leidos,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/databricks-sr-systems-engineer-security-clearance-required-at-leidos-3744511473,2023-12-17,Frederick,United States,Mid senior,Onsite,"Description
Leidos is currently seeking a Systems Engineer (SE) for the Advanced Analyst Augmentation Analytical Cloud Enablement System (4ACES). The SE will analyze 4ACES requirements and assist with design analysis. The SE will coordinate COTS upgrades, coordinate with COTS vendors, user Subject Matter Experts (SMEs), NGA Analytic Product Owners, and other segments when issues arise throughout sustainment. The Systems Engineer serves as the liaison with the PMO, customer user community, COTS vendor, other segment data providers, and will assist with integrations, migrations, Software Approval Process (SWAP), and accreditation activities. The SE will coordinate corrective maintenance management with the COTS vendor.
The SE’s primary responsibility will be to function as a Content Manager by managing data and data services published for the user community. As the Content Manager the SE will support the analysts with new data and Databricks requests as well as resolving any daily issues.
Position can also be performed in the following locations:
Alexandria, VA
St. Louis, MO
Tucson, AZ
Clearance Level Required
Top Secret/SCI with Polygraph
Primary Responsibilities
Interact with the customer Release Train Engineer, Product Owners, and stakeholders to analyze customer requirements and determine how to include requirements in the operational environment through COTS integration and new data sources being added to the Databricks cluster.
Perform the design and requirements analysis of data visualizations and COTS appliances for capability enhancements.
Coordinate with stakeholder and team to determine how to host new data capabilities within the operational environment and troubleshoot issues as the customer is integrating new data or creating new visualizations.
Identify software and hardware dependencies and capacities.
Basic Qualifications
US citizenship is required per contract.
Bachelor's degree and 8-12 years of prior relevant experience or Master’s with 6-10 years of prior relevant experience.
Experience with Databricks deployment, configuration, and support.
Significant experience in Cloud Technologies (specifically AWS).
Experience interacting with cross functional project teams including Software Development, Test, and Security.
Substantial experience working in Agile, SAFe, and Scrum environments.
Knowledge of Software Configuration Management life cycle deliverables.
Preferred Qualifications
Experience with Databricks deployment, configuration, and support.
AWS certifications.
Experience working with Machine Learning and knowledge of data science concepts.
Experience working with DevOps CI/CD related technologies and deployment automation techniques (e.g., Ansible, Git, Jenkins, Docker, Confluence, and Junit).
This position has a target range for compensation of $130,000 - $150,000. This is a firm fixed price contract and does not allow for negotiations outside of this target range.
Pay Range
Pay Range $101,400.00 - $183,300.00
The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
Original Posting Date
12/11/2023
While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","Systems Engineering, Databricks, AWS, Software Configuration Management, Machine Learning, Data Science, Agile, SAFe, Scrum, DevOps, CI/CD, Ansible, Git, Jenkins, Docker, Confluence, Junit","systems engineering, databricks, aws, software configuration management, machine learning, data science, agile, safe, scrum, devops, cicd, ansible, git, jenkins, docker, confluence, junit","agile, ansible, aws, cicd, confluence, data science, databricks, devops, docker, git, jenkins, junit, machine learning, safe, scrum, software configuration management, systems engineering"
Sr. Data Analyst - Remote | WFH,Get It Recruit - Information Technology,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-data-analyst-remote-wfh-at-get-it-recruit-information-technology-3776621875,2023-12-17,Frederick,United States,Mid senior,Remote,"Rooted in the realm of higher education, our organization collaborates with institutions across North America to enhance financial decision-making, streamline assessment/accreditation processes, and foster a more integrated approach to strategic planning and institutional effectiveness. Comprised of dedicated professionals, we are fervently driven by the transformative impact of innovation on the mission of higher education.
Our Mission
We specialize in empowering colleges and universities to measure and assess their effectiveness in fulfilling their mission. By aligning investments with financial, operational, and student learning outcomes, we guide our partners on a journey toward institutional sustainability.
Our Approach
Transforming institutional data into a powerful asset is at the core of what we do. We combine cutting-edge technology platforms with industry best practices to present data in ways that empower stakeholders in higher education to make informed decisions and mitigate risks. Our unparalleled institutional expertise, coupled with proven data models and innovative software, sets us apart.
Who We're Looking For
We're on the lookout for a Senior Data Analyst to join our team of higher education data experts. As a key member of our organization, the ideal candidate will collaborate closely with client partners, leveraging their analytical skills and superior technical acumen to contribute to the success of our mission.
Key Responsibilities
Develop expertise in our data platform, guiding client partners in using it to address strategic institutional questions.
Deliver standardized analysis playbooks developed by our team.
Analyze financial data benchmarks for higher education institutions.
Conduct ad hoc analysis to uncover actionable insights for strategic decision-making.
Act as the primary liaison between our organization and client partners, working with senior leaders to maximize platform utilization.
Manage timelines and scope for client and internal projects.
Utilize enterprise business intelligence tools, SQL queries, and other tools to develop complex dashboards and reports.
Provide ongoing training and support to client stakeholders.
Contribute to the development of our product roadmap for analytical tools and solutions.
Translate and interpret business and data requirements between clients and our technical/data teams.
Ensure accuracy and quality of data extracts.
Qualifications
To thrive in this role, you should have:
Minimum of 5 years of experience in analytic functions in higher education or consulting within the field.
Deep understanding of key areas in higher education (e.g., financial aid, admissions, student affairs).
Excellent communication skills.
Experience with Student Information systems (e.g., Banner, PeopleSoft, Colleague, Workday).
Proficiency in reporting and data visualization tools (e.g., Tableau and Power BI).
Understanding of relational databases.
Ability to manage multiple priorities independently with strict deliverable dates.
Experience working with both business users and technical development teams.
Desirable Skills
Nice-to-haves include:
Experience with HR, CRM, Financial, and/or Learning Management systems in higher education.
Experience presenting findings to mid to senior-level executives.
Significant experience building reports and dashboards in Tableau.
Strong understanding of data warehouse concepts and working with large datasets.
Project Management experience.
Experience with SQL and in one or more analytic/programming software packages such as R, SAS, Python, or comparable.
Projects You Will Contribute To
Client analysis deliverables for Data Analytics clients and Benchmarking consortium members.
Creation of Tableau templates and playbooks to standardize analysis deliverables.
Process efficiency initiatives to streamline and improve existing delivery processes.
Tableau development work, documentation, and presentation materials to support the product roadmap.
Compensation And Benefits
We offer a competitive salary, paid time off, healthcare, vision, dental, 401(k) with company match, remote work flexibility, and a dynamic and collaborative work environment.
Employment Type: Full-Time
Show more
Show less","Data analytics, Business intelligence, SQL, Tableau, Power BI, Relational databases, Higher education, Financial aid, Admissions, Student affairs, Student Information systems, Reporting tools, Data visualization, Data warehouse concepts, Large datasets, Project Management, R, SAS, Python, Analytic/programming software packages","data analytics, business intelligence, sql, tableau, power bi, relational databases, higher education, financial aid, admissions, student affairs, student information systems, reporting tools, data visualization, data warehouse concepts, large datasets, project management, r, sas, python, analyticprogramming software packages","admissions, analyticprogramming software packages, business intelligence, data warehouse concepts, dataanalytics, financial aid, higher education, large datasets, powerbi, project management, python, r, relational databases, reporting tools, sas, sql, student affairs, student information systems, tableau, visualization"
Databricks Sr. Systems Engineer -- Security Clearance REQUIRED with Security Clearance,ClearanceJobs,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/databricks-sr-systems-engineer-security-clearance-required-with-security-clearance-at-clearancejobs-3753467156,2023-12-17,Frederick,United States,Mid senior,Hybrid,"R-00121177 Description Leidos is currently seeking a Systems Engineer (SE) for the Advanced Analyst Augmentation Analytical Cloud Enablement System (4ACES). The SE will analyze 4ACES requirements and assist with design analysis. The SE will coordinate COTS upgrades, coordinate with COTS vendors, user Subject Matter Experts (SMEs), NGA Analytic Product Owners, and other segments when issues arise throughout sustainment. The Systems Engineer serves as the liaison with the PMO, customer user community, COTS vendor, other segment data providers, and will assist with integrations, migrations, Software Approval Process (SWAP), and accreditation activities. The SE will coordinate corrective maintenance management with the COTS vendor. The SE’s primary responsibility will be to function as a Content Manager by managing data and data services published for the user community. As the Content Manager the SE will support the analysts with new data and Databricks requests as well as resolving any daily issues. Position can also be performed in the following locations:  * Alexandria, VA
St. Louis, MO
Tucson, AZ  Clearance Level Required: Top Secret/SCI with Polygraph Primary Responsibilities: * Interact with the customer Release Train Engineer, Product Owners, and stakeholders to analyze customer requirements and determine how to include requirements in the operational environment through COTS integration and new data sources being added to the Databricks cluster. * Perform the design and requirements analysis of data visualizations and COTS appliances for capability enhancements. * Coordinate with stakeholder and team to determine how to host new data capabilities within the operational environment and troubleshoot issues as the customer is integrating new data or creating new visualizations. * Identify software and hardware dependencies and capacities. Basic Qualifications: * US citizenship is required per contract. * Bachelor's degree and 8-12 years of prior relevant experience or Master’s with 6-10 years of prior relevant experience. * Experience with Databricks deployment, configuration, and support.
Significant experience in Cloud Technologies (specifically AWS).
Experience interacting with cross functional project teams including Software Development, Test, and Security.
Substantial experience working in Agile, SAFe, and Scrum environments.
Knowledge of Software Configuration Management life cycle deliverables. Preferred Qualifications: * Experience with Databricks deployment, configuration, and support.
AWS certifications.
Experience working with Machine Learning and knowledge of data science concepts.
Experience working with DevOps CI/CD related technologies and deployment automation techniques (e.g., Ansible, Git, Jenkins, Docker, Confluence, and Junit). This position has a target range for compensation of $130,000 - $150,000. This is a firm fixed price contract and does not allow for negotiations outside of this target range. Pay Range: Pay Range $101,400.00 - $183,300.00 The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. Original Posting Date: 12/11/2023 While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","Systems Engineering, Databricks, AWS, Cloud Technologies, Machine Learning, Data Science, DevOps, CI/CD, Ansible, Git, Jenkins, Docker, Confluence, Junit, SAFe, Scrum, Software Configuration Management","systems engineering, databricks, aws, cloud technologies, machine learning, data science, devops, cicd, ansible, git, jenkins, docker, confluence, junit, safe, scrum, software configuration management","ansible, aws, cicd, cloud technologies, confluence, data science, databricks, devops, docker, git, jenkins, junit, machine learning, safe, scrum, software configuration management, systems engineering"
Data Analyst,The Walt Disney Company,"Burbank, CA",https://www.linkedin.com/jobs/view/data-analyst-at-the-walt-disney-company-3744045615,2023-12-17,Burbank,United States,Mid senior,Onsite,"Department/Group Overview:
The Disney Music Group (DMG) is the music hub for The Walt Disney Company, encompassing all aspects of music commercialization and marketing: recorded music (Walt Disney Records and Hollywood Records), Disney Music Publishing, and Disney Concerts! DMG distributes music both physically and digitally, and also licenses music throughout the world in various forms of media including television, print, gaming, and consumer products.
Job Summary:
The
Data Analyst
supports data development, reporting and quantitative research, serving as a data analytics partner to cross-functional teams and providing insights to advise planning and decisions across the Disney Music Group business areas.
As a member of the Data & Analytics team, the Data Analyst will contribute to the development of analytics and tools that enable a deep understanding of user behaviors within music marketing, discovery, and consumptions channels.
The right person for this role has a deep curiosity and aptitude in data with applied knowledge of measurement and statistics. The Data Analyst will collaborate with team members to translate business questions into analytical and technical methodologies, driving results through analysis of large transaction level datasets. The role will need to communicate effectively, distilling insights into concise and compelling data-driven stories and socialize broadly with audiences across levels and functions.
Responsibilities and Duties of the Role:
Acquire, organize, cleanse and manipulate complex, large-scale, high-dimensional data from multiple disparate sources using tools such as PySpark and advance SQL
Build automated, scalable, repeatable, and integrated analysis using modern big data architectures in data ecosystems such as Databricks, Snowflake, Google Big Query
Apply advanced analytics techniques (data mining, statistical analysis, machine learning) to identify patterns and uncover opportunities in behavioral data
Conduct campaign, product, content, and audience analysis for company priorities. Develop and report on key metrics to measure success
Partner with Engineering and Product teams to execute on data projects, contributing to solution discovery, design, build and QA
Required Education, Experience/Skills/Training:
Basic Qualifications:
3+ years of analytical experience or equivalent combination of education and experience
Bachelor’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
Knowledge of supervised and unsupervised learning techniques
Proficient in using scripting tools such as PySpark/Python/R and SQL
Proficient in data exploration and visualization tools
Strong time management with ability to work in a fast-paced environment across multiple priorities
Preferred Qualifications:
Experience in applying supervised and unsupervised learning techniques, such as linear and logistic regression, decision trees, or k-means clustering to derive, validate, and quantify conclusions from the data
Experience documenting data requirements, rules and, assumptions
Experience data mining in distributed systems such as Databricks/Google Big Query/Snowflake
Experience manipulating large, transaction level datasets and interpreting data trends from multiple disparate data sources
A passion for music and/or entertainment industry data!
Familiarity with music streaming services (Spotify, Apple, Amazon Music, YouTube), social media platforms (TikTok, FB/Instagram) and new technologies (digital assistants/voice)
Master’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
The hiring range for this position in Burbank, CA is $93,398 to $125,290 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.
Show more
Show less","Data Analytics, Data Mining, Machine Learning, Natural Language Processing, Statistical Analysis, Business Intelligence, Big Data, Data Visualization, Cloud Computing, SQL, Python, PySpark, R, Databricks, Snowflake, Google BigQuery, Tableau, Power BI","data analytics, data mining, machine learning, natural language processing, statistical analysis, business intelligence, big data, data visualization, cloud computing, sql, python, pyspark, r, databricks, snowflake, google bigquery, tableau, power bi","big data, business intelligence, cloud computing, data mining, dataanalytics, databricks, google bigquery, machine learning, natural language processing, powerbi, python, r, snowflake, spark, sql, statistical analysis, tableau, visualization"
Sr. Data Engineer,Ontic,"Chatsworth, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-ontic-3779465207,2023-12-17,Burbank,United States,Mid senior,Onsite,"We are a fast-paced business with ambitious growth plans; so if you are dedicated, enthusiastic and always seeking ways to improve, you'll enjoy a career with us!
Job Purpose:
Data is Ontic’s biggest asset. We’re looking for someone who is passionate about data and who has the technical and soft skills to propel our data platform through an exciting period of growth. We are looking for someone with a unique blend of data platform governance and architecture, data and analytics engineering and data science. Somebody with the ability to communicate effectively across the board in a way that makes the complex seem simple.
As part of the Ontic Global Data Team, you will be responsible for designing, developing, and maintaining our data architecture and pipelines. You will also be responsible for analysing data across multiple systems, producing data models, identifying gaps in the data models, and making recommendation for improvements. Your primary focus will be on leveraging Snowflake, dbt, Fivetran, and Power BI to ensure our data is accessible, reliable, and actionable for stakeholders across the organization. In addition to technical proficiency, this role requires strong stakeholder engagement skills to collaborate effectively with teams across the company. This is a very exciting and varied role and therefore will best suit a self-starter willing to deeply understand the broader company objectives and how proposed data projects support them.
Main responsibilities:
Perform exploratory data analysis on a range of complex business problems.
Interpret business processes into data requirements.
Define and enforce key data attribute governance strategies.
Act as geographical data leader, reporting directly to the global director.
Collaborate on data team initiatives impacting Ontic’s global strategic pillars.
Administer and maintain the Snowflake Data Warehouse.
Contribute to data pipelines that clean, transform and aggregate data from disparate data sources into reporting data stores
Provide input on the company data strategy by suggesting and proving new technologies and innovations.
Contributing to the translation of data into valuable insights that inform data driven decisions.
Develop dashboards and reports to present data insights to stakeholders around the business.
Support end-user technical questions and assist in data education
Host on-site data quality clinics
Participate in and audit data governance activities
Knowledge, Skills And Experience
Proven experience of administering and working in a Data Warehouse (experience of Snowflake desirable)
Ability to influence a team of technical and non-technical stakeholders
Experience building data models and flows from various sources using Kimball Dimensional Modelling techniques.
Good knowledge of data warehousing and data modelling principles.
Experience of administering, creating, and maintaining data pipelines in an ELT application that cleans, transforms, and aggregates data from disparate data sources into reporting data stores.
Experience documenting ELT specifications such as source to target mappings, business rules and data validation requirements, ETL functional and non-functional requirements.
Experience of DBT
Strong SQL programming experience in a commercial setting.
Experience of using BI reporting tools (i.e. Power BI) with an ability to develop reports and dashboards.
Strong analytical and problem-solving skills with a keen attention to detail.
Excellent oral and written communication skills for translating data insights into concise executive summaries and presentations.
Ability to present findings in a structured, clear manner with actionable insights.
Possess ability to both multi-task and manage priorities effectively with the ability to work proactively and independently in meeting stakeholder needs whilst adhering to tight deadlines
Appreciation of data security, CCPA, GDPR and PII data handling principles.
A willingness to be utilised as a developer across other enterprise applications.
Experience of working in the Aerospace, Engineering, Manufacturing sectors
Experience with below toolset preferred:
Snowflake data warehouse
Dbt
FiveTran
Microsoft Fabric/Power BI Analytics
GitHub peer review
Kimball Dimensional Modelling
Dynamics 365 Backend
Infor Syteline and/or Infor Visual ERP Backend
SQL
Key Working Relationships:
A variety of key stakeholders including site and enterprise leadership teams
Geographically spread Global Data Team
External mix of strategic technology partners and niche providers.
Ontic is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Note: This job description is intended to convey information essential to understanding the scope of the position and is not an exhaustive list of skills, efforts, duties, responsibilities, or working conditions associated with it. The company may change responsibilities or requirements as the need arises.
Compensation Pay Range
$110-$130. Budgeted $130
Please click here to review Ontic's California Consumer Privacy Act policy.
Ontic Engineering and Manufacturing Inc. is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran
Status, or any other characteristic protected by applicable federal, state, or local law.
This position requires use of information which is subject to the International Traffic in Arms Regulations (ITAR). All applicants must be U.S. persons within the meaning of the ITAR. The ITAR defines a U.S. person as a U.S. Citizen, U.S. Permanent Resident (i.e. 'Green Card Holder'), Political Asylee, or Refugee.
Show more
Show less","Snowflake, dbt, Fivetran, Power BI, Data architecture, Data modeling, Data analysis, SQL, Kimball Dimensional Modelling, Data warehousing, Data pipelines, Data governance, Data security, CCPA, GDPR, PII, Cloud computing, Data visualization, Data cleaning, Data transformation, Data aggregation, Data reporting, Datadriven decision making, Stakeholder engagement, Communication, Problemsolving, Analytical thinking, Attention to detail, Multitasking, Prioritization, Time management, Microsoft Fabric","snowflake, dbt, fivetran, power bi, data architecture, data modeling, data analysis, sql, kimball dimensional modelling, data warehousing, data pipelines, data governance, data security, ccpa, gdpr, pii, cloud computing, data visualization, data cleaning, data transformation, data aggregation, data reporting, datadriven decision making, stakeholder engagement, communication, problemsolving, analytical thinking, attention to detail, multitasking, prioritization, time management, microsoft fabric","analytical thinking, attention to detail, ccpa, cloud computing, communication, data aggregation, data architecture, data cleaning, data governance, data reporting, data security, data transformation, dataanalytics, datadriven decision making, datamodeling, datapipeline, datawarehouse, dbt, fivetran, gdpr, kimball dimensional modelling, microsoft fabric, multitasking, pii, powerbi, prioritization, problemsolving, snowflake, sql, stakeholder engagement, time management, visualization"
Senior Data Engineer,Activision Blizzard,"Santa Monica, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-activision-blizzard-3772911047,2023-12-17,Burbank,United States,Mid senior,Onsite,"Job Title
Senior Data Engineer
Requisition ID
R022176
Job Description
Welcome to Solid State Studios. We’re a new in-house studio within Activision, dedicated to developing the best AAA mobile games in the world. Our first project is Call of Duty: Warzone for Mobile, and we’re looking for great talent, passionate about their work, who share our belief in what AAA experiences on mobile can and should be. We’re incredibly excited to build out our teams and get to work, and invite you to join us in making something special!
Our first project is Call of Duty Warzone for Mobile and we’re looking for great talent from mobile, console and PC backgrounds passionate about their work, who share our belief in what AAA experiences on mobile canand should be. We’re incredibly excited to build out our teams and get to work, and invite you to join us in making something special.
Your Profile
As a Sr. Data Engineer you will be responsible for the millions of events our game will emit each day.
You will collaborate closely with engineers to instrument events in the game code, work with data analysts to define the processing of data through ETLs into our data architecture, and be responsible for the infrastructure that automates reporting of business insights. You will work cross-title with other data teams to evolve the pipeline that processes events across a large-scale data footprint. You will lead the monitoring solutions used to ensure the availability and reliability of our data and our game, including the machine learning based alerting system for anomaly detection and identify a solution to detect slow moving trends.
As the senior member of the team, you will identify new ways to elevate our data systems, increase the self-serve nature of our data ecosystem, and minimize data and game outages. You will also mentor and guide other members of the team and set roadmaps and priorities.
Main Mission
Contribute to the solutions used for processing and storing of data, from code libraries to ETLs to Databricks Lakehouse, dashboards, and in-grown coding projects used for data analysis
Maintain and expand upon our ETL process, consuming data from third parties where needed, and ensure a clean data architecture
Work with multi-functional team members in defining and documenting single sources of truth to ensure consistent and high-quality data
Reconcile data issues and alerts between various systems, finding opportunities to innovate and drive improvements
Work in a highly collaborative team to devise solutions to business problems, bringing your skills and ideas into every discussion
Learn new technical skills when needed and use them to help the team achieve success
Collaborate with other Call of Duty teams to ensure the consistent and coherent growth of our data and tools capabilities
Assist the data scientist community, helping to analyse the results of AB tests and define predictive models
Minimum Requirements
MS/BS in Computer Science, Data Science, Systems Engineering, Applied Mathematics or equivalent experience
8+ years of experience as a Data Engineer or in a similar role
Expert-level knowledge of SQL
Experience with Python including common data science libraries (e.g. Pandas,NumPy, Jupyter, IPython)
Experience working with Google Cloud Platform OR Exposure to Big Data technologies like Databricks and/or BigQuery
Ability to build API integrations with our internal systems and third-party data sources
Strong communication skills with the ability to translate business needs into technical specifications
Highly collaborative work style, but not afraid to take on solo tasks
Curious and inquisitive mind-set
Bonus Objectives
Experience in solving complex data engineering problems at a really large scale
Familiarity with version control tools (Git commands) and basic understanding of containerization, build, and deployment processes
Experience with big data platforms technologies such as Kafka, Spark, Airflow, and others
Familiarity with live monitoring solutions like Anodot
We love hearing from anyone who is enthusiastic about changing the games
industry. Not sure you meet all qualifications? Let us decide! Research shows that women and members of other under-represented groups tend to not apply to jobs when they think they may not meet every qualification, when, in fact, they often do! At Activision Blizzard, we are committed to creating a diverse and inclusive environment and strongly encourage you to apply.
About Activision
Activision Blizzard, Inc. (NASDAQ: ATVI), is one of the world's largest and most
successful interactive entertainment companies and is at the intersection of media, technology and entertainment. We are home to some of the most beloved entertainment franchises including Call of Duty®, World of Warcraft®, Overwatch®, Diablo®, Candy Crush™ and Bubble Witch™. Our combined entertainment network delights hundreds of millions of monthly active users in 196 countries, making us the largest gaming network on the planet!
Our ability to build immersive and innovative worlds is only enhanced by diverse teams working in an inclusive environment. We aspire to have a culture where everyone can thrive in order to connect and engage the world through epic entertainment. We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered!
The videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as advised by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
Rewards
Requirements
We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered! Subject to eligibility requirements, the Company offers comprehensive benefits including:
Medical, dental, vision, health savings account or health reimbursement account, healthcare spending accounts, dependent care spending accounts, life and AD&D insurance, disability insurance;
401(k) with Company match, tuition reimbursement, charitable donation matching;
Paid holidays and vacation, paid sick time, floating holidays, compassion and bereavement leaves, parental leave;
Mental health & wellbeing programs, fitness programs, free and discounted games, and a variety of other voluntary benefit programs like supplemental life & disability, legal service, ID protection, rental insurance, and others;
If the Company requires that you move geographic locations for the job, then you may also be eligible for relocation assistance.
Eligibility to participate in these benefits may vary for part time and temporary full-time employees and interns with the Company. You can learn more by visiting https://www.benefitsforeveryworld.com/.
In the U.S., the standard base pay range for this role is $101,000.00 - $186,754.00 Annual. These values reflect the expected base pay range of new hires across all U.S. locations. Ultimately, your specific range and offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share this role’s range details for your local geography during the hiring process. In addition to a competitive base pay, employees in this role may be eligible for incentive compensation. Incentive compensation is not guaranteed.
Show more
Show less","Data Engineering, Data Analysis, ETL, Data Architecture, Business Intelligence, Machine Learning, Anomaly Detection, Data Science, Python, Pandas, NumPy, Jupyter, IPython, Google Cloud Platform, Databricks, BigQuery, SQL, API Integration, Git, Kafka, Spark, Airflow, Anodot","data engineering, data analysis, etl, data architecture, business intelligence, machine learning, anomaly detection, data science, python, pandas, numpy, jupyter, ipython, google cloud platform, databricks, bigquery, sql, api integration, git, kafka, spark, airflow, anodot","airflow, anodot, anomaly detection, api integration, bigquery, business intelligence, data architecture, data engineering, data science, dataanalytics, databricks, etl, git, google cloud platform, ipython, jupyter, kafka, machine learning, numpy, pandas, python, spark, sql"
Sr Data Analyst,The Walt Disney Company,"Burbank, CA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-the-walt-disney-company-3744050022,2023-12-17,Burbank,United States,Mid senior,Onsite,"Department/Group Overview:
The Disney Music Group (DMG) is the music hub for TWDC, encompassing all aspects of music commercialization and marketing: recorded music (Walt Disney Records and Hollywood Records), Disney Music Publishing, and Disney Concerts. DMG distributes music both physically and digitally, and also licenses music throughout the world in various forms of media including television, print, gaming, and consumer products.
Job Summary:
The
Sr. Data Analyst
supports data development, reporting and quantitative research, serving as a data analytics partner to cross-functional teams and providing insights to inform planning and decisions across the Disney Music Group business areas.
As a member of the Data & Analytics team, the Sr. Data Analyst will contribute to the development of analytics and tools that enable a deep understanding of user behaviors within music marketing, discovery, and consumptions channels.
The right person for this role has a deep curiosity and aptitude in data with applied knowledge of measurement and statistics. The role will be expected to translate business questions into analytical and technical methodologies, driving results through analysis of large transaction level datasets. The role will need to communicate effectively, distilling insights into concise and compelling data-driven stories and socialize broadly with audiences across levels and functions.
Responsibilities and Duties of the Role:
Develop analytical frameworks, defining the data, measurement, and analysis, to answer business questions across the Disney Music Group business areas, including recorded music, publishing and concerts
Acquire, organize, cleanse and manipulate complex, large-scale, high-dimensional data from multiple disparate sources utilizing tools such as PySpark and advance SQL
Build automated, scalable, repeatable, and integrated analysis using modern big data architectures in data ecosystems such as Databricks, Snowflake, Google Big Query
Apply advanced analytics techniques (data mining, statistical analysis, machine learning) to identify patterns and uncover opportunities in behavioral data
Conduct campaign, product, content, and audience analysis for company priorities. Develop and report on key metrics to measure success
Partner with Engineering and Product teams to execute on data projects, contributing to solution discovery, design, build and QA
Required Education, Experience/Skills/Training:
Basic Qualifications:
5+ years of analytical experience or equivalent combination of education and experience
Bachelor’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field
Experience delivering end-to-end analytics projects
Experience in applying supervised and unsupervised learning techniques, such as linear and logistic regression, decision trees, or k-means clustering to derive, validate, and quantify conclusions from the data
Experience documenting data requirements, rules and, assumptions
Proficient in using scripting tools such as PySpark/Python/R and SQL
Proficient in data exploration and visualization tools
Strong time management with ability to work in a fast-paced environment across multiple priorities
Preferred Qualifications:
Experience data mining in distributed systems such as Databricks/Google Big Query/Snowflake
Experience manipulating large, transaction level datasets and interpreting data trends from multiple disparate data sources
A passion for music and/or entertainment industry data
Familiarity with music streaming services (Spotify, Apple, Amazon Music, YouTube), social media platforms (TikTok, FB/Instagram) and emerging technologies (digital assistants/voice)
Master’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
The hiring range for this position in Burbank, CA is $112,586 to $151,030 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.
Show more
Show less","Data analysis, Machine learning, Statistics, PySpark, SQL, Data mining, Data visualization, Data science, Hadoop, Python, R, Data frameworks, Distributed systems, Big data, Data lakes, Data warehousing, Cloud computing, Agile methodologies, SCRUM, Kanban","data analysis, machine learning, statistics, pyspark, sql, data mining, data visualization, data science, hadoop, python, r, data frameworks, distributed systems, big data, data lakes, data warehousing, cloud computing, agile methodologies, scrum, kanban","agile methodologies, big data, cloud computing, data frameworks, data lakes, data mining, data science, dataanalytics, datawarehouse, distributed systems, hadoop, kanban, machine learning, python, r, scrum, spark, sql, statistics, visualization"
Senior Data Analyst,Revenue.io,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-revenue-io-3704074794,2023-12-17,Burbank,United States,Mid senior,Remote,"Revenue.io powers high-performing teams with real-time guidance. By surfacing and recommending what works best, Revenue.io enables customers like HPE, Nutanix, and Fidelity Investments to deliver predictable results and optimize their entire revenue operation.
Loved by customers, employees and analysts, Revenue.io is the only company included in the most recent Forrester Waves for Conversation Intelligence and Sales Engagement, is a Gartner Cool Vendor, and is named one of the ""Best Places to Work"" by BuiltinLA and Comparably.
Revenue.io offers talented candidates a chance to work with executive mentors in a rapid growth environment where teammates are passionate about our transformative technology, vibrant culture and collaborative mindset.
Who We're Looking For
Revenue.io is seeking an amazing, Senior Data Analyst to be a foundational piece of our Data & Analytics (D&A) team. As the first Senior Data Analyst member of the team, you will serve as a critical voice on our reporting and analytics strategy both in product and within Revenue.io. You will work with Leadership to develop a cascading metrics strategy, connecting the benefits our products deliver to how those benefits are realized in our product experiences. You will work with Product Engineering, Operations, and your fellow D&A team members to build and execute on this strategy, leveraging your deep expertise to create the right experiences that generate the most valuable and actionable insights to your customers.
What You'll Do
Collaborate with Leadership to develop a cascading metrics strategy, connecting the benefits our products deliver to how those benefits are realized/experienced within our products. You will leverage your deep statistical expertise to identify correlation and/or causation between experiences and benefits.
Partner with Product Engineering, Operations, and your fellow D&A team members to build and execute on a reporting and analytics strategy that supports our metrics strategy, leveraging your deep expertise to design and create the right experiences in our products that generate the most valuable and actionable insights to your customers
Collaborate with Chief Product Officer and Chief Delivery Officer to develop our product metrics approach, leveraging Pendo, Redshift, and other analytical tools as necessary
Conduct analysis to drive valuable business insights, using internal analytics tools, i.e., SQL to access and manipulate from disparate data sources
Build intuitive data visualizations to tell stories with data that convey key performance metrics, significant trends, etc.
Assist in the design and analysis of A/B tests to improve the user journey in our products
Support others in the development of sales funnel analysis, lifetime value analysis, customer segmentation and forecasting
Be a proactive and amazing teammate!
Qualifications
5+ years of relevant work experience in analytics, data centric role and/or business intelligence related field
Significant analytical skills; background in statistics and/or data science strongly preferred
Strong, demonstrable experience with data visualization tools (Tableau, Powerbi, etc.) and strong storytelling skills for technical and non-technical audiences
Advanced skills in SQL and SQL platforms
Experience in product analytics/telemetry and related platforms - e.g., Amplitude, Pendo, Google Analytics - a strong plus
Experience with prescriptive and predictive analytics a plus
An excellent communicator, with the ability to simplify key messages, present compelling stories and promote technical and personal credibility with internal and external stakeholders
Passion for teamwork and collaboration, adaptability, communication, problem-solving, customer focus, results, and innovation
Compensation
Additional incentive compensation may be offered as stock options and company benefits. Actual compensation within the range will be determined on your skills, experience, qualification, location, and market conditions. As an equal opportunity employer, we are committed to providing a fair and transparent workplace for all employees and applicants.
Salary Range
$120,000—$150,000 USD
Company Benefits Include
Paid parental leave
Flexible time off (US only)
Competitive salary
Multiple medical plans to choose from including HSA and FSA
Work from home flexibility
Anti-Discrimination
We consider applicants without regards to race, color, national origin, sex, age, religion, sexual orientation, gender identity, veteran status, marital status, physical or mental disability, or other protected classes under all local, state, and federal laws and ordinances (AA/EOE/W/M/Vet/Disabled).
California Applicants Only
What Personal Information We Collect
Professional, employment-related, or schooling information. Current or past job history, performance evaluations, and educational background, including grades and transcripts.
How We Use Your Information
For professional, internal analysis, or employment-related purposes, including job applications. all applicants are subject to our Employment Privacy Notice and Global privacy policy.
Global Data Privacy Notice for Job Candidates
Please follow this link to access the document that provides transparency around the way in which Revenue.io handles personal data of employees and job applicants: https://www.revenue.io/privacy-notice-employment
Show more
Show less","Tableau, PowerBI, SQL, Data Analytics, Data Visualization, Data Science, Predictive Analytics, Prescriptive Analytics, Amplitude, Pendo, Google Analytics, Statistics, A/B Testing, Data Science","tableau, powerbi, sql, data analytics, data visualization, data science, predictive analytics, prescriptive analytics, amplitude, pendo, google analytics, statistics, ab testing, data science","ab testing, amplitude, data science, dataanalytics, google analytics, pendo, powerbi, predictive analytics, prescriptive analytics, sql, statistics, tableau, visualization"
Lead Data Scientist (Customer Analytics),Tiger Analytics,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/lead-data-scientist-customer-analytics-at-tiger-analytics-3775698470,2023-12-17,Langley, Canada,Mid senior,Remote,"Tiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce.
We are also market leaders in AI and analytics consulting in the CPG & retail industry with over 40% of our revenues coming from the sector. This is our fastest-growing sector, and we are beefing up our talent in the space.
We are looking for a Lead Data Scientist with a good blend of data analytics background, who holds solid knowledge of Personalization, and Web/Mobile Analytics, and Customer Analytics, quick learner, and has strong coding capabilities to add to our team.
Key Responsibilities
Work on the latest applications of data science to solve business problems in the Customer Analytics space of Retail, in particular omnichannel retailing
Effectively communicate the analytics approach and how it will meet and address objectives to business partners
Lead data analytic and modeling approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts, and developers
Create repeatable, interpretable, dynamic, and scalable models seamlessly incorporated into analytic data products
Collaborate, coach, and learn with a growing team of experienced Data Scientists
Stay connected with external sources of ideas through conferences and community engagements
Support demands from regulators, investor relations, etc., to develop innovative solutions to meet objectives utilizing cutting-edge techniques and tools
Requirements
>7 years of Data Science experience required
Graduate Degree in Data Science, Computer Science, or a related field is required
Deep Knowledge and Understanding of Personalization, Web/Mobile Analytics, and Customer Analytics
Strong python coding with production experience is preferred, MLops knowledge and experience is plus
At least 4 years of experience in CPG and Retail space
Ability to apply various analytical models to business use cases
Exceptional communication and collaboration skills to understand business partner needs and deliver solutions
Bias for action, with the ability to deliver outstanding results through task prioritization and time management
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Python, MLops, Data science, Personalization, Web Analytics, Mobile Analytics, Customer Analytics, Data Engineering, Statistical Modeling, Data Products, Business Intelligence, Communication, Collaboration, Analytical Models, CPG, Retail","python, mlops, data science, personalization, web analytics, mobile analytics, customer analytics, data engineering, statistical modeling, data products, business intelligence, communication, collaboration, analytical models, cpg, retail","analytical models, business intelligence, collaboration, communication, cpg, customer analytics, data engineering, data products, data science, mlops, mobile analytics, personalization, python, retail, statistical modeling, web analytics"
Staff Data Analyst (Product Analytics),Plenty of Fish,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/staff-data-analyst-product-analytics-at-plenty-of-fish-3779921665,2023-12-17,Langley, Canada,Mid senior,Hybrid,"Founded in Vancouver, Canada in 2003, Plenty of Fish is one of the early pioneers in the online dating industry, with one of the largest and most diverse communities of singles. We're one of the top revenue-driving brands in the Match Group (comprised of Tinder, OkCupid, Hinge and Match.com).
We love what we do, and have the ability to profoundly impact millions of people's lives every single day! Named one of BC’s Top Employers, Plenty of Fish is a great place to build friendships, grow your career and collaborate with top talent.
We work hybrid at Plenty of Fish - IRL in our downtown Vancouver office Monday, Tuesday, Wednesday and work remotely the remainder of the week!
We are looking for someone to be a part of our outstanding Product Analytics team, who will be working cross-functionally with the rest of the organization to help identify, understand, and improve business and product challenges facing Plenty of Fish. The Senior Product Analyst will deliver high-quality strategic and tactical analysis; collaborating with the product, engineering, data science, business intelligence, customer service, and marketing teams.
So, what will you do?
Provide a variety of stakeholders from Executives, Product Managers, Engineering Managers, or other Data Analysts, with deep-dive quantitative analyses and present them in an immediately usable format, extracting key insights from extensive, complex data sets
Proactively find opportunities to take analytics framework into the next level and lead stakeholders to adopt new frameworks and methodologies
Quantitatively analyze user behaviour to help determine product strategy and drive business decisions, including driving decisions from A/B test learnings
Take ownership of ambiguous and complex analytical assignments
You'll be a match for this role if you have..
5+ years of progressive experience using quantitative analysis to make business-focused recommendations
Strong communication skills at different levels of organizational hierarchy
Strong understanding of incrementally as well as sizing business problems and opportunities
Proven technical data analytics skills in working with large amounts of data at different levels of aggregation, from raw telemetry to metric stores.
Experience with raw clickstream data (telemetry event data) and datasets of 100 million+ records
Experience with web/digital analytics metrics (Daily Active Users, Retention, Churn)
Understanding of basic statistical concepts such as confidence interval, normal distribution, correlation, linear regression, and multivariate testing
Proficiency with SQL or other query languages
Strong experience with A/B test analysis
In depth experience with data visualization and analytical tools (E.g. Tableau, PowerBI, Looker)
Proactive skill set that encompasses problem identification, analysis, solution definition, results, and communication
Experience with Python or a willingness to learn
$135,000 - $145,000 a year
Factors such as scope and responsibilities of the position, candidate's work experience, education/training, job-related skills, internal peer equity, as well as market and business considerations may influence base pay offered. This salary range is reflective of a position based in Vancouver, British Columbia.
We would love to hear from you, even if you don't match 100% of the requirements**
Why Plenty of Fish?
We're recognized as one of BC's Top Employers 2023 !
Generous vacation, flex days, professional development days
RRSP matching, and employee stock purchase plan
Professional development budget and unlimited access to Udemy from day one
Match Group mentorship program
Parental leave top up and fertility preservation benefits
Extended health & dental benefits from day one
Corporate ClassPass membership and other wellness benefits
And many more on our careers page
Our Values
Be Proud - We own our ship. We see challenges as opportunities and take action.
Make Waves - We profoundly impact millions of peoples' lives, every day.
Dive Deep - We empathize with our members and use data to surface thoughtful decisions.
Crew Together - We're all working towards the same goals and win as a team.
Want To Dive Deeper?
LinkedIn
Glassdoor
We’re committed to creating an equal and inclusive environment; we welcome all crew (and prospective crew) members regardless of race, colour, ancestry, place of origin, political belief, religion, marital status, family status, physical or mental disability, sex, sexual orientation, gender identity or expression, age, conviction unrelated to employment, or any other prohibited ground of discrimination recognized by applicable law. Plenty of Fish is proud to be an equal opportunity workplace.
Show more
Show less","SQL, Tableau, PowerBI, Looker, Python, A/B testing, User behavior analytics, Product strategy, Business intelligence, Statistics, Data visualization, Data analysis, Machine learning, Data science, Raw telemetry data, Web analytics, Digital analytics, Multivariate testing","sql, tableau, powerbi, looker, python, ab testing, user behavior analytics, product strategy, business intelligence, statistics, data visualization, data analysis, machine learning, data science, raw telemetry data, web analytics, digital analytics, multivariate testing","ab testing, business intelligence, data science, dataanalytics, digital analytics, looker, machine learning, multivariate testing, powerbi, product strategy, python, raw telemetry data, sql, statistics, tableau, user behavior analytics, visualization, web analytics"
"Business Analyst, Data & Analytics - BC Cancer - Vancouver",BC Cancer,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/business-analyst-data-analytics-bc-cancer-vancouver-at-bc-cancer-3787517399,2023-12-17,Langley, Canada,Mid senior,Hybrid,"Business Analyst, Data & Analytics
BC Cancer
Vancouver, BC
We are seeking an experienced Business Analyst to join our team and be a key player in supporting analytics and reporting needs related to British Columbia’s 10-year cancer action plan; transforming complex data into actionable insights and reports. In this role, you will help define metrics, develop data collection methodologies, create visually compelling reports/dashboards, and deliver data-driven presentations drawing from your prior experience and proficiency in key developing performance indicators (KPI) , data analysis, visualization, and reporting using tools such as Power BI.
What You’ll Do
Develop and produces program evaluation performance metrics and tools to provide accurate and high quality analysis of key indicators, including outcomes, quality and safety, data standards, in ensuring continuous quality improvement for BC Cancer.
Ensure internal, external and public reporting needs are met by gathering and negotiating reporting requirements from customers and stakeholders, communicating requirements to technical resources and following up with stakeholders regularly to develop and implement an examination process for the final reports.
Analyze operational problems and recommend innovative solutions by critically evaluating information gathered from multiple sources, reconciling conflicts, decomposing high-level information into details, abstracting up from low-level information to a general understanding, distinguishing user requests from the underlying true needs, and driving and challenging business assumptions.
Work in collaboration with program leaders and provincial partners to monitor program plans, measure progress towards goals and objectives, and identify the key factors that account for any deviation from stated goals and objectives.
Identify issues with complex data and takes initiative in working out optimal solutions with appropriate stakeholders to improve accuracy, timelines, coordination and other data issues.
Develop reports, dashboards, presentations and briefing documents for multiple internal and external stakeholders.
Provide health planning, program development, and project management support for new initiatives and business cases. Provide leadership in the development of processes to analyze the functionality and effectiveness of program delivery and recommend improvements.
Keep up-to-date about new initiatives, developments, trends and best practice in evaluation and monitoring through literature reviews and contact with key stakeholders from other organizations. Communicate information and make recommendations to program leaders.
Qualifications
What you bring
A level of education, training and experience equivalent to a Master’s degree in Business, Analytics, Health Informatics, Health Sciences, or related discipline plus a minimum of five (5) years of recent experience in complex data analysis, health evaluation, project/program management, preferably in a large, multi-site health care environment.
You will also have
Demonstrated ability to perform accurate advanced data manipulation, analysis, and information presentation using Microsoft Office suite, database and statistical software.
Knowledge and experience in project management, strategic planning and project implementation.
Strong verbal and written communication skills.
Demonstrated ability in preparing professional reports and presentation materials.
Ability to work under pressure and multi-task without supervision.
Ability to communicate complex and technical ideas in simple language.
Demonstrated ability to exercise tact, good judgment and initiative.
Commitment to upholding the shared responsibility of creating lasting and meaningful reconciliation in Canada as per TRC (2015) and BC's Declaration on the Rights of Indigenous Peoples Act (2019).
Knowledge of social, economic, political and historical realities impacting indigenous communities and familiarity with Indigenous Cultural Safety and anti-racism and accompanying reports (BC DRIPA, TRC, etc.).
What We Bring
Every PHSA employee enables the best possible patient care for our patients and their families. Whether you are providing direct care, conducting research, or making it possible for others to do their work, you impact the lives of British Columbians today and in the future. That’s why we’re focused on your care too – offering health, wellness, development programs to support you – at work and at home.
Join one of BC’s largest employers with province-wide programs, services and operations – offering vast opportunities for growth, development, and recognition programs that honour the commitment and contribution of all employees.
Access to professional development opportunities through our in-house training programs, including +2,000 courses, such as our San’yas Indigenous Cultural Safety Training course, or Core Linx for Leadership roles.
Enjoy a comprehensive benefits package, including municipal pension plan, and psychological health & safety programs and holistic wellness resources.
Annual statutory holidays (13) with generous vacation entitlement and accruement.
PHSA is a remote work friendly employer, welcoming flexible work options to support our people (eligibility may vary, depending on position).
Access to WorkPerks, a premium discount program offering a wide range of local and national discounts on electronics, entertainment, dining, travel, wellness, apparel, and more.
Job Type:
Regular, Full-Time
Salary range:
$72,445 - $104,139 /year.
The starting salary for this position would be determined with consideration of the successful candidate’s relevant education and experience, and would be in alignment with the provincial compensation reference plan. Salary will be prorated accordingly for part time roles.
Location:
601 West Broadway, Vancouver BC
,
V5Z 4C2
Applications will be accepted until position is filled.
Hours of Work:
Monday – Friday, 0830 – 1630
Requisition #
157364E
As per the current Public Health Order, full vaccination against COVID-19 is a condition of employment with PHSA as of October 26, 2021.
What We Do
BC Cancer provides comprehensive cancer control for the people of British Columbia.
BCC is part of the Provincial Health Services Authority (PHSA).
The Provincial Health Services Authority (PHSA) plans, manages and evaluates specialized health services with the BC health authorities to provide equitable and cost-effective health care for people throughout the province. Our values reflect our commitment to excellence and include: Respect people – Be compassionate – Dare to innovate – Cultivate partnerships – Serve with purpose. Learn more about PHSA and our programs: jobs.phsa.ca/programs-and-services
PHSA and BCC are committed to employment equity, encouraging all qualified individuals to apply. We recognize that our ability to provide the best care for our diverse patient populations relies on a rich diversity of skills, knowledge, background and experience, and value a safe, inclusive and welcoming environment.
Reconciliation is an ongoing process and a shared responsibility for all of us. The BC Governments’ unanimous passage of the Declaration on the Rights of Indigenous Peoples Act was a significant step forward in this journey—one that all health authorities are expected to support as we work in cooperation with Indigenous Peoples to establish a clear and sustainable path to lasting reconciliation. True reconciliation will take time and ongoing commitment to work with Indigenous Peoples as they move toward self-determination. Guiding these efforts Crown agencies must remain focused on creating opportunities that implement the Truth and Reconciliation Commission Mandate.
ATTN: PHSA Employees:
To be considered as a PHSA employee (internal applicant) for this position,
you must apply online via your internal profile at
http://internaljobs.phsa.ca
Please note the internal job posting will no longer be accessible after the expiry date of
December 22, 2023
. If the internal job posting has expired, please contact the Internal Jobs Help Desk and advise that you would like to be considered as a late internal applicant for this position.
Please do not apply for the external job posting.
If you have not registered your internal profile, a password is required to log in for the first time. To obtain your password, please contact the
Internal Jobs Help Desk at 604-875-7264 or 1-855-875-7264
. Please note regular business hours are Monday – Friday (excluding stats), 8:30am to 4:30pm. For inquiries outside of regular business hours, please email the Internal Jobs Help Desk at internaljobshelpu@phsa.ca and a Help Desk Representative will contact you the next business day.
Show more
Show less","Microsoft Office Suite, Database software, Statistical Software, Power BI, Data analysis, KPI (key performance indicators), Data visualization, Reporting, Program evaluation, Project management, Strategic planning, Communication, Presentation skills, Public speaking, Tact, Initiative, Reconciliation, Indigenous Cultural Safety, Antiracism, Social economic political and historical realities impacting indigenous communities","microsoft office suite, database software, statistical software, power bi, data analysis, kpi key performance indicators, data visualization, reporting, program evaluation, project management, strategic planning, communication, presentation skills, public speaking, tact, initiative, reconciliation, indigenous cultural safety, antiracism, social economic political and historical realities impacting indigenous communities","antiracism, communication, dataanalytics, database software, indigenous cultural safety, initiative, kpi key performance indicators, microsoft office suite, powerbi, presentation skills, program evaluation, project management, public speaking, reconciliation, reporting, social economic political and historical realities impacting indigenous communities, statistical software, strategic planning, tact, visualization"
Data Scientist,Pragmatic,"Sedgefield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-pragmatic-3780100312,2023-12-17,Teesside, United Kingdom,Associate,Hybrid,"Overview
Advising on or leading analytical projects (including data infrastructure) to enable better capture, understanding, analysis, modelling and reporting/visualization of data – providing actionable insights to stakeholders and decision makers.
Contributing across multi-functional teams, you will promote and drive a data-centric approach to support and enable analysis across operations and production, as well as other key aspects within the business.
Key tasks
Collaborate with engineers and scientists to support or lead key analysis projects into the complex nature of our manufacturing process
Proactively lead projects which capture new data in structured systems: ETL, data wrangling, cleaning and processing – mindful of the impact of clean and structured data systems on analysis
To contribute to a clean and robust code base, using version control, and enabling all members to work collaboratively
Prepare reports/visualisations/dashboards to capture learnings or effectively share/communicate outputs for wider consumption, to support decision making
Work with various technical/cross functional teams to develop data pipelines and dashboard/reports for stakeholder consumption, enabling improved visibility of their KPIs
To proactively suggest case-studies for analyses and modelling; using your interactions with stakeholders (engineers, scientists) look for and pursue opportunities to find correlations, patterns, signals, signatures; and work towards developing models for a production environments
Work with data and ML engineers to enable productionalisation of any relevant predictive models, to support and optimise process development and drive operations
Assist in training team members to enhance their data literacy and promote a data-driven culture within the company
Qualifications and training
Bachelors in STEM discipline and experience in a relevant role within manufacturing
OR Higher degree (masters/PhD) in STEM discipline + minimum 3 years working experience as part of data team
Desirable - Cloud certification (AWS or ideally MS Azure)
Skills and experience
Excellent communication skills (written and spoken) to enable dialogue with stakeholders, requirements gathering and sharing outputs
Highly analytical mindset and personal drive to understand and work to solve problems
Strong Python (or R) & SQL programming knowledge
Good working knowledge of a wide range of statistical techniques and the various modelling approaches esp. dimensionality reduction, supervised/unsupervised, ensemble methods and optimisation strategies
Significant exposure to the standard python DS and visualisation libraries (pandas, numpy, Sklearn, matplotlib/seaborn + Plotly) – or similar in R
Ability to convert exploratory code into robust, functioning, documented modules/libraries for sharing across the team
Knowledge and working experience with Tableau for dashboard design and reporting
Show more
Show less","Data Analysis, Data Visualization, Data Modeling, Data Wrangling, Data Cleaning, Data Processing, ETL, Python, R, SQL, Statistics, Machine Learning, AWS, Azure, Tableau, Dimensionality Reduction, Supervised Learning, Unsupervised Learning, Ensemble Methods, Optimization Strategies, Pandas, NumPy, ScikitLearn, Matplotlib, Seaborn, Plotly","data analysis, data visualization, data modeling, data wrangling, data cleaning, data processing, etl, python, r, sql, statistics, machine learning, aws, azure, tableau, dimensionality reduction, supervised learning, unsupervised learning, ensemble methods, optimization strategies, pandas, numpy, scikitlearn, matplotlib, seaborn, plotly","aws, azure, data cleaning, data processing, data wrangling, dataanalytics, datamodeling, dimensionality reduction, ensemble methods, etl, machine learning, matplotlib, numpy, optimization strategies, pandas, plotly, python, r, scikitlearn, seaborn, sql, statistics, supervised learning, tableau, unsupervised learning, visualization"
Senior Data Engineer,TUI,"Thornaby-on-Tees, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-tui-3778633795,2023-12-17,Teesside, United Kingdom,Mid senior,Hybrid,"It's an exciting time for First Choice as the business looks ahead to a bright future with a bold new strategy. We are on the search for motivated, talented people with a curious outlook, energetic mind-set and can do attitude to join us and help bring the brand to life for our colleagues,customers and collaborators.
We are looking for a talented and dedicated technical enthusiast to join the FirstChoice technology team which Build, Run and Maintain multiple holiday e-commerce websites.
The Senior Data Engineer is a practitioner and an advocate of state-of-the-art data engineering practices. Working in an agile environment and keeping up with the ever-evolving technical landscape the Senior Data Engineer is a lifelong learner and likes to think outside the box.
Please note the closing date for applications is: Sunday 7th January
About The Job
As a Senior Data Engineer, you are a go-to person working as part of a cross-functional team that enables data engineering skills and capabilities across the FirstChoice platform and websites. Being an enthusiast in data engineering, with an advanced DevSecOps mindset, you’ll use your excellent collaboration skills to work with your team to deliver the best answers to our customers’ needs with full responsibility for its data flows, from design to operation. You care diligently about the quality of your work, including proper documentation and security aspects.
You will use your deep technical skills to work closely with your colleagues to ensure an optimal data delivery architecture which is consistent throughout ongoing highly complex projects. You will work on data collection, data pipelines, data delivery to other systems. With your advanced skillset for understanding and solving problems you are able to take full ownership of complex topics or multi-faceted initiatives and outcomes panning across your domain.
You are able to verbalise your thoughts and ideas and take the initiative to translate ideas into outcomes. Together with the domain’s teams as well as the Group Enabler teams you will research, evaluate and test new approaches, processes and tools and help teams to use them effectively. You take part in Communities of Practice, including collaboration in shared initiatives to grow your experience. You regularly coach, guide and develop more junior colleagues.
You love to work in an international, multi-cultural team. You challenge constructively and have high expectations of yourself and others. You always drive for technical excellence, ownership and self-organisation at team and personal level.  You love to learn and acquire new skills and keep up to date with latest developments in your focus areas.
Security is part of everyone’s job. At TUI, we practise secure behaviours first in everything we do.
About You
Experienced in design, implementation and use of different database structures
Outstanding SQL and data profiling skills and experience with building ETL/ELT pipelines (e.g. SSIS)
Strong hands-on experience with modern data storage systems (e.g. AWS RDS, Aurora, AWS S3, DynamoDB, ElastiCache, SOLR and NoSQL)
On the spot conversion of high-level business & technical requirements into technical specs
Customer centric, passionate about delivering great digital products and services
Passionate about continuous improvement, collaboration, and great teams
Compelling coaching and training skills for imparting professional software engineering skills
About Our Offer
Working in the leading global tourism group: We stand for intercultural cooperation and offer the opportunity to work in international projects and teams.
Fantastic holiday benefits including discounts, special offers
Mobile working, flexible working hours and working from abroad: We believe that work is something you do, not where you go. Our offer: TUI Way of Working
Health and Wellbeing support in five key areas – Health, Social, Community, Career and Financial
Development and career opportunities: We offer a wide range of digital training and international career opportunities.
Additional benefits relevant to the local market that you'll be based in
We love to see your uniqueness shine through and inspire the future of travel. If you would like to read more about what Diversity & Inclusion means to us simply visit Our DNA
If you have any questions, please contact the Recruiter for this role via the contact information included in the advert.
Show more
Show less","Data Engineering, Databases, SQL, Data Profiling, ETL/ELT Pipelines, SSIS, Data Storage Systems, AWS RDS, Aurora, AWS S3, DynamoDB, ElastiCache, SOLR, NoSQL, Business Requirements, Technical Specifications, Customer Centricity, Digital Products and Services, Continuous Improvement, Collaboration, Coaching, Training, Software Engineering","data engineering, databases, sql, data profiling, etlelt pipelines, ssis, data storage systems, aws rds, aurora, aws s3, dynamodb, elasticache, solr, nosql, business requirements, technical specifications, customer centricity, digital products and services, continuous improvement, collaboration, coaching, training, software engineering","aurora, aws rds, aws s3, business requirements, coaching, collaboration, continuous improvement, customer centricity, data engineering, data profiling, data storage systems, databases, digital products and services, dynamodb, elasticache, etlelt pipelines, nosql, software engineering, solr, sql, ssis, technical specifications, training"
Systems and Data Analyst,Tempting Recruitment,"Darlington, England, United Kingdom",https://uk.linkedin.com/jobs/view/systems-and-data-analyst-at-tempting-recruitment-3779694546,2023-12-17,Teesside, United Kingdom,Mid senior,Hybrid,"Systems and Data Analyst in Durham- Exciting Career Prospect
Are you a skilled and passionate professional eager to play a key role in a local authority as a Systems and Data Analyst?
Would you like to contribute to a dynamic local authority recognized for its empowering work culture and meaningful initiatives?
If so, we have an excellent opportunity just for you!
About Us
Tempting Recruitment is delighted to present a compelling opportunity with our respected client - a prominent council. We are currently seeking a dedicated Systems and Data Analyst to join their dynamic team.
The Position
The duties you will be performing as part of your role:
Provided high-quality data for Community Safety service by managing and developing ICT software systems, supporting service development, and conducting research.
Gathered, analyzed, and reported relevant community safety data.
Maintained the list of KPIs and PIs for the service.
Developed and managed ICT software systems within Community Safety.
Collaborated with public, private, and voluntary sectors, as well as colleagues, to gather and analyze data supporting efforts to ensure Darlington is a safe place.
Developed and produced reports on service performance as required.
Identified opportunities for data quality improvement without compromising effectiveness.
Presented analysis in an accessible manner to relevant colleagues and partners.
Conducted research, analyzed data, and provided commentary as needed.
Maintained an overview of relevant legislation related to Community Safety.
Produced reports on both planned and ad-hoc bases as required by senior managers.
About You
To be successful in this role, you should possess the following qualifications and skills:
Qualification in a relevant field, educated to degree level or equivalent.
Approximately 2 years experience in data analysis and reporting for diverse audiences.
Experience in developing, implementing, monitoring, and reviewing strategies and policies.
Proven ability to provide comprehensive and concise written reports.
Around 4 years of experience in collating data with a focus on maintaining data quality consistency.
Proficiency in using software packages to input, extract, interpret, and process data for accurate information and detailed reports.
Understanding of community safety principles.
Experience in administering performance management systems.
Familiarity with public service partnership arrangements.
Knowledge of data-sharing protocols and their implementation.
Awareness of data protection regulations.
Flexible approach to working hours to meet business requirements.
Strong communication skills, capable of providing advice in accurate spoken English.
Contract Duration:
3 - 6 Months (likely extension)
Hourly Rate:
£17.12 per Hour (Umbrella)
Commencement Date:
December 2023
Work Hours: Hybrid!
Monday - Friday, 9 am to 5 pm
How To Apply
If you are interested in applying please press the button below, send your CV to desean.pierre-dwyer@temptingrecruitment.co.uk or contact Desean on 02038544913 to discuss further.
To explore other exciting opportunities or learn more about our recruitment process, please visit our website https://www.temptingrecruitment.co.uk or contact Desean on 02038544913.
Show more
Show less","Data Analysis, Data Reporting, ICT Software Systems, Community Safety, KPI and PI Management, Data Quality Improvement, Legislation Maintenance, Report Writing, Data Interpretation and Processing, Performance Management Systems, Public Service Partnership Arrangements, DataSharing Protocols, Data Protection Regulations, English Communication","data analysis, data reporting, ict software systems, community safety, kpi and pi management, data quality improvement, legislation maintenance, report writing, data interpretation and processing, performance management systems, public service partnership arrangements, datasharing protocols, data protection regulations, english communication","community safety, data interpretation and processing, data protection regulations, data quality improvement, data reporting, dataanalytics, datasharing protocols, english communication, ict software systems, kpi and pi management, legislation maintenance, performance management systems, public service partnership arrangements, report writing"
Data Center Engineer,Cloudflare,"Denver, CO",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732380839,2023-12-17,Boulder,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, Jira, Juniper, Cisco, DWDM, Data Center, Networking, Automation, Process Improvement, Incident Management, Remote Contractors, Data Center Operations, Project Management, MS Excel, Google Spreadsheets, Documentation, SQL, Networking, Physical Security, Data Analysis, Server Technologies, RHCSA, CCNA, JNCIA","linux, jira, juniper, cisco, dwdm, data center, networking, automation, process improvement, incident management, remote contractors, data center operations, project management, ms excel, google spreadsheets, documentation, sql, networking, physical security, data analysis, server technologies, rhcsa, ccna, jncia","automation, ccna, cisco, data center, data center operations, dataanalytics, documentation, dwdm, google spreadsheets, incident management, jira, jncia, juniper, linux, ms excel, networking, physical security, process improvement, project management, remote contractors, rhcsa, server technologies, sql"
Big Data Developer,Epsilon,"Westminster, CO",https://www.linkedin.com/jobs/view/big-data-developer-at-epsilon-3782272099,2023-12-17,Boulder,United States,Associate,Onsite,"Job Description
The Data Engineer position will focus on designing, developing, and supporting our Hadoop data solutions in Spark and Python (PySpark) while working with other components of the Hadoop ecosystem such as HDFS, Hive, Hue, Impala, Zeppelin, Jupyter. A successful candidate will work closely with business and portfolio leads to understand requirements then design and build innovative data solutions.
Job Duties & Responsibilities.
Design and development centered around PySpark, Python and Hadoop Framework.
Working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Provide effective solutions to address the business problems – strategic and tactical.
Collaboration with team members, project managers, business analysts and QA teams in conceptualizing, estimating and developing new solutions and enhancements.
Work closely with the stake holders to define and refine the big data platform to achieve sales, product, and strategic objectives.
Collaborate with other technology teams and architects to define and develop cross-function technology stack interactions.
Read, extract, transform, stage and load (ETL) data to multiple targets, including Hadoop and Oracle.
Ingest and streamline incoming files of various layouts/formats as part of Source Prep process.
Develop scripts around Hadoop framework to automate processes and existing flows.
Modify existing programming/code for new requirements.
Estimate work, and track progress through SDLC with JIRA/Confluence
Unit testing and debugging. Perform root cause analysis (RCA) for any failed processes.
Convert business requirements into technical design specifications and execute on them.
Participate in code reviews and keep applications/code base in sync with version control (GIT/Bitbucket).
Effective communication, self-motivation, and ability to work independently while remaining fully aligned within a distributed team environment.
Required Skills
Bachelor’s or Master’s degree in Computer science (or Engineering equivalent).
3+ years of experience with big data ingestion, transformation and staging.
Analysis, design and implementation experience with Hadoop distributed frameworks, including Python & Spark (SparkSQL, PySpark), HDFS, Hive, Impala, Hue, Cloudera Hadoop, Zeppelin, Jupyter, etc.
Extensive experience handling large volumes of data (measured in Terabytes/Billions of Transactions)
Proficient knowledge of SQL with any RDBMS
Familiarity with RDD and Data Frames within Spark
Working knowledge of data analytics
Troubleshooting and complex problem-solving skills
Knowledge of Oracle databases and PL/SQL
Working knowledge of Linux/Unix environments and comfort with Unix Shell scripts (ksh, bash)
Basic Hadoop administration knowledge.
DevOps Knowledge is an advantage
Ability to work within deadlines and effectively prioritize and execute on tasks
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels
Preferred Skills
Working knowledge of Oracle databases and PL/SQL.
Hadoop Admin & Dev-Ops.
ETL Skills (Familiarity with Talend or other ETL tools a plus.
Good analytical thinking and problem-solving skills.
Ability to diagnose and troubleshoot problems quickly.
Motivated to learn new technologies, applications, and domains.
Possess appetite for learning through exploration and reverse engineering.
Strong time management skills.
Ability to take full ownership of tasks and projects.
Team player with excellent interpersonal skills.
Good verbal and written communication.
Possess Can-Do attitude to overcome any kind of challenges.
Preferred Certifications (Any Of These)
CCA Spark and Hadoop Developer.
MapR Certified Spark Developer (MCSD).
MapR Certified Hadoop Developer (MCHD).
HDP Certified Apache Spark Developer.
HDP Certified Developer.
Salary Range $95,000.00 - $100,000.00/year
Additional Information
About Epsilon
Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.
When you’re one of us, you get to run with the best.
For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC
Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
Epsilon is an Equal Opportunity Employer.
Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.
REF216743S
Show more
Show less","Hadoop, Spark, Python (PySpark), HDFS, Hive, Hue, Impala, Zeppelin, Jupyter, SQL, NoSQL, Data Analytics, Big Data, RDD, Data Frames, Hadoop, DevOps, Linux, Unix, Unix Shell scripts (ksh bash), PL/SQL, ETL, CCA Spark and Hadoop Developer, MapR Certified Spark Developer (MCSD), MapR Certified Hadoop Developer (MCHD), HDP Certified Apache Spark Developer, HDP Certified Developer","hadoop, spark, python pyspark, hdfs, hive, hue, impala, zeppelin, jupyter, sql, nosql, data analytics, big data, rdd, data frames, hadoop, devops, linux, unix, unix shell scripts ksh bash, plsql, etl, cca spark and hadoop developer, mapr certified spark developer mcsd, mapr certified hadoop developer mchd, hdp certified apache spark developer, hdp certified developer","big data, cca spark and hadoop developer, data frames, dataanalytics, devops, etl, hadoop, hdfs, hdp certified apache spark developer, hdp certified developer, hive, hue, impala, jupyter, linux, mapr certified hadoop developer mchd, mapr certified spark developer mcsd, nosql, plsql, python pyspark, rdd, spark, sql, unix, unix shell scripts ksh bash, zeppelin"
"Lead, Data Analyst",Strive Health,"Denver, CO",https://www.linkedin.com/jobs/view/lead-data-analyst-at-strive-health-3785774361,2023-12-17,Boulder,United States,Mid senior,Onsite,"What We Strive For
Strive Health is built for purpose — to transform a broken kidney care system. We are fundamentally changing the lives of kidney disease patients through early identification, engagement and comprehensive coordinated care. Strive’s model is driven by a high-touch care team that integrates with local providers and spans the entire care journey from chronic kidney disease through end-stage kidney disease, leveraging comparative and predictive data and analytics to identify at-risk patients. Strive Health’s interventions significantly reduce the rate of emergent dialysis crash, cut inpatient utilization and significantly improve patient outcomes and experience. Come join our journey as we create THE destination for top talent in the healthcare community and set a new standard for how kidney care should be done.
Why We’re Worth the Application
We Strive for excellence and were recognized as one of America’s Best Startup Employers for 2023 by Forbes, Circle of Excellence, American Society of Transplantation, 2023, Best Places to Work – Denver, Comparably.com, 2022, Best Places to Work – Denver, Built in Colorado, 2022
We derive innovation and ideas from through authentic diversity intentionally building a team that represents the populations we serve in partnership with our Employee Resource Groups:
Strive Forward - LGBTQ
Underrepresented Minorities
Women and Allies
We care and support our Strivers within and beyond work to feel fully charged and empowered through our generous wellbeing offerings including:
Flexible time off
Companywide wellbeing days
Volunteer time off
Leave packages including a sabbatical, parental leave and eight weeks paid for living donor
Professional development
A dedicated certified financial planner
Headspace, Carrot Fertility and Gympass for all Strivers
We like to have fun by celebrating our successes as a team through team building, company gatherings, trivia, wellbeing raffles, pajama days, a companywide book club and more.
We value tenacity to help us overcome obstacles with grit and determination to deliver compassionate kidney care.
Lead Data Analyst, Partner Analytics
The Lead Data Analyst, Partner Analytics will partner with the market Operator, Clinical, Implementation, Data Engineering, Provider Integration leaders in the organization to support new and ongoing initiatives. This person will be a leader of an experienced and passionate team and will play a key role in defining the organization's future. The lead analyst’s primary focus will be to provide in-depth analysis of ongoing and developing new initiative across multiple markets, supporting and advancing initiatives across analytics and operational performance in collaboration with both the new market & business development team, as well as the organization overall.
Essential Functions
Synthesize data stories through innovate analytical deep dives by factoring in data science, external, internal, operational, and clinical initiatives.
Research, hypothesize and develop analytical approaches for identifying, analyzing, and interpreting trends or patterns in complex data sets, particularly in healthcare claims & lab data
Conceptualize and design innovative models to assess viability of new and ongoing initiatives and clinical programs across multiple markets and partners
Collaborate with team members from strategy, clinical and new market functions to identify data driven opportunities to add value to existing and potential partners
Establish best practices for more junior analysts and assist in development of team members
Provide insight to design and build self-service dashboards to measure performance and assist in directing team’s efforts towards initiatives with the largest impact
Identify, communicate, and lead the resolution of data gaps that impede the fulfillment of operational reporting and oversight
Minimum Qualifications
Bachelor’s degree with strong academic achievement
4+ years’ experience in healthcare data analysis, preferably with experience in value-based care or population health analytics
Business travel
Preferred Qualifications
Working knowledge of analyzing medical claims, labs, EMR, and clinical data
Understanding of value-based care and risk contracts, including operational and financial levers for success
High degree of proficiency in Microsoft SQL, Excel & PowerPoint
Moderate degree of proficiency in a data visualization tool (preferably Quicksight)
Comfortable speaking and leading analytical discussions with partners
Annual Salary Range:
$93,400.00 - $116,700.00
Strive Health offers competitive compensation and benefits, including Health insurance, Dental insurance, Vision insurance, 401k Retirement Plan with Employer Match, Life and Accidental Death & Dismemberment insurance, Disability insurance, Health Savings Account, Flexible Spending Account, paid company holidays, in addition to Vacation Time Off. An annual performance bonus, determined by company and individual performance, is available for many roles as aligned to Strive Health guidelines.
Strive Health is an equal opportunity employer and drug free workplace. At this time Strive Health is unable to provide work visa sponsorship. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Please apply even if you feel you do not meet all qualifications. If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to talentacquisition@strivehealth.com
Show more
Show less","Data Science, SQL, Claims, EMR, Risk Contracts, Excel, PowerPoint, Quicksight, Tableau, Health Insurance, Dental Insurance, Vision Insurance, 401k Retirement Plan, Life Insurance, Disability Insurance, Health Savings Account, Flexible Spending Account, Python, R","data science, sql, claims, emr, risk contracts, excel, powerpoint, quicksight, tableau, health insurance, dental insurance, vision insurance, 401k retirement plan, life insurance, disability insurance, health savings account, flexible spending account, python, r","401k retirement plan, claims, data science, dental insurance, disability insurance, emr, excel, flexible spending account, health insurance, health savings account, life insurance, powerpoint, python, quicksight, r, risk contracts, sql, tableau, vision insurance"
Principal Data Engineer-Snowflake,InnovAge,"Denver, CO",https://www.linkedin.com/jobs/view/principal-data-engineer-snowflake-at-innovage-3708484076,2023-12-17,Boulder,United States,Mid senior,Onsite,"Responsibilities
Work-from-Home position supporting an 8am to 5pm MST schedule. Less than 10% travel to InnovAge centers and headquarters.
The
Principal Data Engineer
will be responsible for data design, management and development building new and vital data platforms and products for the Data and Intelligence team. Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models, and in implementing and operating machine learning models and services. Leverage internal client relationships to resolve complex business issues. Serve as an expert in their field of data engineering, providing technical guidance to less experienced team members.
Architect cutting edge solutions that are fit for purpose and mentors more junior staff to ensure overall quality.
Collaborate and make proposals, and help their team make informed decisions in alignment with the department’s strategic plans.
Provide technical guidance and oversee all the phases of projects, including research, development and design
Develop new product design and modify design of existing product as required. Review and decide upon improvements to existing system designs. Determine new designs to eliminate future issues.
Analyze product requirements and accordingly decide on technical and functional direction. Design and deliver solutions to improve system performance and reliability.
Be analytical and data driven to understand and apply knowledge from different domains to new technical areas. Lead the definition of coding standards and lead the Peer Code Review processes
Drive the continuous improvement of product development and deployment processes
Lead technical support of system acceptance testing and validation activities
Respond to customer queries and concerns in a timely fashion.
Collaborate with teammates and business partners regarding new ideas, issues and projects
Perform system failure analysis and drive corrective actions. Lead root cause analysis and determine long term solutions.
Build solid relationships with product and business data and technology leaders in the organization.
Partner with key stakeholders to develop and execute operational and strategic interventions.
Work with Enterprise Architecture team to develop cutting edge solutions
Support analytics and data science teams with exploration of new data sources and procuring them appropriately. Identify and deploy appropriate analytic techniques to solve complex business problems and overcome measurement challenges
Understand and translate key client business challenges tied to value delivery into potential analytic solutions to support those business challenges
Work with leadership, finance, and operations to ensure financial viability of new data products
Initiates opportunities and influences stakeholders with ideas that drive business value
Required
10+ yrs total related experience.
5 + Hands on Snowflake experience
5+ yrs Data Management experience proposing, documenting technology solutions and development experience with a focus on database architecture and data integration.
5+ yrs of expertise working with Data in a hands-on, developer role with advanced engineering principles and analytics development
Fluent in SQL (any flavor), having served in various capacities within design, development and deployment of data warehouses or relational/analytical/multi-dimensional data marts
Familiar with Virtual Data Warehousing or use of Snowflake DWaaS
Relational Database Management Systems and expert SQL skills. Extract/Load/Transform (ELT) development skills.
Experience with cloud migrations is desirable.
Experience building data pipelines on Azure following best practices in Cloud deployments
Demonstrated ability to debug complex data issues while working on very large data sets with billions of records
Proven ability to work with business and technical audiences, on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions
Ability to take the initiative to pursue assignments, responsibilities, and intake requests to a logical and final solution and to employ System Development Life Cycle (SDLC) standards is required.
Excellent verbal and written communication skills and strong analytical and problem-solving skills.
Bachelor’s degree in Computer Science or work-related experience in lieu of degree
Benefits
InnovAge’s Program of All-inclusive Care for the Elderly (PACE) is an alternative to nursing facilities. Seniors receive customized healthcare and social support at a nearby PACE center supported by a team of medical experts dedicated to providing personalized healthcare and support to help them age at home. Our greatest assets are our team members who make a difference in the lives of those we serve every day. Elevate your future with co-workers passionate about a patient-centered care model supported by comprehensive services to improve the quality of care while reducing over-utilization of high-cost care settings.
As an equal opportunity/affirmative action employer InnovAge is committed to and values an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender, gender identity/expression, national origin, disability or protected veteran status, pregnancy or any other status prohibited by applicable law.
Salary ranges are dependent on a variety of factors, including qualifications, experience, and geographic location. Range is not inclusive of potential bonus or benefits. Comprehensive benefits include m/d/v, short and long-term disability, life insurance and add, supplemental life insurance, flexible spending accounts, 401(k) savings, paid time off, and company paid holidays.
Posted Salary Range
USD $126,500.00 - USD $144,600.00 /Yr.
Show more
Show less","Data Engineering, Machine Learning, Data Warehousing, SQL, Snowflake, Data Integration, Data Migration, Cloud Migrations, Azure, Data Pipelines, ELT, System Development Life Cycle (SDLC), Computer Science","data engineering, machine learning, data warehousing, sql, snowflake, data integration, data migration, cloud migrations, azure, data pipelines, elt, system development life cycle sdlc, computer science","azure, cloud migrations, computer science, data engineering, data integration, data migration, datapipeline, datawarehouse, elt, machine learning, snowflake, sql, system development life cycle sdlc"
"Data Engineering Lead, Pipeline (487)",Techstars,"Boulder, CO",https://www.linkedin.com/jobs/view/data-engineering-lead-pipeline-487-at-techstars-3298550046,2023-12-17,Boulder,United States,Mid senior,Onsite,"This is a fully remote role and can be located anywhere in the continental US.
As a Data Engineering Lead, you will lead a team of engineers to build innovative solutions that empower entrepreneurs worldwide. Techstars already has one of the largest portfolios in early stage venture capital, with over 2,300 portfolio companies, a combined market cap of more than $193B, and 12 unicorns. In this role, you and your team will build backend streaming data pipelines, integrations with third party SaaS applications, complex analytics features and app facing platform data APIs. You will take an active role in architecture and solution design and help optimize solution performance and reliability. As a result you will help Techstars attract 10x more founders, and contribute directly to scaling the Techstars footprint to serve more entrepreneurs than ever before.
We believe in sustainable software development using Agile Development methodologies and mature DevOps practices to quickly and consistently provide value. We believe in creating robust, performant, maintainable, observable solutions. As an organization we value innovation and collaboration.
What You Will Do
Design, build and deploy quality data pipelines, models, integrations and APIs in an agile team environment.
Lead a team to deliver solutions that allow for 10x growth of Techstars pipeline of startups and our accelerator programs
Mentor team members on standards, best practices and implementation of technology.
Work with Engineering Leadership on design and implementation of technical systems.
Support Product Owner with OKRs, Sprint planning and stakeholder meetings as a technical resource.
Maintain uptime and SLA of deployed software systems.
Foster technical growth within your team by encouraging collaboration and innovation.
What You Bring
5+ years creating secure, reliable and performant enterprise level data cleansing, statistical modeling and analytics solutions (prefer fintech experience).
5+ years working on a data platform using open source technology (Kafka, K8s, Redis) and cloud infrastructure or operating as managed services.
5+ years experience with relational and non-relational database architecture (Postgres a plus)
3+ years as a senior engineer or lead engineer, leading and mentoring other engineers
Experience developing robust APIs
Diverse experience with languages (ie. SQL, Python, Node, Scala)
Knowledge of methodologies for testing quality, release management, incident response and issue resolution
Ability to create risk mitigation strategies for system upgrades and code releases
Ability to analyze appropriate technology stacks and major infrastructural components.
Strong DevOps skills
Track record of building reusable and cohesive architecture across applications
Experience creating technical designs that fulfill product requirements
A team mentality towards accomplishing projects
Ability to breakdown, prioritize and sequence development tasks for other engineers for high team utilization
Desire to mentor and grow engineers on the team
Coding standards utilization (Unit tests, formats, use of libraries, well structured, reusable, high quality)
Strong communication skills with an ability to work with the Product Owner and stakeholders to understand and manifest software outcomes
Compensation range:
$115,000 - $155,000 + 10% Bonus
US Benefits
About Techstars
Techstars is the worldwide network that helps entrepreneurs succeed. Founded in 2006, Techstars began with three simple ideas - entrepreneurs create a better future for everyone, collaboration drives innovation and great ideas can come from anywhere. Now we are on a mission to enable every person on the planet to contribute to, and benefit from, the success of entrepreneurs. In addition to operating accelerator programs and venture capital funds, we do this by connecting startups, investors, corporations and cities to help build thriving startup communities. Techstars has invested in more than 2,300 companies with a combined market cap of more than $29B.
Techstars’ mission is to help entrepreneurs succeed wherever they are in the world and whatever their background is. Regional accelerator programs all around the world are the cornerstone of the strategy. The investment approach is fundamentally driven by the worldwide network of managing directors, who interact with startup founders daily, guiding, mentoring and cultivating them along the journey. The scale of this reach results in a diversified strategy that provides investors with a uniquely qualified deal flow.
We help Techstars founders connect with other entrepreneurs, experts, mentors, alumni, investors, community leaders, and corporations to grow their companies.
www.techstars.com
Techstars is an affirmative action, equal opportunity employer and does not discriminate on the basis of race, sex, age, national origin, religion, physical or mental handicaps or disabilities, marital status, Veteran status, sexual orientation, gender identity nor any other basis prohibited by law.
Show more
Show less","Data Pipelines, Agile Development, DevOps, APIs, Data Cleansing, Statistical Modeling, Analytics, Kafka, Kubernetes, Redis, Cloud Infrastructure, Managed Services, Relational Databases, NonRelational Databases, Postgres, SQL, Python, Node.js, Scala, Software Testing, Quality Assurance, Release Management, Incident Response, Risk Mitigation, Infrastructure Components, Reusable Architecture, Technical Design, Unit Testing, Coding Standards, Communication Skills","data pipelines, agile development, devops, apis, data cleansing, statistical modeling, analytics, kafka, kubernetes, redis, cloud infrastructure, managed services, relational databases, nonrelational databases, postgres, sql, python, nodejs, scala, software testing, quality assurance, release management, incident response, risk mitigation, infrastructure components, reusable architecture, technical design, unit testing, coding standards, communication skills","agile development, analytics, apis, cloud infrastructure, coding standards, communication skills, datacleaning, datapipeline, devops, incident response, infrastructure components, kafka, kubernetes, managed services, nodejs, nonrelational databases, postgres, python, quality assurance, redis, relational databases, release management, reusable architecture, risk mitigation, scala, software testing, sql, statistical modeling, technical design, unit testing"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Boulder, CO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709525,2023-12-17,Boulder,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Docker, Helm, Spark, PySpark, Snowflake, Kubernetes, SQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Python, Java, Bash, Git","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, docker, helm, spark, pyspark, snowflake, kubernetes, sql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, python, java, bash, git","airflow, applied machine learning, bash, data cleaning, data engineering, data mining, data normalization, datamodeling, docker, dynamodb, etl, git, helm, java, kafka, kubeflow, kubernetes, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Data Analyst III- Adobe Campaign Specialist,AAA Life Insurance Company,"Livonia, MI",https://www.linkedin.com/jobs/view/data-analyst-iii-adobe-campaign-specialist-at-aaa-life-insurance-company-3778414658,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"By joining AAA Life, you will have the opportunity to strengthen the name and reputation of the brand that millions have come to rely upon for financial piece of mind. We are company dedicated to our members and our employees. We value the unique attributes and contributions of our associates to build an inclusive, collaborative and innovative workplace where all employees are engaged and feel they belong. Delivering our company’s promise to members is what drives each of our associates every day.
We offer a dynamic work environment, excellent benefits, and competitive compensation, that will allow you will exercise your potential to innovate, finding ways to increase efficiency and enhance our business processes.
Are you a Data Analyst with experience in Adobe Campaign? Our Data Analyst III ensures that AAA Life Insurance makes effective business and operational decisions. As a Data Analyst, you will be at the forefront of transforming data into actionable insights. This role involves extensive use of Adobe Campaign and will regularly extract, analyze, and visualize data to provide valuable recommendations that drive marketing strategies and decision-making.
Use Adobe Campaign to pull together cross-channel customer data create and customize campaigns.
Set up new Adobe campaign workflows for mail and email
Lead & develop automated, easy to understand reports and ad hoc analyses to address specific marketing questions and provide insights to guide decision making.
Utilize statistical techniques and data analysis tools (i.e., Python, R, SQL) to gather, clean, analyze, and provide recommendations regarding large datasets from various sources, identifying trends, patterns, and key performance metrics.
Collaborate closely with marketing teams to interpret data and provide actionable insights to optimize marketing campaigns, customer segmentation, and overall strategy.
Develop and implement data governance and quality assurance processes.
Create and maintain Power BI reports and dashboards to translate complex data into clear visualizations that marketing staff can easily interpret and use to inform their strategies.
Lead and develop key performance indicators (KPIs), tracking marketing initiatives against established goals, and providing regular updates to stakeholders.
Conduct A/B tests and statistical analyses to evaluate the effectiveness of marketing strategies, making data-driven recommendations for improvements.
Collaborate with marketing teams to segment audiences effectively and personalize marketing approaches based on data-driven insights.
Bachelor in Statistics, Marketing, Economics, Computer Science, or related technical field. Master’s degree is a plus.
A minimum of five years’ experience working as a Data Analyst, Marketing Analyst, or similar role.
Expert in Adobe Campaign experience strongly preferred
Able to set up new Adobe campaign workflows for mail and email strongly preferred
Understands architecture needed to support very large weekly campaigns strongly needed
Extensive experience in data analysis tools and programming languages (i.e., Python, R, SQL).
Ability to create and interpret reports and dashboards using Power BI, Tableau, or similar data visualization tools.
Proficiency with marketing analytics tools and platforms (i.e., Google Analytics, Adobe Analytics).
Show more
Show less","Data Analysis, Adobe Campaign, Python, R, SQL, Statistics, Power BI, Tableau, Data Governance, Data Quality Assurance, Data Visualization, Marketing Analytics, Google Analytics, Adobe Analytics","data analysis, adobe campaign, python, r, sql, statistics, power bi, tableau, data governance, data quality assurance, data visualization, marketing analytics, google analytics, adobe analytics","adobe analytics, adobe campaign, data governance, data quality assurance, dataanalytics, google analytics, marketing analytics, powerbi, python, r, sql, statistics, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744396373,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, TDD, Pair Programming, Continuous Integration, Automated Testing, Big Data, Data Engineering, Business Intelligence, Data Science, Relational Databases, Data Modeling, Schema Design","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, etl, data warehouses, tdd, pair programming, continuous integration, automated testing, big data, data engineering, business intelligence, data science, relational databases, data modeling, schema design","airflow, automated testing, big data, business intelligence, continuous integration, data engineering, data science, data warehouses, datamodeling, docker, etl, helm, kafka, kubernetes, pair programming, python, relational databases, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Staff Data Engineer,Recruiting from Scratch,"Livonia, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394418,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Apache Airflow, Kubernetes, Docker, Helm, Apache Spark, PySpark, SQL, Testdriven Development (TDD), Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, apache airflow, kubernetes, docker, helm, apache spark, pyspark, sql, testdriven development tdd, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","apache airflow, apache spark, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, testdriven development tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397154,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention, Legal Compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Livonia, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398085,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707770,2023-12-17,Pittsfield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, ML, NLP, Conversational AI APIs, Recommender systems, Distributed systems, Microservices","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, ml, nlp, conversational ai apis, recommender systems, distributed systems, microservices","airflow, aws, azure, bash, conversational ai apis, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, microservices, ml, nlp, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088713,2023-12-17,Pittsfield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML/DL Pipelines, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Compliance, Legal Compliance, Data Classification, Data Retention, 401K, Equity","data engineering, mldl pipelines, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data compliance, legal compliance, data classification, data retention, 401k, equity","401k, airflow, aws, azure, bash, data classification, data compliance, data engineering, data retention, docker, dynamodb, equity, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, mldl pipelines, nlp, nosql, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Principal Consultant – Data and Analytics,Genesys,"Indiana, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781014611,2023-12-17,Plainfield,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","SQL, SQL analytics, Data analysis, Data visualization, Data engineering, Business intelligence, Data governance, Data management, Snowflake, Elastic (ELK stack), Tableau, Power BI, Software development, Project management, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, Contact Center, Customer Experience, XaaS","sql, sql analytics, data analysis, data visualization, data engineering, business intelligence, data governance, data management, snowflake, elastic elk stack, tableau, power bi, software development, project management, genesys engage, genesys cloud, nice, cisco, avaya, contact center, customer experience, xaas","avaya, business intelligence, cisco, contact center, customer experience, data engineering, data governance, data management, dataanalytics, elastic elk stack, genesys cloud, genesys engage, nice, powerbi, project management, snowflake, software development, sql, sql analytics, tableau, visualization, xaas"
Data Engineer III,VRK IT Vision Inc.,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-iii-at-vrk-it-vision-inc-3677424570,2023-12-17,Stroudsburg,United States,Mid senior,Onsite,"Job Title:- Data Engineer III
Location:- Houston TX (Hybrid 3 Days/week On-Site)
Job Type:- Long Term Contract
Need Local and Senior 12+ Years
Need Oil & Gas Background
Skills
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
Required Skills
Basic Qualification :
Additional Skills
Background Check :Yes
Show more
Show less","SQL, AWS, Big Data, Data Pipelines, Data Transformation, Data Extraction, Data Loading, Analytics, Data Architecture, Data Structures, Metadata, Dependency Management, Workload Management, Message Queuing, Stream Processing, Scalable Data Stores, Project Management, CrossFunctional Teams","sql, aws, big data, data pipelines, data transformation, data extraction, data loading, analytics, data architecture, data structures, metadata, dependency management, workload management, message queuing, stream processing, scalable data stores, project management, crossfunctional teams","analytics, aws, big data, crossfunctional teams, data architecture, data extraction, data loading, data structures, data transformation, datapipeline, dependency management, message queuing, metadata, project management, scalable data stores, sql, stream processing, workload management"
Data Analyst Part Time,Toyandsons,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-toyandsons-3757203932,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393773,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data engineering, Business intelligence, Data science, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Agile engineering practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Streamprocessing systems, Data Warehouses, ETL pipelines, Legal compliance, Data classification, Data retention","python, data engineering, business intelligence, data science, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, data warehouses, etl pipelines, legal compliance, data classification, data retention","agile engineering practices, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, docker, etl pipelines, helm, kafka, kubernetes, legal compliance, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827514,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Kafka, Snowflake, Spark, ETL, Distributed Databases, Data Warehouses, TDD, Pair Programming, Continuous Integration, Automated Testing, Data Engineering, Big Data Technologies, Machine Learning, StreamProcessing Systems, Data Modeling, Data Classification, Data Management Tools","python, sql, kafka, snowflake, spark, etl, distributed databases, data warehouses, tdd, pair programming, continuous integration, automated testing, data engineering, big data technologies, machine learning, streamprocessing systems, data modeling, data classification, data management tools","automated testing, big data technologies, continuous integration, data classification, data engineering, data management tools, data warehouses, datamodeling, distributed databases, etl, kafka, machine learning, pair programming, python, snowflake, spark, sql, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398357,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Retention, Legal Compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Researcher - Data Privacy,"Huawei Technologies Canada Co., Ltd.","Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/researcher-data-privacy-at-huawei-technologies-canada-co-ltd-3764777201,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Our team has an immediate 12-month contract opening for a Researcher.
Responsibilities:
Conduct cutting-edge data privacy research
Participate in identifying opportunities in data privacy protection and collaborate with product teams to propose and design features that can be integrated into real Huawei products
Support software engineers to design, build and test data privacy protection features
Draft patents, papers, and design documents
What you’ll bring to the team:
Master's or Ph.D. degree in Computer Science, Electrical and Computer Engineering, or related fields
2-5 years experience in related fields such as information security, privacy, compliance, data science, audit, or risk management
Extensive knowledge in data privacy-related fields (at least 2 areas): anonymization, private information retrieval, private set intersection, cryptography, differential privacy, searchable encryption, privacy attacks, or any other privacy-preserving technologies
Good knowledge of AI deep learning and experience with deep learning frameworks, e.g. Tensorflow, Pytorch
Solid Python and C++ programming skills
Proficiency working in Linux environments
Knowledge of Computer Vision and Natural Language Processing is an asset
Knowledge of Adversarial Machine Learning/GAN is an asset
Show more
Show less","Data privacy research, Information security, Privacy, Compliance, Data science, Audit, Risk management, Anonymization, Private information retrieval, Private set intersection, Cryptography, Differential privacy, Searchable encryption, Privacy attacks, Privacypreserving technologies, AI deep learning, Deep learning frameworks, Tensorflow, Pytorch, Python, C++, Linux environments, Computer Vision, Natural Language Processing, Adversarial Machine Learning, GAN","data privacy research, information security, privacy, compliance, data science, audit, risk management, anonymization, private information retrieval, private set intersection, cryptography, differential privacy, searchable encryption, privacy attacks, privacypreserving technologies, ai deep learning, deep learning frameworks, tensorflow, pytorch, python, c, linux environments, computer vision, natural language processing, adversarial machine learning, gan","adversarial machine learning, ai deep learning, anonymization, audit, c, compliance, computer vision, cryptography, data privacy research, data science, deep learning frameworks, differential privacy, gan, information security, linux environments, natural language processing, privacy, privacy attacks, privacypreserving technologies, private information retrieval, private set intersection, python, pytorch, risk management, searchable encryption, tensorflow"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751473180,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Mining, Statistical Analysis, Data Modeling, A/B Testing, Data Integrity, Data Manipulation, Data Visualization, SQL, R, Python, Tableau, Power BI, Hypothesis Testing, ETL, Data Management","data analysis, data mining, statistical analysis, data modeling, ab testing, data integrity, data manipulation, data visualization, sql, r, python, tableau, power bi, hypothesis testing, etl, data management","ab testing, data integrity, data management, data manipulation, data mining, dataanalytics, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Senior Technical Data Analyst,System1,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-technical-data-analyst-at-system1-3584580561,2023-12-17,Cambridge, Canada,Mid senior,Remote,"We are seeking a highly skilled and experienced
Senior Technical Data Analyst
to join our growing Data Analytics team. Needs a strong background in data analysis and to be able to effectively communicate complex analytical findings to both technical and non-technical audiences, focusing on our highly profitable Search Engine Monetization business. This role requires a strong background in exploratory data analytics, statistics, data modeling as well as excellent communication and leadership skills.
The Role You Will Have
Analyze data from search engine marketing campaigns, including keyword performance, ad spend, and conversions
Collaborate with cross-functional teams to identify data-driven opportunities and develop strategies to leverage data assets
Develop and maintain analytical dashboards and reports to track key performance indicators and provide insights to stakeholders
Analyze data trends and patterns to identify opportunities for improvement and make recommendations to leadership
Utilize statistical and machine learning techniques to extract insights from data and build predictive models
Train and mentor junior data analysts and team members on data analysis techniques and best practices
Stay up-to-date with industry trends and best practices in search engine marketing and data analysis
What You Will Bring
Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
5+ years of experience in data analysis and visualization of large data sets
Strong experience with statistical and machine learning techniques
Advanced proficiency in SQL, and at least one programming language (e.g. Python, R)
Deep familiarity with all digital marketing metrics including CPA, CPC, CPM and ROAS
Excellent communication and leadership skills
Ability to work independently and as part of a team
Experience with big data technologies (e.g. Hadoop, Spark, BigQuery)
Experience with data visualization tools (e.g. Tableau, Power BI), as well as surfacing insights with interactive notebooks (e.g Jupyter, Google Colab)
Familiarity with pulling data from third party API’s and basic ETL processing
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
Medical, Dental & Vision
RRSP w/matching
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
#Bl-Remote
#BI-Hybrid
The base salary range in Canada for this full-time position is
$102,200 - 142,000
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all U.S. and Canada locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following provinces: Ontario and British Columbia.
Show more
Show less","Data Analysis, Exploratory Data Analytics, Statistical Analysis, Data Modeling, Predictive Analysis, Machine Learning, SQL, Python, R, Digital Marketing Metrics, Data Visualization, Data Visualization Tools, Business Intelligence, ETL Processing, Hadoop, Spark, BigQuery, Tableau, Power BI, Jupyter, Google Colab, Data Communication, Leadership","data analysis, exploratory data analytics, statistical analysis, data modeling, predictive analysis, machine learning, sql, python, r, digital marketing metrics, data visualization, data visualization tools, business intelligence, etl processing, hadoop, spark, bigquery, tableau, power bi, jupyter, google colab, data communication, leadership","bigquery, business intelligence, data communication, data visualization tools, dataanalytics, datamodeling, digital marketing metrics, etl processing, exploratory data analytics, google colab, hadoop, jupyter, leadership, machine learning, powerbi, predictive analysis, python, r, spark, sql, statistical analysis, tableau, visualization"
Lead Technical Data Analyst,System1,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-technical-data-analyst-at-system1-3575258564,2023-12-17,Cambridge, Canada,Mid senior,Remote,"We are seeking a highly skilled and experienced
Lead Technical Data Analyst
to join our growing Data Analytics team. Needs a strong background in data analysis and to be able to effectively communicate complex analytical findings to both technical and non-technical audiences, focusing on our highly profitable Search Engine Monetization business. This role requires a strong background in exploratory data analytics, statistics, data modeling as well as excellent communication and leadership skills.
The Role You Will Have
Serve as a subject matter expert to both the Analytics team and business leaders, providing sound & data-driven strategies that help meet or exceed business goals
Develop and create models in production (classification, clustering, learning models, multivariate regression, time series, k-means, etc.) to identify the best business solution
Use Python and packages like Pandas, NumPy, etc., perform complex statistical analysis on results, and utilize APIs for data ingestion
Use SQL to self-serve data from disparate sources & manipulate multiple large data sets simultaneously for analysis
Perform complex exploratory analysis on historical search engine marketing campaign data - including keyword performance, ad spend, and conversions - and identify opportunities for new expansion & data acquisition
Collaborate with teams in Taxonomy & Performance Marketing to proactively identify keyword trends & develop data-driven optimization strategies
Use tools like Tableau, Looker, or PowerBI to visualize results and present directly to business leaders
Provide mentorship & training to junior analysts and new hires, and evangelize ways to implement better data-driven practices throughout the organization
Stay up-to-date with industry trends and best practices in search engine marketing and data analysis
What You Bring To The Team
Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
7+ years of progressive experience in data analysis/data science and handling of large data sets.
Strong experience with mathematical optimization methods, exploratory data analysis, predictive modeling and machine learning techniques
Advanced proficiency in SQL, strong proficiency in Python & its data science packages (Pandas, NumPy, MatPlotLib, Seaborn, etc.)
Familiarity with digital marketing KPIs including CPA, CPC, CPM and ROAS
Excellent communication, and experience in providing leadership within your team (code-review or pair-programming, interviewing & onboarding new hires, etc.)
Experience with big data technologies (e.g. AWS, Snowflake, Hadoop, Spark, BigQuery)
Experience with data visualization tools (e.g. Tableau, Power BI), as well as surfacing insights with interactive notebooks (e.g Jupyter, Google Colab)
Familiarity with pulling data from third party API’s and basic ETL processing
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
Medical, Dental & Vision
RRSP w/matching
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
#BI-Hybrid
The base salary range in Canada for this full-time position is
$149,200 - $207,300
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all U.S. and Canada locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following provinces: Ontario and British Columbia.
Show more
Show less","Data Analysis, Data Science, Exploratory Data Analysis, Statistics, Data Modeling, Machine Learning, Python, Pandas, NumPy, MatPlotLib, Seaborn, SQL, Tableau, Power BI, Jupyter, Google Colab, AWS, Snowflake, Hadoop, Spark, BigQuery, API, ETL","data analysis, data science, exploratory data analysis, statistics, data modeling, machine learning, python, pandas, numpy, matplotlib, seaborn, sql, tableau, power bi, jupyter, google colab, aws, snowflake, hadoop, spark, bigquery, api, etl","api, aws, bigquery, data science, dataanalytics, datamodeling, etl, exploratory data analysis, google colab, hadoop, jupyter, machine learning, matplotlib, numpy, pandas, powerbi, python, seaborn, snowflake, spark, sql, statistics, tableau"
Data Analyst - CMO Center of Excellence,TeamHealth,"Knoxville, TN",https://www.linkedin.com/jobs/view/data-analyst-cmo-center-of-excellence-at-teamhealth-3774852066,2023-12-17,Andersonville,United States,Associate,Remote,"TeamHealth is named among the ""150 Top Places to Work in Healthcare"" by Becker's Hospital Review
and has ranked three years running as ""The World's Most Admired Companies"" by Fortune Magazine as well as one of America's 100 Most Trustworthy Companies by Forbes Magazine in past years. TeamHealth, an established healthcare organization is physician-led and patient-focused. We continue to grow across the U.S. from our Clinicians to our Corporate Employees and we want you to join us.
OVERVIEW:
TeamHealth is underway bringing all its healthcare data assets, products, and intelligence together to provide scalable value to its customers. In order to accomplish this, we are looking for a rock star data enthusiast to join our CMO Center of Excellence at TeamHealth.
The Data Analyst will be responsible for deliverables related to clinical and operational analytics, reporting, and distribution. Data analysis, modeling, and reporting will encompass the areas of healthcare delivery in TeamHealth service lines. Consumers of the analyses and information will include the CMIO, Senior Executive Leadership, and organizational Operational Leadership.
The Data Analyst will use multiple languages and tools to support data discovery, analytics, modeling, reporting and visualization across large data sets. They will be expected to be comfortable with statistical inference, dashboard development, and SQL.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Under limited supervision, be responsible for multiple analyses and deliverables, while monitoring, summarizing, communicating, and reporting status
Takes analysis and reporting tasks from original concept through final implementation. Develops all aspects of these tasks including a detailed plan, schedule, and distribution methodology.
Responsible for assembling necessary external stakeholders, including business owners, database stewards and application owners to ensure that appropriate resources and needs have been identified to achieve the analyses on the necessary timeline.
Able to work closely with data engineers, database owners, application owners and executive sponsors to ensure that development of new databases, reporting marts, and data infrastructure will support ongoing scaled analytics and reporting requirements
Responsible for creating, maintaining and deploying standard reporting using enterprise tools
Works closely with system engineers, database owners, application owners and executive sponsors to ensure that analytics and reporting requirements are met
Works closely with business and operations units to understand and meet reporting and information needs
Develops and contributes ideas and opinions about collecting relevant data, analyzing data, and developing presentations to present to management
Understands underlying business operations, data, data capture processes, and system integration to ensure that analyses and reports are correctly advising the organization, and accounts for data and/or workflow limitations
Produces analytics, dashboards and reports within the framework of the CMIO | CMO data/analytics processes and the CMO Center of Excellence (CoE); including exploratory data analysis, descriptive analysis and statistics, and using statistical inference and predictive modeling techniques as appropriate
Gains understanding of TeamHealth proprietary and enterprise systems. Aids in the enhancement of current systems and implementation of new systems for the company
QUALIFICATIONS / EXPERIENCE:
Bachelor's Degree in a quantitative field such as engineering, computer science, statistics, mathematics, economics, business analytics or a relevant field of study or equivalent professional experience
Master's degree in analytics or a quantitative science is preferred
Minimum of five (5) years of analytics experience, with knowledge and experience in statistical inference, forecasting/predictive analytics, descriptive analytics, and data visualization
Experience with PowerBI or Tableau for report creation, report distribution, and administration
Database management and database structure experience
Programming experience with SQL, and R or Python
Detailed experience in data acquisition/queries for multiple database and file types, data cleaning, aggregation, and data quality assessment
Excellent computer skills (Excel, Word, PowerPoint, and Visio)
Superior communication skills - verbal, written, presentation and negotiation skills
Strong organizational and analytical skills
Patience and ability to excel under pressure, handling multiple requests often with tight deadlines
Resourcefulness, ingenuity, strong decision making and problem solving skills
Leadership, teamwork and the ability to develop others
Solution driven and flexible in interpersonal style
Willing to gain expertise in TeamHealth data and the underlying business operations that produce such data
Healthcare experience preferred
https://www.teamhealth.com/california-applicant-privacy-notice/
Show more
Show less","SQL, R, Python, PowerBI, Tableau, Dashboard development, Statistical inference, Data visualization, Data acquisition, Data cleaning, Data aggregation, Data quality assessment, Database management, Database structure, Data analysis, Data modeling, Data reporting, Statistical analysis, Predictive modeling, Communication skills, Presentation skills, Negotiation skills, Organizational skills, Analytical skills, Resourcefulness, Ingenuity, Decision making, Problem solving, Leadership, Teamwork, Solution driven, Flexible interpersonal style","sql, r, python, powerbi, tableau, dashboard development, statistical inference, data visualization, data acquisition, data cleaning, data aggregation, data quality assessment, database management, database structure, data analysis, data modeling, data reporting, statistical analysis, predictive modeling, communication skills, presentation skills, negotiation skills, organizational skills, analytical skills, resourcefulness, ingenuity, decision making, problem solving, leadership, teamwork, solution driven, flexible interpersonal style","analytical skills, communication skills, dashboard development, data acquisition, data aggregation, data cleaning, data quality assessment, data reporting, dataanalytics, database management, database structure, datamodeling, decision making, flexible interpersonal style, ingenuity, leadership, negotiation skills, organizational skills, powerbi, predictive modeling, presentation skills, problem solving, python, r, resourcefulness, solution driven, sql, statistical analysis, statistical inference, tableau, teamwork, visualization"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Knoxville, TN",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783189279,2023-12-17,Andersonville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Knoxvill-DataScientist.011
Show more
Show less","Python, JavaScript, JSON, Generative AI, Machine Learning, Data Science, Product Development, Research, Algorithms, OOP languages, Technical Communication, Project Management, Problem Solving, Innovation, Collaboration, AI Training Models, EdTech, Data Analytics, HighPerformance Coaching","python, javascript, json, generative ai, machine learning, data science, product development, research, algorithms, oop languages, technical communication, project management, problem solving, innovation, collaboration, ai training models, edtech, data analytics, highperformance coaching","ai training models, algorithms, collaboration, data science, dataanalytics, edtech, generative ai, highperformance coaching, innovation, javascript, json, machine learning, oop languages, problem solving, product development, project management, python, research, technical communication"
"Engineer III, Big Data",Pilot Company,"Knoxville, TN",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-company-3784214007,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and digital innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research, development and modification of extract, transform, load processes and jobs in support of a Big Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","SQL, Data modeling, Relational Databases (Oracle Postgres SQL Server), Apache Spark, Sparkstreaming, Message Queue technologies, Python, AWS, Cloud Big Data platform, Agile methodologies, Team leadership, Mentoring, Communication","sql, data modeling, relational databases oracle postgres sql server, apache spark, sparkstreaming, message queue technologies, python, aws, cloud big data platform, agile methodologies, team leadership, mentoring, communication","agile methodologies, apache spark, aws, cloud big data platform, communication, datamodeling, mentoring, message queue technologies, python, relational databases oracle postgres sql server, sparkstreaming, sql, team leadership"
Senior Data Engineer,Cellular Sales,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cellular-sales-3768825864,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Cellular Sales
Sr Data Engineer - ETL
About Us
At Cellular Sales, we connect our customers with Verizon, the network more people rely on. We strive to bring people together through technology and work towards great accessibility today and every day. As a trusted partner of Verizon, we share their mission: to give humans the ability to do more in this world. We create the connections that turn innovative ideas into reality.
Do you have experience working with relational databases using ETL tools? Can you design and load data patterns? Do you thrive working in a collaborative environment helping to influence company strategy?
If so, we would love to talk with you!
Summary/Objective
The Sr Data Engineer will be responsible for designing and implementing enterprise level ETL processes in order to consume data from various sources. This position will be responsible for collaboration with the rest of the ETL team to facilitate the growth of the enterprise data warehouse as well as developing ETL solutions for various applications. This position also serves to mentor other team members on ETL best practices.
What We Need From You
Experience effectively using ETL tools (Informatica or other tools).
Develop, Design, and communicate practical enterprise level ETL solutions.
Ability to translate requirements and technical design to an automated ETL solution.
Understanding of data concepts to accommodate both operational and reporting design elements.
Ability to translate requirements and technical design to an automated ETL solution.
Mentor team members in SQL/Query performance tuning as well as ETL design.
Familiarity with Cloud technologies as they relate to Data Engineering
Full knowledge of storage and consumer database best practices.
Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
What We Provide
A collaborative working environment encouraging input from team members.
Strong commitment to the success of the employees and the business.
Support from the team, tools and systems.
Ability to build strong relationships throughout the organization.
Additional Benefits
Health and Dental Benefits
401K with matching
On site gym
Great Culture and an inviting atmosphere
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
Show more
Show less","Data Engineering, ETL, Informatica, Data Warehouse, SQL, Query Performance Tuning, Cloud Technologies, Storage, Consumer Database, Agile, Scrum","data engineering, etl, informatica, data warehouse, sql, query performance tuning, cloud technologies, storage, consumer database, agile, scrum","agile, cloud technologies, consumer database, data engineering, datawarehouse, etl, informatica, query performance tuning, scrum, sql, storage"
"Engineer III, Big Data",Pilot Flying J,"Knoxville, TN",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-flying-j-3783968663,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and digital innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research, development and modification of extract, transform, load processes and jobs in support of a Big Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","Extract Transform Load (ETL), Big Data Lake, Data Analytics, SQL, Oracle, Postgres, SQL Server, Relational modeling, Triggers, Stored procedures, Constraints, Apache Spark, Sparkstreaming, Message Queue technologies, Python, Enterprise data warehouse (EDW), Data modeling, Star Schema, Amazon Web Services (AWS), Cloud Big Data","extract transform load etl, big data lake, data analytics, sql, oracle, postgres, sql server, relational modeling, triggers, stored procedures, constraints, apache spark, sparkstreaming, message queue technologies, python, enterprise data warehouse edw, data modeling, star schema, amazon web services aws, cloud big data","amazon web services aws, apache spark, big data lake, cloud big data, constraints, dataanalytics, datamodeling, enterprise data warehouse edw, extract transform load etl, message queue technologies, oracle, postgres, python, relational modeling, sparkstreaming, sql, sql server, star schema, stored procedures, triggers"
Senior Data Analyst,South College,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-analyst-at-south-college-3742878729,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Description
South College invites capable, energetic, outgoing, applicants who are focused on transforming lives of our customers/students! We are one of the nation’s fastest growing institutions of higher learning with over 10,000 students covering 7 campuses and Online learning sites. We are also one of nation’s highest producers of licensed healthcare professionals offering a myriad of undergraduate and graduate healthcare programs for our students. Our Data Analytics team is growing with a focus on supporting business needs while improving student experience! Come join us in these exciting efforts!
As Senior Data Analyst, you will be responsible for delivering value to the company through actionable insights using descriptive statistics, data science, and general data analyses. We are looking for a talented and motivated analytics professional that can solve business problems through critical and abstract thinking. The person in this role must have excellent communication skills and the ability to take complex problems in data and present simplified solutions with little oversight. This position will work out of our Knoxville - Lonas Campus.
Assist the Analytics team members with any data analysis request.
Deliver data driven insights using descriptive or predictive/prescriptive analytics depending on the needs of the business.
Perform necessary statistical analysis from pulling the data and analyzing to delivering a simplified insight.
Develop strong business acumen, connecting business problems and opportunities to data to increase company profitability and student success/experience.
Develop high quality repeatable processes and deliverables to utilize discovered insights
Requirements
Master's Degree in Analytics preferred
2+ years of experience in Analytics, Data Science, or Data Engineering in the workplace
Deep understanding of statistical concepts and modeling
Self-starter with the experience to work independently on projects with little oversight from start to finish
Desire to learn new technologies and statistical concepts and how to apply them to fit the business needs.
Excellent written and oral communication/presentation skills
Works well in a fast-paced environment and can quickly switch priorities as business needs change
Excellent attention to detail and motivated to double check work
Experience in: Excel, Word, & PowerPoint
Experience with SQL or the desire to learn
Experience in R, Python, Tableau, Power BI, JMP, Data Science, Data Analysis, Marketing Analytics, or the desire to learn.
Show more
Show less","Data Analytics, Descriptive Statistics, Data Science, Predictive Analytics, Prescriptive Analytics, Statistical Analysis, Data Visualization, Tableau, Power BI, JMP, R, Python, Business Acumen, SQL, Excel, Word, PowerPoint, Data Engineering","data analytics, descriptive statistics, data science, predictive analytics, prescriptive analytics, statistical analysis, data visualization, tableau, power bi, jmp, r, python, business acumen, sql, excel, word, powerpoint, data engineering","business acumen, data engineering, data science, dataanalytics, descriptive statistics, excel, jmp, powerbi, powerpoint, predictive analytics, prescriptive analytics, python, r, sql, statistical analysis, tableau, visualization, word"
Senior Data Analyst,South College,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-analyst-at-south-college-3666144183,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Description
South College invites capable, energetic, outgoing, applicants who are focused on transforming lives of our customers/students! We are one of the nation’s fastest growing institutions of higher learning with over 10,000 students covering 7 campuses and Online learning sites. We are also one of nation’s highest producers of licensed healthcare professionals offering a myriad of undergraduate and graduate healthcare programs for our students. Our Data Analytics team is growing with a focus on supporting business needs while improving student experience! Come join us in these exciting efforts!
As Senior Data Analyst, you will be responsible for delivering value to the company through actionable insights using descriptive statistics, data science, and general data analyses. We are looking for a talented and motivated analytics professional that can solve business problems through critical and abstract thinking. The person in this role must have excellent communication skills and the ability to take complex problems in data and present simplified solutions with little oversight.
Assist the Analytics team members with any data analysis request.
Deliver data driven insights using descriptive or predictive/prescriptive analytics depending on the needs of the business.
Perform necessary statistical analysis from pulling the data and analyzing to delivering a simplified insight.
Develop strong business acumen, connecting business problems and opportunities to data to increase company profitability and student success/experience.
Develop high quality repeatable processes and deliverables to utilize discovered insights
Requirements
Master's Degree in Analytics preferred
2+ years of experience in Analytics, Data Science, or Data Engineering in the workplace
Deep understanding of statistical concepts and modeling
Self-starter with the experience to work independently on projects with little oversight from start to finish
Desire to learn new technologies and statistical concepts and how to apply them to fit the business needs.
Excellent written and oral communication/presentation skills
Works well in a fast-paced environment and can quickly switch priorities as business needs change
Excellent attention to detail and motivated to double check work
Experience in: Excel, Word, & PowerPoint
Experience with SQL or the desire to learn
Experience in R, Python, Tableau, Power BI, JMP, Data Science, Data Analysis, or the desire to learn.
Show more
Show less","Analytics, Data Science, Data Engineering, Statistics, Statistical modeling, Data analysis, Predictive analytics, Prescriptive analytics, Business acumen, Datadriven insights, Complex problem solving, Communication, Presentation skills, Fastpaced environment, Attention to detail, Excel, Word, PowerPoint, SQL, R, Python, Tableau, Power BI, JMP","analytics, data science, data engineering, statistics, statistical modeling, data analysis, predictive analytics, prescriptive analytics, business acumen, datadriven insights, complex problem solving, communication, presentation skills, fastpaced environment, attention to detail, excel, word, powerpoint, sql, r, python, tableau, power bi, jmp","analytics, attention to detail, business acumen, communication, complex problem solving, data engineering, data science, dataanalytics, datadriven insights, excel, fastpaced environment, jmp, powerbi, powerpoint, predictive analytics, prescriptive analytics, presentation skills, python, r, sql, statistical modeling, statistics, tableau, word"
"Healthcare Data Analyst or Senior Healthcare Data Analyst, Analytics Hub",ECG Management Consultants,"St Louis, MO",https://www.linkedin.com/jobs/view/healthcare-data-analyst-or-senior-healthcare-data-analyst-analytics-hub-at-ecg-management-consultants-3785921708,2023-12-17,Belleville,United States,Associate,Hybrid,"Overview
ECG is a national management consulting firm working exclusively in the healthcare industry. At ECG, our primary emphasis is on quality—in our people as well as our services—and we’re seeking others who appreciate our high standards of excellence.
What’s in It for You: Consult with Purpose
At ECG, you can have a bigger impact than you ever imagined. The work you do will help health systems deliver care more effectively and efficiently—and that’s just the beginning. We’re looking for innovators, problem-solvers, and self-starters to collaborate across our five divisions, take on challenging projects, and find new ways to improve patient care. ECG is committed to ensuring a friendly work environment that rewards high performance and welcomes, values, and supports all people.
Join Our Analytics Hub
Our growing Analytics Hub works with our consulting teams to deliver new insights, streamlined analysis, and world-class intelligence to the healthcare organizations we serve as clients. Combined with our consulting teams, you will be helping physician and executive leadership make informed decisions that can alter the course of their enterprises in areas such as physician alignment, mergers and acquisitions, service line planning, provider compensation, financial performance, transforming care models, patient access, managed care portfolio optimization, digital health expansion, and more. ECG uses a Microsoft Azure and Microsoft 365 environment, with Microsoft Power BI as ECG’s current analytics visualization platform. The goal of the position is to further capitalize on the existing ECG infrastructure, as well as implement new features and technologies. The ideal candidate is open to new challenges, exceptional at multitasking, and proficient at implementing new solutions.
Our detailed plans incorporate qualitative findings with data-driven strategic, operational, and financial considerations that enable organizations to pursue realistic change. Our consultants bridge the gap between strategic thinking and operational implementation with sensible action plans and tactical recommendations. This approach allows us to stretch our clients’ thinking while ensuring that initiatives with the broadest and deepest impact are prioritized and implementable. Our wide-ranging engagements often focus on helping our clients:
Position themselves for value-based care delivery.
Align hospitals and physician organizations.
Strengthen financial performance.
Enhance clinical programs.
Develop strategic partnerships and/or mergers.
Your Opportunity with ECG: Data Analyst or Senior Data Analyst
As a data analyst or senior data analyst, you will support the firm’s client delivery and business development efforts, working with consultants and senior leaders to manage large data sets, aggregate internal and external data, develop scalable models and other analyses, and support data visualization and presentation sets. In addition, the analyst will work directly with our internal support teams (IT, business development, and L&D) and associated vendors for data management and IT solutions and will assist with a range of innovative analytic opportunities. Here, no two days or projects are alike, which means you’ll have a lot to learn and plenty of support to help you succeed.
Your Responsibilities May Include:
Gathering and synthesizing data from various sources (e.g., national surveys, proprietary research, client interviews, industries, markets)
Preparing analyses related to hospital inpatient databases and provider claims data (Stratasan)
Building flexible, dynamic, and scalable financial and operational models to forecast trends, preparing scenarios for consulting efforts related to delivery and business development (e.g., strategic planning, operational improvement, market assessments)
Enhancing existing models and capabilities built using SQL, primarily, with some use of Python or R in certain models or scenarios
Performing complex analyses on big data, including cleaning, preparing, and interpreting
Identifying and executing analyses in response to consultant business intelligence inquiries
Building relational databases using a Microsoft SQL Server or Fabric
Communicating market insights to pursuit teams, and supporting business development efforts
Configuring, maintaining, and building flexible reports, queries, and visuals within the Power BI environment and the production and development environments of related databases
Researching, testing, and implementing solutions in Power BI
Partnering with various consulting units (strategy, academic health, provider financial services, performance transformation, etc.) to gather requirements related to data analytics
Developing, configuring, and maintaining interfaces to exchange data among Power BI, SQL databases, and other cloud-based applications
Creating and maintaining analytical support documentation and operating procedures
Developing new and innovative methodologies and approaches
Contributing to internal education and learning opportunities for consultants and operations members for the Analytics Hub and analytics in general, and supporting education about ECG’s use cases related to analytics in work
Assisting the director of data analytics in developing analytics and data strategy to support data analytics capabilities at the firm
Our Expectations of You
A bachelor’s degree in either computer science, computer engineering, mathematics, statistics, health information management, health administration, business, or a related degree that relies heavily on critical thinking, logic, and math
Prior data analytics work experience using databases, SQL, and Power BI in a healthcare or enterprise environment (five+ years of experience for senior analyst level; one to three years for analyst)
Experience using Python, R, Fabric, and Tableau a plus
Proficiency in Microsoft Word, Excel, and PowerPoint
Comfort with manipulating and synthesizing large data sets
Strong written and verbal communication skills
Excellent organizational skills
Job Locations
St. Louis and Washington, DC, offices are preferred. Other offices may include Atlanta, Boston, Chicago, Minneapolis, San Diego, or Seattle. Able to work a hybrid schedule with days in the office and remote.
Schedule
Full time/exempt
What You Can Expect Of Us
To reward our driven, innovative, and passionate employees, we’ve built a company culture that’s centered on performance. We offer an attractive compensation package, challenging work, and an entrepreneurial environment where you can take ownership of your career—and get out as much as you put in.
About ECG
ECG is a strategic consulting firm leading healthcare forward using knowledge and expertise built over the course of five decades to help clients see clearly where the industry is going and navigate toward success. We work as trusted, professional partners with hospitals, health systems, medical groups, and academic medical centers across the country. We thrive on delivering smart counsel and pragmatic solutions to the critical challenges facing healthcare providers. Client success is our primary objective. ECG’s national presence includes offices in Atlanta, Boston, Chicago, Dallas, Minneapolis, San Diego, Seattle, St. Louis, and Washington, DC.
Apply now and make an impact for years to come.
To begin the recruitment process, please submit your resume via our career site at https://careers.ecgmc.com.
ECG provides equal employment opportunities to all employees and applicants for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, disability, pregnancy, medical condition (cancer and genetic characteristics), genetic information, gender, gender identity or expression, sexual orientation, marital status, military or veteran status, or any other legally protected characteristic. People of color are encouraged to apply. We participate in E-Verify as part of our onboarding process. Having the permanent legal right to work in the United States is a condition of employment. ECG is not currently able to provide assistance to candidates requiring sponsorship or a visa.
Residents of the states of California or Washington may receive salary information for this job through this link or by contacting the recruiter directly at schavez@ecgmc.com.
Show more
Show less","Power BI, Microsoft Azure, Microsoft 365, SQL, Python, R, Fabric, Tableau, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Data analytics, Statistical analysis, Financial modeling, Market analysis, Data visualization, Data interpretation, Data management, Data mining, Business intelligence, Data warehousing, Data mining","power bi, microsoft azure, microsoft 365, sql, python, r, fabric, tableau, microsoft word, microsoft excel, microsoft powerpoint, data analytics, statistical analysis, financial modeling, market analysis, data visualization, data interpretation, data management, data mining, business intelligence, data warehousing, data mining","business intelligence, data interpretation, data management, data mining, dataanalytics, datawarehouse, fabric, financial modeling, market analysis, microsoft 365, microsoft azure, microsoft excel, microsoft powerpoint, microsoft word, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Sr. Data Analyst,SteadyMD,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-analyst-at-steadymd-3697498895,2023-12-17,Belleville,United States,Mid senior,Onsite,"Be part of a team enabling access to better healthcare at SteadyMD!
SteadyMD is a technology company and healthcare provider that powers high-quality telehealth experiences for its partners, including fast-growing digital healthcare companies, labs, pharmacies, large employers, and Fortune 100 companies like Amazon, AmerisourceBergen, and Abbott. SteadyMD initially launched in two states: California and Missouri. By 2018, the company was licensed, operating, and providing care in all 50 states. We’ve raised over $60 million in funding from top tier investors including Lux Capital, Pelion Ventures and AB Health Ventures.
We are currently seeking a talented
Sr. Data Analyst
to join our Product team. As a Sr. Data Analyst you will play a pivotal role in transforming raw data into meaningful insights. You will collaborate with cross-functional teams to analyze data, identify trends, and provide actionable recommendations. Reporting to the VP of Product Management, this position offers an exciting opportunity to work with different datasets, contribute to data-driven initiatives, and impact the success of our company. This is a hybrid role in which you will be located in the St. Louis, MO area.
At SteadyMD, we value what diverse teams can accomplish together, and we honor each of our unique lived experiences. We look for a diverse pool of applicants, including those from historically marginalized groups, and we are committed to ensuring a safe work environment that is distinctly anti-discriminatory against any person. This is one of the reasons we are ranked #81 on Forbes’ America’s Best Startup Employers List. We know the value of building a team that encompasses a variety of backgrounds, experiences, and skills.
Works with other teams to provide key analytics support in identifying process, software, and data improvements
Define, maintain, and communicate the data “source of truth” for cross-functional teams to use in assessing performance and quality
Provide support and documentation for associated business units to understand how to use data and insights
Creates and owns ETLs: identifies data sources, writes queries, validates, and makes modifications as needed
Write clean code that can be maintained and extended by other technical stakeholders
Ensure safe and secure data handling by partnering with company security leaders
Sets up and manages usage, reliability, quality, and performance of products, services, solutions or processes and proposes improvements
Guides business leaders with data-driven reports, dashboards, and visualizations
Interprets data, analyzes results, and provides insights to support data-driven decision-making for ad hoc and on-going reports
Stay up-to-date with industry technologies and frameworks to identify trends to maintain best and cutting edge practices in data analysis
Participate in special projects and initiatives as needed
Requirements
4+ years of related experience in a business analytics or data management experience
Relevant Industry Experience or Bachelor’s Degree with emphasis in: Information Technology, Mathematics, Management Information System (MIS), Statistics, Engineering, Computer Science, or related
Strong proficiency in:
SQL, Python, writing SQL queries
Advanced SQL aggregation functions
database performance concepts and query optimization techniques
Experience with Looker and Tableau
Can provide helpful insights from dense datasets
Creative self-starter capable of first principles thinking
Strong interpersonal skills that can work across a highly cross-functional environment
Excellent oral and written communication skills required
Detail oriented and strong organizational skills
Previous experience in a B2B SaaS start-up preferred
Experience in healthcare is a bonus
Benefits
Competitive Compensation. The annual salary range for this role is $85,000 - $110,000 depending on experience, and participation in the company bonus program
Fast-paced Startup Environment. An environment that is focused on disrupting the status quo and challenging conventional professional norms. We are focused on the results you can achieve, not how many hours you spend at a desk
Complimentary Lemonaid Primary Care Membership. So that you can experience what we have to offer and be able to speak first-hand about what the future of medicine will look like
Company-paid health, dental, and vision insurance. Also includes Basic Life and ADD offerings
401k with Match & Parental Leave Benefits offered to all full-time employees
Unlimited PTO. We trust our employees to make the right decisions for the business, and we also recognize that means taking time to take care of yourself
Show more
Show less","SQL, Python, Looker, Tableau, ETL, Data analysis, Data management, Datadriven decisionmaking, Database performance concepts, Query optimization, Industry technologies, Frameworks, Healthcare, B2B SaaS","sql, python, looker, tableau, etl, data analysis, data management, datadriven decisionmaking, database performance concepts, query optimization, industry technologies, frameworks, healthcare, b2b saas","b2b saas, data management, dataanalytics, database performance concepts, datadriven decisionmaking, etl, frameworks, healthcare, industry technologies, looker, python, query optimization, sql, tableau"
Lead Data Engineer,Kforce Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-at-kforce-inc-3782232154,2023-12-17,Belleville,United States,Mid senior,Onsite,"Responsibilities
Kforce has a client that is seeking a Lead Data Engineer in Saint Louis, MO. Your role here: From Day 1, you will collaborate closely with the data analytics, Power BI reporting, and functional teams to deliver top notch analytics. Key Tasks:
Lead Data Engineer provides technical leadership in Microsoft Synapse, spanning numerous services (Pipelines, Notebooks, Azure Storage, Logic Apps)
Interacts with various business functions to define requirements and build a strategy to implement within our modern data framework
Interacts with engineering team to implement a high quality, efficient, dependable, and secure data platform while maintaining consistent data standards, patterns, and practices across the organization (Data warehouse, Data Lakehouse)
As a Lead Data Engineer, you will perform code review and verifies compliance with code standards
Owns Data Analytics team delivery outcomes
What You Can Expect From Us
Our salaries are competitive
Comprehensive benefits in medical, dental, and vision insurance
401(k) plan with employer match
Paid time off plus holidays
Tuition reimbursement, and much more
Requirements
To be considered for this position, candidates must have experience in a similar role, or they must possess significant knowledge, experience, and abilities to successfully perform the responsibilities listed
Relevant education and/or training will be considered a plus
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $128,000 - $140,000 per year
Show more
Show less","Data Engineering, Microsoft Synapse, Pipelines, Notebooks, Azure Storage, Logic Apps, Power BI, Data Analytics, Data Framework, Data Warehouse, Data Lakehouse, Code Review, Data Compliance, Medical Insurance, Dental Insurance, Vision Insurance, 401(k) Plan, Paid Time Off, Holidays, Tuition Reimbursement","data engineering, microsoft synapse, pipelines, notebooks, azure storage, logic apps, power bi, data analytics, data framework, data warehouse, data lakehouse, code review, data compliance, medical insurance, dental insurance, vision insurance, 401k plan, paid time off, holidays, tuition reimbursement","401k plan, azure storage, code review, data compliance, data engineering, data framework, data lakehouse, dataanalytics, datawarehouse, dental insurance, holidays, logic apps, medical insurance, microsoft synapse, notebooks, paid time off, pipelines, powerbi, tuition reimbursement, vision insurance"
Senior Software Engineer (Python/Data Pipelines),Diversity Resource Staffing Inc.,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-software-engineer-python-data-pipelines-at-diversity-resource-staffing-inc-3548643839,2023-12-17,Belleville,United States,Mid senior,Onsite,"As Senior Software Engineer specializing in Python, you will be responsible for designing, developing, maintaining, and operationalizing quantitative models and analytic environments supporting NISA’s core line of business. You will work closely with business counterparts and our quantitative research groups to understand their needs, formulate requirements, design creative and extensible solutions for data distribution, analysis, and modeling for consumption by internal business groups.
As a senior member of the IT Solutions team, you will also participate in design and code reviews, will collaborate with other team members, and mentor junior teammate.
Required Qualifications
Bachelor's degree or equivalent experience in a field requiring strong analytical and quantitative skills, such as Computer Science, Engineering, Mathematics, Finance, or Information Systems
Significant experience developing using Python (expertise in other high-level languages considered)
Extensive experience designing queries and data structures in SQL Server or another relational database platform
Experience in data science and data analysis
Preferred Qualifications
Experience building solutions using public cloud platforms (AWS, Azure, GCP)
Experience designing & implementing data pipelines to support quantitative research
Familiarity with MATLAB or R
Experience with unit testing frameworks
Working knowledge of modern application frameworks
Previous professional experience in financial services sector is a plus
Show more
Show less","Python, SQL, Data Science, Data Analysis, Cloud Platforms (AWS Azure GCP), MATLAB, R, Unit Testing Frameworks, Application Frameworks","python, sql, data science, data analysis, cloud platforms aws azure gcp, matlab, r, unit testing frameworks, application frameworks","application frameworks, cloud platforms aws azure gcp, data science, dataanalytics, matlab, python, r, sql, unit testing frameworks"
Data Automation Engineer,The Wise Seeker,"Arnold, MO",https://www.linkedin.com/jobs/view/data-automation-engineer-at-the-wise-seeker-3776917565,2023-12-17,Belleville,United States,Mid senior,Onsite,"As the Data Automation Engineer, you'll be exposed and contribute to the development and application of innovative technologies such as machine learning, artificial intelligence, and advanced data analytics. Our work depends on a Data Engineer joining our team to support our intelligence customer in Springfield, VA or St. Louis, MO.
What you will be working on:
Support the Geospatial Services and Solutions business area to provide high-quality, cost-effective solutions to the customer
Design and implement automation solutions to enhance data capture, data refinement, and processes. Coding examples include:
Interfacing with device APIs in order to collect operational metrics
Providing automated VoIP phone setup
Administering and automating data pipelines between different environments
Produce and deploy code via GitLab projects in collaboration with other team members
Utilize best practices for source control, testing, and deployment of software changes
Work in close collaboration with other automation engineers, infrastructure administrators, and data scientists
Diagnose, isolate, and expediently resolve complex problems pertaining to data structures
Develop methods of ensuring data incompatibilities among systems are systematically eliminated
Develop and recommend data management policies, standards, practices, and security measures to ensure effective and consistent data management operations
Participate in continuous improvement efforts to increase data availability, data quality, and speed of access
Maintain up-to-date documentation of designs/configurations, ensuring team members have continuity of recurring tasks
In office work requirement > 80%
Travel requirement 0%
What you will bring to us:
Bachelor's Degree in Computer Science or related technical discipline, or the equivalent combination of education, technical certifications or training, and work experience
8+ years of related systems engineering experience
Scripting, coding, or software development experience
Comfort with Linux/Windows command-line
Automation mindset
System administration and/or DevOps environment experience
Active TS/SCI clearance and eligibility to obtain a CI poly
Would be nice if you bring the following:
Python experience
Shell scripting experience such as Bash or PowerShell
Experience with Database technologies such as Postgres, SQL Server, Oracle, or MySQL
Experience writing and working with SQL commands
Version control experience with Git
Experience with Gitlab and Git workflows
Familiarity with Agile Scrum methodologies
Time management skills and the drive to work with limited supervision within a small team
Bonus Skills:
Web App development experience such as Flask, Django, React, etc.
UI/UX experience
Experience with Analytics tools such as Tableau
Infrastructure as Code experience
Experience in technical operations at DoD/IC agencies
Cloud experience such as AWS, Azure, GCP, etc.
Show more
Show less","Machine Learning, Artificial Intelligence, Data Analytics, Data Engineering, Geospatial Services, Automation Solutions, Data Capture, Data Refinement, Device APIs, VoIP, Data Pipelines, GitLab, Source Control, Software Deployment, Data Structures, Data Management, Data Security, Data Availability, Data Quality, Data Access, Technical Documentation, Computer Science, Systems Engineering, Scripting, Coding, Software Development, Linux, Windows, Automation, System Administration, DevOps, Clearance, Python, Shell Scripting, Bash, PowerShell, Database Technologies, Postgres, SQL Server, Oracle, MySQL, SQL Commands, Version Control, Git, Gitlab, Agile Scrum, Time Management, Flask, Django, React, UI/UX, Tableau, Infrastructure as Code, DoD/IC agencies, Cloud, AWS, Azure, GCP","machine learning, artificial intelligence, data analytics, data engineering, geospatial services, automation solutions, data capture, data refinement, device apis, voip, data pipelines, gitlab, source control, software deployment, data structures, data management, data security, data availability, data quality, data access, technical documentation, computer science, systems engineering, scripting, coding, software development, linux, windows, automation, system administration, devops, clearance, python, shell scripting, bash, powershell, database technologies, postgres, sql server, oracle, mysql, sql commands, version control, git, gitlab, agile scrum, time management, flask, django, react, uiux, tableau, infrastructure as code, dodic agencies, cloud, aws, azure, gcp","agile scrum, artificial intelligence, automation, automation solutions, aws, azure, bash, clearance, cloud, coding, computer science, data access, data availability, data capture, data engineering, data management, data quality, data refinement, data security, data structures, dataanalytics, database technologies, datapipeline, device apis, devops, django, dodic agencies, flask, gcp, geospatial services, git, gitlab, infrastructure as code, linux, machine learning, mysql, oracle, postgres, powershell, python, react, scripting, shell scripting, software deployment, software development, source control, sql commands, sql server, system administration, systems engineering, tableau, technical documentation, time management, uiux, version control, voip, windows"
Sr Data Analyst,Cushman & Wakefield,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-analyst-at-cushman-wakefield-3779636196,2023-12-17,Belleville,United States,Mid senior,Onsite,"Job Title
Sr Data Analyst
Job Description Summary
The role is for a Senior Data Analyst as part of a team collecting and analyzing data supporting ad hoc and strategic client projects primarily involving architectural building design.
The candidate will provide advanced expertise in data analysis, collaborate with key client partners, support the data team with identifying project requirements, refining project work, and providing recommendations on optimizing data management and workflows.
Job Description
Core Responsibilities
Collect, clean, study, transform, load, and visualize data for ad hoc and strategic projects
Identify trends and provide insights from data that contribute to solving business problems
Code programs, as needed, to help capture and organize relevant data
Collaborate directly with internal and external partners to satisfy project needs
Clearly communicate useful information to business partners derived from data analysis
Assist in managing completion of team data analysis tasks
Lead problem-solving and refinement of project activities and tasks
Lead project identifying requirements from analysis of current state versus desired future state
Identify opportunities for workflow optimization
Qualifications
Three or more years of experience in data analytics, data management, or related roles
Advanced knowledge of data analytics, cleaning, preparation, and visualization techniques
Advanced experience with data analytics tools and programs (Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau)
Advanced understanding of best practices in data management and visualization
Strong critical thinking and problem solving skills
Strong focus on solutions serving client/end user
Ability to write and speak clearly to both technical and non-technical audiences
Keen attention to both technical detail and quality of work acceptable to client/end user
Demonstrated ability to collaborate effectively with partners across multiple teams
Strong ability to prioritize work tasks in alignment with changing project and team needs
Preferred candidate will have experience managing/analyzing architectural design data
Cushman & Wakefield provides equal employment opportunity. Discrimination of any type will not be tolerated. Cushman & Wakefield is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other characteristic protected by state, federal, or local law.
In compliance with the Americans with Disabilities Act Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position at Cushman & Wakefield, please call the ADA line at
1-888-365-5406
or email
HRServices@cushwake.com
. Please refer to the job title and job location when you contact us.
Show more
Show less","Data Analytics, Data Management, Data Analysis, Data Visualization, Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau, ProblemSolving, Data Cleaning, Data Preparation, Data Loading, Critical Thinking, Attention to Detail, Communication, Collaboration, Prioritization, Workflow Optimization, Business Intelligence, Architecture Design Data","data analytics, data management, data analysis, data visualization, microsoft excel, microsoft power bi, python, sql, tableau, problemsolving, data cleaning, data preparation, data loading, critical thinking, attention to detail, communication, collaboration, prioritization, workflow optimization, business intelligence, architecture design data","architecture design data, attention to detail, business intelligence, collaboration, communication, critical thinking, data cleaning, data loading, data management, data preparation, dataanalytics, microsoft excel, microsoft power bi, prioritization, problemsolving, python, sql, tableau, visualization, workflow optimization"
Lead Data Engineer,TricorBraun,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-at-tricorbraun-3733281824,2023-12-17,Belleville,United States,Mid senior,Onsite,"Make our IT Team your best move ever
We’re looking for a highly detailed Lead Data Engineer to join our IT team to deliver reliable and scalable analytics.
Why here
?
TricorBraun is a global packaging leader, with team members working in locations throughout the Americas, Europe, Asia, Australia and New Zealand. As North America’s largest distributor of primary packaging, we provide innovative solutions to customers from a wide variety of industries. Our customers range from cutting-edge start-ups to the world’s most iconic brands. We put people first and live by that every day. Join us and you will be welcomed by our friendly, motivated and supportive team. Many of the products we distribute are already sitting in your home.
Your role here
From Day 1, you will collaborate closely with the data analytics, Power BI reporting, and functional teams to deliver top notch analytics.
Your Experience And Background
Provides technical leadership in Microsoft Synapse, spanning numerous services (Pipelines, Notebooks, Azure Storage, Logic Apps)
Interacts with various business functions to define requirements and build a strategy to implement within our modern data framework.
Interacts with engineering team to implement a high quality, efficient, dependable, and secure data platform while maintaining consistent data standards, patterns, and practices across the organization (Data warehouse, Data Lakehouse)
Performs code review and verifies compliance with code standards.
Owns Data Analytics team delivery outcomes.
What You Can Expect From Us
Because we’re a well-known and respected leader in packaging, we have many opportunities here. We’ll get you started with an exceptional training program providing classroom, online and hands on work with colleagues. There’s always someone to answer any questions and ensure you’re getting the right information you need to excel. And the compensation and benefits are what you can expect from a people-first company.
Our salaries are competitive
Comprehensive benefits in medical, dental, and vision insurance
401(k) plan with employer match
Paid time off plus holidays
Tuition reimbursement, and much more
We are proudly an equal-opportunity employer and will consider all applications.
Show more
Show less","Data Engineering, Microsoft Synapse, Azure Storage, Logic Apps, Power BI, Data Analytics, Data Warehouse, Data Lakehouse, Code Review, Data Standards, Data Patterns, Data Practices","data engineering, microsoft synapse, azure storage, logic apps, power bi, data analytics, data warehouse, data lakehouse, code review, data standards, data patterns, data practices","azure storage, code review, data engineering, data lakehouse, data patterns, data practices, data standards, dataanalytics, datawarehouse, logic apps, microsoft synapse, powerbi"
Security Engineer III - Data Security,Stifel Financial Corp.,"St Louis, MO",https://www.linkedin.com/jobs/view/security-engineer-iii-data-security-at-stifel-financial-corp-3739533819,2023-12-17,Belleville,United States,Mid senior,Onsite,"Summary
Under minimal supervision, the Data Security Engineer III is a front-line member of the Data Security team that has responsibility for protecting corporate information assets. The Data Security Engineer III will be responsible for configuring and improving DLP policies on multiple tools, working towards increasing DLP Program coverage, crafting and maintaining DLP Program process documentation, defining new processes and controls to further mature the DLP Program, and addressing gaps that impact the DLP process.
Essential Duties & Responsibilities
Deploys and manages technology and process solutions to reduce the potential of data compromise
Develops technical requirements, evaluating vendor solutions, and testing of data security solutions
Utilizes security tools to enhance data loss prevention capabilities across the Enterprise
Tune DLP policies on a continuous basis to maintain a mature set of policies within the scope of the DLP Program.
Implement security policies to comply with data privacy, governance and regulatory requirements
Performs data protection monitoring and reporting, analyzes security alerts and escalates security alerts to local support teams.
Proposes improvements and assists in the implementation of enterprise wide security policies, procedures and standards to meet compliance responsibilities.
Prepares status reports to develop security risk analysis scenarios.
Assist in documenting standard operating procedures and protocols for the Data Security Pillar
Assist in the development of technical solutions and processes to help mitigate security vulnerabilities and automate repeatable tasks.
Partner with teams as needed to enhance DLP monitoring / response processes on an ongoing basis.
Qualifications
Strong understand of how to identify and prioritize security incidents and/or escalate to management or other team members.
Experience translating data security questions into data analytical approaches
Hands-on experience in security systems, including data loss prevention, data classification, etc.
Understanding of data classification frameworks and processes
Knowledge of information security standards including CIS Critical controls and the NIST Cybersecurity Framework.
Ability to systematically assess a problem or situation to accurately identify probable causes and solutions.
Understanding of a broad range of IT disciplines that would impact overall security posture.
Proficiency in relating complex technical situations to non-technical customers.
Ability to prioritize workload and consistently meet deadlines
Education & Experience
Bachelor's degree in Computer Science, Information Systems, Cybersecurity, or related field; or related combination of education and experience
4+ years experience in an information technology or information security role
CISSP, CISM, or Security+ certifications preferred
About Stifel
Stifel is a more than 130 years old and still thinking like a start-up. We are a global wealth management and investment banking firm serious about innovation and fresh ideas. Built on a simple premise of safeguarding our clients’ money as if it were our own, coined by our namesake, Herman Stifel, our success is intimately tied to our commitment to helping families, companies, and municipalities find their own success.
While our headquarters is in St. Louis, we have offices in New York, San Francisco, Baltimore, London, Frankfurt, Toronto, and more than 400 other locations. Stifel is home to approximately 9,000 individuals who are currently building their careers as financial advisors, research analysts, project managers, marketing specialists, developers, bankers, operations associates, among hundreds more. Let’s talk about how you can find your place here at Stifel, where success meets success.
At Stifel we offer an entrepreneurial environment, comprehensive benefits package to include health, dental and vision care, 401k, wellness initiatives, life insurance, and paid time off.
Stifel is an Equal Opportunity Employer.
Show more
Show less","Data Security, DLP Tools, Data Loss Prevention, Security Policies, Security Alerts, Security Standards, Risk Analysis, Data Classification Frameworks, NIST Cybersecurity Framework, CIS Critical Controls, IT Disciplines, Technical Communication, Workload Prioritization, Computer Science, Information Systems, Cybersecurity, CISSP, CISM, Security+","data security, dlp tools, data loss prevention, security policies, security alerts, security standards, risk analysis, data classification frameworks, nist cybersecurity framework, cis critical controls, it disciplines, technical communication, workload prioritization, computer science, information systems, cybersecurity, cissp, cism, security","cis critical controls, cism, cissp, computer science, cybersecurity, data classification frameworks, data loss prevention, data security, dlp tools, information systems, it disciplines, nist cybersecurity framework, risk analysis, security, security alerts, security policies, security standards, technical communication, workload prioritization"
Staff Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395481,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Data Analyst,Kforce Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-analyst-at-kforce-inc-3782228566,2023-12-17,Belleville,United States,Mid senior,Onsite,"Responsibilities
Kforce has a client in Saint Louis, MO that is seeking a Senior Data Analyst. Responsibilities:
Provides guidance on the most complex and unique matters related to data reporting and analytic techniques that have a significant impact to the region and ministry; May lead work of others within business operations
Ensures collaboration with key decision makers to identify and solve a variety of business or operational problems and objectives
Interprets complex data, identifies trends, establishes/utilizes benchmark data, creates data visualization, and applies information to the business to determine impact, significance, and opportunities
Responsible for developing specifications of analytic and reporting needs through participation in project teams or meetings with internal/external customers for projects and/or ad hoc requests
Utilizes a variety of databases and advanced query tools to gather data and information
Ensures all requirements are well defined from the customer to meet reporting and analysis specifications
Ensures data integrity and data quality; Supplies business and leadership with deep insights into the daily operations and overall business operations; Viewed as a subject matter expert for the team
Partners closely with technical and build team members to establish reports and dashboards within applications that will support operations
Partners with customers, peers and cross functional teams to discover new business needs; Proactively offer options and solutions
Oversees the evaluation, analysis, and development of reporting and analytical tools to analyze department and division performance indicators
Leverages advanced analytic techniques necessary to meet needs of business users supported by the analytic team
Supports both new and ongoing operational reporting needs, business process redesign, process improvement initiatives by providing data insights
Requirements
Bachelor's degree or equivalent combination of education and experience
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $65 - $75 per hour
Show more
Show less","Data analysis, Data reporting, Data visualization, Business operations, Decision making, Data interpretation, Trend analysis, Benchmarking, Data integrity, Data quality, Deep insights, Reporting needs, Analytical tools, Business insights, Data mining, Process redesign, Process improvement, Datadriven decision making, Advanced analytic techniques, Compensation, Benefits, Equal Opportunity/Affirmative Action","data analysis, data reporting, data visualization, business operations, decision making, data interpretation, trend analysis, benchmarking, data integrity, data quality, deep insights, reporting needs, analytical tools, business insights, data mining, process redesign, process improvement, datadriven decision making, advanced analytic techniques, compensation, benefits, equal opportunityaffirmative action","advanced analytic techniques, analytical tools, benchmarking, benefits, business insights, business operations, compensation, data integrity, data interpretation, data mining, data quality, data reporting, dataanalytics, datadriven decision making, decision making, deep insights, equal opportunityaffirmative action, process improvement, process redesign, reporting needs, trend analysis, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827522,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, ETL, Dimensional Data Modeling, Schema Design, Legal Compliance, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, etl, dimensional data modeling, schema design, legal compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744399380,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Data Management Tools, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data management tools, data classification, data retention","agile engineering practices, airflow, automation, continuous delivery, data classification, data engineering, data management tools, data retention, data warehouses, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Junior Data Scientist,Concero,Greater St. Louis,https://www.linkedin.com/jobs/view/junior-data-scientist-at-concero-3771636398,2023-12-17,Belleville,United States,Mid senior,Onsite,"Job Description
Onsite St. Louis, MO
Data Scientist
- Design and analyze lift experiments to drive product improvements with cross-functional teams.
- Conduct research and analysis to improve the statistical/ML model lift using experimental design and causal inference methods
- Write SQL queries to clean, aggregate, and/or impute data from multiple tables and/or across jump servers
- Manage IT tickets for simultaneous projects and tasks
- Maintain proper project documentation (Project scope, business requirements, workflow charts, data mapping, etc.)
- Build cross-functional relationships with SQL developers, business analysts, product marketing, Customer Success Managers (CSM), and other key stakeholders to identify opportunities to improve products, drive product launches, and influence product roadmaps
Qualifications
- Bachelor's degree in a quantitative discipline (e.g., Statistics, Engineering, Biostatistics, Economics, Computer Science, Mathematics, Physics) and/or 2+ years of equivalent practical experience
- knowledge and experience in statistical methodologies, especially probability, hypothesis testing, experimental design, and causal inference
- Familiarity with machine learning algorithms and A.I. methodologies
- Understanding of project management principles
- Basic understanding of A/B testing
- Practical experience querying and analyzing large datasets using SQL
- Proficient in scripting languages like Java, JavaScript, Python, and/or R
- Able to work in an innovative and fast-paced environment
#5737
Show more
Show less","Data Analysis, Hypothesis Testing, Experimental Design, AI, Project Management, A/B Testing, Machine Learning, Causal Inference, SQL, Statistical Methods, Python, Java, JavaScript, R","data analysis, hypothesis testing, experimental design, ai, project management, ab testing, machine learning, causal inference, sql, statistical methods, python, java, javascript, r","ab testing, ai, causal inference, dataanalytics, experimental design, hypothesis testing, java, javascript, machine learning, project management, python, r, sql, statistical methods"
Senior Statistical Data Analyst - Biostatistics,Washington University in St. Louis,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-statistical-data-analyst-biostatistics-at-washington-university-in-st-louis-3727312628,2023-12-17,Belleville,United States,Mid senior,Onsite,"Scheduled Hours
40
Position Summary
This position is in the Xiong Lab in the Center for Biostatistics. The Center for Biostatistics is part of the Institute for Informatics, Data Science and Biostatistics and the Office of Health Information and Data Science (OHIDS). At OHIDS we have a people-centric approach where our team members are our number one asset. We are committed to providing safe and inclusive working conditions and take great care to support employees’ health and well-being. We support a flexible work environment in order to support a range of work schedules and better work-life guardrails. Ensuring the well-being of our team members is a top priority.
Performs database management, and data harmonization across different studies, and data sharing, as well as data analysis expertly using statistical packages. Assists investigators in the design of experiments, clinical trials, and epidemiological studies. May be required to assume major responsibility on large project.
Job Description
Primary Duties & Responsibilities
Performs management duties as part of several large NIH-funded longitudinal data repository containing clinical, psychological, imaging, neuropathologic and biomarker data.
Harmonizes databases across multiple independent studies by working with database team of each study, standardizing database dictionary and creating standard metadata template for data sharing with both internal and external investigators.
Transfers QC’d data to national and local data repository as well as to individual investigators who requested the data, and track the data sharing.
Monitors and evaluates data integrity.
Performs quality control on all forms of data (paper, electronic, etc.);
Assists with research coordination of projects.
Assists with research management (administration, scheduling, tracking, recruitment, etc.).
Assists in development of data capture forms and screens.
Utilizes REDCap and other EDC systems as required for generation of such forms and reports.
Generate routine reports on statistics and figures and tables summarizing the current status of databases, using standard statistical packages, SAS, R.
Performs other duties as assigned (e.g. may assist in manuscript preparation).
Preferred Qualifications
Master’s degree in Biostatistics, Statistics, Informatics, Computer Science or related field.
Expert knowledge base and experience with database design, management and analysis.
PC/UNIX knowledge.
Skills and demonstrated success in designing and executing appropriate statistical data analysis in SAS, R Python, or other packages.
Experience in REDCap.
Capable of assuming a leadership role.
Demonstrated effective verbal, written and interpersonal communication skills.
Demonstrated organization skills.
Required Qualifications
Bachelor’s degree and three years of related experience; a combination of college education and/or relevant experience equaling seven years may substitute for this requirement.
Grade
G13
Salary Range
$64,700.00 - $110,500.00 / Annually
The salary range reflects base salaries paid for positions in a given job grade across the University. Individual rates within the range will be determined by factors including one's qualifications and performance, equity with others in the department, market rates for positions within the same grade and department budget.
Accommodation
If you are unable to use our online application system and would like an accommodation, please email CandidateQuestions@wustl.edu or call the dedicated accommodation inquiry number at 314-935-1149 and leave a voicemail with the nature of your request.
Pre-Employment Screening
All external candidates receiving an offer for employment will be required to submit to pre-employment screening for this position. The screenings will include criminal background check and, as applicable for the position, other background checks, drug screen, an employment and education or licensure/certification verification, physical examination, certain vaccinations and/or governmental registry checks. All offers are contingent upon successful completion of required screening.
Benefits Statement
Personal
Up to 22 days of vacation, 10 recognized holidays, and sick time.
Competitive health insurance packages with priority appointments and lower copays/coinsurance.
Want to Live Near Your Work and/or improve your commute? Take advantage of our free Metro transit U-Pass for eligible employees. We also offer a forgivable home loan of up to $12,500 for closing costs and a down payment for homes in eligible neighborhoods.
WashU provides eligible employees with a defined contribution (403(b)) Retirement Savings Plan, which combines employee contributions and university contributions starting at 7%.
Wellness
Wellness challenges, annual health screenings, mental health resources, mindfulness programs and courses, employee assistance program (EAP), financial resources, access to dietitians, and more!
Family
We offer 4 weeks of caregiver leave to bond with your new child. Family care resources are also available for your continued childcare needs. Need adult care? We’ve got you covered.
WashU covers the cost of tuition for you and your family, including dependent undergraduate-level college tuition up to 100% at WashU and 40% elsewhere after seven years with us.
For policies, detailed benefits, and eligibility, please visit: https://hr.wustl.edu/benefits/
EEO/AA Statement
Washington University in St. Louis is committed to the principles and practices of equal employment opportunity and especially encourages applications by those from underrepresented groups. It is the University’s policy to provide equal opportunity and access to persons in all job titles without regard to race, ethnicity, color, national origin, age, religion, sex, sexual orientation, gender identity or expression, disability, protected veteran status, or genetic information.
Diversity Statement
Washington University is dedicated to building a diverse community of individuals who are committed to contributing to an inclusive environment – fostering respect for all and welcoming individuals from diverse backgrounds, experiences and perspectives. Individuals with a commitment to these values are encouraged to apply.
Show more
Show less","Data management, Data harmonization, Data analysis, Statistical packages, SAS, R, Python, REDCap, Database design, Database management, PC/UNIX, Statistical data analysis, Verbal communication, Written communication, Interpersonal communication, Organization skills","data management, data harmonization, data analysis, statistical packages, sas, r, python, redcap, database design, database management, pcunix, statistical data analysis, verbal communication, written communication, interpersonal communication, organization skills","data harmonization, data management, dataanalytics, database design, database management, interpersonal communication, organization skills, pcunix, python, r, redcap, sas, statistical data analysis, statistical packages, verbal communication, written communication"
Healthcare Data Analyst (Remote),Conexess Group,"St Louis, MO",https://www.linkedin.com/jobs/view/healthcare-data-analyst-remote-at-conexess-group-3784395054,2023-12-17,Belleville,United States,Mid senior,Remote,"Our History:
From our start in 2009, Conexess has established itself in 3 markets, employing nearly 200+ individuals nation-wide. Operating in over 15 states, our client base ranges from Fortune 500/1000 companies to mid-small range companies. For the majority of the mid-small range companies, we are exclusively used due to our outstanding staffing track record.
Who We Are:
Conexess is a full-service staffing firm offering contract, contract-to hire, and direct placements. We have a wide range of recruiting capabilities extending from help desk technicians to CIOs. We are also capable of offering project-based work.
Conexess Group is aiding a large healthcare client in their search for a Healthcare Data Analyst in a remote capacity. This is a long-term opportunity with a competitive compensation package.
Responsibilities:
Provides advanced professional input to complex Business Analytics assignments/projects.
Works in collaboration with business partners to provide analyses and reports to aide decision making based on data, facts, and analytical findings across different parts of the organization.
Analyzes and interprets collected data, spots trends, writes reports and recommendations and completes data modeling.
Supports and provides direction to more junior professionals.
Works autonomously, only requiring “expert” level technical support from others.
Exercises judgment in the evaluation, selection, and adaptation of both standard and complex techniques and procedures.
Utilizes in-depth professional knowledge and acumen to develop models and procedures, and monitor trends, within Business Analytics.
Qualifications:
Bachelor's Degree in related field required. Advanced degree preferred.
Experience with SQL, Relational Models, Hadoop, Tableau, Alteryx
Experience with Databricks Platform.
Ability to work effectively in a dynamic, rapidly changing environment; operates well with ambiguity.
2+ years relevant work experience in healthcare advanced analytics.
Show more
Show less","SQL, Relational Models, Hadoop, Tableau, Alteryx, Databricks Platform, Business Analytics, Data Analytics, Data Modeling, Reporting, Data Interpretation, Trend Analysis, Data Visualization, Decision Making, Project Management, Communication, Collaboration, Problem Solving, Analytical Thinking, Critical Thinking, Attention to Detail, Time Management, Microsoft Office Suite","sql, relational models, hadoop, tableau, alteryx, databricks platform, business analytics, data analytics, data modeling, reporting, data interpretation, trend analysis, data visualization, decision making, project management, communication, collaboration, problem solving, analytical thinking, critical thinking, attention to detail, time management, microsoft office suite","alteryx, analytical thinking, attention to detail, business analytics, collaboration, communication, critical thinking, data interpretation, dataanalytics, databricks platform, datamodeling, decision making, hadoop, microsoft office suite, problem solving, project management, relational models, reporting, sql, tableau, time management, trend analysis, visualization"
Senior Database Engineer - SQL,Enterprise Mobility,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-database-engineer-sql-at-enterprise-mobility-3769040147,2023-12-17,Belleville,United States,Mid senior,Remote,"Description
Enterprise Mobility is the world’s largest car rental operator and an industry leader in mobility and technology. We’re one of the top global travel companies, ranking ahead of many airlines and most cruise lines and hotels. And no matter what transportation challenges our customers face, we have an innovative solution.
We operate the Enterprise Rent-A-Car National Car Rental and Alamo Rent A Car brands via more than 10,000 fully staffed neighborhood and airport offices, including franchisee branches, in over 90 countries and territories.
Through this robust global network, we operate a fleet of over 2.3 million vehicles and provide a comprehensive portfolio of transportation solutions, including car rental, carsharing, vanpooling, car sales, truck rental, vehicle-subscription and affiliated fleet management services. As a total mobility provider, we serve the needs of a wide variety of customers, businesses, government agencies and organizations every day.
At the center of it all, our dedicated IT teams innovate, design and develop the technology that is redefining how customers rent, buy and share vehicles from our family of brands. Here, you will be part of a diverse and talented team that creates and delivers powerful technology solutions for our customers and employees across the world with the resources and support to develop in a variety of career paths.
As an Enterprise employee, we offer an excellent package with market-competitive pay, comprehensive healthcare packages, 401k matching & profit sharing, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.
This position is open to candidates who wish to work from home (WFH). Employees who choose virtual / remote work should have an adequate space to serve as their home office.
Responsibilties
As an
Senior Database Engineer
, you will be responsible for using your technical knowledge of professional concepts to solve business problems. We are looking for a talented individual that can work on a diverse work tasks and work with others in the department on complex assignments. You will be responsible for evaluating elements of technology's effectiveness through requirements gathering, testing, research and investigation and make recommendations for improvements that result in increased quality and effectiveness.
In this role, you will play a significant role in the implementation, migration, and maintenance of SQL Server On Prem and Azure SQL production databases. In addition, you will monitor daily database activities and overall performance, growth, and tuning configurations as well as the security patching of the databases. You will assist with process improvements and change activities, as well as participate in the teams on-call. We are looking for a self-starter that will maintain established service level agreements to meet customer expectations and quality standards.
Equal Opportunity Employer/Disability/Veterans
Qualifications
Required:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3+ years of related experience
3+ years of SQL Database engineering experience
3+ years of experience with Microsoft and SQL scripting to automate tasks such as monitoring, patching, and generating reports
2+ years of Azure Cloud Database experience
Strong detail-oriented skills combined with sound problem solving and time management
Strong verbal and written communication skills
Proven experience working in a fast-paced production environment
Must be able to effectively collaborate and work with others in a remote work environment
Must be able to work effectively in a change management environment
Preferred
Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferred
Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance
Knowledge of JIRA and Confluence a plus
Knowledge of BitBucket and Git a plus
Knowledge of Office 365 Windows environment
Knowledge of Database Client software
Knowledge of ServiceNow a plus
Knowledge of SQL to Azure migration processes
#IT
Show more
Show less","SQL Server, Azure SQL, Microsoft SQL, JIRA, Confluence, BitBucket, Git, Office 365, Database Client software, ServiceNow, Azure Cloud Database, SQL to Azure migration processes","sql server, azure sql, microsoft sql, jira, confluence, bitbucket, git, office 365, database client software, servicenow, azure cloud database, sql to azure migration processes","azure cloud database, azure sql, bitbucket, confluence, database client software, git, jira, microsoft sql, office 365, servicenow, sql server, sql to azure migration processes"
Data Engineer - Scala(U.S. remote),Railroad19,"St Louis, MO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782294502,2023-12-17,Belleville,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, RESTful APIs, AWS, EMR clusters, S3, Relational databases, Nonrelational databases, Hadoop, Java","scala 212, spark 24, restful apis, aws, emr clusters, s3, relational databases, nonrelational databases, hadoop, java","aws, emr clusters, hadoop, java, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
CRM Data Analyst,Netskope,"St Louis, MO",https://www.linkedin.com/jobs/view/crm-data-analyst-at-netskope-3756678621,2023-12-17,Belleville,United States,Mid senior,Remote,"About Netskope
Today, there's more data and users outside the enterprise than inside, causing the network perimeter as we know it to dissolve. We realized a new perimeter was needed, one that is built in the cloud and follows and protects data wherever it goes, so we started Netskope to redefine Cloud, Network and Data Security.
Since 2012, we have built the market-leading cloud security company and an award-winning culture powered by hundreds of employees spread across offices in Santa Clara, St. Louis, Bangalore, London, Melbourne, and Tokyo. Our core values are openness, honesty, and transparency, and we purposely developed our open desk layouts and large meeting spaces to support and promote partnerships, collaboration, and teamwork. From catered lunches and office celebrations to employee recognition events (pre and hopefully post-Covid) and social professional groups such as the Awesome Women of Netskope (AWON), we strive to keep work fun, supportive and interactive. Visit us at Netskope Careers. Please follow us on LinkedIn and Twitter@Netskope.
About the position:
The Netskope Data Operations team is looking for a CRM (Customer Relationship Management) Data Analyst, who will help drive data operational excellence. As a member of the Data Operations team, this candidate will be responsible for the integrity, completeness and quality of data within the CRM. This individual will develop and implement standard procedures for capturing, cleansing, and updating data in the CRM and document data governance policies.
Netskope’s Go-to-Market (GTM) Data Operations team exists to maintain accurate, actionable, and non-duplicative Account, Lead and Contact data that accurately reflects corporate hierarchies and key data points enabling Sales to better manage Customers and target Prospects. This role requires subject matter expertise on enrichment, normalization and cleansing of data in Salesforce and an understanding of how changes can impact other systems and processes.
Responsibilities:
Regularly perform data cleansing and validation processes to ensure data accuracy, completeness, and consistency
Proactively identify data quality issues and work with relevant teams to implement improvements
Perform tasks related to data acquisition, validation and enrichment as they relate to Accounts, Leads and Contacts
Work cross-functionally within the organization to ensure data from other systems (such as marketing automation tools, sales and customer service platforms) are properly integrated with Salesforce to ensure data integrity is maintained
Propose and implement data governance solutions for the proper management of CRM data
Identify opportunities to streamline data-related processes and workflows to drive greater efficiency and accuracy.
Provide support to cross functional teams such as: training on data entry best practices and troubleshooting issues related to data quality
Requirements:
5+ years of work experience as a technical data steward with a strong background in Data Governance Policies, Processes and Tools
Experience in Salesforce, Excel, 3rd Party Data Tools (e.g. Zoominfo, D&B) and Data Cleansing Tools (e.g. Ringlead, Demandtools, Openprise)
Strong attention to detail with a focus on building standardized and consistent best practices
Understanding of marketing and privacy regulations (e.g. Opt In/Out, GDPR)
Proven ability to collaborate well with others and meet deadlines
Persistent in finding accurate information leveraging all available resources
Experience documenting business processes and developing training materials for data integrity and governance
Preferred Qualifications:
High level of integrity and transparency in working with peers and stakeholders
Comfort with ambiguity and proven ability to work independently
Strong written and verbal communication skills
Ability to multitask and meet Service Level Agreements (SLAs), specific to work efforts
Proven ability to meet timelines and deliver business results
A self-starter who is detail-oriented and can engage across stakeholders with confidence
Education:
Bachelor’s degree
Netskope is committed to implementing equal employment opportunities for all employees and applicants for employment. Netskope does not discriminate in employment opportunities or practices based on religion, race, color, sex, marital or veteran statues, age, national origin, ancestry, physical or mental disability, medical condition, sexual orientation, gender identity/expression, genetic information, pregnancy (including childbirth, lactation and related medical conditions), or any other characteristic protected by the laws or regulations of any jurisdiction in which we operate.
Netskope respects your privacy and is committed to protecting the personal information you share with us, please refer to Netskope's Privacy Policy for more details.
Show more
Show less","Salesforce, Data Governance, Data Cleansing, Data Integrity, Data Quality, Data Enrichment, Data Validation, Data Acquisition, Data Stewardship, Data Governance Policies, Data Governance Processes, Data Governance Tools, Excel, Zoominfo, D&B, Ringlead, Demandtools, Openprise, GDPR, Service Level Agreements","salesforce, data governance, data cleansing, data integrity, data quality, data enrichment, data validation, data acquisition, data stewardship, data governance policies, data governance processes, data governance tools, excel, zoominfo, db, ringlead, demandtools, openprise, gdpr, service level agreements","data acquisition, data enrichment, data governance, data governance policies, data governance processes, data governance tools, data integrity, data quality, data stewardship, data validation, datacleaning, db, demandtools, excel, gdpr, openprise, ringlead, salesforce, service level agreements, zoominfo"
Big Data Developer,Sigmaways Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/big-data-developer-at-sigmaways-inc-3778728506,2023-12-17,Belleville,United States,Mid senior,Hybrid,"We are looking for a
Big data Developer
who will contribute to high-quality technology solutions that address business needs by developing data applications for the customer business lines. You will contribute to the development and ongoing maintenance of several strategic data initiatives and data and analytic applications.
Responsibilities:
Hands-on development role focused on creating big data and analytics solutions.
Coding of mission-critical components.
A strong Big data developer/coder who can write database queries and also tune those queries to perform optimally.
Analyze business and functional requirements and contribute to the overall solution.
Participate in design reviews, and provide input to the design recommendations.
Participate in project planning sessions with project managers, business analysts, and team members.
Required skills:
BS or MS in Computer Science or equivalent experience.
Software development experience with solid working experience in Big Data technologies.
Knowledge of the architecture and internals of technologies in the Hadoop ecosystem
Experience designing and implementing large, scalable distributed systems.
Must have solid expertise and hands-on experience in
Spark,
Scala and SQL
S
olid experience of the Hadoop Ecosystem ( HDFS, Yarn, MapReduce, Spark, Hive, Impala )
and should be able to mentor and lead junior team members.
Good understanding of database technologies, including SQL and NoSQL databases
Ability to debug and promptly resolve production issues.
Proficiency with advanced object-oriented programming.
Excellent problem-solving and analytical skills.
Excellent written and oral communication skills.
Show more
Show less","Big Data, Data Analytics, Hadoop Ecosystem, NoSQL Databases, Spark, Scala, SQL, HDFS, Yarn, MapReduce, Hive, Impala, Database Management Systems, ObjectOriented Programming, ProblemSolving, Analytical Skills, Communication Skills","big data, data analytics, hadoop ecosystem, nosql databases, spark, scala, sql, hdfs, yarn, mapreduce, hive, impala, database management systems, objectoriented programming, problemsolving, analytical skills, communication skills","analytical skills, big data, communication skills, dataanalytics, database management systems, hadoop ecosystem, hdfs, hive, impala, mapreduce, nosql databases, objectoriented programming, problemsolving, scala, spark, sql, yarn"
Looking for Azure Database Engineer -St. Louis MO (Hybrid) - Contract,Extend Information Systems Inc.,"St Louis, MO",https://www.linkedin.com/jobs/view/looking-for-azure-database-engineer-st-louis-mo-hybrid-contract-at-extend-information-systems-inc-3704057850,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Hi,
I hope you are doing well!
We have an opportunity for
Azure Database Engineer
with one of our clients for
St. Louis MO (Hybrid).
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Azure Database Engineer
Location: St. Louis MO (Hybrid)
Terms:
Contract
Job Details
Job Responsibility:
Collaborate with data architects and other stakeholders to design scalable and optimized data warehouse solutions on the Azure platform.
Develop and maintain data warehouse architectural documentation, including schema designs, data flow diagrams, and integration patterns.
Move SSIS / ETL Processes and execute SSIS within Azure Data Factory.
Design the NextGen Data Warehouse using Azure.
Expertise in using Master Data Management strategies, design Data Warehouse structure and schema with Azure Data services..
Execute SSIS packages in Azure from SSDT to assess the cloud compatibility of SSIS packages and run them on Azure-SSIS IR within ADF.
Fix any issues found in IR testing and can migrate all SSIS packages to run in Azure.
Re-write all SSIS packages into ADF and expand analytics using Azure Synapse and Power BI
Investigate use of Azure Fabric for end to end data processes from Data Science to Analytics
Write efficient T-SQL queries for data transformation, extraction, loading, and reporting purposes.
Implement data partitioning, indexing, and distribution strategies to enhance query performance.
Integrate data from various sources, both on-premises and in the cloud, into the Azure Data Warehouse using appropriate ETL/ELT techniques.
Work with data integration tools such as Azure Data Factory or SSIS to automate data pipelines.
Monitor and analyze query performance and data warehouse health, identifying and resolving bottlenecks and performance issues.
Optimize query execution plans, indexing strategies, and data distribution to achieve optimal performance.
Implement security measures to protect sensitive data within the Azure Data Warehouse, following best practices for data encryption, access controls, and data masking.
Ensure compliance with relevant data privacy and security regulations, such as GDPR or HIPAA.
Provide training and guidance to junior team members on data warehouse best practices and technologies.
Qualifications
Bachelor's or higher degree in Computer Science, Information Technology, or a related field.
Proven experience in designing, implementing, and optimizing data warehousing solutions using Azure services (Azure Synapse Analytics, Azure Data Factory, Azure SQL Data Warehouse).
Plan the ETL process for the data warehouse design.
Choose appropriate Azure data services (Synapse, ADF, ?)
Create data warehouse operations runbook
Manage Data Quality ongoing
.
--
Thanks & Regards
Monika Singh
Extend Information System Inc
Phone: (571)-622-3980
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","Azure Data Factory, Azure Data Warehouse, Azure SQL Data Warehouse, Azure Synapse Analytics, AzureSSIS IR, ADF, Data Science, Data Warehouse, ETL, GDPR, HIPAA, Master Data Management, Power BI, SSIS, SSDT, TSQL","azure data factory, azure data warehouse, azure sql data warehouse, azure synapse analytics, azuressis ir, adf, data science, data warehouse, etl, gdpr, hipaa, master data management, power bi, ssis, ssdt, tsql","adf, azure data factory, azure data warehouse, azure sql data warehouse, azure synapse analytics, azuressis ir, data science, datawarehouse, etl, gdpr, hipaa, master data management, powerbi, ssdt, ssis, tsql"
Sr. Power BI / Data Engineer-expert,neteffects,"St. Louis City County, MO",https://www.linkedin.com/jobs/view/sr-power-bi-data-engineer-expert-at-neteffects-3779391637,2023-12-17,Belleville,United States,Mid senior,Hybrid,"At neteffects, we are looking for a
Sr. Power BI/ Data Engineer-expert
for our direct client in
St. Louis MO.
Contract -No c2c.
Hybrid in St. Louis MO
Education: Bachelor's degree or Master's degree with six years of experience
Top Technical Skills/Tools/Experience Needed for the Role:
Technical Skills:
•
Programming Languages:
Python, Scala, Go, SQL (required), R (preferred)
Business Intelligence: Power BI, Power Apps
Power BI and Data Visualizations
Power Apps
•
Cloud Platforms
: AWS, GCP
•
Data Warehousing
: BigQuery, Redshift, Snowflake-
expertise on any one
•
Streaming Technologies
: Kafka
•
Data Pipelines
: Spark, AWS SQS, Lambda, Step Functions, ECS, Fargate, Athena, BigQuery, GCP PubSub, Cloud Functions, Cloud Run, Kubernetes
• I
nfrastructure as Code
: Terraform, AWS CloudFormation
•
DevOps:
Continuous Integration/Continuous Delivery (CI/CD)
•
Version Control:
Git
•
Documentation:
Haystack, SharePoint
•
• Other:
Statistical and/or mathematical programming packages, machine learning (ML) (optional)
Experience:
• Building data models
• Engineering data-intensive software
• Implementing data pipelines
• Working with cloud platforms
• Collaborating with cross-functional teams
• Leading and participating in design sessions
• Providing technical recommendations
• Estimating project timelines
• Communicating technical information effectively
Additional Desired Skills:
• Business Acumen: Understanding of business processes and needs
• Leadership: Leading and motivating teams
• Communication: Excellent written and oral communication skills
• Project Management: Successfully managing projects from start to finish
• Organizational Skills: Prioritizing tasks and meeting deadlines
• Interpersonal Skills: Building and maintaining positive relationships
• Global Experience: Working with culturally diverse teams
• Learning: Continuously learning and staying up-to-date with new technologies
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Python, Scala, Go, SQL, R, Power BI, Power Apps, AWS, GCP, BigQuery, Redshift, Snowflake, Kafka, Spark, AWS SQS, Lambda, Step Functions, ECS, Fargate, Athena, GCP PubSub, Cloud Functions, Cloud Run, Kubernetes, Terraform, AWS CloudFormation, Git, Haystack, SharePoint, Machine Learning, Statistical Programming, Mathematical Programming Packages, Data Modeling, DataIntensive Software Engineering, Data Pipeline Implementation, Cloud Platform Experience, CrossFunctional Team Collaboration, Design Session Participation, Technical Recommendation, Project Timeline Estimation, Technical Information Communication, Business Acumen, Leadership, Communication, Project Management, Organizational Skills, Interpersonal Skills, Global Experience, Learning","python, scala, go, sql, r, power bi, power apps, aws, gcp, bigquery, redshift, snowflake, kafka, spark, aws sqs, lambda, step functions, ecs, fargate, athena, gcp pubsub, cloud functions, cloud run, kubernetes, terraform, aws cloudformation, git, haystack, sharepoint, machine learning, statistical programming, mathematical programming packages, data modeling, dataintensive software engineering, data pipeline implementation, cloud platform experience, crossfunctional team collaboration, design session participation, technical recommendation, project timeline estimation, technical information communication, business acumen, leadership, communication, project management, organizational skills, interpersonal skills, global experience, learning","athena, aws, aws cloudformation, aws sqs, bigquery, business acumen, cloud functions, cloud platform experience, cloud run, communication, crossfunctional team collaboration, data pipeline implementation, dataintensive software engineering, datamodeling, design session participation, ecs, fargate, gcp, gcp pubsub, git, global experience, go, haystack, interpersonal skills, kafka, kubernetes, lambda, leadership, learning, machine learning, mathematical programming packages, organizational skills, power apps, powerbi, project management, project timeline estimation, python, r, redshift, scala, sharepoint, snowflake, spark, sql, statistical programming, step functions, technical information communication, technical recommendation, terraform"
"Data DevOps Engineer // St. Louis, MO",Motion Recruitment,"St Louis, MO",https://www.linkedin.com/jobs/view/data-devops-engineer-st-louis-mo-at-motion-recruitment-3785829212,2023-12-17,Belleville,United States,Mid senior,Hybrid,"A well-known brewing and beverage company is looking for a DevOps Engineer to join their team on a contract-to-hire basis. This will be a hybrid role working onsite in St. Louis, MO 2-3 days per week.
As a DevOps Engineer, you will be partnering up with Data Engineering and Data Science teams to architect data solutions. You will be using a DevOps approach to move data across their platforms and environments, so they are looking for someone who is knowledgeable on a variety of data systems.
Skills & Experience
4+ years of experience in DevOps/Cloud Engineering
2+ years of experience in an Azure cloud environment
Hands-on experience with Databricks, Snowflake, and/or Apache Airflow
CI/CD experience using GitHub Actions is a plus
Posted By:
Carolyn Regimbal
Show more
Show less","DevOps, Cloud Engineering, Azure, Databricks, Snowflake, Apache Airflow, GitHub Actions, CI/CD","devops, cloud engineering, azure, databricks, snowflake, apache airflow, github actions, cicd","apache airflow, azure, cicd, cloud engineering, databricks, devops, github actions, snowflake"
Senior Cloud Data Engineer,BDO USA,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467837,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, SQL, Data Warehousing, Data Modeling, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch, Streaming, AI Algorithms, Machine Learning, Automation, Computer Vision, UiPath, Alteryx, Tableau, Qlik, PySpark, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS, Snowflake, Athena, Glue, RedShift, QuickSight, SageMaker, S3, Databricks","data analytics, business intelligence, artificial intelligence, sql, data warehousing, data modeling, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch, streaming, ai algorithms, machine learning, automation, computer vision, uipath, alteryx, tableau, qlik, pyspark, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs, snowflake, athena, glue, redshift, quicksight, sagemaker, s3, databricks","ai algorithms, alteryx, artificial intelligence, athena, automation, aws, azure, azure analysis services, batch, bicep, business intelligence, c, computer vision, data lake medallion architecture, data ops, dataanalytics, databricks, datamodeling, datawarehouse, delta, devops, git, glue, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, streaming, tableau, terraform, uipath"
"Lead Data Engineer java sparc sqlLocation: St Louis, MO (Onsite)",Executive Staff Recruiters / ESR Healthcare,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-java-sparc-sqllocation-st-louis-mo-onsite-at-executive-staff-recruiters-esr-healthcare-3645958534,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Company Profile
esrhealthcare.com.mysmartjobboard.com
Lead Data Engineer
>
> Location: St Louis, MO (Onsite)
>
> Duration: 12 month with possible extension
>
> Required Work Experience:
>
>
5+ years of experience in an engineering role using Python,
> Java, Spark, and SQL.
>
>
GCP Cloud Experience Required
>
>
Strong working experience in HealthCare domain is a must
>
>
Strong Linux/Unix background and hands on knowledge.
>
>
Experience with Shell scripting and bash.
>
>
Experience with version control platform github
>
>
Experience with development ecosystem including Jenkins,
> Artifactory, CI/CD, and Terraform.
>
>
Pipeline creation and automation for Data Acquisition
>
>
Metadata extraction pipeline design and creation between
> raw and finally transformed datasets
>
>
Past experience with big data technologies including HDFS,
> Spark, Impala, Hive
>
>
Able to collaborate with scrum team including scrum master,
> product owner, data analysts, Quality Assurance, business owners, and
> data architecture to produce the best possible end products
>
>
Works on problems of diverse scope and complexity ranging
> from moderate to substantial
>
>
Assists senior professionals in determining methods and
> procedures for new tasks
>
>
Leads basic or moderately complex projects/activities on
> semi-regular basis
>
>
Must possess excellent written and verbal communication
> skills
>
>
Ability to understand and analyze complex data sets
>
>
Exercises independent judgment on basic or moderately
> complex issues regarding job and related tasks
>
>
Makes recommendations to management on new processes, tools
> and techniques, or development of new products and services
>
>
Works under minimal supervision, uses independent judgment
> requiring analysis of variable factors
>
>
Collaborates with senior professionals in the development
> of methods, techniques and analytical approach
>
>
Able to effectively communicate highly technical
> information to numerous audiences, including management, the user
> community, and less-experienced staff.
>
>
Consistently communicate on status of project deliverables
>
>
Consistently provide work effort estimates to management to
> assist in setting priorities
>
>
Deliver timely work in accordance with estimates
>
>
Solve problems as they arise and communicate potential
> roadblocks to manage expectations
>
>
Adhere strictly to all security policies
>
> Regards...!!!!
>
> Aravind
Powered by Webbtree
Show more
Show less","Python, Java, Spark, SQL, GCP Cloud, Linux/Unix, Shell scripting, Bash, GitHub, Jenkins, Artifactory, CI/CD, Terraform, Data Acquisition Pipeline, Metadata extraction pipeline, HDFS, Impala, Hive, Scrum, Data analysis, Quality Assurance, Data Architecture, Communication, Data analysis, Independent judgment, Recommendations, Supervision, Collaboration, Technical communication, Status updates, Work effort estimates, problem solving, Security policies","python, java, spark, sql, gcp cloud, linuxunix, shell scripting, bash, github, jenkins, artifactory, cicd, terraform, data acquisition pipeline, metadata extraction pipeline, hdfs, impala, hive, scrum, data analysis, quality assurance, data architecture, communication, data analysis, independent judgment, recommendations, supervision, collaboration, technical communication, status updates, work effort estimates, problem solving, security policies","artifactory, bash, cicd, collaboration, communication, data acquisition pipeline, data architecture, dataanalytics, gcp cloud, github, hdfs, hive, impala, independent judgment, java, jenkins, linuxunix, metadata extraction pipeline, problem solving, python, quality assurance, recommendations, scrum, security policies, shell scripting, spark, sql, status updates, supervision, technical communication, terraform, work effort estimates"
Senior Data Analyst,Equifax,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-analyst-at-equifax-3781960084,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
Equifax Workforce Solutions (EWS), headquartered in St. Louis, MO
is EFX’s fastest growing and most profitable business unit. Learn more about EWS and our workforce solutions here: https://workforce.equifax.com/
Equifax (EFX)
,
HQ Atlanta, GA is a global data, analytics, and technology company. We believe knowledge drives progress. We blend unique data, analytics, and technology with a passion for serving customers globally, to create insights that power decisions to move people forward.
The senior data analyst will understand the needs of different audiences, deliver content both technically and non-technically, and have a knack for visually telling the story with data.
What You’ll Do
This role will be responsible for driving analytics that support sales enablement, marketing, and product development in a state of the art analytics cloud environment. Must be self motivated, collaborative, and results oriented.
Develop innovative analytical solutions that drive actionable insights to enrich a variety of revenue driving functions such as sales enablement, customer lifetime value, and market penetration
Consultative approach to data projects, with the ability to artfully articulate the analysis to non-technical audiences
Take ownership of delivering quality projects to the business, applying rigor to business requirements, design, and aftercare of the solution when deployed
Utilizes Equifax’s diverse data sources to craft solutions using various modeling techniques through the most appropriate open source and/or proprietary tooling
What Experience You'll Need
Bachelor’s degree in Computer Science, Information Systems, Economics, Engineering, or a comparable discipline
5+ years of work experience in identifying, gathering, transforming and analyzing data within and across database platforms (SQL, Python)
Strong development skills in visualization software such as Tableau, Data Studio, or other business intelligence tools
Strong analytical skills including the ability to rapidly learn new business, data assets and how to apply them to drive business impacts
You possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders
What Could Set You Apart
Advanced degree in Statistics, Engineering, Business Analytics or a comparable discipline or experience
Work experience in the SaaS industry driving sales or marketing functions
Work experience building, loading, transforming, and analyzing data within Google Cloud Platform (GCP) or other similar cloud environment
Work experience with regulated data
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
Are you ready to power your possible? Apply today, and get started on a path toward an exciting new career at Equifax, where you can make a difference!
Equifax is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","SQL, Python, Tableau, Data Studio, Statistics, Engineering, Business Analytics, Google Cloud Platform, GCP","sql, python, tableau, data studio, statistics, engineering, business analytics, google cloud platform, gcp","business analytics, data studio, engineering, gcp, google cloud platform, python, sql, statistics, tableau"
Data Scientist-Deep Learning-Bioinformatics,neteffects,"St Louis, MO",https://www.linkedin.com/jobs/view/data-scientist-deep-learning-bioinformatics-at-neteffects-3775479912,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Our direct client in St. Louis MO is looking for a DataScientist with Deep Learning & Bioinformatics/ Biomolecular focus.
Contract
Hybrid / Remote for exceptional candidates
No c2c
Basic requirements:
The candidate will join a cross divisional, global team to generate innovative machine learning solutions to better understand biomolecules.
The successful candidate will be responsible for
implementing machine learning models and work collaboratively with data scientists and research scientists leveraging available datasets and influencing new dataset generation.
Must Haves:
·
PhD degree (or M.Sc. with 4 years of working experience) in computer sciences, computational chemistry, computational biology, physics
or related fields.
· Profound experience with state-of-the-art
advanced mathematical models, machine learning methods and model selection concepts; previous experience with deep learning
would be of advantage.
· Real interest in
biology and the life sciences with knowledge of proteins
. Proficient in applying deep learning algorithms to solve biomolecular problems.
· E
xcellent programmin
g and software engineering skills in
Python are essential.
· Expertise in Python libraries like
Biopython and NumPy
would be beneficial.
· The ability to write clean, efficient, and well-documented Python code is crucial.
· Proficiency in writing code and experience in cloud computing.
· Highly creative, independent, fast-learning person with outstanding problem-solving ability and the willingness to undertake challenging analysis tasks autonomously and in a timely fashion.
· Strong interpersonal skills, excellent written and verbal communication, and the ability to work effectively both independently and in cross-functional teams.
· Willingness to travel between research sites domestically and globally.
· Fluency in English, both written and spoken.
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Deep Learning, Bioinformatics, Biomolecular, Machine Learning, Python, Biopython, NumPy, Cloud Computing","deep learning, bioinformatics, biomolecular, machine learning, python, biopython, numpy, cloud computing","bioinformatics, biomolecular, biopython, cloud computing, deep learning, machine learning, numpy, python"
Sr. Data Governance Analyst,Accounting Career Consultants & HR Career Consultants,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-governance-analyst-at-accounting-career-consultants-hr-career-consultants-3768713926,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Why is This a Great Opportunity?
-great boss
-autonomy in role
-tremendous flexibility
-high-level exposure role
-tons of growth
-top company
-strong comp package
Job Description:
-Expertise in Data Management: Demonstrate a profound understanding of master data management, data standards, data quality, and data lineage, ensuring a comprehensive approach to Data Governance.
-Partnership and Documentation: Collaborate closely with functional stakeholders to develop and document Data Governance best practices, standards, principles, and policies that align with organizational objectives.
-Data Quality Reporting: Contribute to the development of data quality reports and dashboards, providing valuable insights to stakeholders and ensuring the health of data aligns with business goals.
-Continuous Improvement: Embrace a continuous improvement mindset, identifying opportunities to enhance processes and systems that align with Data Governance policies and standards.
-Promotion of Data Governance Tools: Actively facilitate and promote the adoption of data governance tools, processes, and procedures among functional business users, ensuring widespread understanding and compliance.
Qualifications:
-2+ years of data governance experience
-self-starter
-strong
Show more
Show less","Data Management, Data Governance, Master Data Management, Data Standards, Data Quality, Data Lineage, Partnership, Documentation, Data Quality Reporting, Dashboards, Continuous Improvement, Data Governance Tools, Data Governance Policies, Data Governance Standards","data management, data governance, master data management, data standards, data quality, data lineage, partnership, documentation, data quality reporting, dashboards, continuous improvement, data governance tools, data governance policies, data governance standards","continuous improvement, dashboard, data governance, data governance policies, data governance standards, data governance tools, data lineage, data management, data quality, data quality reporting, data standards, documentation, master data management, partnership"
Senior Staff AI Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087762,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, ML, NLP, LLM, Data engineering, Data mining, Data cleaning, Data normalizing, Data modeling, Data platforms, Data frameworks, Data processing, Data pipelines, Data governance, Data risk, Data compliance, Data infrastructure, Statistical analysis, Data visualization, Pandas, R, Conversational AI, Recommender systems, Distributed systems, Microservices, Docker images, Streamprocessing systems","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, ml, nlp, llm, data engineering, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, data processing, data pipelines, data governance, data risk, data compliance, data infrastructure, statistical analysis, data visualization, pandas, r, conversational ai, recommender systems, distributed systems, microservices, docker images, streamprocessing systems","airflow, aws, azure, bash, conversational ai, data cleaning, data compliance, data engineering, data frameworks, data governance, data infrastructure, data mining, data normalizing, data platforms, data processing, data risk, datamodeling, datapipeline, distributed systems, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, llm, microservices, ml, nlp, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, streamprocessing systems, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709551,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML Data Engine, ML Data Ops, Data pre/post processing, ML models, Data mining, Data cleaning, Data normalizing, Data modeling, Data platforms, Data frameworks, Big data, Data governance, Data risk, Data compliance, Data infrastructure, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, SQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","data engineering, ml data engine, ml data ops, data prepost processing, ml models, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, big data, data governance, data risk, data compliance, data infrastructure, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, sql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, aws, azure, bash, big data, data classification, data cleaning, data compliance, data engineering, data frameworks, data governance, data infrastructure, data management, data mining, data normalizing, data platforms, data prepost processing, data retention, data risk, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, ml data engine, ml data ops, ml models, python, snowflake, spark, sparkstreaming, sql, storm"
Data Migration Analyst - Hybrid,"Swank Motion Pictures, Inc.","St Louis, MO",https://www.linkedin.com/jobs/view/data-migration-analyst-hybrid-at-swank-motion-pictures-inc-3766477789,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Swank Motion Pictures is looking for a technical business analyst to join an existing and growing team working on innovative and leading-edge technologies. We are looking for someone with strong data analysis skills necessary to work on multiple projects simultaneously with minimal oversight in an agile/iterative IT environment. These projects are primarily custom software development and business process improvements. This position is a liaison between non-technical business stakeholders and technology teams. This position is in the IT department and will interact with multiple levels of management within the company. This position is intended to be a year-long position with the opportunity of becoming a permanent role within the group. At Swank Motion Pictures, we are not just looking for employees; we are investing in future leaders. For the Data Migration Analyst role, we offer a pathway for professional growth and development. As our company evolves, there will be abundant opportunities for you to expand your skill set, take on new challenges, and advance your career. We are committed to fostering a culture of learning and innovation where your contributions will be recognized and your professional aspirations supported.
Responsibilities
This position has the primary responsibility to analyze corporate data and assist in its migration from one system to another. Corporate data includes but is not limited to accounts, contacts, agreements, orders, and invoices. The data migration analyst will elicit, verify, and document business-defined data migration rules; analyze data based on those rules for consistency, cleanliness, and outliers; perform quality control checks against data transfer files; and share findings from those checks with the data migration team. The Data Migration Analyst will be expected to develop and maintain comprehensive documentation, including data mappings, transformation rules, and workflow diagrams, to ensure clarity and consistency throughout the data migration process. This individual will be an integral part of a team consisting of data architects, system specialists, and project managers. The Data Migration Analyst will collaborate closely with data governance teams to uphold meaningful data quality and compliance standards across all stages of the migration process. This position is a hybrid position – the individual must be willing to be in the office at least 2 days per week.
Requirements
Skills Needed
Interpersonal skills to negotiate priorities and collaborate with both business and IT peers
Interview skills to ask the proper questions for gathering essential requirements and listen attentively to their feedback
Analytical skills to critically evaluate data of different types, discern data patterns, and identify data outliers using a high level of attention to detail
Communication skills to effectively share ideas and requirements with both technical and non-technical audiences through meetings, working group sessions, whiteboard sessions using data visualization skills and too
Creativity skills to be flexible and think outside the box when solving problems
Organizational skills to meet deadlines, ensure quality deliverables, and cope with rapidly changing information in a hybrid work environment
Experience Needed
3 to 5 years of experience executing business analysis with a focus on data analysis and migration
Experience working with and analyzing large amounts of data via Excel – SQL experience is nice to have but not required
An understanding of best practices for eliciting, analyzing, documenting, validating, and managing requirements, along with knowing when to apply them
Experience facilitating meetings with both business and IT stakeholders from any level within the organization
Experience eliciting requirements via one-on-one interviews, group meetings, brainstorming sessions, and other methods as needed
Experience collaborating with Project Management, Development, and Quality Assurance personnel on software development projects
Detailed expertise using Microsoft Word, PowerPoint, and Excel a must; familiarity with Atlassian Jira and Confluence preferred but not required
Educational Requirements
Bachelor’s degree in technology related field required
Benefits
We are pleased to offer:
Comprehensive compensation and healthcare packages, including medical, dental, vision, and life insurance products
401(K) plan with employer match
Competitive paid time off: vacation, personal time, holidays and winter break
Company sponsored volunteer & community outreach opportunities
Organizational growth potential through our company sponsored online learning platform
EOE, including disability/vets
Show more
Show less","Data Analysis, Agile/Iterative IT, Custom Software Development, Business Process Improvement, Data Migration, Data Mapping, Transformation Rules, Workflow Diagrams, Data Governance, Interpersonal Skills, Negotiation, Interview Skills, Analytical Skills, Communication Skills, Creativity, Organizational Skills, Business Analysis, Excel, SQL, Requirements Gathering, Microsoft Word, PowerPoint, Jira, Confluence, Bachelor's in Technology","data analysis, agileiterative it, custom software development, business process improvement, data migration, data mapping, transformation rules, workflow diagrams, data governance, interpersonal skills, negotiation, interview skills, analytical skills, communication skills, creativity, organizational skills, business analysis, excel, sql, requirements gathering, microsoft word, powerpoint, jira, confluence, bachelors in technology","agileiterative it, analytical skills, bachelors in technology, business analysis, business process improvement, communication skills, confluence, creativity, custom software development, data governance, data mapping, data migration, dataanalytics, excel, interpersonal skills, interview skills, jira, microsoft word, negotiation, organizational skills, powerpoint, requirements gathering, sql, transformation rules, workflow diagrams"
Voice Data Communications Engineer III,"All Native Group, The Federal Services Division of Ho-Chunk Inc.","Scott AFB, IL",https://www.linkedin.com/jobs/view/voice-data-communications-engineer-iii-at-all-native-group-the-federal-services-division-of-ho-chunk-inc-3724381768,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Summary
All Native Group is looking for a Voice/Data Communications Engineer I provide technical direction and engineering knowledge for communications activities including planning, designing, developing, testing, installing, and maintaining large communications networks.
Essential Functions
Installs and maintains voice, wireless, video, and data communications systems.
Coordinates with users to determine requirements.
Applies fundamental telco and data network installation and maintenance concepts, processes, practices, and procedures on technical assignments.
Supervisory Responsibility
None required for this position
Competencies
Network Installation (Cable) Experience (Inside and/or Outside Plant)
Network (Cable) Termination and Splicing Experience (Fiber and/or Copper)
Network (Cable) Maintenance & Inspection Experience (Inside and/or Outside Plant)
Work Environment
Outside environment and inside environment with some work in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines. Work may also require the use of tools and machinery required to install, splice, and terminate network infrastructure (e.g., cables, etc.).
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Must be able to remain in a stationary position 75% of the time.
Occasionally moves about inside the office to access
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
Expresses or exchanges ideas by means of the spoken word. Those activities in which they must convey detailed or important spoken instructions to other workers accurately, loudly, or quickly.
Frequently moves standard office equipment up to 25 pounds.
Must be able to work indoor conditions 90% of the time.
While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand; walk; use hands to finger, handle or feel; and reach with hands and arms.
Position Type/Expected Hours of Work
This is a full-time position. Typical days and hours of work are Monday through Friday, 8:00 a.m. to 5:00 p.m. Evening and weekend hours required, as required by business need.
Travel
No travel is required for this position.
Experience
A minimum of 5-7 years of relevant experience is required.
Education
Associates Degree, Bachelors or higher desired.
One or more of the following certifications are required:
DURALINE
CTNS
iNARTE
IPEP
NCTI
RCDD
Security Clearance
Active Secret Security Clearance is required
Executive Order 14043 COVID-19 Vaccination Requirement
Pursuant to Executive Order 14043, this position requires full vaccination against COVID-19 regardless of work location.
AAP/EEO Statement
All Native Group is an equal opportunity employer. All applicants are considered without regard to age, sex, race, national origin, religion, marital status, or physical disability. However, preference may be extended to persons of Indian descent in accordance with applicable laws.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with
or without notice.
Apply Now
Show more
Show less","Telecommunication, Network Installation, Network Maintenance, Network Inspection, Network Termination, Network Splicing, Fiber Optics, Copper Cabling, DURALINE, CTNS, iNARTE, IPEP, NCTI, RCDD","telecommunication, network installation, network maintenance, network inspection, network termination, network splicing, fiber optics, copper cabling, duraline, ctns, inarte, ipep, ncti, rcdd","copper cabling, ctns, duraline, fiber optics, inarte, ipep, ncti, network inspection, network installation, network maintenance, network splicing, network termination, rcdd, telecommunication"
Data Scientist- Journeyman,ECS,"St Louis, MO",https://www.linkedin.com/jobs/view/data-scientist-journeyman-at-ecs-3784918694,2023-12-17,Belleville,United States,Mid senior,Hybrid,"ECS is seeking a
Data Scientist- Journeyman
to work in our
St Louis, MO
office .
Job Description:
ECS is seeking a Data Scientist- Journeyman to work in our St Louis, MO office. This offer is contingent upon acceptance by the government customer, validation of appropriate clearances and approval from a cognizant government contracting officer. Candidate will be submitted for Counterintelligence Polygraph after customer indoctrination. The primary role for these data analysts are to provide support to data management and automation activities as part of SFP’s technical operations team. The scope of work aligns to SFP’s efforts to modernize and advance requirements management and production feedback capabilities. The contractor shall support the office in modernizing the Community’s requirements management process, to include automation, data management, statistical analysis, data visualization, training, and process documentation. The primary result of this effort will be the implementation of enhanced requirements management capabilities in order to support Community stakeholders and SF leadership.
The contractor shall:
Work in a team environment and interact with production offices, analysts, and data owners as required.
Advise and consult SF stakeholders on data management best practices.
Assist SF analysts and external partners on data management procedures and best practices.
Implement enterprise database management processes.
Perform data management tasks, to include data manipulation.
Perform data analysis to assess integrity, identify patterns, and determine and correct shortfalls.
Develop and document data management workflows.
Transfer data between multiple formats.
Automate processes through the use of Python scripting and model building.
Perform statistical analysis of data to identify trends and patterns and to build reports for SF leadership and IC community (or NSG).
Provide advanced data visualization to convey findings and communicate insights utilizing capabilities such as IC Portal and other dynamic viewers.
Independently identify, document, and address technical challenges.
Deliver schema mapping and entity resolution across disparate geospatial data sources through data scientist applied expertise in the subject.
Understand and communicate the interrelationships across many disparate datasets.
Document and visualize data both temporally and spatially to assist in data integrity checks, ask the next question, and display analytical assessments.
Develop and apply methods to identify, collect, process, and analyze large volumes of data to build and enhance GEOINT processes, and systems.
Provide support to produce GEOINT web services publication to develop application-ready content, support GEOINT content metadata-tagging to enable content discovery; and publish GEOINT web services.
Implement ESRI Story Maps in a classified environment.
Work with ESRI Portal technology to develop and maintain map services.
Support and manage geodatabase versioning workflows.
Support, manage, and automate geodatabase synchronization workflows.
Required Skills:
Active TS/SCI CI-Poly Clearance.
Minimum of a Bachelor’s Degree or 4+ years of experience.
Demonstrated advanced proficiency with the following programs:
Python o ArcGIS, ArcSDE, ArcPro, ArcGIS Online.
PostgreSQL, Postgis, PgAdmin.
Application programming interfaces (APIs).
Demonstrated advanced proficiency of database principles and technology.
Demonstrated advanced data transformation skills.
Demonstrated advanced data processing and analytic skills.
Demonstrated advanced experience with enterprise geodatabase management.
Demonstrated advanced experience geodatabase versioning workflows.
Demonstrated advanced experience with geodatabase synchronization and integration.
Demonstrated advanced experience with Python (i.e. numpy, pandas, matplotlib libraries).
Demonstrated advanced experience in SQL and NoSQL technologies.
Demonstrated advanced experience text parsing.
Demonstrated advanced experience in metadata tagging (data storage, processing, and & analyzing).
Demonstrated advanced experience in work with a variety of geospatial data formats.
Desired Skills:
Demonstrated experience with JAVA programming language.
Demonstrated experience with cloud-based database solutions.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Python, ArcGIS, ArcSDE, ArcPro, ArcGIS Online, PostgreSQL, PostGIS, PgAdmin, Application programming interfaces (APIs), SQL, NoSQL, Geodatabase, Geospatial data formats, JAVA, Cloudbased database solutions, Data management, Data analysis, Data visualization, Data manipulation, Statistical analysis, Data integration, Data transformation, Metadata tagging, Geospatial data","python, arcgis, arcsde, arcpro, arcgis online, postgresql, postgis, pgadmin, application programming interfaces apis, sql, nosql, geodatabase, geospatial data formats, java, cloudbased database solutions, data management, data analysis, data visualization, data manipulation, statistical analysis, data integration, data transformation, metadata tagging, geospatial data","application programming interfaces apis, arcgis, arcgis online, arcpro, arcsde, cloudbased database solutions, data integration, data management, data manipulation, data transformation, dataanalytics, geodatabase, geospatial data, geospatial data formats, java, metadata tagging, nosql, pgadmin, postgis, postgresql, python, sql, statistical analysis, visualization"
Senior Data Engineer,Kforce Inc,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3775449352,2023-12-17,Utah,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is looking for a Senior Data Engineer in Salt Lake City, UT. Job Summary: We are currently seeking an experienced Senior Data Engineer to join our dynamic team. The ideal candidate will possess a strong technical skill set in data engineering. A proven track record in designing, developing, and deploying data-intensive solutions in cloud environments is essential. This role involves utilizing languages such as Python and SQL to construct data pipelines, prioritizing data quality and resiliency. The Senior Data Engineer will play a pivotal role in ensuring the reliability, efficiency, and maintainability of data pipeline infrastructure, contributing significantly to the evolution of our data capabilities.
Requirements
Degree in a required field
5+ years of experience as a Data Engineer
Hands-on experience with orchestration and pipeline tools like Apache Airflow, Azure Data Factory, Prefect, Fivetran, or Mage
Experience with cloud platforms
Expertise in Snowflake
Proficiency in data engineering languages such as Python, SQL, Java, or Scala
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $140,000 - $150,000 per year
Show more
Show less","Data Engineering, Python, SQL, Cloud Platforms, Snowflake, Apache Airflow, Azure Data Factory, Prefect, Fivetran, Mage, Java, Scala","data engineering, python, sql, cloud platforms, snowflake, apache airflow, azure data factory, prefect, fivetran, mage, java, scala","apache airflow, azure data factory, cloud platforms, data engineering, fivetran, java, mage, prefect, python, scala, snowflake, sql"
Lead Data Analyst,Serenity Healthcare,"Lehi, UT",https://www.linkedin.com/jobs/view/lead-data-analyst-at-serenity-healthcare-3728004829,2023-12-17,Utah,United States,Associate,Onsite,"Serenity Healthcare is looking for a seasoned data analyst for our Lehi, UT headquarters. Title will depend on prior experience. Preference given to those with full time experience in SQL, building/managing data pipelines, and Exploratory Data Analysis.
This is an onsite position located in Lehi, UT
Desired Skillsets
SQL – the most important skillset (we use SQL SERVER)
SSIS – previous experience making and managing packages.
PowerBI
Day-to-day Work Description
Using automated tools to extract data from primary and secondary sources
Removing corrupted data and fixing coding errors and related problems
Developing and maintaining databases, data systems – reorganizing data in a readable format
Performing analysis to assess quality and meaning of data
Filter Data by reviewing reports and performance indicators to identify and correct code problems
Using statistical tools to identify, analyze, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction
Assigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.
Preparing reports for management stating trends, patterns, and predictions using relevant data
Job Fit
Capable of “Deep work”
Problem Solver
Reliable, consistent, and thorough
What We Offer To You
Competitive pay, including additional target compensation
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.
Show more
Show less","SQL, SSIS, PowerBI, Data extraction, Data cleaning, Database development, Data analysis, Data interpretation, Statistical analysis, Reporting, Problem solving, Reliability, Consistency, Thoroughness","sql, ssis, powerbi, data extraction, data cleaning, database development, data analysis, data interpretation, statistical analysis, reporting, problem solving, reliability, consistency, thoroughness","consistency, data cleaning, data extraction, data interpretation, dataanalytics, database development, powerbi, problem solving, reliability, reporting, sql, ssis, statistical analysis, thoroughness"
Senior Data Engineer,Jobot,"Lehi, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobot-3786066516,2023-12-17,Utah,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Join Jobot as a Senior Database Engineer and revolutionize auto financing! Work with a talented team in Silicon Slopes, enjoy a 4-day work week, and help scale our data infrastructure to the next level. Apply now!
This Jobot Job is hosted by Jeff Sorensen
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $85,000 - $110,000 per year
A Bit About Us
Join our dynamic and thriving FinTech startup based in the vibrant Silicon Slopes region, where innovation meets opportunity! We're on the lookout for a seasoned Senior Database Engineer to play a pivotal role in shaping the future of auto financing without mentioning Lease End.
As a key member of our talented team, you'll have the unique opportunity to contribute to the evolution of our data infrastructure and performance. In this role, you'll be the driving force behind database design, performance optimization, and the establishment of industry-leading standards. Your expertise will extend to designing, developing, and enhancing various back-end services.
Tech Stack
React
Node.js
TypeScript
PostgreSQL
GraphQL
AWS (CloudFront, ECS, Aurora, SQS, Lambda)
Responsibilities
Lead the architectural, performance, scalability, and security aspects of our databases
Take charge as a senior engineer, focusing on the development and deployment of back-end data-related services
Fine-tune and establish best practices for queries, indexing, partitioning, and read/write efficiency
Guide and actively participate in the database design and architecture for new product initiatives
Troubleshoot and perform root cause analysis on database and application issues
Establish deployment procedures in collaboration with the infrastructure team
Maintain monitoring and alerting for databases, contributing to a rotating on-call schedule
Ensure adherence to best practices for AWS database infrastructure and services
Oversee data backup processes and contribute to disaster recovery planning
Cultivate a positive culture of innovation and problem-solving
Why join us?
Collaborate with a talented team in a dynamic startup environment
Work with cutting-edge technologies and innovate in the AI and DeFi space
Enjoy a 4-day work week for improved work-life balance
Opportunity to make a significant impact and contribute to the company's growth
Competitive salary and benefits package
Job Details
Qualifications
Must live in Utah (NO REMOTE CANDIDATES - Only Utah Based)
Must be a US Citizen
Graduate degree or equivalent real-world experience in Software Engineering
8+ years of professional experience specializing in relational databases, back-end data-related services, and SQL tuning
Proficiency in Node.js, TypeScript, PostgreSQL, and GraphQL
Solid experience managing relational databases on AWS infrastructure (RDS, Aurora, etc.)
Advanced communication skills, both verbal and written
Detail-oriented with critical thinking skills
Experience with AI/ML technologies is a plus
Ability to thrive in a fast-paced startup environment
If you're passionate about cutting-edge technology, problem-solving, and making a meaningful impact in the FinTech space, we invite you to be part of our innovative journey. Join us and be at the forefront of revolutionizing auto financing!
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","React, Node.js, TypeScript, PostgreSQL, GraphQL, AWS (CloudFront ECS Aurora SQS Lambda), SQL, AWS (RDS Aurora), Relational databases, Software Engineering, SQL tuning, Critical thinking, AI/ML, FinTech","react, nodejs, typescript, postgresql, graphql, aws cloudfront ecs aurora sqs lambda, sql, aws rds aurora, relational databases, software engineering, sql tuning, critical thinking, aiml, fintech","aiml, aws cloudfront ecs aurora sqs lambda, aws rds aurora, critical thinking, fintech, graphql, nodejs, postgresql, react, relational databases, software engineering, sql, sql tuning, typescript"
Senior Data Engineer,Strider Technologies,"South Jordan, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-strider-technologies-3725127890,2023-12-17,Utah,United States,Mid senior,Onsite,"Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to nation-state directed IP theft and supply chain vulnerabilities.
The purpose of Strider is to protect the ideals and innovations of the free world. If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Job Description
Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to foreign government directed IP theft and supply chain vulnerabilities. We are a data company. We have billions of documents online and are collecting millions of new documents each day. The Data Engineering team is responsible for building structured data assets from these raw documents to build new products and enhance existing ones.
If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Responsibilities For This Role
Develop large scale data processing pipelines
Create data assets from unstructured data
Build and support internal and external facing APIs
Author design documents
Mentor teammates
Drive software engineering best practices
Participate in code reviews
Collaborate with a talented cross-functional team of engineers, data scientists and subject-matter experts
You are a good fit for this role if you:
Have 4-5+ years of software engineering experience
Previous experience working in a data heavy role
Understand how to use cloud infrastructure effectively
Are a natural problem solver with an affinity for data
Are opinionated about how software is built
Are proficient at breaking down large, sometimes ambiguous, problems into well-defined tasks
Value shipping code early and often
Have a well-honed mental model for how software systems execute and interact
Technologies We Use
Python
Elasticsearch
Memgraph
Flyte
AWS (EC2, S3, SQS, DynamoDB, Lambda, EKS)
Benefits
Competitive Compensation
Company Equity Options
Flexible PTO
Wellness Reimbursement
US Holidays (Office Closed)
Paid Parental Leave
Comprehensive Medical, Dental, and Vision Insurance
401(k) Plan
Strider provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Strider complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Learn more about us by visiting https://www.striderintel.com/
Show more
Show less","Software Engineering, Data Engineering, Problem Solving, Data Processing, Cloud Infrastructure, APIs, Design Documentation, Mentorship, Code Reviews, CrossFunctional Collaboration, Python, Elasticsearch, Memgraph, Flyte, AWS, EC2, S3, SQS, DynamoDB, Lambda, EKS","software engineering, data engineering, problem solving, data processing, cloud infrastructure, apis, design documentation, mentorship, code reviews, crossfunctional collaboration, python, elasticsearch, memgraph, flyte, aws, ec2, s3, sqs, dynamodb, lambda, eks","apis, aws, cloud infrastructure, code reviews, crossfunctional collaboration, data engineering, data processing, design documentation, dynamodb, ec2, eks, elasticsearch, flyte, lambda, memgraph, mentorship, problem solving, python, s3, software engineering, sqs"
Staff Data Engineer,Strider Technologies,"South Jordan, UT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-strider-technologies-3725130477,2023-12-17,Utah,United States,Mid senior,Onsite,"Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to nation-state directed IP theft and supply chain vulnerabilities.
The purpose of Strider is to protect the ideals and innovations of the free world. If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Job Description
Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to foreign government directed IP theft and supply chain vulnerabilities. We are a data company. We have billions of documents online and are collecting millions of new documents each day. The Data Engineering team is responsible for building structured data assets from these raw documents to build new products and enhance existing ones.
If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Responsibilities For This Role
Develop large scale data processing pipelines
Create data assets from unstructured data
Build and support internal and external facing APIs
Author design documents
Mentor teammates
Drive software engineering best practices
Participate in code reviews
Collaborate with a talented cross-functional team of engineers, data scientists and subject-matter experts
You are a good fit for this role if you:
Have 8+ years of software engineering experience
Previous experience working in a data heavy role
Understand how to use cloud infrastructure effectively
Are a natural problem solver with an affinity for data
Are opinionated about how software is built
Are proficient at breaking down large, sometimes ambiguous, problems into well-defined tasks
Value shipping code early and often
Have a well-honed mental model for how software systems execute and interact
Technologies We Use
Python
Elasticsearch
Memgraph
Flyte
AWS (EC2, S3, SQS, DynamoDB, Lambda, EKS)
Benefits
Competitive Compensation
Company Equity Options
Flexible PTO
Wellness Reimbursement
US Holidays (Office Closed)
Paid Parental Leave
Comprehensive Medical, Dental, and Vision Insurance
401(k) Plan
Strider provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Strider complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Learn more about us by visiting https://www.striderintel.com/
Show more
Show less","Python, Elasticsearch, Memgraph, Flyte, AWS (EC2 S3 SQS DynamoDB Lambda EKS), Software engineering, Data engineering, Data science, Cloud infrastructure, Problem solving, Software design, Collaboration","python, elasticsearch, memgraph, flyte, aws ec2 s3 sqs dynamodb lambda eks, software engineering, data engineering, data science, cloud infrastructure, problem solving, software design, collaboration","aws ec2 s3 sqs dynamodb lambda eks, cloud infrastructure, collaboration, data engineering, data science, elasticsearch, flyte, memgraph, problem solving, python, software design, software engineering"
Data - Staff Software Engineer,Backcountry,"Park City, UT",https://www.linkedin.com/jobs/view/data-staff-software-engineer-at-backcountry-3755347577,2023-12-17,Utah,United States,Mid senior,Onsite,"EMPLOYER: Backcountry.com, LLC
POSITION: Staff Software Engineer - Data
DUTIES: Provide analytic and strategic technical leadership and support to Backcountry’s Data Team which is responsible for maintaining the data platform and maturing the analytical capabilities of the organization. Act as a steward of our core platform and data pipelines that power analytical capabilities. Evolve data models in several components of the data stack and help architect, build, and launch scalable data pipelines to support BC's growing data processing and analytics needs. Collaborate with functional and business leaders and teams and work closely with the Data Engineering team and managers to enable decision support and key insights. Work with a team of high-performing analytics, data science, data engineering professionals, and cross-functional teams to identify business opportunities, monitor data platform performance and optimize analytical capabilities. Build and maintain the infrastructure required for optimal transformation and integration from a wide variety of data sources using appropriate data integration technologies. Deploy pipelines using scheduling and orchestration frameworks. Build data expertise, manage complex data systems for the Data team. Take ownership of core data pipelines that power analytical metrics. Evolve data models in several components of the data stack and help architect, build, and launch scalable data pipelines to support growing data processing and analytics needs. Implement data governance practices in partnership with business stakeholders and peers. Initiate and drive projects to completion with minimal guidance. Create proof of concepts as per business requirements. Contribute to data and engineering innovations that fuel the company’s vision and mission.
REQUIREMENTS: Bachelor’s degree or foreign equivalent in Computer Science, Electrical Engineering, Computer Engineering, Engineering, or a related field of study , and Four (4) years of experience building data intensive applications, tackling challenging architectural, scalability and reliability problems as a Software Engineer or similar role . Work experience or academic coursework must have included: Knowledge of different phases of SDLC including Analysis, High Level and Detailed Design, Development, Testing, Implementation and Production Support activities; experience building highly scalable ETL pipelines; experience with on-prem and relational database platforms including Oracle, SQL Server, PostGreSQL and Snowflake; experience with one of the cloud platforms: GCP, Azure or AWS; experience with Python; experience maintaining data quality frameworks, data observability and monitoring frameworks; knowledge of application monitoring, handling user tickets and analyzing data issues; and experience manipulating and analyzing large datasets.
JOB SITE: 1678 W. Redstone Ctr., Dr., Suite 210, Park City, UT 84098 and various unanticipated work locations. Telecommuting permitted from anywhere within the U.S.
Apply Now
Show more
Show less","Data Engineering, Analytics, Data Pipelines, Data Modeling, Data Integration, Data Governance, Software Development, Python, SQL, Oracle, SQL Server, PostgreSQL, Snowflake, GCP, Azure, AWS, ETL, Data Observability, Monitoring, Application Monitoring, Data Analysis, Data Processing","data engineering, analytics, data pipelines, data modeling, data integration, data governance, software development, python, sql, oracle, sql server, postgresql, snowflake, gcp, azure, aws, etl, data observability, monitoring, application monitoring, data analysis, data processing","analytics, application monitoring, aws, azure, data engineering, data governance, data integration, data observability, data processing, dataanalytics, datamodeling, datapipeline, etl, gcp, monitoring, oracle, postgresql, python, snowflake, software development, sql, sql server"
Senior Data Engineer,"Medifast, Inc","Lehi, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-medifast-inc-3741968685,2023-12-17,Utah,United States,Mid senior,Onsite,"About The Opportunity
At Medifast, our team members are relentless in our mission of driving Lifelong Transformation, One Healthy Habit at a Time®. When you join Medifast, you become part of a dynamic, fast-growing community of highly motivated, like-hearted people who share a passion for promoting health and wellness. Just as OPTAVIA Coaches inspire Clients to reach their personal wellness goals, at Medifast, we inspire each other to bring our best to work each day to further our shared mission. If you want to build a rewarding career that makes lives better on a daily basis, Medifast may be the perfect place for you.
Medifast is looking for a talented Senior Data Engineer to join our team in our exciting pursuit to transform and leverage a data-driven enterprise to impact every customer along their health journey. If you are creative and love building data products, we are innovating with new technologies and seeking curious, passionate, and diverse team members to drive change and impact our business. In this role, our Senior Data Engineer will collaborate across all of Medifast. While this position will exist in our data enterprise, you will partner with many amazing technologists, product managers, and internal and external business partners.
Overview Of Position
A Senior Data Engineer is responsible for developing and maintaining data processing software like databases, data structures, or algorithms to process data. Their duties include coordinating with company Executives and other professionals to create unique data infrastructure, running tests on their designs to isolate errors and updating systems to accommodate changes in company needs. They should be comfortable as both a player and a coach in this dynamic work environment, be naturally curious, and have a proven track record of mentoring and influencing senior engineers.
II. Job Responsibilities
Assembling large, complex sets of data that meet non-functional and functional business requirements
Leads the creation of software and data architecture and designs. Identifies and implements complex data products with best practices focused on greater scalability, optimizing data delivery, and automating manual processes
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
Acts as “Player and Coach” to team of software and data engineers – Mentoring, Coaching, and Collaborating
Lead projects and work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
III. Scope
Data engineering needs across business functions. Managing geographically dispersed internal/external team members.
IV. Knowledge, Education, Skills & Abilities
Bachelor’s Degree in CS, IT, or Engineering
Expert proficiency writing software to build data structures and algorithms
In-depth knowledge and experience creating high performance, fault-tolerant data and software architectures
Proven track record identifying and delivering relational and distributed database performance tuning best practices
Practitioner experience of the full data engineering lifecycle through maintenance and monitoring
Works independently and serves as aIV subject matter expert in data engineer solutions
Mentors, coaches, and acts as a resource for colleagues with less experience
5+ years of experience programming in Python
5+ years of experience programming in SQL
7+ years in data engineering focused on data enrichment, data integration and data warehouse projects
5+ years building data analytic pipelines to enable artificial intelligence or machine learning efforts
5+ years writing artificial intelligence software in an advanced analytic enterprise
5+ years writing software to build complex data pipelines between relational databases, NoSQL databases, API’s, flat files, and external sources
4+ years of AWS experience in services like RedShift, Amazon Redshift Spectrum, AWS Glue, AWS DMS, Kafka, etc.
1-3 years of Linux experience
5+ years of experience with Relational DB / NoSQL experience (PostgreSQL and Amazon Redshift or similar) with specific implementation on AWS cloud
5+ yrs experience with different DBMS like Oracle, SQL Server, MySQL, etc
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow for complex problem solving
Experience with data serialization languages such as JSON, XML, YAML
Hands-on experience with Transactional and Dimensional data modeling methodologies (eg. Normalization, Star Schema, Snowflake Schema)
5+ yrs experience with data mapping, data transformation and handling different data formats – CSV, XML, JSON, JMS
At Medifast, Relationships Are At The Center Of What We Do!
We thrive by elevating our connections with one another as well as with our Coaches & Clients. We believe that everyone has the potential to be
OUTSTANDING
. The Medifast culture is built on seven core values:
integrity, courage, teaming, accountability, empowerment, partnership and diversity
. These values aren’t just words on a page – they are celebrated as a core part of the company’s philosophy.
We Lead By…
Mastering Relationships
: We build trust, promote collaboration and we are reliable.
Being Innovative
: We strive to improve things in our areas of influence; test, refine and expand within the business strategy; and reach beyond real and perceived boundaries.
Simplifying
: We are committed to making things measurable, repeatable and scalable; focusing on outcomes not activities; and eliminating complexity to increase focus.
Anticipating
: We predict long-term business and organizational needs; challenge assumptions; and expect and prepare for the unexpected.
More About Medifast
About Medifast®:
Medifast (NYSE: MED) is the health and wellness company known for its habit-based and coach-guided lifestyle solution
OPTA
VIA®, which provides people with a simple, yet comprehensive approach to help them achieve lasting optimal health and wellbeing.
OPTA
VIA's lifestyle plans deliver clinically proven health benefits as well as evidence-based tools, including scientifically developed products and a framework for habit creation reinforced by independent Coaches and Community support. As a physician-founded company with a 40+ year history, Medifast is a leader in the U.S. weight management industry. Through a collaboration with the national virtual primary care provider LifeMD,
OPTA
VIA customers have access to board-certified affiliated clinicians and medications, such as GLP-1s, that support treatment plans for obesity and other health conditions. The company continues to innovate and build upon its scientific and clinical heritage to fulfill its mission of offering the world Lifelong Transformation, One Healthy Habit at a Time®. Medifast was recognized in 2023 by Financial Times as one of The Americas' Fastest Growing Companies and in 2022 as one of America's Best Mid-Sized Companies by Forbes. For more information, visit MedifastInc.com and
OPTAVIA.com
and follow @Medifast on X.
Thank you for taking the time to learn more about Medifast.
Show more
Show less","Python, SQL, AWS, Linux, PostgreSQL, NoSQL, JSON, XML, YAML, Airflow, Glue, Dataflow, Data mining, Data transformation, Data visualization, Data analytics, Data modeling, Database management, Software development, Data pipelines, Data architecture, Data engineering, Artificial intelligence, Machine learning, Agile methodology, Scrum, Jira, Data structures, Algorithms, Problemsolving, Communication, Teamwork, Leadership, Mentoring","python, sql, aws, linux, postgresql, nosql, json, xml, yaml, airflow, glue, dataflow, data mining, data transformation, data visualization, data analytics, data modeling, database management, software development, data pipelines, data architecture, data engineering, artificial intelligence, machine learning, agile methodology, scrum, jira, data structures, algorithms, problemsolving, communication, teamwork, leadership, mentoring","agile methodology, airflow, algorithms, artificial intelligence, aws, communication, data architecture, data engineering, data mining, data structures, data transformation, dataanalytics, database management, dataflow, datamodeling, datapipeline, glue, jira, json, leadership, linux, machine learning, mentoring, nosql, postgresql, problemsolving, python, scrum, software development, sql, teamwork, visualization, xml, yaml"
Senior Cloud Database Engineer,NICE,"Sandy, UT",https://www.linkedin.com/jobs/view/senior-cloud-database-engineer-at-nice-3757568363,2023-12-17,Utah,United States,Mid senior,Onsite,"At NICE, we don’t limit our challenges. We challenge our limits. Constantly. We’re relentless. We’re ambitious. And we make an impact. Our NICErs bring their A game and spend each day turning it into an A+. And if you’re like us, we can offer you the kind of challenge that will light a fire within you.
Sr. Cloud Database Engineer
Location: Salt Lake City, UT
The Sr. Cloud Database Engineer will ensure SQL Server database performance and availability in a 24x7 environment.
Typical Day Might Include the Following:
Install, configure, upgrade, monitor, maintain and manage multiple SQL Server databases.
Perform database and application tuning.
Establish and maintain sound backup and recovery policies and procedures.
Implement and maintain database security
Create, maintain and monitor SQL Server replication.
Perform code review and provide consultation to development teams.
Provide 24x7 support in an on call rotation with other staff members.
Setup and maintain documentation and standards.
Review, approve and create database logical and physical designs.
Integrate third party software and databases with corporate databases.
Train other staff as necessary.
Attend meetings and training as required.
To Land This Gig You'll Need:
Bachelor’s degree in Computer Science, Business Information Systems, or related field or equivalent work experience required.
5+ years maintaining SQL Server mission critical databases
5+ years TSQL programming
2+ years implementing and maintaining database replication
Ability to work with technical and non-technical people
Problem solving skills are a must
Ability to meet deadlines
Bonus Experience:
Newer data/streaming platforms: Kafka, MSK, Snowflake, Aurora DB
HP Blade and EVA SAN optimization
Veritas Net Backup
MCSE/MCDBA/OCP
NET technologies
PowerShell
C#
About NICE
NICE Ltd. (NASDAQ: NICE) software products are used by 25,000+ global businesses, including 85 of the Fortune 100 corporations, to deliver extraordinary customer experiences, fight financial crime and ensure public safety. Every day, NICE software manages more than 120 million customer interactions and monitors 3+ billion financial transactions.
Known as an innovation powerhouse that excels in AI, cloud and digital, NICE is consistently recognized as the market leader in its domains, with over 8,500 employees across 30+ countries.
NICE is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, age, sex, marital status, ancestry, neurotype, physical or mental disability, veteran status, gender identity, sexual orientation or any other category protected by law.
Show more
Show less","SQL Server, TSQL, Database replication, Veritas Net Backup, MCSE/MCDBA/OCP, NET technologies, PowerShell, C#, Kafka, MSK, Snowflake, Aurora DB, HP Blade, EVA SAN","sql server, tsql, database replication, veritas net backup, mcsemcdbaocp, net technologies, powershell, c, kafka, msk, snowflake, aurora db, hp blade, eva san","aurora db, c, database replication, eva san, hp blade, kafka, mcsemcdbaocp, msk, net technologies, powershell, snowflake, sql server, tsql, veritas net backup"
Staff Data Engineer (ML/AI) - Remote,Plentific,United Kingdom,https://uk.linkedin.com/jobs/view/staff-data-engineer-ml-ai-remote-at-plentific-3719616578,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"We're Plentific, the world’s leading real-time property solution, and we're looking for top talent to join our ambitious team. We’re a global company, headquartered in London, and operating across the United Kingdom, Germany and North America.
As a B2B company, we're dedicated to helping landlords, letting agents and property managers streamline operations, unlock revenue, increase tenant satisfaction, and remain compliant through our award-winning SaaS technology platform. We also work with SMEs and large service providers, helping them access more work and grow their businesses.
We're not just any proptech - we're backed by some of the biggest names in the business, including A/O PropTech, Highland Europe, Mubadala, RXR Digital Ventures and Target Global and work with some of the world’s most prominent real estate players.
But we're not just about business - we're also building stronger communities where people can thrive by ensuring the quality and safety of buildings, supporting decarbonisation through our ESG Retrofit Centre of Excellence and championing diversity across the sector through the Women’s Trade Network. We're committed to creating exceptional experiences for our team members, too. Our culture is open and empowering, and we're always looking for passionate, driven individuals to join us on our mission.
So, what's in it for you?
A fast-paced, friendly, collaborative and hybrid/flexible working environment
Ample opportunities for career growth and progression
A multicultural workplace with over 20 nationalities that value diversity, equity, and inclusion
Prioritisation of well-being with social events, digital learning, career development programs and much more
If you're ready to join a dynamic and innovative team that’s pioneering change in real estate, we'd love to hear from you.
The Role
This is a fully remote position based anywhere in the UK.
We are looking for an experienced Staff Software Engineer to join the Data Engineering team. You'll be reporting to the Head of Data Engineering and will have responsibility across large production code bases and will also be tasked with software architecture designs and reviews. The role is heavy hands-on coding, as an individual contributor, with no people management responsibilities, though you will be expected to enjoy mentoring other engineers.
The Data Engineering team, alongside the Frontend, Backend and DevOps teams, sits at the centre of everything we do at Plentific and is constantly tackling challenging problems, such as online payments, quoting, invoicing, booking, search / scoring algorithms, ETL, data pipelines, in-app messaging, Public APIs, ML/AI, real-time notifications and fraud prevention.
The team has a stronger focus and ownership of the production Machine Learning systems, the data warehouse, Public APIs, real-time data pipelines, LLM's and AI in general, and our flagship interactive analytics module named 'Advanced Analytics'. Almost all code so far is written in Python and SQL.
Our tech stack is made of, amongst others: AWS, Kubernetes, Django, PostgreSQL, Snowflake, ElasticSearch, dbt, Looker, scikit-learn, FastAPI, Celery, Jenkins, GitHub.
Responsibilities
Be the main individual contributor to the most strategic and high impact projects in Plentific
Own and defend the software architecture designs from conception to release, including build vs buy discussions
Collaborate with Product stakeholders, to understand, define, develop and implement scalable, cutting-edge solutions that amaze our customers
Serve as a tech lead to look up to, providing guidance and mentorship to help develop the skills and talents of others
Champion best coding practices, documentation, reviews, unit and integrations testing, data quality and security, and performance
Requirements
Skills
Excellent Python and SQL skills, but you should be open to picking up other programming languages
A strong interest in learning and developing AI-powered systems (such as LLM-based)
A self-starter who assumes responsibility for their work, accepts direction and feedback from co-workers and managers, and happily helps make anyone’s good idea a reality
Ability to think out of the box with a can-do attitude to get things done efficiently
Excellent communication skills in English
Experience And Qualifications
7+ years of relevant experience as a Software Engineer
Experience with Machine Learning is a must, since we are heavily expanding our capabilities in this area
Benefits
As you can see, we are quickly progressing with our ambitious plans and are eager to grow our team of doers to achieve our vision of managing over 2 million properties through our platform across various countries. You can help us shape the future of property management across the globe. Here’s what we offer:
A competitive compensation package
A flexible working environment + 25 days annual holiday
Private health care including discounted gym membership
Enhanced parental leave
Life insurance
Employee assistance program
Company volunteering day and charity salary sacrifice scheme
Learning management system by SAP Litmos
Learning and development fund
Referral bonus and charity donation if someone you introduce joins the company
Season ticket loan, Cycle to work, Electric vehicle and Techscheme programs
Pension - 3% employer contribution, 5% employee contribution
Lunch of your choice once a week for office based employees
Regular company-sponsored lunches, dinners and social gatherings
Fully stocked kitchen with drinks, snacks, fruit, breakfast cereal etc
Show more
Show less","Python, SQL, AI, Machine Learning, LLM, AWS, Kubernetes, Django, PostgreSQL, Snowflake, ElasticSearch, dbt, Looker, scikitlearn, FastAPI, Celery, Jenkins, GitHub, DevOps","python, sql, ai, machine learning, llm, aws, kubernetes, django, postgresql, snowflake, elasticsearch, dbt, looker, scikitlearn, fastapi, celery, jenkins, github, devops","ai, aws, celery, dbt, devops, django, elasticsearch, fastapi, github, jenkins, kubernetes, llm, looker, machine learning, postgresql, python, scikitlearn, snowflake, sql"
Senior Data Engineer,TechAIte Solutions Inc,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-techaite-solutions-inc-3776650191,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Data Engineer
Must have 8
+ Years
of exp in Data Engineering
Must have skills :
Hadoop,Pyspark,Bigdata, SQL ,ETL
Should have deep knowledge in any cloud platform:
AWS, Azurs & GCP
Role Type
: Freelance @ WFH
What we are looking for
We are seeking an experienced Senior Data Engineer with experience in architecture, design, and development of highly scalable data integration and data engineering processes
The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL/ELT workload patterns, and related BI & analytics systems
Strong in scripting languages like Python, Scala
7+ years of hands-on experience with one or more of these data integration/ETL tools.
Experience building on-prem data warehousing solutions.
Experience with designing and developing ETLs, Data Marts, Star Schema
Designing a data warehouse solution using Synapse or SQL DB
Advanced working SQL knowledge and experience working with relational databases, and queries. authoring (SQL) as well as working familiarity with a variety of database
Show more
Show less","Data Engineering, Hadoop, Pyspark, Bigdata, SQL, ETL, AWS, Azure, GCP, Python, Scala, Data Integration Tools, Data Warehousing Solutions, Data Lakes, ETL/ELT Workload Patterns, BI & Analytics Systems, Synapse, SQL DB, Relational Databases, Queries","data engineering, hadoop, pyspark, bigdata, sql, etl, aws, azure, gcp, python, scala, data integration tools, data warehousing solutions, data lakes, etlelt workload patterns, bi analytics systems, synapse, sql db, relational databases, queries","aws, azure, bi analytics systems, bigdata, data engineering, data integration tools, data lakes, data warehousing solutions, etl, etlelt workload patterns, gcp, hadoop, python, queries, relational databases, scala, spark, sql, sql db, synapse"
Data Cabling Engineer,Digital Waffle,United Kingdom,https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3750300452,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Job Description
Position: Data Cabling Engineer- (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Dubai
Rate: £400 p/d
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Hold either a 'Security Cleared' or 'Develop Vetted' level of security clearance
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Security Cleared Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Data Cabling, Network Infrastructure, Cat6, Cat6a, Copper Cables, Metal Trunking, Containment, IT, Network Expansion, Network Performance, Cable Installation, Cable Routing, Cable Pathways, Cable Trays, Conduits, Wall Penetrations, Cable Connectors, Outlets, Termination Panels, Design Specifications, Cable Termination, Jack, Patch Panel, Industry Standards, Cable Labeling, Cable Layout Documentation, Cable Testing, Signal Quality, Continuity, Performance, Troubleshooting, Connectivity Issues, Signal Degradation, Cable Testers, Certification Tools, Quality Assurance, Quality Checks, Network Expansion Projects, Network Infrastructure Components, Safety Protocols, Accident Prevention, Safe Working Environment, Tool Operation, Security Clearance, Cabling Standards, TIA/EIA, ISO/IEC, Network Topologies, Protocols, Network Equipment, Technical Drawings, Schematics, Cabling Diagrams, ProblemSolving, Attention to Detail, Communication Skills, Collaboration, Teamwork, Office Spaces, Data Centers, Remote Locations, Safety Commitment, Industry Regulations, Cable Stripping, Cable Cutting, RJ45 Crimping, PunchDown Tool, Cable Continuity, Signal Quality, Fault Detection, Tone Generator, Probe, Cable Labels, Cable Markers, Label Printer, Tape Measure, Ruler, Level, Cable Ties, Velcro Straps, Cable Clips, Mounts, Power Drill, Bits, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, Personal Protective Equipment, SteelToed Boots, Hard Hat, Cable Fish Tape, Cable Rods, Cable Lubricant","data cabling, network infrastructure, cat6, cat6a, copper cables, metal trunking, containment, it, network expansion, network performance, cable installation, cable routing, cable pathways, cable trays, conduits, wall penetrations, cable connectors, outlets, termination panels, design specifications, cable termination, jack, patch panel, industry standards, cable labeling, cable layout documentation, cable testing, signal quality, continuity, performance, troubleshooting, connectivity issues, signal degradation, cable testers, certification tools, quality assurance, quality checks, network expansion projects, network infrastructure components, safety protocols, accident prevention, safe working environment, tool operation, security clearance, cabling standards, tiaeia, isoiec, network topologies, protocols, network equipment, technical drawings, schematics, cabling diagrams, problemsolving, attention to detail, communication skills, collaboration, teamwork, office spaces, data centers, remote locations, safety commitment, industry regulations, cable stripping, cable cutting, rj45 crimping, punchdown tool, cable continuity, signal quality, fault detection, tone generator, probe, cable labels, cable markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, personal protective equipment, steeltoed boots, hard hat, cable fish tape, cable rods, cable lubricant","accident prevention, attention to detail, bits, cable clips, cable connectors, cable continuity, cable cutting, cable fish tape, cable installation, cable labeling, cable labels, cable layout documentation, cable lubricant, cable markers, cable pathways, cable rods, cable routing, cable stripping, cable termination, cable testers, cable testing, cable ties, cable trays, cabling diagrams, cabling standards, cat6, cat6a, certification tools, collaboration, communication skills, conduits, connectivity issues, containment, continuity, copper cables, data cabling, data centers, design specifications, fault detection, hard hat, industry regulations, industry standards, isoiec, it, jack, label printer, level, metal trunking, mobile device, mounts, network equipment, network expansion, network expansion projects, network infrastructure, network infrastructure components, network performance, network topologies, notepad, office spaces, outlets, patch panel, pen, performance, personal protective equipment, power drill, probe, problemsolving, protocols, punchdown tool, quality assurance, quality checks, remote locations, rj45 crimping, ruler, safe working environment, safety commitment, safety glasses, safety protocols, schematics, screwdrivers, security clearance, signal degradation, signal quality, steeltoed boots, tape measure, teamwork, technical drawings, termination panels, tiaeia, tone generator, tool bag, tool operation, troubleshooting, velcro straps, wall anchors, wall penetrations, work gloves"
Data Warehouse Engineer,Holland & Barrett,United Kingdom,https://uk.linkedin.com/jobs/view/data-warehouse-engineer-at-holland-barrett-3749531120,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Are you an experienced Data Warehouse Engineer with a passion for transforming raw data into actionable insights? Do you have deep expertise in Cloud Data Warehouse solutions and can confidently describe yourself as an SQL guru, blended with a proven track record of designing and optimizing data warehouses for performance and scalability? Do you want to play a pivotal role in shaping data architecture and driving data-driven decision-making?
Holland & Barrett is a company at the forefront of technology innovation, revolutionizing the Well-Tech industry with our groundbreaking products and solutions. Our commitment to excellence and data-driven insights drives us to seek a Data Warehouse Engineer who shares our passion for leveraging data to unlock business potential.
As a Data Warehouse Engineer, you will be required to:
Design, implement, and manage our AWS Redshift data warehouse, ensuring optimal performance, scalability, and reliability across a broader AWS ecosystem.
Collaborate with cross-functional teams to gather data requirements, define data models, and develop ETL workflows (Matillion) and processes to transform and load data from various sources.
Optimize SQL queries and data pipelines to deliver fast and accurate results to end-users.
Work closely with Data Scientists and Analysts to understand their data needs and provide the necessary infrastructure and tools to integrate all required raw data sources.
Build data cubes for BI reporting, marketing automation, customer segmentation, and content personalization.
Continuously monitor and troubleshoot the data warehouse environment to proactively address performance bottlenecks and issues.
Stay up to date with the latest industry trends and best practices in data warehousing, AWS services, and SQL technologies.
Mentor junior team members, sharing your expertise and knowledge to foster their growth and development.
Integrate and ingest numerous raw data sources such as sales, supply chain, and clickstream data.
Apply a strong quantitative approach to drive the H&B technical data agenda.
As a Data Warehouse Engineer, you will bring to the role:
Experience in data warehouse design, development, visualization, and management.
Strong proficiency in AWS Redshift or similar Cloud Data Warehouse, including setup, configuration, and optimization.
Ability to transform data sets into world-class analytics and business intelligence solutions.
Expertise in writing complex SQL queries for data transformation and analysis.
Proven experience with ETL processes and tools, data modeling, and data integration.
Solid understanding of performance tuning, indexing, and query optimization techniques.
Familiarity with data warehousing best practices, data governance, and security protocols.
Excellent problem-solving skills and the ability to troubleshoot complex data-related issues.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Good all-round tech knowledge to work across a diverse Data Tech Stack is essential.
Our Benefits
This role is fully remote with occasional travel to our London hub.
We offer a 10% performance incentive scheme for our tech teams.
Learn from the best – at H&B you will have the opportunity to enhance and expand your skills and shape your career.
We want you to produce your best, so we provide you with the best tech equipment.
Stay healthy with a 25% discount on all product ranges to help you live well.
We like to recognize and celebrate our people with our Colleague Recognition Scheme, so your hard work will not go unnoticed.
About H&B
Holland & Barrett is one of the nation's most loved and trusted brands, known for offering quality health food, vitamins, and supplements all sold by highly trained and qualified advisors.
Bucking the current trend of high street retailers, we forecast significant growth and expansion plans in the coming years, with considerable investment going into all areas of the business. We certainly embrace change and drive speed in everything we do. Every day presents a different challenge, but every day is also filled with fun, teamwork, and passion to succeed and surpass every expectation.
Our culture respects equality, values diversity, and encourages individuality – because this allows our people to unlock their potential and be their best. We welcome everyone who shares our EPIC values regardless of background, culture, disability, ethnicity, gender identity, or sexual orientation.
Show more
Show less","Data Warehouse Engineer, Data Warehouse Design, Cloud Data Warehouse, SQL, Data Modeling, ETL, Matillion, Data Analytics, Business Intelligence, AWS Redshift, AWS, Data Integration, Performance Tuning, Indexing, Query Optimization, Data Governance, Data Security, Problem Solving, Communication, Team Collaboration, Tech Stack","data warehouse engineer, data warehouse design, cloud data warehouse, sql, data modeling, etl, matillion, data analytics, business intelligence, aws redshift, aws, data integration, performance tuning, indexing, query optimization, data governance, data security, problem solving, communication, team collaboration, tech stack","aws, aws redshift, business intelligence, cloud data warehouse, communication, data governance, data integration, data security, data warehouse design, data warehouse engineer, dataanalytics, datamodeling, etl, indexing, matillion, performance tuning, problem solving, query optimization, sql, team collaboration, tech stack"
Lead Data Engineer,Oho Group Ltd,United Kingdom,https://uk.linkedin.com/jobs/view/lead-data-engineer-at-oho-group-ltd-3782234418,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Lead Data Engineer - United Kingdom - Exciting Start-Up
🚀
Permanent + Fully Remote + Share Options
Are you looking for the next exciting step in your Data Engineering career? Do you have a strong interest in Machine Learning and/or Data Science too?
Here’s a fantastic opportunity for you to be a core member of a fast-paced and growing start-up! As the Lead Data Engineer, you will be the lead in developing their spatial intelligence platform and will take ownership of their data analytics stack. In this Lead Data Engineering role, you will also maintain their codebase and extend its functionalities while working with their clients. You will also be able to work directly under the CEO and have the opportunity to work alongside the CTO in your position as the Lead Data Engineer. This Lead Data Engineer role will allow you to truly make a long-lasting impact on the business as it expands.
What you will ideally have these skills 🖥️:
Python (+ NumPy, SciPy)
Machine Learning and MLOps (including CI/CD, monitoring and deployment)
Linux
Docker
AWS
Kafka
Preferably an MSc or PhD in Computer Science, Mathematics, Statistics or related technical field
If you’re passionate about technology and this sounds like the role for you, apply for immediate consideration!
Lead Data Engineer - United Kingdom - Exciting Start-Up
Show more
Show less","Python, NumPy, SciPy, Machine Learning, MLOps, CI/CD, Monitoring, Deployment, Linux, Docker, AWS, Kafka, Computer Science, Mathematics, Statistics","python, numpy, scipy, machine learning, mlops, cicd, monitoring, deployment, linux, docker, aws, kafka, computer science, mathematics, statistics","aws, cicd, computer science, deployment, docker, kafka, linux, machine learning, mathematics, mlops, monitoring, numpy, python, scipy, statistics"
Data  Engineer,Cypher Consulting Europe S.L.,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-cypher-consulting-europe-s-l-3780809195,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"We are looking for a skilled data engineer to help build and maintain our data infrastructure. The ideal candidate will have expertise working with cloud data platforms and architecting robust and scalable data pipelines.
Tasks
As a data engineer, you will work cross-functionally to intake, process, transform, and store large amounts of data from various sources. You will build and maintain the core data infrastructure that powers the organisation's advanced analytics, machine learning applications, and data-driven decision-making. This role requires strong technical skills, problem-solving abilities, and excellent communication with stakeholders at all levels.
Requirements
Proficient in data tools and services on Google Cloud Platform and/or Azure
Experience with building and maintaining data warehouses, data lakes, and other large-scale data storage systems
Knowledge of ETL processes and data transformation techniques
Ability to design and develop data aggregation and integration processes across different systems and environments
Understanding of data architecture patterns such as lambda and kappa architecture
Snowflake
Data Vault
AWS
Nice to Have:
Experience with blockchain infrastructure and data integration
Knowledge of SQL and NoSQL databases
Experience with data visualization and business intelligence tools
Familiarity with machine learning pipelines and model deployment
All candidates must be authorised to work in the EU.
Preferred candidate locations:
Cezch
Slovakia
Hungary
Romania
Ukraine
Lithuania
Latvia
Portugal
Poland
Show more
Show less","Cloud data platforms, Data pipelines, Data infrastructure, Advanced analytics, Machine learning, Datadriven decisionmaking, Data tools, Google Cloud Platform, Azure, Data warehouses, Data lakes, Data storage systems, ETL processes, Data transformation, Data aggregation, Data integration, Data architecture patterns, Lambda architecture, Kappa architecture, Snowflake, Data Vault, AWS, Blockchain infrastructure, SQL, NoSQL databases, Data visualization, Business intelligence tools, Machine learning pipelines, Model deployment","cloud data platforms, data pipelines, data infrastructure, advanced analytics, machine learning, datadriven decisionmaking, data tools, google cloud platform, azure, data warehouses, data lakes, data storage systems, etl processes, data transformation, data aggregation, data integration, data architecture patterns, lambda architecture, kappa architecture, snowflake, data vault, aws, blockchain infrastructure, sql, nosql databases, data visualization, business intelligence tools, machine learning pipelines, model deployment","advanced analytics, aws, azure, blockchain infrastructure, business intelligence tools, cloud data platforms, data aggregation, data architecture patterns, data infrastructure, data integration, data lakes, data storage systems, data tools, data transformation, data vault, data warehouses, datadriven decisionmaking, datapipeline, etl, google cloud platform, kappa architecture, lambda architecture, machine learning, machine learning pipelines, model deployment, nosql databases, snowflake, sql, visualization"
Lead Data Engineer : Fully Remote,TipTopJob,United Kingdom,https://uk.linkedin.com/jobs/view/lead-data-engineer-fully-remote-at-tiptopjob-3788317730,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Lead Data Engineer : Exiting Spatial Intelligence Start Up : Fully Remote A Lead Data Engineer is required for a pioneering company at the forefront of revolutionizing industrial operations. Coming in as a Lead Data Engineer youll have the opportunity to spearhead the development of this Spatial Intelligence Platform. Lead Data Engineer / ML : The Role: As a Data Engineer, youll leverage your expertise in data analysis, machine learning, and software development to take charge of our cloud:based data analytics stack. Your responsibilities will include refining and expanding the existing codebase, translating raw data into actionable insights, and leading the evolution of our Spatial Intelligence Platform. Lead Data Engineer / ML : About You:
: Top academics in a relevant field (Computer Science, Statistics, Mathematics)
: Strong command of Python and its data analysis libraries (e.g., numpy, scipy)
: Proven expertise in Python:based machine learning, optimization, and process mining
: Knowledgeable about MLOps principles, CI/CD, model versioning, monitoring, and deployment
: Competent in utilizing Linux, Docker, AWS, Kafka, and git
: Consistent practice of best software development methodologies, inclusive of testing and deployment
: Capability to work autonomously, excellent collaborative skills, and effective communication in English
: Demonstrated history of successful project delivery coupled with an enthusiastic drive for innovation
: Cloud Experience (AWS, Azure, GCP)
: Knowledge of ERP/WMS software landscape Lead data Engineer / ML : Benefits:
: Remote working
: Amazing progression opportunities
: Competitive Salary
: Stock Options
: Working with a very high level skilled team Lead Data Engineer : Exiting Spatial Intelligence Start Up : Fully Remote
Show more
Show less","Data Engineer, Spatial Intelligence, Data Analysis, Machine Learning, Software Development, Python, Numpy, Scipy, Optimization, Process Mining, MLOps, CI/CD, Model Versioning, Monitoring, Deployment, Linux, Docker, AWS, Kafka, Git, Cloud Experience, ERP/WMS","data engineer, spatial intelligence, data analysis, machine learning, software development, python, numpy, scipy, optimization, process mining, mlops, cicd, model versioning, monitoring, deployment, linux, docker, aws, kafka, git, cloud experience, erpwms","aws, cicd, cloud experience, dataanalytics, dataengineering, deployment, docker, erpwms, git, kafka, linux, machine learning, mlops, model versioning, monitoring, numpy, optimization, process mining, python, scipy, software development, spatial intelligence"
Lead Data Engineer,Noir,United Kingdom,https://uk.linkedin.com/jobs/view/lead-data-engineer-at-noir-3787082824,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Lead Data Engineer (Python, PySpark) - Remote
(Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer, Python, PySpark, SQL, Big Data, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer)
Our client is a global innovator and world leader with one of the most recognisable names within technology. They are looking for a Lead Data Engineer with significant Python and PySpark experience as well as management responsibility to run an exceptional Agile engineering team and provide technical and team leadership through coaching and mentorship.
We are seeking a Lead Data Engineer and line manager capable of creating a dynamic and positive environment for your team to excel. This will include coaching your team, working with architects, creating automated tests, instilling a culture of continuous improvement and setting standards for the team. You will be responsible for building a greenfield modern data platform using cutting-edge technologies, architecting big data solutions and developing complex enterprise data ETL and ML pipelines and projections.
The successful candidate will have strong Python, PySpark and SQL experience, possess a clear understanding of databricks, as well as a passion for Data Science (R, Machine Learning and AI). Database experience with SQL and No-SQL – Aurora, MS SQL Server, MySQL is expected, as well as significant Agile and Scrum exposure along with SOLID principles. Continuous Integration tools, Infrastructure as code and strong Cloud Platform knowledge, ideally with AWS is also key.
We are keen to hear from talented Lead Data Engineer candidates from all backgrounds.
This is a truly amazing opportunity to work for a prestigious brand that will do wonders for your career. They invest heavily in training and career development with unlimited career progression for top performers.
Location: Remote
Salary: £75k - £95k + Bonus + Pension + Benefits
To apply for this position please send your CV to Nathan Warner at Noir Consulting.
NOIRUKTECHREC
NOIRUKREC
Show more
Show less","Python, PySpark, SQL, DataBricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, GitHub, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Aurora, MS SQL Server, MySQL","python, pyspark, sql, databricks, r, machine learning, ai, agile, scrum, tdd, bdd, ci cd, solid principles, github, azure devops, jenkins, terraform, aws cdk, aws cloudformation, azure, aurora, ms sql server, mysql","agile, ai, aurora, aws cdk, aws cloudformation, azure, azure devops, bdd, ci cd, databricks, github, jenkins, machine learning, ms sql server, mysql, python, r, scrum, solid principles, spark, sql, tdd, terraform"
Senior Data Engineer,Distributed,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-distributed-3711546196,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Invite to join the Elastic Team Community
Distributed has created the world's best freelance developer community. Our mission is to provide freelancers with the same variety and freedom of freelancing but with more benefits than permanent employment.
This is an opportunity to become part of our community and access projects unavailable to the broader market. While entrance to the Community doesn't guarantee selection for a project, it gives you expedited access to some of the most innovative projects in the world. Once accepted, you'll become part of a group of like-minded, skilled individuals.
All members are required to complete basic assessments in order to join the Elastic Team Community.
About Joining The Elastic Team
If approved
to join our Community by qualifying through the assessment process, you will automatically be one of the individuals who could be considered for exciting customer-centric projects that we are currently engaged in for our clients.
Once on a project,
you will be working closely with teams of engineers, QAs, and designers, producing detailed specifications and writing the programme codes, testing the product in controlled, real situations before going live.
We're proud to have Enterprise companies like Capita and BT, BBC, NHS, Virgin, Money, BP, Master Card, Tesco and Suzuki select us for delivering user-centric products.
Our mission as an organization is to provide our Community with access to the most exciting tech projects and to build a freelance career with us as we continue to scale as an organisation.
We're looking for Data Engineers with +4 years of experience in
data architecture, ETL processes, and data modelling.
Once approved to one of our projects you will work closely with cross-functional teams, including data scientists, analysts, and software engineers, to ensure the availability and accessibility of high-quality data for analysis and decision-making.
Qualifications:
4+ years of experience in data engineering, including ETL development, data modelling, and data warehousing.
Strong proficiency in SQL and one or more programming languages (e.g., Python, Java, Scala).
Experience with big data technologies and frameworks (e.g., Hadoop, Spark, Kafka).
Proficiency with cloud-based data storage and processing platforms (e.g., AWS Redshift, Azure Data Lake, Google BigQuery).
Knowledge of data integration tools and practices.
Excellent problem-solving and communication skills.
Ability to work effectively in a project-based environment with changing requirements.
About Us
Distributed is proud to be an equal opportunities employer. Employees and contractors, as well as prospective employees and contractors, will all be treated equally and fairly. Distributed is committed to ensuring no less favourable treatment is experienced by any current or prospective employee because of any of the protected characteristics under the UK Equality Act 2010 or equivalent local equality legislation.
By submitting your application you give us permission to store and use the information from your CV and your answers to application questions.
Show more
Show less","Data Architecture, ETL, Data Modelling, SQL, Python, Java, Scala, Hadoop, Spark, Kafka, AWS Redshift, Azure Data Lake, Google BigQuery, Data Integration, Cloudbased Data Storage, Cloudbased Data Processing","data architecture, etl, data modelling, sql, python, java, scala, hadoop, spark, kafka, aws redshift, azure data lake, google bigquery, data integration, cloudbased data storage, cloudbased data processing","aws redshift, azure data lake, cloudbased data processing, cloudbased data storage, data architecture, data integration, data modelling, etl, google bigquery, hadoop, java, kafka, python, scala, spark, sql"
Senior Data Engineer,GetHarley,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-getharley-3774170797,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"About GetHarley
GetHarley empowers people to look, feel and be their best selves — at every age. We are the first platform to combine technology with the human instinct and experience of a Clinician to deliver a deeply personalized and effective experience. Our company is transforming the skincare and healthy ageing experience as we know it.
Launched in May 2019, we are backed by top-tier venture capitalist firms, raising $52 million in our funding to date. Currently, at ~70 employees and growing, we are an award-winning, fast-paced business, growing triple-digits year over year with no market competition. Today, we empower over 1000 clinicians to increase their skincare product sales and drive patient loyalty via the GetHarley platform and we plan to grow this significantly in the coming year. We are looking for ambitious, dynamic, kind people to help us continue to scale.
About GetHarley
GetHarley empowers people to look, feel and be their best selves — at every age. We are the first platform to combine technology with the human instinct and experience of a Clinician to deliver a deeply personalized and effective experience. Our company is transforming the skincare and healthy ageing experience as we know it.
Launched in May 2019, we are backed by top-tier venture capitalist firms, raising $52 million in funding to date. Currently, at ~80 employees and growing, we are an award-winning, fast-paced business, growing triple-digits year over year with no market competition. Today, we empower over 1000 clinicians to increase their skincare product sales and drive patient loyalty via the GetHarley platform and we plan to grow this significantly in the coming year. We are looking for ambitious, dynamic, kind people to help us continue to scale.
Your Work
As the founding Senior Data Engineer in the Product & Engineering Team, you will work closely with the team to lead, design and build the foundational data infrastructure for GetHarley. This is just the beginning and will be a unique opportunity to influence key architectural decisions at an early stage. You will be pivotal in defining our data engineering approach, the design and implementation of an advanced data ecosystem that will unlock the power of data to scale the company to the next level.
In This Role, You Will:
Be part of the making of technical decisions that will have a direct impact on how our platform grows and evolve
Build our data warehouse to enable efficient data storage, retrieval, and analysis.
Develop and maintain data pipelines that collect, process, and transform data from various sources into usable formats for analysis.
Automate data ingestion, build quality control for data consistency, and cataloguing new data sets for data discovery
Be part of a lean, effective, engaged team that is mission-driven and focused on high impact.
Work closely with Backend Engineers, Data Analyst, CEO, and other team members to understand data requirements
Champion strong practices and ethics for data security, privacy, and compliance with relevant regulations.
Experience
You will be a great fit if you:
5+ years of experience as a data engineer, working with big complex datasets in both a start-up and corporate environment.
A passion for data architecture and the ability to envision and implement efficient data storage and retrieval strategies.
Expertise in Python and SQL
Familiarity with cloud platforms e.g. AWS, GCP
Familiarity with ETL tooling
Familiarity with CDP tools e.g. Segment, Oracle or Salesforce
Good understanding and value of the best of the software practices, maintainable codebase, rich CI/CD pipelines and quality design documentation
Proactive attitude towards problem-solving and a willingness to tackle challenging technical issues.
Exceptional attention to data security, privacy, and compliance considerations.
Strong communication skills with proven success in influencing without authority.
Excitement for exploring new technologies and approaches, with a willingness to learn and adapt.
Location
This role can be fully remote or hybrid.
Working Hours
This role is full-time hours (40 hours per week, with 2 days off a week).
Why choose GetHarley
It’s an exciting challenge. No two days are the same! We are an ambitious company that move fast and hustle
You are making a difference. We are empowering our clinicians and supporting our patient's skincare goals
Growth opportunities. We take personal development seriously and support your growth ambitions
What We Can Offer You
25 days of annual leave + bank holidays
Laptop and required software provided
Wide range of growth opportunities in a scale-up environment
Regular team socials
Discounted skincare products
In-office manicures and facials, regular team workouts at exclusive gyms and studios
Quarterly all-company GetTogethers including a countryside retreat!
A birthday treat delivered to your door
Dog-friendly office located in Marylebone
Seasonal merch drops
GetHarley is an equal opportunities employer ensuring that all applicants are treated equally and fairly throughout our recruitment process. We are determined that no applicant experiences discrimination on the basis of sex, race, ethnicity, religion or belief, disability, age, gender identity, ancestry, sexual orientation, veteran status, marriage and civil partnership, pregnancy and maternity, or any other basis prohibited by applicable law.
We want to leverage this diversity by building an inclusive culture where everyone is respected, can be themselves and strive to be their best. That way we can build a better future for our employees, our patients and clinicians.
Show more
Show less","Data architecture, Data engineering, Python, SQL, AWS, GCP, ETL tooling, CDP tools, Segment, Oracle, Salesforce, CI/CD pipelines, Data security, Data privacy, Data compliance","data architecture, data engineering, python, sql, aws, gcp, etl tooling, cdp tools, segment, oracle, salesforce, cicd pipelines, data security, data privacy, data compliance","aws, cdp tools, cicd pipelines, data architecture, data compliance, data engineering, data privacy, data security, etl tooling, gcp, oracle, python, salesforce, segment, sql"
Lead Data Engineer,ZOE,United Kingdom,https://uk.linkedin.com/jobs/view/lead-data-engineer-at-zoe-3784954290,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"We Are Redefining How People Approach Their Health
ZOE is combining scientific research at a scale never before imagined and cutting-edge AI to improve the health of millions .
Created by the world’s top scientists, our personalised nutrition program is reimagining a fundamental human need – eating well for your own body. Currently available in the US and the UK, ZOE is already helping > 100k ZOE members to adopt healthier habits and live better. Our work and expertise in biology, engineering, data science, and nutrition science have led to multiple breakthrough papers in leading scientific journals such as Nature Medicine, Science, The Lancet, and more.
To learn more, head to Spotify , Apple Podcasts , or Audible to listen to our Science & Nutrition Podcast (with 3 million listens!)
A remote-first, high-growth startup, we are backed by founders, investors, and entrepreneurs who have built multi-billion dollar technology companies. We are always looking for innovative thinkers and builders to join our team on a thrilling mission to tackle epic health problems. Together, we can improve human health and touch millions of lives.
We value inclusivity, transparency, ownership, open-mindedness and diversity. We are passionate about delivering great results and learning in the open. We want our teams to have the freedom to make long-term, high-impact decisions, and the well-being of our teammates and the people around us is a top priority.
Check out what life is like for our tech team on ZOE Tech .
We’re looking for a
Data Platform Engineer
to take ZOE even further.
Meet the team
The mission of the Platform team is to accelerate ZOEntists' path to production. We provide the building blocks to enable our teams to move fast and maximise their focus time on solving business problems.
We are responsible for building and maintaining the technical components that enable our organisation to develop, deploy, and operate high-quality software products at scale. We primarily focus on cloud infrastructure, data platform and devx.
At ZOE we strongly believe in DevOps as a culture ” You build it, you run it”. To achieve this vision, we heavily focus on enablement and strive to provide the best self-serve platform experience to make teams highly productive.We operate in a fast-paced environment. We work closely with engineers to understand what slows them down and continuously improve their productivity. We standardise best practices, push for consistency and raise the technical bar across our teams. We also provide technical guidance and support.On our Data Platform, our systems make use of GCP, Apache Airflow, Fivetran, DataStream, BigQuery, dbt, HEX or Holistics. Our code is primarily written in Python and we also manage our Data Platform with IaaC using Terraform and Config Connector.
About The Role
As a Data Platform Engineer, you will be working with a large number of stakeholders including engineers, data scientists, data analysts and/or analytics engineers.
You will play a key role in supporting ZOEntists to accelerate data-driven innovations and decision-making. You will have the opportunity to drive change and mentor teams to ingest, transform, store and visualise data in a performant, reliable, scalable and cost-effective manner.
You'll be...
Spearheading the development, testing, tuning, and maintenance of our Data Platform.
Collaborating with various teams to enhance system integration and augment data quality; solving complex business problems with data-powered solutions.
We are enabling ease and speed of data discovery for teams.
Making substantial contributions to the design of our data platform. Exploring and integrating innovative technologies within our tech stack.
Liaising with stakeholders within and outside the Technology department to deliver state-of-the-art, self-serve solutions that empower our engineers and drive business success.
Mentoring and guiding other engineers to foster a culture of continuous growth and knowledge sharing.
Leading by example, advocating for high consistency and best-in-class developer experience that optimises for lead time.
Driving the adoption of best industry practices and standards proactively, enhancing the overall quality of our engineering processes and output.
Collaborating closely with our Cloud Infrastructure leaders to create a secure, reliable, and observable Data Platform.
We think you’ll be a great fit if you…
Experience in a similar role, with a proven track record of working on data integration, ETL processes, and data warehousing.
Bring expertise in managing large data platforms in cloud-native environments such as GCP, AWS or Microsoft Azure.
Have a solid Software Engineering background.
Can deal with ambiguity. As a Series B Start-up, we often need to iterate fast and clarify on the go.
Believe in outcome over output. Focus on what matters most and measure the impact of your work.
Are passionate about operation excellence and comfortable leading by example.
Enjoy working in a collaborative environment. You will work closely with different stakeholders and lead cross-functional projects.
Are passionate about ML modelling and Ops.
Our hiring process consists of the following stages:
➡️ Talent Acquisition Screening:
Selected candidates will be contacted for an initial phone interview with our Talent Acquisition Partner. This conversation will help us assess your qualifications, experience, and your fit for our team. Duration 45min
➡️ Hiring Manager Screening:
Selected candidates will be moved forward in the process and meet the Hiring Manager. This conversation will focus on diving deeper in regards to your qualifications, experience, and your fit for ZOE. Duration 30min
➡️ Technical Interviews:
You will be asked to complete two technical interviews, which evaluate your coding and system design experience. Duration 60min/each
➡️ Behavioural Interviews:
You will be asked to complete two behavioural interviews, which evaluate your present and past experience from a behavioural standpoint. Duration 45min/each
⏰
Expected Timeline:
The entire hiring process typically takes around 3-4 weeks from the Talent Acquisition Interview to the job offer. We understand the importance of your time and aim to provide timely feedback and updates at each stage.
These are the ideal skills, attributes, and experience we’re looking for in this role. Don’t worry if you don’t tick all the boxes, especially on the skills and experience front, we’re happy to upskill for the right candidate.
Life as a ZOEntist – what you can expect from us:
As well as industry-benchmarked compensation and all the hardware and software you need, we offer a thoughtfully-curated list of benefits. We expect this list to evolve as we continue supporting our team members’ long-term personal and professional growth, and their wellbeing.
Remote-first:
Work flexibly – from home, our London office, or anywhere within the EU
Stock options:
So you can share in our growth
Paid time off:
28 days paid leave (25 holiday days, plus 2 company-wide reset days, and 1 “life event” day)
Enhanced Parental Leave:
On top of the statutory offering
Flexible private healthcare and life assurance options
Pension contribution:
Pay monthly or top up – your choice.
Health and wellbeing:
Like our Employee Assistance Program and Cycle to Work Scheme
Social, WFH, and Growth (L&D) budgets.
Plus, multiple opportunities to connect, grow, and socialise
We’re All About Equal Opportunities
We know that a successful team is made up of diverse people, able to be their authentic selves. To continue growing our team in the best way, we believe that equal opportunities matter, so we encourage candidates from any underrepresented backgrounds to apply for this role. You can view our Equal Opportunities statement in full here .
A closer look at ZOE
Think you’ve heard our name somewhere before? We were the team behind the COVID Symptom Study, which has since become the ZOE Health Study (ZHS). We use the power of community science to conduct large-scale research from the comfort of contributors’ own homes. Our collective work and expertise in biology, engineering, and data/nutrition science have led to multiple breakthrough papers in leading scientific journals such as Nature Medicine, Science, The Lancet, and more.
Seen ZOE in the media recently? Catch our co-founder Professor Tim Spector (one of the world’s most cited scientists) and our Chief Scientist Dr Sarah Berry on this BBC Panorama, and listen to CEO Jonathan Wolf unpack the latest in science and nutrition on our ZOE podcast .
Oh, and if you’re wondering why ZOE? It translates to “Life” in Greek, which we’re helping ZOE members enjoy to the fullest.
Show more
Show less","Data engineering, Data integration, Data warehousing, GCP, AWS, Microsoft Azure, Apache Airflow, Fivetran, DataStream, BigQuery, dbt, HEX, Holistics, Python, Terraform, Config Connector, ETL processes, Software engineering, Cloud infrastructure, DevOps, Machine learning modelling, Ops","data engineering, data integration, data warehousing, gcp, aws, microsoft azure, apache airflow, fivetran, datastream, bigquery, dbt, hex, holistics, python, terraform, config connector, etl processes, software engineering, cloud infrastructure, devops, machine learning modelling, ops","apache airflow, aws, bigquery, cloud infrastructure, config connector, data engineering, data integration, datastream, datawarehouse, dbt, devops, etl, fivetran, gcp, hex, holistics, machine learning modelling, microsoft azure, ops, python, software engineering, terraform"
Data Analyst - Real World Evidence (RWE),OPEN Health,United Kingdom,https://uk.linkedin.com/jobs/view/data-analyst-real-world-evidence-rwe-at-open-health-3782034141,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Graduate Data Analyst
Reports to Data Analyst Team Lead
Job Summary
You will be part of a team of dynamic and talented analysts who are responsible for planning and delivering data analysis across multiple therapeutic areas. The projects involve creating reports and working on studies using a variety of source data such as HES, CPRD, Prescribing and QoF. You will contribute to UK and Global projects, the complexity ranging from basic to advanced analytics.
Essential Duties & Responsibilities
Conducting analysis using SQL which may involve statistical analysis
Liaising with both clients and Business Development Managers to understand requirements and interrogate the data to produce the best outcomes
Assisting in the interpretation of the results by way of producing excel documents which may be used in report writing
Creating data outputs that are used for building dashboards
Discuss data and results with clients, internal members of staff and other COE's (Centre of Excellence) within the Open Health Group.
Experience, Skills, And Qualifications
Undergraduate or Postgraduate degree in Epidemiology, Medical Statistics, related Health Data Science subject or evidence of working with SQL or healthcare data
Working with patient identifiable data and knowledge of Information Governance to treat the data accordingly
Good working knowledge of SQL and R
Experience in healthcare data analysis and reporting
Fluent in written and spoken English
Experience working in the healthcare industry (CRO or pharmaceutical) or the NHS
Experience of working with medical datasets (CPRD, HES, other)
Familiarity with clinical terminology (ICD10, OPCS, other)
Travel Requirements
Remote — may be required to travel to the London office for specific occasions
About OPEN Health
OPEN Health unites deep scientific knowledge with wide-ranging specialist expertise to unlock possibilities that improve health outcomes and patient wellbeing. Working in partnership with our clients, we embrace our different perspectives and strengths to deliver fresh thinking and solutions that make a difference.
OPEN Health is a flexible global organization that solves complex healthcare challenges across HEOR and market access, medical communications and creative omnichannel campaigns.
What we offer:
As a global organization, OPEN Health is committed to supporting our employees and their families through a comprehensive benefits program
Competitive pay, generous paid vacation and holidays, and health insurance programs across all our locations
Ongoing training and development opportunities which foster and shape your individual career path
An active and growing commitment to bettering the communities our employees call home through our Corporate Social Responsibility program
The opportunity to thrive in a global, collaborative environment while working every day to improve health outcomes and patient wellbeing
Diverse, inclusive culture that encourages you to bring your whole self to work
If we sound like the sort of business environment in which you would thrive, then we would love to hear from you.
Show more
Show less","SQL, R, Epidemiology, Medical Statistics, Health Data Science, Information Governance, Healthcare Data Analysis, Healthcare Reporting, English (written and spoken), Medical Datasets (CPRD HES), Clinical Terminology (ICD10 OPCS), Excel","sql, r, epidemiology, medical statistics, health data science, information governance, healthcare data analysis, healthcare reporting, english written and spoken, medical datasets cprd hes, clinical terminology icd10 opcs, excel","clinical terminology icd10 opcs, english written and spoken, epidemiology, excel, health data science, healthcare data analysis, healthcare reporting, information governance, medical datasets cprd hes, medical statistics, r, sql"
Data Engineer,Cloud Decisions,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-cloud-decisions-3787096845,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Data Consultant / BI Developer
Fully remote - Based from UK home.
c.£45-55k + Benefits (DOE)
Work for a well-respected Data & AI Microsoft Solutions Partner and land yourself an opportunity to drive your career forward.
Seeking a personable individual who is passionate about technology and working with customers to implement effective solutions.
As the successful Data Consultant / BI Developer you will have:
Proven experience in a Data Engineering / Business Intelligence hands-on role
Experience designing and developing ELT/ETL processes
Experience with Azure and/or SQL Server stack (SSIS, SSRS, SSAS)
A willingness to develop Azure Platform / services skills
Python and/or SQL skills
Excellent communication skills to develop in to a rounded consultant
Any of the following would be a real plus but not necessary: Relevant Microsoft Certifications (DP900, DP203), ADF, Databricks, Synapse knowledge.
As the successful Data Consultant / BI developer you will:
Deliver effective Data solutions to a variety of customers
Migrate on-prem to the cloud
Cover Data acquisition, engineering, modelling, analysis and visualisation
Always be learning!
If you are an experienced Data Engineer / BI Developer looking to take that next step up then this role is worth considering. Please do click ‘apply’ now.
Get in touch with Kerry Foreman - 07495306696 | kerry@clouddecisions.co.uk | Kerry Foreman | LinkedIn
Show more
Show less","Data Engineering, Business Intelligence, ELT/ETL, Azure, SQL Server, SSIS, SSRS, SSAS, Azure Platform, Python, SQL, Data acquisition, Data engineering, Data modelling, Data analysis, Data visualisation, Cloud migrations, Microsoft Certifications (DP900 DP203), ADF, Databricks, Synapse","data engineering, business intelligence, eltetl, azure, sql server, ssis, ssrs, ssas, azure platform, python, sql, data acquisition, data engineering, data modelling, data analysis, data visualisation, cloud migrations, microsoft certifications dp900 dp203, adf, databricks, synapse","adf, azure, azure platform, business intelligence, cloud migrations, data acquisition, data engineering, data modelling, data visualisation, dataanalytics, databricks, eltetl, microsoft certifications dp900 dp203, python, sql, sql server, ssas, ssis, ssrs, synapse"
Data Engineer,The Future Project Ltd,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-the-future-project-ltd-3781729482,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Data Engineer
£500 - £520 per day
6 months
Job Specification: Senior Data Engineer (Contractor) - HR Data Transition Project
Project Overview:
They are embarking on a significant enterprise-wide HR program to transition from IFS v9 to IFS v11 for HCM processes. This transition includes a shift from existing ETL processes to a modern data infrastructure, leveraging Data Vault 2.0 modeling, Snowflake for database management, and dbt for data transformation to establish robust data pipelines.
Key Responsibilities:
· Develop and implement Data Vault 2.0 models to support their advanced HR data requirements.
· Design and generate dimensional (star schema) and fact tables based on Data Vault models.
· Utilize dbt (data build tool) for data transformation within these pipelines, with a preference for experience in using Automate DV in dbt.
· Understand and document current ETL processes to effectively transition them into scalable data pipelines.
· Ensure high data quality and integrity throughout the data pipeline development and maintenance.
· Provide insights and recommendations to optimize new data pipelines.
· Collaborate closely with HR and IT departments to ensure reporting requirements are fulfilled in the new system.
Required Skills and Experience:
· Extensive experience in data engineering, with a strong focus on Data Vault 2.0 modelling.
· Proficiency in Snowflake for data storage and management.
· Expertise in dbt for data transformation, particularly with experience in Automate DV.
· Familiarity with HR data processes and systems, including experience with IFS versions.
· Solid understanding of dimensional modelling and fact table generation for data pipelines.
· Ability to translate complex data requirements into efficient and effective data models.
· Excellent problem-solving skills and meticulous attention to detail.
· Strong communication skills to effectively collaborate with various teams.
Show more
Show less","Data Engineering, Data Vault 2.0, Snowflake, dbt, Data Transformation, HR Data Processes, IFS, Dimensional Modelling, Fact Table Generation, Data Pipelines, Data Quality, Data Integrity, Automate DV","data engineering, data vault 20, snowflake, dbt, data transformation, hr data processes, ifs, dimensional modelling, fact table generation, data pipelines, data quality, data integrity, automate dv","automate dv, data engineering, data integrity, data quality, data transformation, data vault 20, datapipeline, dbt, dimensional modelling, fact table generation, hr data processes, ifs, snowflake"
Data Engineer,US Tech Solutions,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-us-tech-solutions-3780717739,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Duration:
6 months contract, Full-Time
Job Description:
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
Experience:
7+ years experience in the data warehouse space.
7+ years experience in custom ETL design, implementation and maintenance.
7+ years experience with programming languages (Python or Java), Python preferred.
7+ years experience in writing efficient SQL statements.
Experience working with either a Map Reduce or an MPP system.
Hands on and deep experience with schema design and dimensional data modeling.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Excellent communication skills including the ability to identify and communicate data driven insights.
Ability and interest in managing and communicating data warehouse plans to internal clients
Education:
BS/BA in Technical Field, Computer Science or Mathematics.
About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.
Recruiter Details:
Name:
Abhishek Mishra
Email:
ukjobboardusers@ustechsolutionsinc.com
Internal Reference Id:
23-00083
Show more
Show less","Data Warehouse Management, Data Modeling, Data Extraction, Data Transformation, Data Loading, Data Quality Management, SLA Management, Infrastructure Triaging, Programming Languages (Python Java), SQL, Map Reduce, MPP Systems, Schema Design, Dimensional Data Modeling, Data Analysis, Communication Skills, Data Visualization","data warehouse management, data modeling, data extraction, data transformation, data loading, data quality management, sla management, infrastructure triaging, programming languages python java, sql, map reduce, mpp systems, schema design, dimensional data modeling, data analysis, communication skills, data visualization","communication skills, data extraction, data loading, data quality management, data transformation, data warehouse management, dataanalytics, datamodeling, dimensional data modeling, infrastructure triaging, map reduce, mpp systems, programming languages python java, schema design, sla management, sql, visualization"
Data Engineer,Harrington Starr,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-harrington-starr-3782397722,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Data Engineer
InsurTech
Remote (UK)
Salary up to £50K + bonuses and comprehensive benefits package
Harrington Starr has partnered with one of the leading insurance brokers in the UK as they seek to add a Data Engineer to their ranks. Our client is an award-winning organisation and one of the fastest growing their field
You will be joining a team that strives to be the top digital insurance broker through innovation and collaboration with their major partners. They make substantial investments in their employees, ensuring your career advances in the direction you aspire it to
Within this role, you will collaborate with a team of data engineers and help construct a robust, cost-efficient data platform. You will help uphold coding standards, ensure regulatory data security and governance compliance, and implement modern development practices
What you need:
2+ years of development experience in SQL programming and systems integration
Professional ETL development experience
Strong knowledge of SQL, Data Modeling, Data Warehousing concepts
Excellent project management and stakeholder management skills
Experience with SSRS, SSIS and PowerBI very desirable
Knowledge in Azure highly desirable
A background in Insurance seen as a nice-to-have
This role is largely remote with the requirement to visit one of their offices once quarterly. They are offering a starting salary of £45 - 50K plus bonuses and excellent wider benefits
Please send your CV via the link or reach out directly to Graeme King at Harrington Starr for a confidential chat!
Show more
Show less","SQL, Data Modeling, Data Warehousing, SSRS, SSIS, PowerBI, Azure, ETL, Python","sql, data modeling, data warehousing, ssrs, ssis, powerbi, azure, etl, python","azure, datamodeling, datawarehouse, etl, powerbi, python, sql, ssis, ssrs"
Senior Data Engineer,Wave Talent,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-wave-talent-3780374295,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"SENIOR DATA ENGINEER - Industry Leader in Gen-AI
At Wave Talent, we don't want to consume any more of your time trying to decipher job descriptions to identify the information you need.
Instead, we spoke with you all to understand the key information you'd like to know about a job position before you enter the process. Hopefully, you'll find this listed below and if the job role sounds like it could be up your street; we'd love to arrange a chat to give you more info.
💰 Salary: up to £120,000
💻 Benefits: Equity / stock options included, enhanced holiday, genuinely amazing culture
🚀 Industry/Type of business: Generative AI
🏠 Remote working: Fully remote (but come to the London HQ whenever you!)
🧩 Interview process: 3 stages, including a technical task
🦄 Reporting to: Engineering Lead
👏 Company size: Series B, ~300 (and growing!)
🎯 Working on: 2 hires across the business in data & product
We'd love to chat with you about this position if:
You have a passion for all things Gen-AI, data and media
Have the technical skills to get involved with an advanced AI product platform
Have relevant experience working with Snowflake, DBT, Python & SQL, Segment, Fivetran, Hex, Metabase
Show more
Show less","Generative AI, Snowflake, DBT, Python, SQL, Segment, Fivetran, Hex, Metabase","generative ai, snowflake, dbt, python, sql, segment, fivetran, hex, metabase","dbt, fivetran, generative ai, hex, metabase, python, segment, snowflake, sql"
Senior Data Engineer,Premier Group Recruitment,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-premier-group-recruitment-3784532416,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Role: Senior Data Engineer
Skills:
- Data Engineer, SQL, Synapse, Bigquery, Snowflake, Databuild Tools.
£65,000 – Senior Data Engineer – Fully Remote (UK Based)
I have a client who are looking to hire a
Senior Data Engineer with
experience in SQL, Synapse, Bigquery, Snowflake and Databuild Tools on a permanent basis.
The company are a medium-sized Development company based in Cambridge, however, this role will be fully remote with no need to travel into the office.
Our Client is offering the successful candidate a salary up to £65,000 based on experience.
Skills needed:
Excellent communication and people skills
Data Engineer experience
Advanced SQL Skills
Synapse
Bigquery
Snowflake
DBT (Data build tools)
If this
Senior Data Engineer with experience
in SQL, Synapse, Bigquery, Snowflake and Databuild Tools opportunity is of interest then please forward your CV along with references and salary expectations and I will be in contact to discuss further.
Please Note: If this
Senior Data Engineer
opportunity is not of interest and you know of anyone whom it may be suitable for then please forward this on as our client is looking to interview ASAP.
Show more
Show less","Data Engineer, SQL, Synapse, Bigquery, Snowflake, DBT (Data build tools), Communication skills, People skills","data engineer, sql, synapse, bigquery, snowflake, dbt data build tools, communication skills, people skills","bigquery, communication skills, dataengineering, dbt data build tools, people skills, snowflake, sql, synapse"
Senior Data Engineer,Qinecsa Solutions,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-qinecsa-solutions-3777983870,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Commonwealth Informatics (a qinecsa company) is a global team of technology innovators and entrepreneurs with deep experience in clinical research, medical product safety and public health surveillance. We are working with regulators, major pharmaceutical companies, and academia to develop the next generation of analytical solutions used to improve drug, vaccine and device safety monitoring and public health surveillance.
Position Title: Senior Data Engineer
This position is responsible for developing data driven products and services for our Commonwealth Vigilance Workbench SaaS product suite, including data warehouses, data streaming, Data Access APIs and reporting.
Responsibilities
Contribute to all phases of the software development lifecycle.
Design and maintain conceptual, logical, and physical product data models.
Design, develop and maintain robust, efficient and performant production data integration processes (e.g. ETL/ELT)
Create unit tests for both existing and new code to ensure stability and accuracy.
Apply secure coding techniques.
Design and architect future-state solutions
Help refine business requirements into technical requirements.
Help guide and mentor less experienced software developers.
Support and resolve base product defects and incidents client testing and production environments.
Staying up to date with recent technology trends and tools.
Technical Stack
SQL
Python
Apache Kafka
PostgreSQL
Linux/Ubuntu
Education and Experience
· Bachelor’s degree in computer science, informatics, or a related field
· Minimum of 6 years of fulltime experience in data engineering / modelling
Commonwealth Informatics (a Qinecsa company) is an equal employment opportunity and affirmative action employer. We do not discriminate based upon race, color, religion, sex, national origin, age, genetic information, gender identity or expression, sexual orientation, disability, or any other characteristic protected by law.
Seniority Level
Mid-Senior level
Industry
Pharmaceutical Manufacturing
Employment Type
Full-time
Job Functions
Information Technology
Screening questions
Required qualifications
How many years of work experience do you have with Software as a Service (SaaS)?
Ideal Answer: Minimum: 5
Will you now or in the future require sponsorship for employment visa status?
Ideal Answer: No
How many years of work experience do you have with Data Warehousing?
Ideal Answer: Minimum: 4
can you please share the below salary details : current CTC: Expected CTC: Notice Period:
Ideal Answer: Minimum: 4
Preferred qualifications
How many years of work experience do you have with Python (Programming Language)?
Ideal Answer: Minimum: 1
Show more
Show less","Data Engineering, Data Warehousing, Data Integration, Python, Unit Testing, Secure Coding, Apache Kafka, PostgreSQL, Linux/Ubuntu, SQL","data engineering, data warehousing, data integration, python, unit testing, secure coding, apache kafka, postgresql, linuxubuntu, sql","apache kafka, data engineering, data integration, datawarehouse, linuxubuntu, postgresql, python, secure coding, sql, unit testing"
Contract Data Engineer,F5 Consultants,United Kingdom,https://uk.linkedin.com/jobs/view/contract-data-engineer-at-f5-consultants-3784163102,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Contract: Data Engineer/Data Analyst
Contract Duration: 12 weeks (January - March)
Rate: Up to £450pd Outside IR35
Location: Fully Remote
About the Role
:
Our client is seeking a Data Engineer/Data Analyst for a 12-week contract role, with potential for extension.
This role involves accessing, extracting, and analysing data stored in M365 tools such as Outlook, Teams, and OneDrive.
Role Highlights:
Analysing and rectifying data pull issues within existing pipelines.
Segregating data across M365 tools (Outline, Teams, OneDrive) to establish ownership structures for multiple businesses.
Engineering the data flow effectively, maintaining strict adherence to data governance standards.
What We're Looking For:
A skill set encompassing Azure, M365 suite, Purview, Graph API, PowerShell, SQL, Power BI, and Databricks.
Exceptional analytical skills to decipher intricate data structures and flows, and resolve data integration challenges.
Excellent communication skills are pivotal for collaborating effectively with internal teams.
Show more
Show less","Data Engineering, Data Analysis, Azure, M365 suite, Purview, Graph API, PowerShell, SQL, Power BI, Databricks, Data Governance, Data Structures, Data Flows, Data Integration","data engineering, data analysis, azure, m365 suite, purview, graph api, powershell, sql, power bi, databricks, data governance, data structures, data flows, data integration","azure, data engineering, data flows, data governance, data integration, data structures, dataanalytics, databricks, graph api, m365 suite, powerbi, powershell, purview, sql"
Data Engineer,IDR,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-idr-3777084413,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Are you ready to be a part of a dynamic fintech company that's bringing people and technology together to change the way the private markets operate? We’re on a mission to be the trusted hub of the private markets, connecting fund managers and investors around the globe. Through building cutting-edge technology enabled solutions, we're revolutionizing the private markets investor journey, with a universally compliant single access token.
We do this by living true to our core values: We
challenge
the norm, we
change
the way we think and work, by
connecting
systems and people, while
committing
to best practice. We're seeking a skilled Data Engineer to join our global team of experts.
Location:
UK (Remote) or Cape Town, South Africa (Hybrid)
Position Overview:
We are seeking a highly skilled and experienced professional to join our data team as a Data Engineer and Data Scientist. This role will focus on collecting, processing, and analysing data to drive actionable insights for our organisation. The ideal candidate will have a strong background in data engineering and data science, along with the ability to work with large datasets, develop machine learning models, and contribute to data-driven decision-making.
Responsibilities:
Design and develop data pipelines, ETL processes, and data integration solutions.
Maintain and optimise data infrastructure, databases, and data warehousing systems.
Ensure data quality, reliability, and consistency.
Collaborate with other teams to understand data requirements and implement data solutions.
Utilise machine learning and statistical techniques to analyse large datasets.
Develop and deploy predictive models for various business applications.
Create data visualisations and reports to communicate findings and insights.
Collaborate with domain experts to identify data-driven opportunities and solutions.
Evaluate and incorporate emerging data science tools and techniques.
Qualifications:
Bachelor's or higher degree in a relevant field such as Computer Science, Data Science, or Engineering.
Relevant experience in data engineering and data science roles.
Strong programming skills in languages such as Python, SQL, and proficiency with data manipulation libraries (e.g., pandas).
Experience with distributed computing frameworks (e.g., Hadoop, Spark) and data processing tools.
Proficiency with data storage and data warehousing technologies (e.g., SQL databases, NoSQL databases, data lakes).
Strong knowledge of ETL processes, data integration, and data transformation.
Experience with data orchestration tools and workflow management.
Strong understanding of machine learning algorithms, statistical analysis, and data modeling.
Experience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch, Azure).
Data visualisation skills using tools like Tableau, Power BI, or Matplotlib/Seaborn.
Familiarity with cloud platforms (e.g. Azure) for deploying data solutions.
Excellent problem-solving and analytical skills.
Effective communication and the ability to translate technical findings into business insights.
Strong teamwork and collaboration skills to work with cross-functional teams.
Preferred Qualifications:
Degree in a relevant field.
Experience in Fintech / RegTech
Knowledge of Big Data technologies (e.g., Hadoop, Spark, Hive).
Being part of the #IDRTeam provides a collaborative and inclusive work culture that values innovation and diversity. We believe in the power of our unique mission and we all work together towards that one single goal. We also believe in being real. We’re not a big corporate. Everyone has an important role to fulfil, and your contribution will be an integral part of our success story.
We are a scaling business which is not for everyone. We are building as we go, and we believe we are creating something remarkable. If you are excited by the prospect of creating something great, while collaborating with people across the globe and being okay with feeling uncomfortable, then IDR is your next great decision.
Benefits:
Competitive compensation package
Hybrid work arrangements, accommodating remote work options – region specific.
Opportunities for professional growth and career advancement.
Show more
Show less","Python, SQL, Hadoop, Spark, Data warehousing, Data pipelines, Machine learning, Data science, Data engineering, Statistics, Tableau, Power BI, Matplotlib, Seaborn, Azure, ETL, Big data, Hive, Pandas, Scikitlearn, TensorFlow, PyTorch","python, sql, hadoop, spark, data warehousing, data pipelines, machine learning, data science, data engineering, statistics, tableau, power bi, matplotlib, seaborn, azure, etl, big data, hive, pandas, scikitlearn, tensorflow, pytorch","azure, big data, data engineering, data science, datapipeline, datawarehouse, etl, hadoop, hive, machine learning, matplotlib, pandas, powerbi, python, pytorch, scikitlearn, seaborn, spark, sql, statistics, tableau, tensorflow"
Data Analyst,Careers Plus,United Kingdom,https://uk.linkedin.com/jobs/view/data-analyst-at-careers-plus-3783677458,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Data Analyst (Integrations)
Must NOT require any form of sponsorship and Hold Full Right to Work in the UK.
Salary - Up to £55,000 DOE
Client Overview: Our client is an Award Winning Software Solutions Provider who specialise in providing services to the Legal Sector.
The Data Analyst will specialize in developing integrations between our platform and third-party applications. This customer-facing role involves assisting clients in designing, developing, and maintaining business intelligence solutions, creating innovative reporting tools and dashboards, and supporting data migration activities.
We are seeking a disciplined thinker capable of thriving in a high-output environment. The ideal candidate possesses outstanding organizational, communication, and presentation skills, with a keen eye for detail while maintaining a broader perspective. Proactivity, adaptability, and a results-driven mindset are essential, along with the ability to work collaboratively, under pressure, and take initiative to meet deadlines.
CORE RESPONSIBILITIES
As a member of the Data Services team, you will be responsible for:
Developing, testing, and maintaining integration and ETL interfaces between our platform and third-party applications.
Writing custom scripts for data transformations to meet data model requirements.
Designing, developing, and maintaining business intelligence solutions.
Crafting and executing queries upon request for data and presenting information through reports and visualizations.
Collaborating with consultants and customers to enhance their product experience through the creation of bespoke innovative reports and dashboards, providing actionable business intelligence.
Assisting Senior Data Services Analyst with ongoing optimization of technology tools, scripts, and documentation supporting migration and integration processes.
Communicating effectively with all stakeholders involved in the project, including clients, third-party partners, and internal staff.
Analysing data issues and implementing resolutions.
Operating our data migration tools to produce working platform systems.
Producing migration scope documents and mapping schema detailing migration transformation processes into our platform.
Supplementary responsibilities:
As required, supporting the Data Services team to migrate customer data from their existing legacy system to our platform. This includes operating our highly developed data migration tooling to produce a working system from customers' source data.
KNOWLEDGE & SKILLS REQUIRED
Essential:
In-depth knowledge in own discipline and basic knowledge of related disciplines.
Integration – delivering solutions to customers using Microsoft tools (ADF, Logic Apps, Power Automate, and SSIS).
Data migration experience.
Client-facing role experience.
Problem-solving skills and the ability to take a new perspective on existing solutions.
Excellent working knowledge of database structures, particularly T-SQL.
Experience with API protocols.
Data visualization and data warehousing experience.
Proven experience with the extraction, transformation, and loading of data from various sources.
Strong planning and organizational skills.
Can-do attitude and a commitment to providing a great customer experience.
Strong work ethic and willingness to work extended hours as required.
Embraces new technology and engages in continuous professional development to maximize opportunities for our customers.
Desired:
Knowledge of MS Dynamics, Power BI, PowerShell, JSON, and Visual C#.
Show more
Show less","Microsoft Tools (ADF Logic Apps Power Automate SSIS), Data migration, Clientfacing role experience, Problemsolving, TSQL, API protocols, Data visualization, Data warehousing, Data extraction transformation and loading, Planning and organizational skills, Cando attitude, Strong work ethic, Embrace new technology, MS Dynamics, Power BI, PowerShell, JSON, Visual C#","microsoft tools adf logic apps power automate ssis, data migration, clientfacing role experience, problemsolving, tsql, api protocols, data visualization, data warehousing, data extraction transformation and loading, planning and organizational skills, cando attitude, strong work ethic, embrace new technology, ms dynamics, power bi, powershell, json, visual c","api protocols, cando attitude, clientfacing role experience, data extraction transformation and loading, data migration, datawarehouse, embrace new technology, json, microsoft tools adf logic apps power automate ssis, ms dynamics, planning and organizational skills, powerbi, powershell, problemsolving, strong work ethic, tsql, visual c, visualization"
Lead Data Engineer,MBN Solutions,United Kingdom,https://uk.linkedin.com/jobs/view/lead-data-engineer-at-mbn-solutions-3780397754,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Lead Data Engineer
Remote:
Home Based UK wide
Salary:
£70,000 - £80,000 + Bonus
Hybrid working:
Home based with quarterly meetings in the London office
* Greenfield * Azure * Python * Autonomous * Scale Up * DWH *
Benefits:
Bonus, Birthday holiday, Private medical insurance, Life Assurance, Critical Illness, Group Income Protection, Holiday buy and sell, Dental Insurance, Discounted gym membership, Eye tests etc.
We are partnering with a rapidly growing scale up finance company with large growth plans who are looking for a Lead Data Engineer.
The Lead Data Engineer will own and evolve the Azure Data Warehouse including design, build, migration and ETL. Whilst there is a team of Subject Matter Experts who will assist, this is an autonomous role and we are looking for someone with a breadth of experience who is keen to get involved in a number of areas and really make their mark whilst supporting the business’ growth ambitions.
The team is small (but expanding) so you will need to wear several hats. You’ll be involved in business (data) analysis, architecting and designing solutions, implementation, and testing. This is an excellent opportunity for you to come in and make the role your own. The foundations of the data warehouse have been built– you will operationalise this and expand it further to deliver value.
You will
:
Build a roadmap of business use cases to ingest into the data warehouse to fuel reporting & dashboards
Own, monitor, and evolve the current Azure Data warehouse
Create and maintain ETL processes, data mappings & transformations
Build a strategy and supporting plan to deliver a great data warehouse and reporting capability
Do you have:
Hands on experience in a Senior / Lead Data Engineering role?
Experience working autonomously to establish a greenfield data engineering environment migrating from on prem to cloud?
Strong Python based Azure experience?
The desire to own the Data engineering function whilst working in an autonomous position in a greenfield area?
Strong SQL experience, including programming i.e. SQL queries and stored procedures, and formal database design methodologies.
Experience working in a broad role ideally with a startup mentality?
Breadth of data experience across databases, pipelines, modelling and architecture?
A strong database background i.e. SQL, ETL, data modelling, data warehouse design etc.?
For more info press apply now!
* Greenfield * Azure * Python * Autonomous * Scale Up * DWH *
Show more
Show less","Lead Data Engineer, Azure, Python, SQL, ETL, Data Warehouse, DWH, Data Analysis, Data Mapping, Data Modeling, Data Architecture, Cloud Migration, Reporting, Dashboards, Green Field","lead data engineer, azure, python, sql, etl, data warehouse, dwh, data analysis, data mapping, data modeling, data architecture, cloud migration, reporting, dashboards, green field","azure, cloud migration, dashboard, data architecture, data mapping, dataanalytics, datamodeling, datawarehouse, dwh, etl, green field, lead data engineer, python, reporting, sql"
Data Engineer Manager,RSM UK,"Preston, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-manager-at-rsm-uk-3763011706,2023-12-17,Preston, United Kingdom,Mid senior,Hybrid,"As one of the world's largest networks of audit, tax and consulting firms, RSM delivers big ideas and premium service to help middle-market businesses thrive. We are a fast-growing firm with big ambitions -- we have a clear goal to become the premium adviser to the middle market, globally. This vision touches everything we do, motivating and inspiring us to become better every day. If you are looking for a firm where you can build a future and make an impact, then RSM is the place for you.
Background to the role
RSM is a leading professional services firm with over 4,000 staff and over 30 offices in the UK. As part of ongoing investment in RSM’s IT and digital capabilities, the need has arisen for a skilled and experienced Data Engineering Manager within our National Technology department.
Role Description
The role will be responsible for the technical design and delivery of a variety of solutions, pipelines and feeds facilitating a range of analytical projects within RSM. The pace of innovation within RSM is increasing all the time and this has created the need for high quality skills within the firm relating specifically to production, and support of a range of data products. The core of the role will be to lead and support team of data engineers as they deliver innovative and valuable products. They will work with stakeholders across the business to help facilitate the development of new solutions in a consistent manner. This will be formed by building excellent relationships with technical and business leaders through providing trusted advice and guidance.
The data engineering manager will be expected to:
Drive business value through the creation and timely delivery of high-quality data products and pipelines
Co-ordinate and manage the workloads of a growing team of data engineers, based around clearly defined sprint goals
Provide leadership, support, guidance, and objectives for the team through tailored individual plans that facilitate growth, personal development, and delivery of high-quality code and products
Create processes and key metrics to ensure efficient and consistent delivery
Actively own strategic relationships with business architects to create technical designs and product roadmaps
Develop a data-product approach which focuses on one-touch integration solutions
Build a profile within RSM as a trusted and highly skilled professional
Provide leadership in key projects that may be multi-team, client focused, or business critical
Contribute to the continuous improvement of analytic best practices and frameworks within RSM
Work with the Data Analyst Manager to ensure the efficient transfer of work items, to unblock issues, and to resolve any common challenges
Lead in the delivery of RSM’s data platform and integration solutions, adopting a cloud first approach while balancing technical debt alongside benefits versus complexity of moving legacy products and pipelines to the cloud
Identify systems and processes that negatively impact data on-onboarding, quality, and management
Take responsibility for the ongoing maintenance of published solutions and the delegation of regular helpdesk support
Work with external partners to build strategic partnerships which focus on optimisation of the code stack
The preferred candidate will bring the ability to focus on getting the job done, maintaining a view of the business benefits, and proving resilient in the face of obstacles.
Candidate
This is a key leadership role within an agile team, guiding team members, and collaborating with colleagues and stakeholders so excellent communication skills are essential. The right candidate will be experienced in leading a team of dedicated data professionals. They will be experienced in working on a variety data platforms, but be primarily experienced in the development, support and management of cloud created pipelines. They should bring demonstrable experience of delivering projects against agreed requirements and additionally how to manage ongoing support, enhancements and service. Have a desire and passion for continual development and bringing best in breed tools to the business.
Essential Skills And Experience
Experienced manager – demonstrable experience of successful team delivery
Experienced mentor and facilitator – ability to demonstrate experience of building supportive team structures
Experience of working within the Azure platform developing data pipelines and associated processes
Demonstrate successful interactions with business and technical stakeholders
High standard of written and verbal communication skills
Experienced in use of Git / DevOps and can articulate its importance and necessity
Focused and versatile team player who is comfortable under pressure, ambiguity, frequent change, or unpredictability
Experience in delivering projects under different process frameworks of lifecycles (i.e. Agile, SCRUM, Waterfall)
Experience in working with both structured and unstructured data
Experience in working with a range of data sources including API, flat files, and databases
Desirable Skills
Ability to define technical implementations that confirm with Architectural principles
Ability to own and manage pathfinding technical projects that enhance the digital product catalogue.
Ability to translate new technologies and platforms to support business objectives.
Experience of working within an Agile methodology
Experience of working with analytical languages (Python/R/T-SQL)
Experience of working with data modelling and visualisation tools (Power BI)
Experience of working with data lakehouses and the medallion methodology
Diversity and Inclusion at RSM
At RSM, we want to create a strong sense of belonging so that people of all identities, backgrounds, and cultures feel they can bring their true self to work. Our clients come from all walks of life. We aim to achieve that same diversity of background, experience and perspective in our own teams, so that we can genuinely understand our client's needs. Diverse teams bring a broader range of ideas and insights to work. That's why we're working together to ensure our firm's principles and processes support a firm culture that embraces difference and strengthens inclusion.
At RSM we work hard to create an environment where our people can make a difference - to themselves, their career, their teams, and to the success of our firm and clients. We support all our people to work flexibly, to manage their family and other responsibilities alongside their work commitments; we believe this is key to achieving an inspiring and fulfilling working environment.
Show more
Show less","Data Engineering, Data Analytics, Data Warehousing, Data Lakehouses, Data Modeling, Data Visualization, Python, R, TSQL, Power BI, Azure, DevOps, Git, Waterfall, Agile, SCRUM, API, Databases, Flat Files","data engineering, data analytics, data warehousing, data lakehouses, data modeling, data visualization, python, r, tsql, power bi, azure, devops, git, waterfall, agile, scrum, api, databases, flat files","agile, api, azure, data engineering, data lakehouses, dataanalytics, databases, datamodeling, datawarehouse, devops, flat files, git, powerbi, python, r, scrum, tsql, visualization, waterfall"
Defence Business Services (DBS) - Cognos Datastore Developer,UK Ministry of Defence,"Thornton, England, United Kingdom",https://uk.linkedin.com/jobs/view/defence-business-services-dbs-cognos-datastore-developer-at-uk-ministry-of-defence-3779053587,2023-12-17,Preston, United Kingdom,Mid senior,Hybrid,"As a CPFMI Data Warehouse Developer/Data Engineer you will be responsible for the Extraction, Transformation and Load (ETL) of data from various Oracle Business Suite modules, archives, and other feeds into the Contracting, Purchasing and Finance Management Information (CPFMI) data warehouse. The warehouse stores data extracted from CP&F (Contracting, Purchasing and Finance), the Ministry of Defences (MoDs) primary commerce and finance system. The CP&FMI user base is large and diverse. It includes the MoDs Finance, Commercial and Project Management communities. You will have technical responsibilities across all stages and iterations of ETL design and development. You will model, optimise and support data warehousing solutions based upon functional and technical specifications. Those solutions will enable users to deliver Business Intelligence (BI) reports and dashboards using standard IBM Cognos analytical and report writing tools.
This is an exciting time to join the CPFMI ETL team. We are about to migrate from IBM DataManager to a new data integration tool named IBM DataStage. The team has also begun to work closely with the Project, Budgeting and Forecasting (PB&F) team as DBS looks to consolidate into a cohesive single data warehousing solution. You will be given sufficient time to familiarise yourself with the new data integration tool. You will receive tailored support from MoD colleagues and contractors to enable you to successfully support and in time lead ETL development. Duties will range from the resolution of incidents to contributions in the delivery of large-scale pan MoD projects.
This role is within Finance and Commercial DIT Team which supports a large and complex change programme. It is suitable for hybrid working, which is an informal, non-contractual and voluntary arrangement, blending a balance of attendance in the workplace (your permanent duty station in Norcross, Blackpool) and working from home. If you are successful, any opportunities for hybrid working will be discussed with you prior to you taking up your post. There is, however, an expectation that all DBS staff, irrespective of their agreed duty station, will travel to other DBS and/or MOD site(s), as required, to complete all necessary training and to participate in team or additional collaborative activities – detached duty terms may apply. Terms of transfer have been negotiated for DBS posts relocating in the North West. DBS also supports Flexible Working, which can be discussed in more detail at interview.
Responsibilities
Accountable to the CPFMI Datastore Technical Lead for the design, development and integration of CP&F ETL solutions.
Monitoring of the Overnight service logs and management of the CPFMI Datastore Remedy workgroup to ensure incident management performance indicators are met and problems are prevented from reoccurrence.
Analysis of the impact of change and incidents on Corporate Services Management Information (CSMI). Interpretation of data and processes to provide meaningful and actionable insights.
Effective Customer engagement to understand customer requirements, manage expectations and ensure customer satisfaction.
Liaison with the CP&F, Live Service and Test teams to manage satisfactory restoration of service.
Support ETL modelling for internally delivered change and for ETL input into high level and detailed designs. This includes reverse engineering CP&F and other data models to map, transform and integrate new data feeds from multiple systems into the CSMI data warehouse.
Use of agreed standards and tools to design, code, test and document programs and scripts from agreed specifications to deliver efficient and effective solutions.
Engagement with CSMI colleagues, Oracle resource and external contractors to exchange knowledge, review and progress incidents and change. Collaboration and exploitation of shared skills and work practices to develop efficiencies and improve service.
Delivery of data services that are fit for purpose, resilient, scalable and future-proof.
Liaison with Project Management and other appropriate parties to provide timely updates, manage risk and address blockers.
Observation of security and configuration control policies and conventions.
Execution of test and release processes to minimise risk and ensure that planned milestones are met.
Recognition, identification, and exploitation of opportunities for innovation, transformation, and continuous improvement. Exploration of new, more efficient and effective ways of deriving value from data. Optimising and designing data to support business opportunities.
Participation in the design and delivery of ETL development to support the rework of the CPF Presentation schema and migration from the DataManager development tool to DataStage.
Show more
Show less","ETL, Data Warehousing, Data Integration, IBM DataManager, IBM DataStage, IBM Cognos, Business Intelligence (BI), Project Budgeting and Forecasting (PB&F), Corporate Services Management Information (CSMI), Oracle, SQL, XML, Java, Python, Unix, Linux, Windows","etl, data warehousing, data integration, ibm datamanager, ibm datastage, ibm cognos, business intelligence bi, project budgeting and forecasting pbf, corporate services management information csmi, oracle, sql, xml, java, python, unix, linux, windows","business intelligence bi, corporate services management information csmi, data integration, datawarehouse, etl, ibm cognos, ibm datamanager, ibm datastage, java, linux, oracle, project budgeting and forecasting pbf, python, sql, unix, windows, xml"
Data Scientist - Advana,Zencon Group,"Washington, DC",https://www.linkedin.com/jobs/view/data-scientist-advana-at-zencon-group-3767584943,2023-12-17,Saint Charles,United States,Associate,Onsite,"Currently looking for Secret cleared and higher Data Scientists local to the DC Metro. On-site 2-3 days/week, some flexibility, but preference given to candidates willing to go on-site more.
Booz Allen is seeking a skilled and highly motivated Data Scientist to join our Advana team. As a Data Scientist, you will play a crucial role in building data pipelines to efficiently collect and process data from various external sources. Your expertise in developing validation and analytics processes to ensure our team is fully developed to provide data accuracy and reliability for executing mission-critical work.
Responsibilities
Collaborate with analysts and product leads to identify and implement data solutions.
Optimize data pipelines to ensure efficient performance with minimal human intervention.
Build data pipelines for collecting and processing data from various external sources.
Work with data engineers to manage data pipelines and troubleshoot issues to maintain data quality and reliability.
Develop validation and analytics processes that are consistent with business needs and strategic goals.
Assist with machine learning and data visualization initiatives as needed.
Continuously improve data solutions to effectively address customer needs.
Communicate and coordinate with teams to accomplish objectives and deliver excellence.
Requirements:
Secret clearance or higher is required.
Minimum of 4 years with SQL and modern Big Data ETL technologies like NiFi or StreamSets.
Bachelor's degree in a related field.
Strong background in a modern programming language, such as Python or Java, with at least 4 years of experience working in a big data and cloud environment.
Ability to quickly grasp technical concepts and collaborate with multiple functional groups
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Detail-oriented mindset with a commitment to delivering high-quality results.
Must be in the DC Metro area and available to work onsite (Crystal City, VA and Alexandria, VA) 2-3 days per week.
Nice to Have:
Recent DoD or IC-related experience.
Experience in working in an agile development environment.
Previous experience with Advana is a plus.
Show more
Show less","Data Pipelines, Data Science, Data Collection, Data Processing, Data Quality, Data Reliability, Data Visualization, Machine Learning, SQL, Apache NiFi, StreamSets, Python, Java, Crossfunctional Teams, DoD, ICrelated Experience, Agile Development","data pipelines, data science, data collection, data processing, data quality, data reliability, data visualization, machine learning, sql, apache nifi, streamsets, python, java, crossfunctional teams, dod, icrelated experience, agile development","agile development, apache nifi, crossfunctional teams, data collection, data processing, data quality, data reliability, data science, datapipeline, dod, icrelated experience, java, machine learning, python, sql, streamsets, visualization"
AWS Data Engineer,Latitude Inc,"Alexandria, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-at-latitude-inc-3787729100,2023-12-17,Saint Charles,United States,Mid senior,Onsite,"Job Summary:
As an AWS Data Engineer with expertise in AWS Glue and RedShift, you will play a pivotal role in our data infrastructure, ensuring efficient data integration, transformation, and storage. You will collaborate with cross-functional teams to design, implement, and maintain data pipelines that enable data-driven decision-making for our organization and clients.
50/50 hybrid flexibility.
Key Responsibilities:
Data Pipeline Development: Design, develop, and maintain robust, scalable, and efficient data pipelines using AWS Glue, AWS Data Pipeline, or other relevant tools.
ETL Processes: Build and optimize ETL (Extract, Transform, Load) processes to ingest, cleanse, transform, and store data from various sources into Amazon Redshift.
Data Modeling: Create and maintain data models within Amazon Redshift, ensuring data accuracy, performance, and scalability.
Performance Optimization: Monitor and optimize AWS Glue jobs and Amazon Redshift clusters for performance, scalability, and cost efficiency.
Data Integration: Integrate data from diverse sources, including databases, APIs, and data lakes, into a unified data platform.
Data Quality: Implement data quality checks and data validation processes to ensure the accuracy and integrity of data.
Security: Implement and maintain data security measures, including access controls, encryption, and compliance with industry standards.
Documentation: Maintain thorough documentation of data pipelines, ETL processes, and data models.
Troubleshooting: Identify and resolve data-related issues, including data discrepancies, performance bottlenecks, and system failures.
Collaboration: Collaborate with data analysts, data scientists, and other stakeholders to understand data requirements and deliver data solutions that meet their needs.
AWS Expertise: Stay current with AWS technologies and best practices, applying them to enhance data engineering capabilities.
Powered by JazzHR
oZXelclxtq
Show more
Show less","AWS, AWS Glue, RedShift, AWS Data Pipeline, ETL, Data Modeling, Data Quality, Data Security, Documentation, Troubleshooting, Collaboration","aws, aws glue, redshift, aws data pipeline, etl, data modeling, data quality, data security, documentation, troubleshooting, collaboration","aws, aws data pipeline, aws glue, collaboration, data quality, data security, datamodeling, documentation, etl, redshift, troubleshooting"
Senior Data Engineer - Data & Machine Learning Platform,Jobs for Humanity,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-machine-learning-platform-at-jobs-for-humanity-3784210444,2023-12-17,Saint Charles,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Booking to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Booking
Job Description
Booking.com, a company dedicated to making travel accessible to everyone, is looking to hire a Senior Data Engineer. We believe that everyone should have the opportunity to explore the world, and through our products, partners, and people, we aim to make that possible. As a Senior Data Engineer, you will be a technical leader responsible for driving data engineering strategies and implementation. Your role will involve developing scalable data models, mapping data flows, and ensuring standardization in line with data governance requirements. You will also create real-time data pipelines, enable efficient data ingestion solutions, and connect disparate datasets into a unified solution. Owning end-to-end data processes and applications, you will monitor system performance, handle incidents, and ensure compliance with regulations. Additionally, you will contribute to the development of software applications, write readable code, and stay up to date with the latest advancements in data engineering. We are looking for individuals who thrive in an ever-changing environment, demand excellence, and embrace opportunities for improvement. Success in this role requires accountability, teamwork, curiosity, and a willingness to learn. We value diversity and believe that it strengthens us as a company. To be considered for this position, you should have a Bachelor's or Master's degree in Computer Science or a related field. In terms of relevant job knowledge, we prefer candidates with at least 6 years of experience in data engineering or a related field, proficiency in server-side programming languages like Scala, Java, Python, or Perl, and experience with technologies such as Hadoop, Cassandra, Kafka, Spark, HBase, and MySQL. Knowledge of data modeling, data streaming, data governance requirements, and data quality implementation methodologies is also desired. Excellent interpersonal skills in English, both written and verbal, are essential for effective communication. At Booking.com, we understand the importance of work-life balance and overall well-being. In addition to competitive compensation, we offer a range of benefits including medical, life, and disability insurance, paid time off, generous leave schemes, industry-leading product discounts, access to online learning platforms, mentorship programs, a complimentary Headspace membership, and a collaborative and diverse work culture. We also have a referral program and opportunities for bonuses. Please note that benefits may differ depending on the office or country. If you require accommodation to fulfill the responsibilities of this job, please inform us. If your application is successful, your personal data may be used for pre-employment screening. This may include checks on employment history, education, and other relevant information to determine your qualifications and suitability for the position, as allowed by applicable law. To apply for the Senior Data Engineer position, please fill out the following form: [Insert HTML-formatted response form here] Thank you for your interest in joining Booking.com! Bullet points: - Booking.com is hiring a Senior Data Engineer to empower everyone to experience the world - Responsibilities include driving data engineering strategies, developing scalable data models, and creating real-time data pipelines - Qualifications include a degree in Computer Science or a related field, experience in data engineering, proficiency in server-side programming languages, and knowledge of data modeling and governance requirements - Booking.com offers competitive compensation, medical and insurance benefits, generous leave schemes, industry discounts, access to learning platforms, and a diverse work culture - If accommodation is needed, please inform us - Personal data may be used for pre-employment screening checks
Show more
Show less","Data Engineering, Scalable Data Models, Data Flows, Data Governance, Realtime Data Pipelines, Data Ingestion, Disparate Datasets, Endtoend Data Processes, System Monitoring, Incident Handling, Compliance, Software Applications, Serverside Programming Languages, Hadoop, Cassandra, Kafka, Spark, HBase, MySQL, Data Modeling, Data Streaming, Data Quality, Interpersonal Skills, English","data engineering, scalable data models, data flows, data governance, realtime data pipelines, data ingestion, disparate datasets, endtoend data processes, system monitoring, incident handling, compliance, software applications, serverside programming languages, hadoop, cassandra, kafka, spark, hbase, mysql, data modeling, data streaming, data quality, interpersonal skills, english","cassandra, compliance, data engineering, data flows, data governance, data ingestion, data quality, data streaming, datamodeling, disparate datasets, endtoend data processes, english, hadoop, hbase, incident handling, interpersonal skills, kafka, mysql, realtime data pipelines, scalable data models, serverside programming languages, software applications, spark, system monitoring"
AWS Data Engineer (Hybrid),Latitude Inc,"Alexandria, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-hybrid-at-latitude-inc-3787726710,2023-12-17,Saint Charles,United States,Mid senior,Onsite,"This is a hybrid position. You can work 3 days remote and the other 2 days on-site in Fairfax, VA. The salary is $85-100k/yr. This role will require you to obtain a Public Trust Clearance
Job Description
Work with engineering and software engineering team to migrate from on-premise infrastructure to cloud
Apply expertise and insight across AWS services
Work in AWS data environments and data warehouses
Work on automating the migration process in AWS from development to production
Requirements
Bachelors Degree in Computer Science, Information Technology or related field
Strong Data experience
3+ years professional IT experience and 1+ years AWS experience
Experienced with Glue and RedShift
Powered by JazzHR
w59q8YQZl3
Show more
Show less","AWS, Glue, RedShift, Data warehousing, Data migration, Cloud infrastructure, Data environments, Automation","aws, glue, redshift, data warehousing, data migration, cloud infrastructure, data environments, automation","automation, aws, cloud infrastructure, data environments, data migration, datawarehouse, glue, redshift"
Data Architect,Analytica,"Washington, DC",https://www.linkedin.com/jobs/view/data-architect-at-analytica-3763392378,2023-12-17,Saint Charles,United States,Mid senior,Remote,"ANALYTICA is seeking a
Senior
Data Engineer
to support a federal government client in the DC metro area (Note - your work location is REMOTE). In this assignment, you will be a team member serving the client in advancing the customers use data, metadata, as well as explore new technologies to better meet those needs.
This is a mission that takes some serious smarts, intense curiosity and a background in developing data solutions across the data lifecycle.
Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.
Responsibilities include (But Are Not Necessarily Limited To):
Research, design, build, optimize and maintain reliable, efficient, and accessible data models, systems and pipelines/APIs etc.
Support, with guidance, the analytic and/or operational use of data.
Align closely with Enterprise partners in data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.
Collaborate with business owners to optimize data collection, movement, storage, and usage to data process and data quality.
Convert concepts & ideas into workable prototypes (custom or COTS products) for client reviews and acceptance.
Translate business needs into:
data architecture solutions development within supported data systems.
data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.
Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams
Competency and skill set will determine level of placement within the posted job family.
Qualifications:
Bachelor’s degree incomputer science, information systems management or similarly related degree.
7+ years of professional data solutions development and implementation experience with:
AWS (Glue, Athena, API Gateway)
SQL, NoSQL
Data developments with modeling tools such as Neo4J, Erwin, Embarcadero, transforming logical, physical, conceptual, reverse engineering & forward engineering.
Development with Alation and/or EASparx
Data Movement tools such as Informatica & others…
Unit testing
RESTful API Development
Desire and willingness to learn new data tools
​​​Has an Agile mindset and iterative development process background
Help promote a culture of diversity and inclusion within the department and the larger organization
Value different ideas and opinions
Listen courageously and remain curious in all that you do
CMS data experience a must
CMS Public Trust clearance, EUA highly preferred
Valuable Experience:
AWS CDK and/or other AWS services (or comparable cloud data solutioning tools)
Experience with Git and CICD pipelines
Relational database design
Microservices / Containers (Docker, Kubernetes)
Informatica Intelligent Cloud Services (IICS)
Prior experience with CMS, preferably within clinical quality or standards area
About
ANALYTICA
: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established8(a) small businessthat has been recognized by
Inc. Magazine
each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) atCMMI® Maturity Level 3and is anISO 9001:2008 certifiedprovider.
As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.
Show more
Show less","Data Engineering, Data Solutions Development, Data Modeling, Data Pipelines, Data Quality, Data Warehousing, Data Architecture, Data Science, Data Analysis, Data Governance, Data Security, AWS, SQL, NoSQL, Neo4J, Erwin, Embarcadero, Alation, EASparx, Informatica, Unit Testing, RESTful API Development, Agile Development, CMS, Relational Database Design, Microservices, Containers, Docker, Kubernetes, Informatica Intelligent Cloud Services (IICS)","data engineering, data solutions development, data modeling, data pipelines, data quality, data warehousing, data architecture, data science, data analysis, data governance, data security, aws, sql, nosql, neo4j, erwin, embarcadero, alation, easparx, informatica, unit testing, restful api development, agile development, cms, relational database design, microservices, containers, docker, kubernetes, informatica intelligent cloud services iics","agile development, alation, aws, cms, containers, data architecture, data engineering, data governance, data quality, data science, data security, data solutions development, dataanalytics, datamodeling, datapipeline, datawarehouse, docker, easparx, embarcadero, erwin, informatica, informatica intelligent cloud services iics, kubernetes, microservices, neo4j, nosql, relational database design, restful api development, sql, unit testing"
Senior Data Governance Analyst,First Merchants Corporation,"Daleville, IN",https://www.linkedin.com/jobs/view/senior-data-governance-analyst-at-first-merchants-corporation-3786540388,2023-12-17,Muncie,United States,Mid senior,Onsite,"Position Goals
Advance the development and maturation of the Data Governance program through execution of the program roadmap and implementation of data governance principles and standards in data related projects.
Essential Duties And Responsibilities
Lead the execution of projects and tsks identified in the Data Governance program roadmap.
Provide guidance in data related projects for the application of data governance principles and standards.
Conduct data governance activities through collaboration with data stewards, data owners and business and functional area subject matter experts.
Contribute to the creation and maintenance of data governance documentation that includes enterprise governance standards, training materials, date quality reporting and compliance reporting.
Research best practices and make recommendations to direct manager in the areas of data profiling, data quality monitoring, and date management solutions.
Provide training to peers, date stewards, and other data consumers to maximize understanding and use of data governance standards and technologies.
Lead data flow mapping exercises and partner with data owners and functional area subject matter experts to ensure data lineage documentation is kept up to date.
Organize a data workflow and communication program with the data stewards to ensure critical issues are communicated timely for remediation and document remediation decisions.
Collaborate with the data stewards and data owners to enlist business unit support to take ownership and responsibility for consistent definitions and quality of their data and ensure that documentation is maintained.
Collaborate with Data Engineering, Data Architecture, and Enterprise Application Management to support alignment of data governance principles and standards with enterprise-wide objectives.
Position Requirements
Education - Bachelor’s degree or equivalent experience in Business, Computer Science, MIS or related field of study OR Associate Degree/ Two years of College + Two (2) years of related work/military experience -OR four (4) years of related work/military experience (plus additional required experience detailed below).
Experience - Minimum of five (5) years of experience in data governance, data analysis, business analysis, or risk management environment, or related experience.
Minimum of two (2) years of experience using SQL, Power BI, or similar technology.
Other - None.
Preferred Requirements
Experience in finance, credit, risk, or operations related data projects, specifically around data transformation.
Experience in the field of data management, specifically with data quality, data stewardship and/or data governance.
Experience in banking or the financial services industry.
Experience with data dictionary or other data governance management tools.
Show more
Show less","Data Governance, Data Quality Monitoring, Data Management, Data Profiling, Data Flow Mapping, Data Stewardship, Data Architecture, Enterprise Application Management, Business Analysis, Risk Management, SQL, Power BI, Data Dictionary, Data Transformation","data governance, data quality monitoring, data management, data profiling, data flow mapping, data stewardship, data architecture, enterprise application management, business analysis, risk management, sql, power bi, data dictionary, data transformation","business analysis, data architecture, data dictionary, data flow mapping, data governance, data management, data profiling, data quality monitoring, data stewardship, data transformation, enterprise application management, powerbi, risk management, sql"
Data Analyst,"The Clearing, Inc.","Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-at-the-clearing-inc-3780464559,2023-12-17,Gananoque, Canada,Associate,Hybrid,"Description
COMPANY DESCRIPTION
The Clearing is a management consulting firm that helps leaders identify underlying causes of organizational obstacles, resolve highly complex challenges, prioritize the fewest, most important initiatives to tackle regardless of conflicting needs, and make informed decisions in the context of an agreed upon mission, vision, and strategy.
We help create peak performance organizations through strategy, organizational development, leadership training, and change management. Our experienced consultants bring a powerful blend of analytic and creative skills from diverse fields, including change management, finance, engineering, communications, education, policy, design, and corporate leadership.
POSITION DESCRIPTION
In this position, data meets consulting. Data Analysts infuse scientific thinking into TC’s change strategy consulting work by viewing things with a data-centric lens.
This position requires a data analyst who has client relations experience with a customer-centric focus. He/she/they executes data projects to put analytics into the hands and brains of one or more clients. The Data Analyst supports client decision-making and improves processes through data insights, accuracy, and integrity.
WHAT YOU’LL GET TO DO
Use data analysis to support change strategies and process improvement, infusing efficiency and automation into many analytical tasks for customers
Develop and/or code data-focused presentations and reports, including data visualizations (dashboards, charts, graphs, and plots)
Perform advanced development/coding work in Google sheets and/or Microsoft Excel and basic development/coding in Tableau and SQL
Run Google Analytics to look for trends
Clean datasets to facilitate analyses when needed
Ensure data quality by performing extensive peer Quality Assurance (QA) reviews
Translate client objectives into clear data-centered change and other strategies, using non-technical language
Keep a pulse on where data tools are headed
Maintain client relationships
Support opportunities to expand project scope and actively share information about emerging customer support needs and trends with team members and management
Facilitate and manage client meetings to successfully achieve identified, desired outcomes
Work on a full-time project or juggle multiple (2-3) part-time projects
Act as support on proposals with a data-centric lens
May lead small-sized project teams
May develop short-term project plans
Deliver work on time, within budget, and to requested specifications; deliver error proof files at the completion of a project
Requirements
WHAT YOU BRING
Minimum of 4 years experience, at least 2 of which involve working as a data analyst and/or data scientist, in a consultative capacity, preferably with a background in management consulting, client relations, data analytics, data science, and/or coding
Bachelor’s Degree required for the position.
Coursework and/or certifications in a highly analytical field are preferred (e.g., STEM or social sciences, data analytics or data science).
Curiosity, initiative, and willingness to learn, including an interest in upskilling data analytics capabilities
Strong detail-orientation in all data and consulting tasks
Analytical and critical thinking abilities, including creative, out of the box thinking
Ability to communicate data into digestible, simple ways to non-technical customers
Ability to communicate with senior leaders and multiple stakeholders
A passionate commitment to quality
Ability to work with some autonomy, but partner with and ask questions of project leadership regularly
Ability to review the work of peers and provide constructive, detailed feedback
Ability to adapt quickly to emerging requirements and prioritize as appropriate
Comfort working through times of ambiguity and in an environment of rapid growth
Strong verbal and written communication skills; ability to effectively present data work with supporting reasoning
Ability to understand issues quickly and apply problem solving skills to data-centered solutions
Time management and organization skills
Willingness and ability to obtain and maintain a U.S. security clearance
Willingness and ability to travel to client sites in the surrounding DC and Baltimore metro areas; occasional travel outside of these regions could be expected (5%)
WHAT TOOLS/LANGUAGES YOU HAVE DEMONSTRATED EXPERIENCE APPLYING:
Advanced In
Basic In
Bonus Points For
Excel, Google Sheets
Tableau, SQL
Statistics
Python, R
PHYSICAL AND MENTAL REQUIREMENTS
While performing duties of this job, an employee may be required to perform any, or all of the following: Attend meetings in and out of the office, travel (sometimes extensively); Occasional evening and weekend work may be required as job duties demand; Communicate effectively (both orally and in writing); Ability to effectively use computers and other electronic and standard office equipment; Occasionally exerting up to 10 pounds to lift, carry, push, pull or otherwise move objects, including the human body. Work is primarily sedentary and involves sitting for several hours at a time. Occasionally walking, climbing stairs, and standing occur in this role. Additionally, this job requires certain mental demands, including the ability to use judgment, withstand moderate amounts of stress, and maintain attention to detail. The Clearing is committed to partnering with all candidates and employees to ensure reasonable accommodations are made to meet these requirements.
EEO
The Clearing is committed to providing equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, religion, color, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, family medical history or genetic information, political affiliation, military service, or other non-merit based factors. In addition to federal legal requirements, The Clearing complies with applicable state and local laws governing nondiscrimination in employment. These protections extend to all terms and conditions of employment, including recruiting and hiring practices, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training and career development programs.
Show more
Show less","Data analysis, Data visualization, Data science, Business intelligence, Customer relationship management, Project management, Communication, Analytical thinking, Critical thinking, Problem solving, Programming, Excel, Google Sheets, Tableau, SQL, Statistics, Python, R","data analysis, data visualization, data science, business intelligence, customer relationship management, project management, communication, analytical thinking, critical thinking, problem solving, programming, excel, google sheets, tableau, sql, statistics, python, r","analytical thinking, business intelligence, communication, critical thinking, customer relationship management, data science, dataanalytics, excel, google sheets, problem solving, programming, project management, python, r, sql, statistics, tableau, visualization"
Data Analyst,Accelerate Professional Talent Solutions,"Milwaukee County, WI",https://www.linkedin.com/jobs/view/data-analyst-at-accelerate-professional-talent-solutions-3771204129,2023-12-17,Gananoque, Canada,Associate,Hybrid,"seeking a Data Analyst experienced in gathering, assembling, and monetizing data across different platforms. The ideal candidate will be an organized self-starter, highly analytical, have a can-do attitude and work well in a fast-paced environment.
Data Analyst Responsibilities include: Synthesize Large Sets of Data
Collect and organize data from various internal and 3rd party sources including Hubspot & NetSuite
Drive Data Integrity
Maintain data quality and integrity through data cleaning/hygiene and validation.
Identify/implement automation opportunities for data merges
Data Analysis & Visualization
Perform data analysis to identify trends, patterns, buying behaviors and pricing intelligence
Datamine & serve up priority opportunities (leads) to recapture, increase share of wallet, cross sell
Use data visualization tools such as tableau, power BI, or excel to effectively communicate findings to non-technical stakeholders.
Develop predictive models and response triggers
Data-driven Decision Support
Collaborate with cross functional teams to provide data driven recommendations and insights to inform business strategies.
Continuous Improvement
Stay updated on Industry trends, best practices, and emerging technologies in data analysis
Carefully leverage AI for analysis and automation
Qualifications:
Highly analytical -- ability to manipulate large sets of data and make data driven recommendations to key stakeholders.
Flexible & Adaptable -- We are a small business – each employee wears many hats.
Min 3-5 years’ experience in data analysis, experience in data visualization tools: Power BI, tableau, excel, etc.
Strong oral and written communication to non-technical stakeholders
Show more
Show less","Data Analysis, Data Mining, Data Visualization, Data Cleaning/Hygiene, Data Validation, Large Data Set Manipulation, Datadriven Decision Support, Business Intelligence, Predictive Modeling, Response Triggers, Tableau, Power BI, Excel, AI, Hubspot, NetSuite","data analysis, data mining, data visualization, data cleaninghygiene, data validation, large data set manipulation, datadriven decision support, business intelligence, predictive modeling, response triggers, tableau, power bi, excel, ai, hubspot, netsuite","ai, business intelligence, data cleaninghygiene, data mining, data validation, dataanalytics, datadriven decision support, excel, hubspot, large data set manipulation, netsuite, powerbi, predictive modeling, response triggers, tableau, visualization"
Data Analyst Part Time,Toyandsons,"Kingston, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-toyandsons-3757208255,2023-12-17,Gananoque, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Statistical Techniques, DataDriven DecisionMaking, A/B Testing, Data Quality Management, Data Collection, Data Cleansing, Data Manipulation, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, data interpretation, statistical techniques, datadriven decisionmaking, ab testing, data quality management, data collection, data cleansing, data manipulation, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, data collection, data interpretation, data manipulation, data quality management, dataanalytics, datacleaning, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Virtual Data Analyst Part Time,Voxmediallc,"Kingston, Ontario, Canada",https://ca.linkedin.com/jobs/view/virtual-data-analyst-part-time-at-voxmediallc-3757239881,2023-12-17,Gananoque, Canada,Mid senior,Onsite,"Summary:
The Virtual Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, A/B Testing, ETL, Hypothesis Testing, Data Visualization, Statistical Modeling, Data Manipulation, Data Collection, Data Quality, Data Integrity, Analytics, Algorithms, Performance Metrics, Business Intelligence, Datadriven decisionmaking","data analysis, sql, r, python, tableau, power bi, ab testing, etl, hypothesis testing, data visualization, statistical modeling, data manipulation, data collection, data quality, data integrity, analytics, algorithms, performance metrics, business intelligence, datadriven decisionmaking","ab testing, algorithms, analytics, business intelligence, data collection, data integrity, data manipulation, data quality, dataanalytics, datadriven decisionmaking, etl, hypothesis testing, performance metrics, powerbi, python, r, sql, statistical modeling, tableau, visualization"
"Senior Data engineer - Austin, TX",Diverse Lynx,"Austin, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-austin-tx-at-diverse-lynx-3776527757,2023-12-17,Austin,United States,Mid senior,Onsite,"Responsibilities
We are looking for a Senior Data Engineer to join our team and help us build and maintain our data infrastructure and pipelines. As a Senior Data Engineer, you will be responsible for designing, developing, and implementing scalable and reliable data solutions to support our business needs. You will also work closely with other engineers and data scientists to ensure that our data is clean, accessible, and secure.
Responsibilities
Design, develop, and implement scalable and reliable data pipelines and architectures
Work with data scientists and analysts to understand their data needs and develop solutions to meet those needs
Build and maintain data warehouses and data marts
Optimize data pipelines and architectures for performance and efficiency
Implement data security and governance measures
Monitor and troubleshoot data pipelines and architectures
Stay up-to-date on the latest data engineering technologies and trends
Qualifications
8&plus; years of experience in a data engineering role
Strong experience with SQL, Python, and other data engineering technologies
Experience with cloud computing platforms such as AWS, Azure, or GCP
Experience with big data technologies such as Hadoop, Spark, and Kafka
Experience with data warehousing and data mart technologies such as Snowflake, Redshift, and BigQuery
Experience with data security and governance best practices
Excellent problem-solving and analytical skills
Strong communication and collaboration skills
Bonus Points
Experience with machine learning and deep learning
Experience with data visualization tools such as Tableau and Power BI
Experience with data streaming technologies such as Kafka and Flink
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","SQL, Python, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Apache Hadoop, Apache Spark, Apache Kafka, Snowflake, Amazon Redshift, Google BigQuery, Machine learning, Deep learning, Tableau, Power BI, Data streaming, Kafka, Flink","sql, python, amazon web services aws, microsoft azure, google cloud platform gcp, apache hadoop, apache spark, apache kafka, snowflake, amazon redshift, google bigquery, machine learning, deep learning, tableau, power bi, data streaming, kafka, flink","amazon redshift, amazon web services aws, apache hadoop, apache kafka, apache spark, data streaming, deep learning, flink, google bigquery, google cloud platform gcp, kafka, machine learning, microsoft azure, powerbi, python, snowflake, sql, tableau"
Software Engineer - Data Infrastructure,Canonical,"Austin, TX",https://www.linkedin.com/jobs/view/software-engineer-data-infrastructure-at-canonical-2540512445,2023-12-17,Austin,United States,Mid senior,Remote,"Canonical is building a comprehensive automation suite to provide multi-cloud and on-premise data solutions for the enterprise. The data platform team is a collaborative team that develops a full range of data stores and data technologies, spanning from big data, through NoSQL, cache-layer capabilities, and analytics; all the way to structured SQL engines.
We are facing the interesting problem of fault-tolerant mission-critical distributed systems and intend to deliver the world's best automation solution for delivering data platforms.
We have a number of openings ranging anywhere from junior to senior level. We will help you identify a suitable position depending on your experience and interests. Engineers who thrive at Canonical are mindful of open-source community dynamics and equally aware of the needs of large, innovative organisations.
Location: This is a Globally remote role
What your day will look like
The data platform team is responsible for the automation of data platform operations. This includes ensuring fault-tolerant replication, TLS, installation, and much more; but also provides domain-specific expertise on the actual data system to other teams within Canonical. This role is focused on the creation and automation of features of data platforms, not analysing the data in them.
Collaborate proactively with a distributed team
Write high-quality, idiomatic Python code to create new features
Debug issues and interact with upstream communities publicly
Work with helpful and talented engineers including experts in many fields
Discuss ideas and collaborate on finding good solutions
Work from home with global travel for 2 to 4 weeks per year for internal and external events
What we are looking for in you
Proven hands-on experience in software development using Python
Proven hands-on experience in distributed systems
Have a Bachelor’s or equivalent in Computer Science, STEM, or a similar degree
Willingness to travel up to 4 times a year for internal events
Additional Skills That You Might Also Bring
You might also bring a subset of experience from the following, which will determine the exact role and level we consider you for:
Experience operating and managing data platform technologies like PostgreSQL, MySQL, MongoDB, OpenSearch, Kafka, Yugabyte, Trino, Superset, Atlas, Ranger, and Redis
Experience with Linux systems administration, package management, and operations
Experience with the public cloud or a private cloud solution like OpenStack
Experience with operating Kubernetes clusters and a belief that it can be used for serious persistent data services
What we offer you
Your base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilisation. Our compensation philosophy is to ensure equity right across our global workforce.
In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, please ask your Talent Partner.
Fully remote working environment - we’ve been working remotely since 2004!
Personal learning and development budget of 2,000USD per annum
Annual compensation review
Recognition rewards
Annual holiday leave
Parental Leave
Employee Assistance Programme
Opportunity to travel to new locations to meet colleagues at ‘sprints’
Priority Pass for travel and travel upgrades for long haul company events
About Canonical
Canonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.
Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.
Canonical is an equal-opportunity employer
We are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration.
Show more
Show less","Python, Distributed Systems, Linux Systems Administration, Package Management, Operations, Public Cloud, Private Cloud, Kubernetes Clusters, PostgreSQL, MySQL, MongoDB, OpenSearch, Kafka, Yugabyte, Trino, Superset, Atlas, Ranger, Redis","python, distributed systems, linux systems administration, package management, operations, public cloud, private cloud, kubernetes clusters, postgresql, mysql, mongodb, opensearch, kafka, yugabyte, trino, superset, atlas, ranger, redis","atlas, distributed systems, kafka, kubernetes clusters, linux systems administration, mongodb, mysql, opensearch, operations, package management, postgresql, private cloud, public cloud, python, ranger, redis, superset, trino, yugabyte"
Senior Data Engineer,"HeartFlow, Inc","Austin, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-heartflow-inc-3780093387,2023-12-17,Austin,United States,Mid senior,Remote,"HeartFlow, Inc. is a medical technology company transforming the diagnosis and management of coronary artery disease, the #1 cause of death worldwide, using cutting-edge technology. The flagship product—an AI-based, non-invasive cardiac test called the HeartFlow Analysis—provides a color-coded, 3D model of a patient’s coronary arteries indicating the impact blockages have on blood flow to the heart. It offers physicians a completely novel way to diagnose and treat cardiac patients. Our pipeline of products is growing and so is our team; join us in helping to revolutionize precision heartcare.
HeartFlow is a VC-backed, pre-IPO company that has received international recognition for exceptional strides in healthcare innovation, is supported by medical societies around the world, cleared for use in the US, UK, Europe, Japan and Canada, and has been used for more than 200,000 patients worldwide.
The Sr. Data Engineer brings DataOps, data engineering, DevOps and cloud computing expertise in expanding and optimizing our data lake processing pipeline and data analytics architecture. They bring technical expertise to design, architect, implement and support solutions to scale and optimize data flow into and out of cloud-based data lakes for cross functional teams to consume for Tableau reporting, Ad-hoc querying and Machine learning processing. ;
In addition, expertise in cross-team (NetOps, DevOps, InfoSec, SecOPs, etc.) collaboration via use of professional skills are utilized to perform stretch roles as Sr. Data Engineer.
The Sr. Data Engineer will be a mentor and SME to other members of the team, as well as consult to management and leadership.
Job Responsibilities:
Design, build, and maintain a scalable and low-latency infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and targets using AWS data lake related services and analytics-related technologies.
Support and collaborate with down-stream consumers, i.e. Tableau, Ad-hoc and Machine Learning to ensure their success by assembling and delivering required data sets, security, and acceptable query times
Perform exporting/sync of data to third-party products, i.e. Salesforce, FTP Servers, etc. using AWS AppFlow, SFTP SSH, etc.
Be SME from within the team to provide expertise to bring innovative ideas from concept to fruition via collaborative POCs, Spikes and /or discussions with team members.
Identify, design, document and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Understand and guide a comprehensive testing, continuous integration and development framework for schema, data, and functional processes/pipelines, etc.
Be SME for tools used by Data Engineering, i.e. GitHub, Jira, Confluence, etc.
Manage AWS accounts dedicated to DataOps team with root level access that have DevOps, NetOps, InfoSec and DataOps stretch-role responsibilities.
Skills Needed:
Strong experience with AWS cloud services: S3, DMS, RDS, Lambda, Redshift, Glue, AppFlow, EC2, in cross-region and cross-account implementations
Strong experience with AWS security: IAM Roles/Policies/Users, Okta/SSO
Strong experience with AWS networking: VPC, Subnets, security groups, route tables, gateways, etc.
Strong experience with technical writing to document processes, best practices, create architecture diagrams, etc. using Atlassian Confluence, Lucid Charts, etc.
Strong experience with Agile Scrum: Managing, creating and maintaining Jira board, issues, sprint shutdown, grooming, status updates with management, supporting documentation and sprint demos. Perform stretch-role of Scrum Master.
Strong experience with Infrastructure as Code via Terraform, AWS CDK for Python, AWS CloudFormation, etc.
Strong experience with NoSQL databases, i.e. DynamoDB, DocumentDB and/or MongoDB
Strong experience with relational/DW SQL databases such as MySQL, PostgreSQL, and Redshift
Strong experience with programming in Python via python.org and Anaconda python using AWS Boto3, PyCharm IDE, etc. and data engineering related packages: pandas, numpy, scipy, sqlalchemy, pyarrow, redshift connector
Strong experience building and optimizing data pipelines, architectures, data lakes, data sets, etc.
Strong experience with reading and writing of multiple file formats, i.e. JSON, text/CSV, and parquet
Strong experience with transferring and receiving of files via SFTP SSH using ssh CLI, Python, Filezilla, etc.
Strong experience with Code repo management - Git and GitHub, managing and overseeing repos, branches, pull requests, etc. using standalone CLI and GUI tools, and integrated with PyCharm.
Strong experience with Salesforce: reading and writing of data to and from the data lake, Salesforce Query Language, and Salesforce objects and structures.
Strong experience API usage and connectivity via Python and Postman REST API client tool.
Experience with AWS CloudWatch alerting and notification integration with third-party tools: PagerDuty, Slack, etc.
Experience with managing and overseeing dedicated AWS accounts: billing, tuning, security and access management, network and connectivity management, root level login/access/tasks, etc.
Experience with containerization technologies, such as Docker, AWS ECR, AWS Lambda
Experience with third-party Data and File management tools, i.e. DBeaver, FileZilla, etc.
Experience with AWS and third-party ETL tools: AWS Glue, Hevo, Boomi, etc.
Experience with master data, metadata, versioning, tagging, cataloging of data sources
Familiarity with data visualization and analytics tools like Tableau
Educational Requirements & Work Experience:
5+ years of experience in a Data Engineering role in a team setting
5+ years of experience working with AWS cloud using data related services
5+ years of experience working with RDS, Redshift and NoSQL
2+ years of experience working with Data Lakes in AWS
Undergraduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Graduate degree preferred.
AWS certifications a plus
Experience leading data analysts or managing teams a plus
The pay range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to experience and training; skill sets; licensure and certifications; and other business and organizational needs. A reasonable estimate of the base salary compensation range is $122,445 to $165,000 per year.
We also offer a range of benefits and programs to meet employee needs based on eligibility. These benefits include comprehensive health care coverage, a health savings account, disability, and life insurance, a Critical Illness and accident plan, a flex spending account (medical and dependent care), a 401k plan with a company match, mental health support TaskHuman, EAP, financial coaching, Rocket Lawyer, and more. HeartFlow offers 12 paid holidays, 15 vacation days, and 80 hours of sick leave.
About HeartFlow, Inc.
HeartFlow, Inc. is a medical technology company redefining the way heart disease is diagnosed and treated. Our non-invasive HeartFlow FFRct Analysis leverages deep learning to create a personalized 3D model of the heart. By using this model, clinicians can better evaluate the impact a blockage has on blood flow and determine the best treatment for patients. Our technology is reflective of our Silicon Valley roots and incorporates decades of scientific evidence with the latest advances in artificial intelligence. The HeartFlow FFRct Analysis is commercially available in the United States, Canada, Europe and Japan. For more information, visit www.heartflow.com .
HeartFlow, Inc. is an Equal Opportunity Employer. We are committed to a work environment that supports, inspires, and respects all individuals and do not discriminate against any employee or applicant because of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. This policy applies to every aspect of employment at HeartFlow, including recruitment, hiring, training, relocation, promotion, and termination.
Positions posted for HeartFlow are not intended for or open to third party recruiters / agencies. Submission of any unsolicited resumes for these positions will be considered to be free referrals.
US Locations Only: All employees and contingent workers (contractor, consultant, interns or temporary personnel) are required to be vaccinated against SARS-CoV-2 as recommended by CDC, unless a reasonable accommodation is approved. All prospective hires will be expected to provide proof of vaccination on their first day of employment.
Show more
Show less","AI, Deep Learning, Machine Learning, Python, AWS, S3, DMS, RDS, Lambda, Redshift, Glue, AppFlow, EC2, IAM, VPC, Subnets, Route tables, Gateways, Lucid Charts, Confluence, Atlassian, Jira, Scrum, Terraform, AWS CDK, Python.org, Anaconda, PyCharm, Boto3, pandas, numpy, scipy, sqlalchemy, pyarrow, Redshift connector, JSON, CSV, Parquet, SFTP SSH, Git, GitHub, DBeaver, FileZilla, AWS Glue, Hevo, Boomi, Docker, AWS ECR, Salesforce, Tableau, Docker, AWS ECR, Salesforce Query Language","ai, deep learning, machine learning, python, aws, s3, dms, rds, lambda, redshift, glue, appflow, ec2, iam, vpc, subnets, route tables, gateways, lucid charts, confluence, atlassian, jira, scrum, terraform, aws cdk, pythonorg, anaconda, pycharm, boto3, pandas, numpy, scipy, sqlalchemy, pyarrow, redshift connector, json, csv, parquet, sftp ssh, git, github, dbeaver, filezilla, aws glue, hevo, boomi, docker, aws ecr, salesforce, tableau, docker, aws ecr, salesforce query language","ai, anaconda, appflow, atlassian, aws, aws cdk, aws ecr, aws glue, boomi, boto3, confluence, csv, dbeaver, deep learning, dms, docker, ec2, filezilla, gateways, git, github, glue, hevo, iam, jira, json, lambda, lucid charts, machine learning, numpy, pandas, parquet, pyarrow, pycharm, python, pythonorg, rds, redshift, redshift connector, route tables, s3, salesforce, salesforce query language, scipy, scrum, sftp ssh, sqlalchemy, subnets, tableau, terraform, vpc"
Sr. Graph Data Developer,ASCENDING Inc.,"Fairfax, VA",https://www.linkedin.com/jobs/view/sr-graph-data-developer-at-ascending-inc-3787769867,2023-12-17,Clinton,United States,Mid senior,Onsite,"Job Type:
Full Time/Long Term.
Location:
Fully Remote.
No
C2C. Able to do W2 or 1099 Individual Contractor.
About:
We are seeking an experienced
Data Analyst
to join our team developing the next generation of graph-driven data solutions. Our products connect the people, places, and things enabling pattern detection, self-exploration, and machine learning applications
Required Skills:
5-7 years of experience performing data analysis tasks in big data, graph, business intelligence, or similar environments
6+ years experience with Python or Scala development (OOP and scripting
3 or more years of experience performing data analysis tasks in big data, graph, busineSS intelligence, or similar environments
2+ years of experience in AWS
Demonstrated expert level skills in SQL
1+ years of experience in Neo4j
Experience with large scale data processing engines (Spark, Presto, or equivalent
Naturally inquisitive and enjoys exploring data and business outcomes
Strongly Preferred/Beneficial Experience:
Financial industry experience
Experience in machine learning environments
Typical Job Activities
Data analytics and processing in Spark
Data profiling new data sources
Design and support of graph architecture
Identification of patterns in data
Engage with upstream and downstream engineering partners in value pipeline improvements
Agile ceremonies and team engagement
Powered by JazzHR
rnYwDQrnSG
Show more
Show less","Data Analysis, Big Data, Graph Databases, Business Intelligence, Python, Scala, OOP, Scripting, AWS, SQL, Neo4j, Spark, Presto, Financial Industry, Machine Learning","data analysis, big data, graph databases, business intelligence, python, scala, oop, scripting, aws, sql, neo4j, spark, presto, financial industry, machine learning","aws, big data, business intelligence, dataanalytics, financial industry, graph databases, machine learning, neo4j, oop, presto, python, scala, scripting, spark, sql"
(Full-time) Data and Reporting Analyst (SC),Shasta Tehama Trinity Joint Community College District,"Redding, CA",https://www.linkedin.com/jobs/view/full-time-data-and-reporting-analyst-sc-at-shasta-tehama-trinity-joint-community-college-district-3783917006,2023-12-17,Redding,United States,Mid senior,Onsite,"Overview
Description of Basic Functions and Responsibilities
The Shasta-Tehama-Trinity Joint Community College District (Shasta College) sits at the northern end of the Sacramento Valley, surrounded by mountains to the north, east, and west. The area provides a wide array of outdoor amenities and activities with miles of hiking and biking trails, national parks, two lakes, and the Sacramento River. The main campus is located in the city of Redding, population 90,000. The area is characterized by excellent schools with high graduation rates, low traffic, and low housing prices (Zillow average $393,000 vs. California state average of $770,000).
Shasta College is committed to providing its diverse student population with equitable education outcomes, contributing to the social, cultural, intellectual, and economic development of our communities. The college has been recognized as a leader in developing and implementing innovative education programs to increase student success, including accelerated pathways for high school students to achieve a bachelor's degree and for returning students to complete their Associate and Bachelor's degrees. Shasta College enrolls more than 11,000 students annually and provides educational services to students with a wide range of skills and abilities.
Our ideal candidate is motivated to join a campus community prioritizing the success of our racially and socio-economically diverse staff and student population. Providing equitable outcomes so all members of our community may enjoy safe, healthy, and vibrant educational and employment environments is critical to our success.
Administrator positions in the California Community College system offer a unique opportunity to contribute to the empowerment of our students seeking to improve their lives and communities. Our administrators manage a wide range of programs and lead faculty and staff in engaging our diverse employee and student populations, facilitating equitable outcomes for all through the management of our various employment and academic programs. As an administrator at Shasta College, you will join a vibrant community of professionals working collaboratively to provide a positive, healthy educational environment where our students experience diverse academic and cultural perspectives. A career at Shasta College offers a highly rewarding and enriching employment experience.
Description Of Basic Functions And Responsibilities
To perform a variety of functions in support of the District's data analyst and reporting services; to assist in and facilitate the district's required audit, state and federal reporting processes including complex data analysis; and to conduct assessments, present findings and make recommendations to improve the processes related to the 320, IPEDS, MIS and all other state/federal reporting. Along with the data analysis and discovery, this position would also be responsible for working directly with departments to assist in process and data review from start to finish. Employees in this classification receive limited supervision within a broad framework of policies and procedures and provide specialized functions in the analysis, tracking, processing, and reporting transactions requiring a thorough knowledge of district research and reporting policies, procedures, and requirements. Employees in this classification will provide key stakeholders of data for planning and evidence-based decision-making. They also support the District's equity goals by designing and conducting research related to the effectiveness of support services and instructional programs for students of diverse backgrounds. This position will be responsible for supporting any California Community College reporting initiatives such as, but not limited to, the Vision Aligned Reporting Initiative. This job class is designated as administrative, overtime exempt, and requires effective organizational, problem solving and decision-making skills.
CLASSIFICATION:
Range 20 on the Administrative Confidential Salary Schedule, 40 hours per week, 12 months per year. Work assignment hours are 8:00 a.m. - 5:00p.m., Monday through Friday.
SUPERVISOR
: Associate Vice President of Information Services and Technology or designee
BENEFITS:
Holiday, vacation and sick time will be provided. Our competitive benefits package includes medical, dental, and vision insurance provided at a share of cost on a pretax basis and an employee assistance program (EAP). This is a classified administrator position with a default CalPERS retirement.
Employees Have The Option To Contribute To
Health Savings Account (HSA)
403(b) and 457 Retirement Plans
Section 125 Plan including Flexible Spending Accounts (FSA) and Dependent Day Care Accounts
In Addition, Employees Have The Option To Purchase
Disability Income Insurance
Cancer Insurance
Life Insurance
Accident Only Insurance
Critical Illness Insurance
For more information regarding Employment and Benefit Policies, please view the Administrative Agreement - Appendix A.
Typical Duties, Knowledge and Ability
Essential Functions
Stays up to date on all federal, state and internal reporting requirements and initiatives.
May act as the Districts POC for specific reporting programs and initiatives.
Monitors, maintains and manages assigned data analysis and reporting projects.
Develops, coordinates and trains area lead on best practices for data collection, processing, validating, tracking, storing and reporting for all areas assigned.
Designs, writes, and creates data queries and dashboards.
Monitors performance of data through custom reports and constant monitoring.
Provides technical and analytical support to all areas associated with reporting.
Assists with submitting reports to auditors, state and federal systems and others as needed.
Conducts in-depth assessment of processes and procedures related to data collection and processing.
Presents analytical findings and makes recommendations for improvement.
Assists with change management in areas assigned related to data and reporting.
Analyzes existing programs/systems and makes recommendations for modifications; assumes the lead on testing and validating changes related to reporting.
Researches and analyzes federal, state, and local requirements, policies and procedures to assist design and implement computer solutions for the District needs.
Engages with software and application representatives to submit system enhancements, requirements and issues found related to data collection, processing, storing and reporting on behalf of the District.
Writes and revises technical manuals using proper operating procedures for use of software for areas assigned.
Conducts training sessions to end users.
Reviews, redesigns, and makes recommendations to modify existing systems to improve reporting effectiveness and efficiency.
Learns, implements and trains others on new technologies as required.
Serves as a primary contact for all areas of the District for reporting support.
Designs, creates and delivers reports through writing queries for ad-hoc, operational or general data reporting needs.
Supports the District with State MIS, 320, IPEDS, and all other reporting needs as assigned.
Works directly and collaboratively with the Director of Information Services and Reporting, Programmer Analyst, and the Office of Research and Planning to assess and meet the data needs for State, Federal and other reporting.
Attends various meetings, conferences and seminars related to data processing operations and reporting.
Supervises other staff as needed.
Performs other related duties similar to the above in scope and function as required.
Knowledge of:
Data collection, processing, validating, storing and reporting techniques appropriate to community college and other educational institutions.
Record keeping, tracking, project management and change management.
SQL and other related enterprise database management systems (DBMS) architectures.
Complex SQL query writing.
FERPA, HIPPA, PCI, GLBA, NIST, CIS and all other regulations relevant to data and protection use in higher education.
Basic principles of supervision including evaluation and professional growth.
Ability to
:
Learn the California Community College reporting requirements and process.
Lead and support change management.
Train end users on technical processes and document procedures.
Analyze and recommend improvements to processes related to data collection and reporting.
Learn the Districts operating systems and write reports using different computer languages.
Effectively plan, coordinate, and organize meetings with goals to improve processes that lead to State, Federal and other required District reporting.
Communicate tactfully and effectively in both oral and written forms.
Work effectively and collegially with students and staff from diverse backgrounds.
Establish and maintain effective working relationships with those contacted.
Experience/Education
Education Required:
Associate's degree with coursework in computer information systems, database technologies, research, statistics, mathematics, business administration, engineering or related field.
Education Preferred:
Bachelor's degree with major coursework in computer information systems, database technologies, research, statistics, mathematics, business administration, engineering or related field.
Experience Required:
Demonstrated sensitivity to, and understanding of, the diverse academic, socioeconomic, cultural, and ethnic backgrounds of staff and students, as well as staff members and students with physical and/or learning disabilities.
Experience Preferred:
Progressively responsible experience in supporting project management including data analysis and reporting.
Additional Information
To be considered a candidate for an Administrative position, the applicant must submit ALL of the following materials:
Online Application with the following materials attached:
Cover letter addressing criteria listed in the position announcement
Current resume and/or placement file
College transcripts (unofficial will be accepted at the time of application)
Attention Applicants
After application has been submitted, all application materials will be screened.
Internal candidates must also submit all required materials to be considered.
Candidate evaluations will not be considered.
Reference letters from any member of the Hiring Committee or Board Members will not be considered.
Interviews will be by invitation only.
Interviews may be by Zoom.
The District does not reimburse for new hire moving expenses.
New Administrative Appointments will be placed at the step 1 on the Administrative Salary Schedule and will graduate to the next step on July 1st of the following year if they have rendered 6 months of satisfactory service, with subsequent steps annually thereafter to a maximum fifth step.
Salary placement would only be negotiable if the candidate held the same position with another community college and step 2 or 3 would be the maximum considered depending on the number of years of service.
The Shasta-Tehama-Trinity Joint Community College District (""Shasta College"") does not discriminate against any person on the basis of race, color, national origin, sex, religious preference, age, disability (physical and mental), pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy or childbirth), gender identity, sexual orientation, genetics, military or veteran status or any other characteristic protected by applicable law in admission and access to, or treatment in employment, educational programs or activities at any of its campuses. Shasta College also prohibits harassment on any of these bases, including sexual harassment, as well as sexual assault, domestic violence, dating violence, and stalking.
Copyright ©2022 Jobelephant.com Inc. All rights reserved.
Posted by the FREE value-added recruitment advertising agency
jeid-c034650e8acb064789729b835af0845f Education Required:Associates degree with coursework in computer information systems, database technologies, research, statistics, mathematics, business administration, engineering or related field.Experience Required:Demonstrated sensitivity to, and understanding of, the diverse academic, socioeconomic, cultural, and ethnic backgrounds of staff and students, as well as staff members and students with physical and/or learning disabilities.
Show more
Show less","SQL, Data analysis, Data processing, Data reporting, Data collection, Data validation, Data storage, Record keeping, Project management, Change management, FERPA, HIPPA, PCI, GLBA, NIST, CIS, SQL, Database management systems, Complex SQL query writing, Computer information systems, Database technologies, Research, Statistics, Mathematics, Business administration, Engineering","sql, data analysis, data processing, data reporting, data collection, data validation, data storage, record keeping, project management, change management, ferpa, hippa, pci, glba, nist, cis, sql, database management systems, complex sql query writing, computer information systems, database technologies, research, statistics, mathematics, business administration, engineering","business administration, change management, cis, complex sql query writing, computer information systems, data collection, data processing, data reporting, data storage, data validation, dataanalytics, database management systems, database technologies, engineering, ferpa, glba, hippa, mathematics, nist, pci, project management, record keeping, research, sql, statistics"
SAP DATA CONVERSION ANALYST - SuccessFactors || Must be local to NY/NJ./CT,Steneral Consulting,"Stamford, CT",https://www.linkedin.com/jobs/view/sap-data-conversion-analyst-successfactors-must-be-local-to-ny-nj-ct-at-steneral-consulting-3750153521,2023-12-17,Casa Grande,United States,Associate,Hybrid,"1 day oniste each week
Must be local to NY/NJ./CT
Need valid LinkedIn
Looking SAP Data conversion analyst.
SAP HR and SuccessFactors Employee Central, Data Migration
EC Configuration (Data Models, XML, Foundation Objects, Workflow, Data Imports, Permissions, Reporting (Adhoc & ORD), Data Migrations
SuccessFactors Reporting
Data Mappings from SAP HR
Expert in SAP Testing and QA processes in multiple modules across industry verticals
SuccessFactors WFA module
Expert in Data compare and Data validation reports in SQL
Data migration tool DSP (Data Stewardship Platform)
Expert in Data Load Templates and Load Sequence
Managed Data Loads via SuccessFactors Templates, Managed Data Cleansing Activities
Create reports based on Data fetched via Boomi and create reports to validate Data
Show more
Show less","SAP, SAP HR, SuccessFactors Employee Central, Data Migration, EC Configuration, Data Models, XML, Foundation Objects, Workflow, Data Imports, Permissions, Reporting, Adhoc, ORD, SuccessFactors Reporting, Data Mappings, SQL, DSP (Data Stewardship Platform), Data Load Templates, Load Sequence, Boomi, Data Cleansing, Data Validation","sap, sap hr, successfactors employee central, data migration, ec configuration, data models, xml, foundation objects, workflow, data imports, permissions, reporting, adhoc, ord, successfactors reporting, data mappings, sql, dsp data stewardship platform, data load templates, load sequence, boomi, data cleansing, data validation","adhoc, boomi, data imports, data load templates, data mappings, data migration, data models, data validation, datacleaning, dsp data stewardship platform, ec configuration, foundation objects, load sequence, ord, permissions, reporting, sap, sap hr, sql, successfactors employee central, successfactors reporting, workflow, xml"
Data Warehouse Analyst,ExecuSource,"Austell, GA",https://www.linkedin.com/jobs/view/data-warehouse-analyst-at-execusource-3756109637,2023-12-17,Casa Grande,United States,Associate,Hybrid,"ExecuSource is seeking a Data Warehouse Analyst in the Austell/Lithia Springs area. In this role, you will be responsible for collecting, cleaning, analyzing, and reporting data for internal and external customers. Must be detail-oriented and bring both strong technical acumen with data and business acumen to help inform and influence our team's continued growth and evolution. You will need to thrive in a start-up like environment and enjoy a blend of supporting the day-to-day application needs of the business while working on the longer-term alignment to the company’s objectives. Excellent company benefits.
Responsibilities
Interpret data, analyze results using statistical techniques and provide ongoing reports.
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.
Acquire data from primary or secondary data sources and maintain databases/data systems.
Identify, analyze, and interpret trends or patterns in complex data sets.
Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.
Work with management to prioritize business and information needs.
Locate and define new process improvement opportunities.
Requirements:
Associate’s degree required, Bachelor’s degree in related field preferred.
Minimum (3) years of experience in an administrative or similar role.
Excellent verbal and written communication skills.
Excellent interpersonal and customer service skills
Proficient in Microsoft Office Suite or related software
Excellent analytical, organizational skills and attention to detail.
Basic understanding of clerical procedures and systems such as recordkeeping and filing.
Ability to work independently.
#FA123
Show more
Show less","Data analysis, Data mining, Data reporting, Data visualization, Data warehousing, Databases, Data collection, Data management, Statistical techniques, Microsoft Office","data analysis, data mining, data reporting, data visualization, data warehousing, databases, data collection, data management, statistical techniques, microsoft office","data collection, data management, data mining, data reporting, dataanalytics, databases, datawarehouse, microsoft office, statistical techniques, visualization"
Data Analyst - Hybrid,Stryker,"Portage, MI",https://www.linkedin.com/jobs/view/data-analyst-hybrid-at-stryker-3771375675,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Why join Stryker?
We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com
Who We Want
Data translators.
Highly effective communicators who can transform data findings into recommendations to compose reports and executive level presentations.
Strategic thinkers.
People who enjoy analyzing data or trends for the purposes of planning, forecasting, advising, budgeting, reporting, or sales opportunities.
Collaborative partners.
People who build and leverage cross-functional relationships to bring together ideas, information, use cases, and industry analyses to develop best practices.
What You Will Do
We are looking for a highly motivated and experienced Senior PMO Analyst to join our team. The ideal candidate will have a strong understanding of project management principles and practices, as well as experience with data, analytics, and reporting.
As the Senior PMO Analyst, you will work with cross functional teams as you drive data integrity as you enter, review, and analyze data in the resource capacity platform. In this role, you will be responsible for supporting and maintaining the resource capacity platform associated with data collection, retrieval, accessibility, and usage for internal resource planning and activities. You will play a crucial role in ensuring data integrity, generating reports, and providing training to users. Additionally, you will have the opportunity to recommend changes in application development, maintenance, and system standards.
Other daily activities include but are not limited to:
Support and maintain resource capacity platform/tool for data collection and usage.
Partner with cross-functional teams SME’s on entering and ensuring clean and consistent data inputs into established tables, fields, and system databases.
Acts as the PMO point of contact for cross-functional users for all platform questions, request, and modifications.
Responsible for processing and validating platform changes in accordance with platform guidance and governance.
Partner with Lead PMO Analyst triage issues and determine if root cause is process, people, or system.
Set up user access and provide training on platform usage.
Create and maintain seamless end-user experience for all reporting needs.
Partner with Lead PMO Analyst to drive visibility, understanding, trust, and improvements within the resource capacity platform and reports.
Partner with cross-functional users to identify and resolve data gaps and issues.
Partner with the Lead PMO Analyst to ensure business readiness for minor and major releases and updates.
Build and produce reports using query and flexible reporting tools to meet the requirements of cross-functional users and resource managers.
Investigate reporting issues and provide expertise to resolve and follow-up in a timely manner.
Identify opportunities to gain efficiencies, automate, and improve data quality.
In addition to the activities listed above, the Senior PMO Analyst must be able to build strong relationships with stakeholders, communicate effectively with stakeholders, stay calm under pressure, think critically and make sound decisions.
The ideal candidate will also be highly motivated and experienced, passionate about data, analytics, project portfolio management software, and project management, able to work independently with minimal supervision, be able to understand and apply procedures and concepts of their discipline, pay attention to detail in making evaluative judgments based on the analysis of factual information.
What You Will Need
Bachelor's Degree is required
Minimum of 2 years of professional experience
Strong analytical and problem-solving skills.
Experience with data analysis tools such as Excel, PowerBI and Python.
Ability to communicate data insights to stakeholders.
Contribute to the development of the resource capacity platform, recommend improvements through data and reporting.
Assist in the development of the PMO tools and reports. Alert Lead PMO Analyst of variances or gaps.
Complete assigned tasks according to established timeline. Assist in the development of the resource capacity platform rollout and training timelines.
Participate in the development of PMO tools that support data quality.
Partner in the establishment of risk management plans and support effective risk analysis and risk mitigation planning for the PMO.
Provide support on rescheduling, and resource allocation in the program plan and support program gating processes.
Establish and maintain partnerships with internal and external PMO stakeholders.
Produce weekly and monthly PMO metric reports using dashboards and metrics to measure success.
Contributes data analysis expertise for short- and long-term strategic planning process activities.
Leads projects. May support a high complexity program.
Builds productive working relationships with departments that are key partners (e.g., R&D and Marketing).
About Stryker
Our benefits:
12 paid holidays annually
Health benefits include: Medical and prescription drug insurance, dental insurance, vision insurance, critical illness insurance, accident insurance, hospital indemnity insurance, personalized healthcare support, wellbeing program and tobacco cessation program.
Financial benefits include Health Savings Account (HSA), Flexible Spending Accounts (FSAs), 401(k) plan, Employee Stock Purchase Plan (ESPP), basic life and AD&D insurance, and short-term disability insurance.
For a more detailed overview of our benefits or time off, please follow this link to learn more: US Stryker employee benefits
About Stryker
Stryker is one of the world’s leading medical technology companies and, together with its customers, is driven to make healthcare better. The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopaedics and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 130 million patients annually. More information is available at stryker.com.
Know someone at Stryker?
Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program on our referral page
Stryker is driven to work together with our customers to make healthcare better. Employees and new hires in sales and field roles that require access to customer accounts as a function of the job may be required, depending on customer requirements, to obtain various vaccinations as an essential function of their role.
R503899_3
Show more
Show less","Project Management, Data Analytics, Excel, PowerBI, Python, Stakeholder Communication, Data Visualization, Reporting, PMO Tools, Risk Management, Strategic Planning, Team Collaboration","project management, data analytics, excel, powerbi, python, stakeholder communication, data visualization, reporting, pmo tools, risk management, strategic planning, team collaboration","dataanalytics, excel, pmo tools, powerbi, project management, python, reporting, risk management, stakeholder communication, strategic planning, team collaboration, visualization"
Data Analyst/Reports Developer (Top Secret Clearance Required),ECS,"Reston, VA",https://www.linkedin.com/jobs/view/data-analyst-reports-developer-top-secret-clearance-required-at-ecs-3679388618,2023-12-17,Casa Grande,United States,Associate,Hybrid,"ECS is seeking a
Data Analyst/Reports Developer
to work in our
Reston, VA
office.
Job Description:
ECS seeks a Data Analyst/Reports Developer to work in support of an Intelligence Community program. The Data Analyst/Reports Developer will join a team supporting data warehousing, business intelligence and reporting professionals. Duties and responsibilities include:
Gather and analyze business reporting and data requirements.
Design highly professional reports, dashboards and visual representation of business and warehouse data for process owners and executive leadership.
Develop ad-hoc queries and reports based on requirements gathered from customers utilizing complex, custom-built SQL queries for the purposes of analytical reporting.
Validate data and troubleshoot discrepancies in data.
Test and troubleshoot report and dashboard functionality, validate data and create test data and table structures through use of SQL.
Performance tune SQL statements executed by reports and dashboards as required.
Provide timely on-site customer support for users.
Required Skills:
Bachelor's Degree
Current TS/SCI clearance.
Significant PL/SQL experience.
Experience using one or more enterprise business intelligence tools (e.g. IBM Cognos, SAP Business Objects, Oracle BIEE).
Significant understanding of relational databases and data warehouse concepts.
Desired Skills:
Experience with Tableau.
Experience with ETL tools.
Ability to work in a fast paced environment.
Outstanding customer service and communications skills.
Knowledge of Federal acquisition processes.
Ability to document processes as needed.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3000+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Data Analysis, Reporting, Data Warehousing, Business Intelligence, SQL, PL/SQL, Tableau, ETL Tools, Relational Databases, Data Warehouse Concepts, Federal Acquisition Processes, Documentation","data analysis, reporting, data warehousing, business intelligence, sql, plsql, tableau, etl tools, relational databases, data warehouse concepts, federal acquisition processes, documentation","business intelligence, data warehouse concepts, dataanalytics, datawarehouse, documentation, etl tools, federal acquisition processes, plsql, relational databases, reporting, sql, tableau"
Advana/ DoD Data Engineer,Zencon Group,"Washington, DC",https://www.linkedin.com/jobs/view/advana-dod-data-engineer-at-zencon-group-3758128792,2023-12-17,Casa Grande,United States,Associate,Hybrid,"We are seeking experienced Data Engineers to join our Advana team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our Advana data infrastructure and systems. Your expertise in ETL, DataBricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks will be essential in ensuring efficient data processing and analysis.
Responsibilities:
Design, develop, and implement end-to-end data pipelines, utilizing ETL processes and technologies such as DataBricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.
Work closely with cross-functional teams to understand data requirements and design optimal data models and architectures.
Create and optimize data pipelines from scratch, ensuring scalability, reliability, and high-performance processing.
Perform data cleansing, data integration, and data quality assurance activities to maintain the accuracy and integrity of large datasets.
Leverage big data technologies to efficiently process and analyze very large datasets, similar to those encountered in a federal agency.
Collaborate with data scientists, analysts, and stakeholders to provide timely and accurate data insights and support decision-making processes.
Troubleshoot data-related problems and provide innovative solutions to address complex data challenges.
Implement and enforce data governance policies and procedures, ensuring compliance with regulatory requirements and industry best practices.
Utilize your experience with DoD (Department of Defense) projects to meet specific data requirements and adhere to relevant standards.
Stay updated with emerging trends and advancements in data engineering and recommend suitable tools and technologies for continuous improvement.
Requirements:
Minimum of 5 years of experience as a Data Engineer, with
demonstrated
experience creating data pipelines from scratch.
Bachelor's degree in a related field.
High level of proficiency in ETL processes and
demonstrated
hands-on experience with technologies such as DataBricks, Python, Spark, Scala, JavaScript/ JSON, SQL, and Jupyter Notebooks.
Strong problem-solving skills and ability to solve complex data-related issues.
Secret clearance is required; TS/SCI clearance is preferred.
Recent DoD or IC-related experience.
Demonstrated
experience working with very large datasets and leveraging big data technologies to process and analyze data efficiently.
Understanding of data modeling/ visualization, database design principles, and data governance practices.
Knowledge of Qlik/ Qlik Sense, QVD/ QlikView, and Qlik Production Application Standards (QPAS) is a huge plus.
Previous experience with Advana is a plus.
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Detail-oriented mindset with a commitment to delivering high-quality results.
Must be in the DC Metro area and available to work onsite (Crystal City, VA and Alexandria, VA) 2-3 days per week.
Show more
Show less","Data Engineering, ETL, DataBricks, Python, Spark, Scala, JavaScript/JSON, SQL, Jupyter Notebooks, Data Pipelines, Data Models, Data Architectures, Data Cleansing, Data Integration, Data Quality Assurance, Big Data Technologies, Data Analysis, Data Insights, Data Governance, DoD Projects, Emerging Trends, Data Engineering Tools and Technologies, ProblemSolving Skills, Secret Clearance, DoD or ICRelated Experience, Data Modeling/ Visualization, Database Design Principles, Qlik/ Qlik Sense, QVD/ QlikView, Qlik Production Application Standards (QPAS), Advana Experience, Communication and Collaboration Skills, DetailOriented Mindset, DC Metro Area Presence","data engineering, etl, databricks, python, spark, scala, javascriptjson, sql, jupyter notebooks, data pipelines, data models, data architectures, data cleansing, data integration, data quality assurance, big data technologies, data analysis, data insights, data governance, dod projects, emerging trends, data engineering tools and technologies, problemsolving skills, secret clearance, dod or icrelated experience, data modeling visualization, database design principles, qlik qlik sense, qvd qlikview, qlik production application standards qpas, advana experience, communication and collaboration skills, detailoriented mindset, dc metro area presence","advana experience, big data technologies, communication and collaboration skills, data architectures, data engineering, data engineering tools and technologies, data governance, data insights, data integration, data modeling visualization, data models, data quality assurance, dataanalytics, database design principles, databricks, datacleaning, datapipeline, dc metro area presence, detailoriented mindset, dod or icrelated experience, dod projects, emerging trends, etl, javascriptjson, jupyter notebooks, problemsolving skills, python, qlik production application standards qpas, qlik qlik sense, qvd qlikview, scala, secret clearance, spark, sql"
"Healthcare Data Analyst or Senior Healthcare Data Analyst, Analytics Hub",ECG Management Consultants,"Minneapolis, MN",https://www.linkedin.com/jobs/view/healthcare-data-analyst-or-senior-healthcare-data-analyst-analytics-hub-at-ecg-management-consultants-3785921709,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Overview
ECG is a national management consulting firm working exclusively in the healthcare industry. At ECG, our primary emphasis is on quality—in our people as well as our services—and we’re seeking others who appreciate our high standards of excellence.
What’s in It for You: Consult with Purpose
At ECG, you can have a bigger impact than you ever imagined. The work you do will help health systems deliver care more effectively and efficiently—and that’s just the beginning. We’re looking for innovators, problem-solvers, and self-starters to collaborate across our five divisions, take on challenging projects, and find new ways to improve patient care. ECG is committed to ensuring a friendly work environment that rewards high performance and welcomes, values, and supports all people.
Join Our Analytics Hub
Our growing Analytics Hub works with our consulting teams to deliver new insights, streamlined analysis, and world-class intelligence to the healthcare organizations we serve as clients. Combined with our consulting teams, you will be helping physician and executive leadership make informed decisions that can alter the course of their enterprises in areas such as physician alignment, mergers and acquisitions, service line planning, provider compensation, financial performance, transforming care models, patient access, managed care portfolio optimization, digital health expansion, and more. ECG uses a Microsoft Azure and Microsoft 365 environment, with Microsoft Power BI as ECG’s current analytics visualization platform. The goal of the position is to further capitalize on the existing ECG infrastructure, as well as implement new features and technologies. The ideal candidate is open to new challenges, exceptional at multitasking, and proficient at implementing new solutions.
Our detailed plans incorporate qualitative findings with data-driven strategic, operational, and financial considerations that enable organizations to pursue realistic change. Our consultants bridge the gap between strategic thinking and operational implementation with sensible action plans and tactical recommendations. This approach allows us to stretch our clients’ thinking while ensuring that initiatives with the broadest and deepest impact are prioritized and implementable. Our wide-ranging engagements often focus on helping our clients:
Position themselves for value-based care delivery.
Align hospitals and physician organizations.
Strengthen financial performance.
Enhance clinical programs.
Develop strategic partnerships and/or mergers.
Your Opportunity with ECG: Data Analyst or Senior Data Analyst
As a data analyst or senior data analyst, you will support the firm’s client delivery and business development efforts, working with consultants and senior leaders to manage large data sets, aggregate internal and external data, develop scalable models and other analyses, and support data visualization and presentation sets. In addition, the analyst will work directly with our internal support teams (IT, business development, and L&D) and associated vendors for data management and IT solutions and will assist with a range of innovative analytic opportunities. Here, no two days or projects are alike, which means you’ll have a lot to learn and plenty of support to help you succeed.
Your Responsibilities May Include:
Gathering and synthesizing data from various sources (e.g., national surveys, proprietary research, client interviews, industries, markets)
Preparing analyses related to hospital inpatient databases and provider claims data (Stratasan)
Building flexible, dynamic, and scalable financial and operational models to forecast trends, preparing scenarios for consulting efforts related to delivery and business development (e.g., strategic planning, operational improvement, market assessments)
Enhancing existing models and capabilities built using SQL, primarily, with some use of Python or R in certain models or scenarios
Performing complex analyses on big data, including cleaning, preparing, and interpreting
Identifying and executing analyses in response to consultant business intelligence inquiries
Building relational databases using a Microsoft SQL Server or Fabric
Communicating market insights to pursuit teams, and supporting business development efforts
Configuring, maintaining, and building flexible reports, queries, and visuals within the Power BI environment and the production and development environments of related databases
Researching, testing, and implementing solutions in Power BI
Partnering with various consulting units (strategy, academic health, provider financial services, performance transformation, etc.) to gather requirements related to data analytics
Developing, configuring, and maintaining interfaces to exchange data among Power BI, SQL databases, and other cloud-based applications
Creating and maintaining analytical support documentation and operating procedures
Developing new and innovative methodologies and approaches
Contributing to internal education and learning opportunities for consultants and operations members for the Analytics Hub and analytics in general, and supporting education about ECG’s use cases related to analytics in work
Assisting the director of data analytics in developing analytics and data strategy to support data analytics capabilities at the firm
Our Expectations of You
A bachelor’s degree in either computer science, computer engineering, mathematics, statistics, health information management, health administration, business, or a related degree that relies heavily on critical thinking, logic, and math
Prior data analytics work experience using databases, SQL, and Power BI in a healthcare or enterprise environment (five+ years of experience for senior analyst level; one to three years for analyst)
Experience using Python, R, Fabric, and Tableau a plus
Proficiency in Microsoft Word, Excel, and PowerPoint
Comfort with manipulating and synthesizing large data sets
Strong written and verbal communication skills
Excellent organizational skills
Job Locations
St. Louis and Washington, DC, offices are preferred. Other offices may include Atlanta, Boston, Chicago, Minneapolis, San Diego, or Seattle. Able to work a hybrid schedule with days in the office and remote.
Schedule
Full time/exempt
What You Can Expect Of Us
To reward our driven, innovative, and passionate employees, we’ve built a company culture that’s centered on performance. We offer an attractive compensation package, challenging work, and an entrepreneurial environment where you can take ownership of your career—and get out as much as you put in.
About ECG
ECG is a strategic consulting firm leading healthcare forward using knowledge and expertise built over the course of five decades to help clients see clearly where the industry is going and navigate toward success. We work as trusted, professional partners with hospitals, health systems, medical groups, and academic medical centers across the country. We thrive on delivering smart counsel and pragmatic solutions to the critical challenges facing healthcare providers. Client success is our primary objective. ECG’s national presence includes offices in Atlanta, Boston, Chicago, Dallas, Minneapolis, San Diego, Seattle, St. Louis, and Washington, DC.
Apply now and make an impact for years to come.
To begin the recruitment process, please submit your resume via our career site at https://careers.ecgmc.com.
ECG provides equal employment opportunities to all employees and applicants for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, disability, pregnancy, medical condition (cancer and genetic characteristics), genetic information, gender, gender identity or expression, sexual orientation, marital status, military or veteran status, or any other legally protected characteristic. People of color are encouraged to apply. We participate in E-Verify as part of our onboarding process. Having the permanent legal right to work in the United States is a condition of employment. ECG is not currently able to provide assistance to candidates requiring sponsorship or a visa.
Residents of the states of California or Washington may receive salary information for this job through this link or by contacting the recruiter directly at schavez@ecgmc.com.
Show more
Show less","Data Analysis, Data Visualization, Microsoft Azure, Microsoft 365, Microsoft Power BI, SQL, Python, R, Fabric, Tableau, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Strategic Consulting, Healthcare Consulting, Financial Modeling, Operational Modeling, Market Assessment, Business Intelligence, Relational Databases, Data Management, Data Strategy, Analytics","data analysis, data visualization, microsoft azure, microsoft 365, microsoft power bi, sql, python, r, fabric, tableau, microsoft word, microsoft excel, microsoft powerpoint, strategic consulting, healthcare consulting, financial modeling, operational modeling, market assessment, business intelligence, relational databases, data management, data strategy, analytics","analytics, business intelligence, data management, data strategy, dataanalytics, fabric, financial modeling, healthcare consulting, market assessment, microsoft 365, microsoft azure, microsoft excel, microsoft power bi, microsoft powerpoint, microsoft word, operational modeling, python, r, relational databases, sql, strategic consulting, tableau, visualization"
MuleSoft Enterprise Data Engineer,Schrödinger,Greater Boston,https://www.linkedin.com/jobs/view/mulesoft-enterprise-data-engineer-at-schr%C3%B6dinger-3719233988,2023-12-17,Casa Grande,United States,Associate,Hybrid,"We’re seeking a
MuleSoft Enterprise
Data Engineer to fill a temporary-to-permanent position
and join us in our mission to improve human health and quality of life through the development, distribution, and application of advanced computational methods!
Schrödinger is on the cutting edge of computer-aided drug discovery and materials science, collaborating with companies like Takeda, Nimbus, Pfizer, and Sanofi. We set the record for the largest and fastest cloud computing run, and our software suites continue to revolutionize the design of therapeutics and materials. WaterMap, Maestro, and LiveDesign are just a few examples of the programs we’ve created.
As a member of our Data team, you’ll play a critical role in the administration and improvement of our corporate data management system.
Who Will Love This Job
A data engineer with meaningful architecture experience
A creative problem-solver with an attention to detail
An excellent team player who wants to make a sizable impact working on a small, dedicated department
A lifelong student who’s eager to learn new and varied technologies and business processes on the job
A humanitarian who cares deeply about improving human health
What You’ll Do
Provide technical leadership towards architecting and delivering end to end solutions
Work with business partners for prioritization, impact assessment, and resolution
Collaborate with other teams for multi-functional initiatives
Design and build reusable components, frameworks and libraries at scale to support analytics products
Devise and implement product features in collaboration with business and technology partners
Develop architecture and design patterns to process and store high volume data sets
Identify and tackle issues concerning data management to improve data quality
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Tackle data issues and perform root cause analysis to proactively resolve product and operational issues
What You Should Have
At least four years of relevant experience, with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT, and reporting/analytic tools
At least three years of experience in cloud environments like GCP and BigQuery
Experience developing custom features and solutions for ERP/CRM systems in similar roles, especially using Apex in Salesforce
Understanding of relational and non-relational SQL
Familiarity with building real-time streaming data pipelines
Background in pub/sub modes like Kafka
Experience in building lambda, kappa, microservice and batch architecture
Knowledge of CI/CD processes and source control tools such as GitHub and related dev processes
Experience with MuleSoft
Experience with NetSuite and/or Salesforce Lightning is a major plus
If you don’t consider yourself an expert in all of the fields
above
, that’s OK - we’re looking to hire an adaptable polymath who’s excited to learn on the job. Our team will be happy to help you get up to speed in any unfamiliar areas!
Pay And Perks
Schrödinger understands it’s people that make a company great. Because of this, we’re prepared to offer a competitive salary, stock options, and a wide range of benefits that include healthcare (with dental and vision), a 401k, pre-tax commuter benefits, a flexible work schedule, and a parental leave program. We have catered meals in the office every day, a company culture that is relaxed but engaged, and over a month of paid vacation time. Our Administrative and Human Resources departments also plan a myriad of fun company-wide events. New York is home to our largest office, but we have teams all over the world. Schrödinger is honored to have been selected as one of Crain's New York Best Places to Work in 2018 and 2019.
Sound exciting? Apply today and join us!
Estimated base salary range: $105,000 - $165,000. Actual compensation package is dependent on a number of factors, including, for example, experience, education, degrees held, market data, and business needs. If you have any questions regarding the compensation for this role, do not hesitate to reach out to a member of our Strategic Growth team.
As an equal opportunity employer, Schrödinger hires outstanding individuals into every position in the company. People who work with us have a high degree of engagement, a commitment to working effectively in teams, and a passion for the company's mission. We place the highest value on creating a safe environment where our employees can grow and contribute, and refuse to discriminate on the basis of race, color, religious belief, sex, age, disability, national origin, alienage or citizenship status, marital status, partnership status, caregiver status, sexual and reproductive health decisions, gender identity or expression, sexual orientation, or any other protected characteristic. To us, ""diversity"" isn't just a buzzword, but an important element of our core principles and key business practices. We believe that diverse companies innovate better and think more creatively than homogenous ones because they take into account a wide range of viewpoints. For us, greater diversity doesn't mean better headlines or public images - it means increased adaptability and profitability.
Show more
Show less","Data engineering, MuleSoft, Salesforce, Apex, NetSuite, GCP, BigQuery, Cloud computing, SQL, Kafka, Microservices, CI/CD, GitHub, ETL/ELT, Reporting, Analytics, Data management, Data architecture, Data quality, Data pipelines, Data warehouse, ERP/CRM","data engineering, mulesoft, salesforce, apex, netsuite, gcp, bigquery, cloud computing, sql, kafka, microservices, cicd, github, etlelt, reporting, analytics, data management, data architecture, data quality, data pipelines, data warehouse, erpcrm","analytics, apex, bigquery, cicd, cloud computing, data architecture, data engineering, data management, data quality, datapipeline, datawarehouse, erpcrm, etlelt, gcp, github, kafka, microservices, mulesoft, netsuite, reporting, salesforce, sql"
Workday Data Conversion Analyst,Stellar Professionals,"Atlanta, GA",https://www.linkedin.com/jobs/view/workday-data-conversion-analyst-at-stellar-professionals-3770741343,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Skills Required
Solid grasp of relational database structures and strong use of SQL
Familiarity with a variety of database types and interfaces (Microsoft Access, Oracle, various text formats, ODBC).
Solid understanding and usage of Microsoft Excel
Experience with PeopleSoft, Oracle, Taleo, Workday or other SaaS and On-Premises ERP systems.
Experience in ERP Human Resource, Financial, or Procurement domains
Show more
Show less","SQL, Relational Databases, Microsoft Access, Oracle, ODBC, Microsoft Excel, PeopleSoft, Taleo, Workday, ERP Systems, SaaS, Human Resources, Financial, Procurement","sql, relational databases, microsoft access, oracle, odbc, microsoft excel, peoplesoft, taleo, workday, erp systems, saas, human resources, financial, procurement","erp systems, financial, human resources, microsoft access, microsoft excel, odbc, oracle, peoplesoft, procurement, relational databases, saas, sql, taleo, workday"
Data Analyst II (Marketing),eStaffing Inc.,"Georgia, United States",https://www.linkedin.com/jobs/view/data-analyst-ii-marketing-at-estaffing-inc-3785774827,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Title -
Data Analyst II (Marketing)
Job location -
Atlanta GA,
30313
Job type - Contract-full time
Duration - 01 Year
Shift Time -
Regular Shift
Job Description
Analyst
Location: Northyards Office, ATL Hybrid 2-3 days/wk onsite but this is flexible.
A Data Analyst will play a key role, responsible for (1) custom analytics execution for our stakeholders across the system (incl. Bottlers and Customers); and (2) PowerBI development / engagement with our Freestyle Connected Insights capability.
What You’ll Do For Us
Execute custom analytics on our proprietary data for Customers, Bottlers, NAOU and EOU account teams, and Functional stakeholders.
Develop new reports and maintain/update existing reports for our Freestyle Connected Insights (PowerBI) capabilities.
Lead Mastercard Test & Learn program (statistical testing and business optimization) for Freestyle Marketing, Commercial, Operations, and Sales-related activations and experiments.
Qualifications & Requirements
:
Strong fundamental analytics knowledge (financial analysis, testing, marketing analytics)
Experience working with big data and building data visualizations, dashboards, and datasets using BI tools (PowerBI, Alteryx)
Great teammate with ability to work cross-functionally in resolving complex issues and take a new perspective on existing solutions
Curious data innovator with passion for experimenting and iterating with emerging approaches and ideas
Tech Plusses: AWS, Azure, and PowerPlatform; SQL and Python; Mastercard Test & Learn for Sites
Functional Skills
Analytics
Data Storytelling
Data Product Management
Show more
Show less","Data Analysis, Power BI, Statistical Testing, Business Optimization, AWS, Azure, PowerPlatform, SQL, Python, Alteryx, Mastercard Test & Learn, Financial Analysis, Data Visualizations, Data Dashboards, Data Warehousing, Data Modeling, Data Storytelling, Data Product Management","data analysis, power bi, statistical testing, business optimization, aws, azure, powerplatform, sql, python, alteryx, mastercard test learn, financial analysis, data visualizations, data dashboards, data warehousing, data modeling, data storytelling, data product management","alteryx, aws, azure, business optimization, data dashboards, data product management, data storytelling, data visualizations, dataanalytics, datamodeling, datawarehouse, financial analysis, mastercard test learn, powerbi, powerplatform, python, sql, statistical testing"
Senior Data Analyst,Steneral Consulting,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-steneral-consulting-3759355837,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Candidate’s Linked In Profile:
2 Managerial References:
First Manager’s Name:
Company:
Title:
Email:
Phone Number:
Second Manager’s Name:
Company:
Title:
Email:
Phone Number:
Attached resume
Job Title:
Senior Data Analyst
Location:
Deerfield Beach Florida 33064 – hybrid work environment, must be onsite a minimum of 3 days per week
Duration:
12+ month contract
Other Requirements
: candidates must be willing to live in the Deerfield Beach Florida area and go into the office a minimum of 3 days/week
Number of Positions
: 1
Job Overview
Selected candidate must be onsite a minimum of 3 days per week in Deerfield Beach, FL.*
Your Future Duties And Responsibilities
Responsible for eliciting, understanding, interpreting and representing business requirements and act as the conduit between the customer and technical teams to ensure requirements are understood.
Provide subject matter expertise on the use of data as well as educate teams on business model, metadata and standards.
Responsible for understanding source systems and its data models.
Develop source to target mappings for data lineage.
Document source architecture to include data flows.
Responsible for analyzing data to validate business domains and requirements.
Responsible for data profiling and ensuring data quality requirements are accurate and complete.
Act in an advisory capacity in data model reviews, architecture approach and solution design to ensure high quality deliverables.
Responsible for partnering with management and business units on innovative ways to successfully utilize data and related tools to advance business objectives.
Works with governance council to establish data governance standards and guidelines.
Assist with business data lake testing / experimentation
Assist with coordinating data dictionary completions
Mentor Project DA resources
Required Qualifications To Be Successful In This Role
Validated experience on projects involving data analysis and profiling, data integration, data cleansing, data mapping, and data conversion activities
Proficient in data management concepts, data lifecycle and methodologies
Knowledge and experience with an ERP system highly preferred. Experience using Sales Force a plus.
Experience and hands-on involvement in operational system modernization and transformation. Knowledge of Microsoft Dynamics 360 a big plus.
Excellent analytical, problem-solving, and decision-making skills, demonstrating both logic and creativity
Excellent written and verbal communication, as well as, strong organizational and presentation skills
Highly motivated and a strong desire to understand the organization, its industry, and its strategies
Resourceful at applying business and technical skills to drive innovation and performance improvement
Demonstrated ability to balance multiple contending priorities in a dynamic environment
Demonstrated facilitations and meeting management skills
Proven ability to work with business representatives to understand and detail their business and functional requirements and document it in an organized ‘functional design’ format
Excellent interpersonal skills with the ability to build relationships within and between individuals and multi-functional teams
Must be a self-starter and show strong initiative
Must exhibit strong customer service orientation
Ability to influence and motivate individuals and teams to drive mutually beneficial outcomes
Solid grasp of agile methodology framework is a plus
4+ years experience working as a data analyst using SQL, BI and other data analysis tools
3+ years hands-on experience working with SQL and a solid understanding of different data structures (flat files, relational, etc.)
Understands data modeling concepts and techniques
Experience working BI/Analytics tools such as Power BI and Tableau is a plus
Experience with MICROSOFT DYNAMICS 360 A BIG PLUS
Education
Bachelor’s degree or equivalent plus 5+ years of related professional experience
Degree in Technology and/or Finance related area preferred
Show more
Show less","SQL, BI, Data analysis tools, Power BI, Tableau, Data management, Data integration, Data cleansing, Data mapping, Data conversion, Microsoft Dynamics 360, Agile methodology, Data modeling, Data profiling, Data quality","sql, bi, data analysis tools, power bi, tableau, data management, data integration, data cleansing, data mapping, data conversion, microsoft dynamics 360, agile methodology, data modeling, data profiling, data quality","agile methodology, bi, data analysis tools, data conversion, data integration, data management, data mapping, data profiling, data quality, datacleaning, datamodeling, microsoft dynamics 360, powerbi, sql, tableau"
"Director, Data Scientist - Biopharma",Pfizer,"Tampa, FL",https://www.linkedin.com/jobs/view/director-data-scientist-biopharma-at-pfizer-3729746278,2023-12-17,Casa Grande,United States,Associate,Hybrid,"ROLE SUMMARY:
The Commercial Analytics team at Pfizer is looking for a Director, Data Science who is passionate about crafting and implementing predictive modeling and statistical analysis to build end-to-end solutions and insights that have a direct impact on patient's lives and the future of Pfizer as a data-driven organization. You will be a thought partner to the business, understand strategic goals, and then use your skills and subject matter expertise to surface impactful insights that drive business decisions and patient benefits. With colleagues across the globe, our rigorous analytical expertise is relied upon as the compass and decision support for the enterprise. Our dynamic, exciting team of subject-matter experts comes from diverse backgrounds and experiences, including market research, data science, digital analytics, finance, and consulting. Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way. We believe Data Science is a team sport, but we strive for independent decision-making and taking smart risks.
ROLE RESPONSIBILITIES:
This role is accountable for delivering data science driven support for an assigned brand and will partner with US Commercial, business, and digital teams, to develop and implement models, insights, and data products that drive brands’ strategic priorities. This is an individual contributor role.
Take deep dives in large-scale data to identify key insights that will shape future product/brand strategy for a specific therapeutic area.
Collaborate with cross-functional teams to identify new growth opportunities, develop data requirements, establish critical metrics, and evangelize data products.
Design, deploy, and evaluate experiments that help define opportunities for higher adoption, improved business performance, and better patient experience.
Conduct hypothesis-driven exploratory analyses, select appropriate ML algorithms, and build complex optimization engines to deliver impactful data solutions!
Research new technologies and methods across data science and data engineering to improve the technical capabilities of the team.
Communicate insights to senior management by distilling complex analysis and concepts into concise business-focused takeaways.
QUALIFICATIONS:
Bachelor’s degree with 10+ years of experience OR Masters Degree with 9+ years of experience OR PhD with 7+ years of experience
Degree preferably in engineering, economics, statistics, computer science, or related quantitative field.
Preferred experience in Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering, or related field preferred.
Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and or working in Snowflake/Databricks
Ability to implement data science pipelines and applications in a general programming language such as Python, Scala, Java, or R.
Practical experience with and theoretical understanding of ML algorithms for classification, regression, clustering, and anomaly detection
Well versed with and have experience applying various statistical methodologies including Bayesian and non-parametric techniques, hypothesis testing, ANOVA, Regression, fixed and random effects etc. to measure the impact of experiments
Hands on experience working in big data environments such as Hadoop, Spark, and using Python / SQL or comparable languages for manipulating and analyzing complex clickstream and or unstructured data
Ability to extract significant business insights from data and identify the roots behind the patterns
Experience working with a data visualization tool/package, including Dash, Tableau, and Angular etc.
Communication skills for communicating complex quantitative analyses to senior business executives
Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.
Other Job Details:
Last Date to Apply for Job: October 31, 2023
The annual base salary for this position ranges from $144,900.00 to $241,500.00.* In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.
The annual base salary for this position in Tampa, FL ranges from $130,400.00 to $217,300.00.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Marketing and Market Research
Show more
Show less","Data Science, Predictive Modeling, Statistical Analysis, Machine Learning, Data Mining, Optimization, Business Intelligence, Big Data, Hadoop, Spark, SQL, Python, Scala, Java, R, Tableau, Angular, Dash","data science, predictive modeling, statistical analysis, machine learning, data mining, optimization, business intelligence, big data, hadoop, spark, sql, python, scala, java, r, tableau, angular, dash","angular, big data, business intelligence, dash, data mining, data science, hadoop, java, machine learning, optimization, predictive modeling, python, r, scala, spark, sql, statistical analysis, tableau"
MuleSoft Enterprise Data Engineer,Schrödinger,"Portland, Oregon Metropolitan Area",https://www.linkedin.com/jobs/view/mulesoft-enterprise-data-engineer-at-schr%C3%B6dinger-3739284082,2023-12-17,Casa Grande,United States,Associate,Hybrid,"We’re seeking a
MuleSoft Enterprise
Data Engineer to fill a temporary-to-permanent position
and join us in our mission to improve human health and quality of life through the development, distribution, and application of advanced computational methods!
Schrödinger is on the cutting edge of computer-aided drug discovery and materials science, collaborating with companies like Takeda, Nimbus, Pfizer, and Sanofi. We set the record for the largest and fastest cloud computing run, and our software suites continue to revolutionize the design of therapeutics and materials. WaterMap, Maestro, and LiveDesign are just a few examples of the programs we’ve created.
As a member of our Data team, you’ll play a critical role in the administration and improvement of our corporate data management system.
Who Will Love This Job
A data engineer with meaningful architecture experience
A creative problem-solver with an attention to detail
An excellent team player who wants to make a sizable impact working on a small, dedicated department
A lifelong student who’s eager to learn new and varied technologies and business processes on the job
A humanitarian who cares deeply about improving human health
What You’ll Do
Provide technical leadership towards architecting and delivering end to end solutions
Work with business partners for prioritization, impact assessment, and resolution
Collaborate with other teams for multi-functional initiatives
Design and build reusable components, frameworks and libraries at scale to support analytics products
Devise and implement product features in collaboration with business and technology partners
Develop architecture and design patterns to process and store high volume data sets
Identify and tackle issues concerning data management to improve data quality
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Tackle data issues and perform root cause analysis to proactively resolve product and operational issues
What You Should Have
At least four years of relevant experience, with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT, and reporting/analytic tools
At least three years of experience in cloud environments like GCP and BigQuery
Experience developing custom features and solutions for ERP/CRM systems in similar roles, especially using Apex in Salesforce
Understanding of relational and non-relational SQL
Familiarity with building real-time streaming data pipelines
Background in pub/sub modes like Kafka
Experience in building lambda, kappa, microservice and batch architecture
Knowledge of CI/CD processes and source control tools such as GitHub and related dev processes
Experience with MuleSoft
Experience with NetSuite and/or Salesforce Lightning is a major plus
If you don’t consider yourself an expert in all of the fields
above
, that’s OK - we’re looking to hire an adaptable polymath who’s excited to learn on the job. Our team will be happy to help you get up to speed in any unfamiliar areas!
Pay And Perks
Schrödinger understands it’s people that make a company great. Because of this, we’re prepared to offer a competitive salary, stock options, and a wide range of benefits that include healthcare (with dental and vision), a 401k, pre-tax commuter benefits, a flexible work schedule, and a parental leave program. We have catered meals in the office every day, a company culture that is relaxed but engaged, and over a month of paid vacation time. Our Administrative and Human Resources departments also plan a myriad of fun company-wide events. New York is home to our largest office, but we have teams all over the world. Schrödinger is honored to have been selected as one of Crain's New York Best Places to Work in 2018 and 2019.
Sound exciting? Apply today and join us!
Estimated base salary range: $105,000 - $165,000. Actual compensation package is dependent on a number of factors, including, for example, experience, education, degrees held, market data, and business needs. If you have any questions regarding the compensation for this role, do not hesitate to reach out to a member of our Strategic Growth team.
As an equal opportunity employer, Schrödinger hires outstanding individuals into every position in the company. People who work with us have a high degree of engagement, a commitment to working effectively in teams, and a passion for the company's mission. We place the highest value on creating a safe environment where our employees can grow and contribute, and refuse to discriminate on the basis of race, color, religious belief, sex, age, disability, national origin, alienage or citizenship status, marital status, partnership status, caregiver status, sexual and reproductive health decisions, gender identity or expression, sexual orientation, or any other protected characteristic. To us, ""diversity"" isn't just a buzzword, but an important element of our core principles and key business practices. We believe that diverse companies innovate better and think more creatively than homogenous ones because they take into account a wide range of viewpoints. For us, greater diversity doesn't mean better headlines or public images - it means increased adaptability and profitability.
Show more
Show less","MuleSoft, Data Engineering, Data Management, Cloud Computing, GCP, BigQuery, Salesforce, Apex, SQL, Streaming Data Pipelines, Kafka, Lambda Architecture, Kappa Architecture, Microservice Architecture, Batch Architecture, CI/CD, GitHub, NetSuite, Salesforce Lightning","mulesoft, data engineering, data management, cloud computing, gcp, bigquery, salesforce, apex, sql, streaming data pipelines, kafka, lambda architecture, kappa architecture, microservice architecture, batch architecture, cicd, github, netsuite, salesforce lightning","apex, batch architecture, bigquery, cicd, cloud computing, data engineering, data management, gcp, github, kafka, kappa architecture, lambda architecture, microservice architecture, mulesoft, netsuite, salesforce, salesforce lightning, sql, streaming data pipelines"
Data Analyst,FinTech LLC,"Avenel, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-fintech-llc-3781702260,2023-12-17,Casa Grande,United States,Associate,Hybrid,"About Client:
The client delivers workplace insights through comparative workplace analytics that shape the future in addition to educating the workforce and bringing clarity to the business benefits of workplace fairness.
Rate Range: $45.00 - $55.00/Hr
Job Description:
We are seeking a seasoned Data Analyst with over five years of experience to drive strategic insights and shape our data-driven initiatives within a startup environment.
This role demands a proactive individual capable of translating complex data into actionable strategies while overseeing the data team's operations.
Responsibilities:
Strategic Data Insights:
Collaborate closely with the data team to identify critical milestones derived from data insights and develop comprehensive strategies aligned with organizational goals.
Team Oversight and Performance:
Take charge of the data team's operations, ensuring efficiency and alignment with strategic objectives within a fast-paced startup setting, making significant improvements within a 6-month timeframe.
DEI Metrics and Solutions:
Utilize DEI metrics and analytics expertise to provide valuable insights and solutions. Capable of analyzing, cataloging, and crafting compelling data-driven narratives and solutions, particularly in Diversity, Equity, and Inclusion spaces.
Data Management on AWS:
Proficiency in utilizing the AWS platform for data analysis, collection, and management. Establish a streamlined data collection and management organization, optimizing efficiency and accuracy.
Requirements:
Bachelor's degree or higher in a relevant field with a minimum of 5 years’ experience as a Data Analyst, with a strong focus on driving strategic insights and managing teams.
Proven track record of collecting and cataloging data to be leveraged to drive strategic decision-making, working on a team, and delivering results within a startup or dynamic environment.
Expertise in handling DEI metrics and analytics, translating data into actionable solutions, and showcasing a knack for storytelling through data.
Proficiency in utilizing the AWS platform for data management and analysis, with the ability to streamline data collection processes.
About ApTask:
Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work. We offer a comprehensive suite of offerings, including staffing and recruitment services, managed services, IT consulting, and project management, providing unparalleled opportunities for professional growth and development. As a member of our dynamic team, you'll have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success for our clients across diverse industries. If you are passionate about excellence, collaboration, and innovation, and aspire to make a meaningful impact in the world of work, come join us at ApTask and be a part of our mission to empower organizations to thrive.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
Candidate Data Collection Disclaimer:
At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.
If you have any concerns or queries about your personal information, please feel free to contact our compliance team at
businessexcellence@aptask.com
.
Show more
Show less","Data Analytics, Datadriven Insights, Strategic Insights, Data Team Management, Datadriven Initiatives, AWS, Data Management, DEI Metrics, Data Storytelling, Data Collection, Data Cataloguing, Datadriven Narratives","data analytics, datadriven insights, strategic insights, data team management, datadriven initiatives, aws, data management, dei metrics, data storytelling, data collection, data cataloguing, datadriven narratives","aws, data cataloguing, data collection, data management, data storytelling, data team management, dataanalytics, datadriven initiatives, datadriven insights, datadriven narratives, dei metrics, strategic insights"
"Data Analyst-local || Hybrid in Pittsburgh, PA || USC,GC only",Steneral Consulting,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-analyst-local-hybrid-in-pittsburgh-pa-usc-gc-only-at-steneral-consulting-3727930088,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Hello,
Greetings from
Steneral Consulting
We have an urgent need for a
Data Analyst
one of our clients in
Hybrid in Pittsburgh, PA
Please go through the below requirement if you or your consultants are open for projects and interested in the below requirement, Please respond back with latest resume along with the details
ASAP
.
Work Authorization:
USC,GC only
Position:
Data Analyst
Location: Hybrid in Pittsburgh, PA
Duration: 12 months
Interview:
F2F
Job Description
Business Sector Data Engineering
Skills: Issue Management and ServiceNow and Issue Management, business analysis, and tools integration (requirements and UAT)
Job Description The primary responsibilities of the candidate will include:
Issue Management analysis and triage through self-identified issues and data quality issues across the enterprise
Liase between stakeholders to identify and agree upon remediation plans and timelines for open issues
Use of ServiceNow tool for issue management process
Document technical requirements for ServiceNow tool enhancements and perform UAT
Assist with ServiceNow end user training including live demos, user guides, best practices, FAQs, etc.
Manager Release Notes
Is the goal to convert the candidate to FTE? Potentially
Show more
Show less","Data Engineering, Issue Management, ServiceNow, Business Analysis, Requirements Gathering, UAT, Technical Documentation, End User Training","data engineering, issue management, servicenow, business analysis, requirements gathering, uat, technical documentation, end user training","business analysis, data engineering, end user training, issue management, requirements gathering, servicenow, technical documentation, uat"
Data Migration Analyst II,Monument Consulting,"Richmond, VA",https://www.linkedin.com/jobs/view/data-migration-analyst-ii-at-monument-consulting-3779382311,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Data Migration Analyst II: (Intermediate)
Position Summary
Monument Consulting is currently seeking a Data Migration Analyst II (DMA) to join our Configuration Team and facilitate the implementation and enhancement of Vendor Management Systems (VMS) for our clients. As a Managed Service Provide (MSP), our clients seek our services to consult on, design, and implement managed procurement solutions for their contingent labor programs. New client implementations typically last 3 - 6 months and a DMA may work on 2 - 4 projects at a time. All DMAs will complete strategic departmental initiatives related to their workstreams.
The DMA will work as part of a project team owning the collection and review of foundational data and incumbent worker data. The DMA will be responsible for manipulating large datasets in Microsoft Excel and learning VMS uploads across multiple software including SAP Fieldglass, Beeline, and VNDLY. The DMA will sanitize client source system HRIS and financial data and create early estimates for key project metrics. The DMA will create a Data Validation Template (DVT) and coordinate distribution to key external stakeholders. The DMA will answer stakeholder questions, collect returned data, and combine into a Worker Data Master (WDM) file. The DMA will then identify and call out any discrepancies or gaps and coordinate the closure of gaps. After building and testing uploads in a test environment, the DMA will upload client data into the VMS and audit results for accuracy and completeness. The DMA will also own change request and/or expansion work for existing clients.
Responsibilities
Project Responsibilities
Learning new VMS data uploads (SAP Fieldglass, Beeline, VNDLY, etc.)
Learning and employing data management best practices
Reviewing, sanitizing, and combining client source data to establish a Data Validation Template (DVT)
Auditing, cleaning, and standardizing large Excel datasets including formatting, file combination, field mapping, and gap analysis
Coordinating the data validation project plan with our internal project management team
Coordinating data validation with client, vendor, and internal stakeholders
Conducting data validation call outlining the process for client, vendor, and internal stakeholders
Validating and uploading foundational datasets into VMS
Building and maintaining a Worker Data Master (WDM) Excel dataset of collected data
Maintaining through backups and audit trails for master data files
Validating returned data, communicating data gaps, and coordinating gap closure
Reporting on data collection progress and key spend metrics
Executing rigorous VMS upload testing and troubleshooting errors
Executing production uploads
Ensuring error-free imports of client data from master files to VMS
Exceeding key performance metrics related to data import completeness and accuracy
Performing systematic audits of production data
Desired Skills & Experience
Experience
Bachelor's degree (Computer Science, Information Services, or related study preferred)
3+ year(s) of experience in data analysis, data entry, or related data study
Familiarity with navigating and loading data into SAP Fieldglass, Beeline, VNDLY, Coupa Contingent Workforce, 3Story Software, Vector, Wand, or equivalent
3+ year(s) of experience in a corporate setting
Desired Skills
Strong Microsoft Excel skills (familiarity with XLOOKUP, VLOOKUP, Pivot Tables)
Working knowledge of relational database concepts (preferred)
Strong detail-oriented organizational skills
Working knowledge of Microsoft Office (Word and PowerPoint)
Working knowledge of internet browsers (Google Chrome, Microsoft Edge)
Clear verbal and written communication skills
Ability to set and manage expectations around deliverables and timelines
Willingness to speak up, ask for help, and receive coaching
Ability to break technical concepts into component parts and problem solve
Demonstrated ability to speak on technical issues to audiences of various sizes and technical abilities
Ability to learn complex concepts through formal training, on the job coaching, and self-study
Ability to travel up to 4 times a year for annual conference and on-site client visits
Our Shared Values
Everyone matters
We take care of each other
Leadership is here to serve
High character and hard work above all else
Always know where you stand
Excellence isn’t the expectation, it’s the Monument standard
Show more
Show less","Data Migration, Data Analysis, Data Entry, SQL, Microsoft Excel, SAP Fieldglass, Beeline, VNDLY, Coupa Contingent Workforce, 3Story Software, Vector, Wand, Microsoft Office, Microsoft Word, Microsoft PowerPoint, Google Chrome, Microsoft Edge, Relational Databases","data migration, data analysis, data entry, sql, microsoft excel, sap fieldglass, beeline, vndly, coupa contingent workforce, 3story software, vector, wand, microsoft office, microsoft word, microsoft powerpoint, google chrome, microsoft edge, relational databases","3story software, beeline, coupa contingent workforce, data entry, data migration, dataanalytics, google chrome, microsoft edge, microsoft excel, microsoft office, microsoft powerpoint, microsoft word, relational databases, sap fieldglass, sql, vector, vndly, wand"
Data Analyst (Candidate must currently reside in the state of IN),"Veridian Tech Solutions, Inc.","Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-candidate-must-currently-reside-in-the-state-of-in-at-veridian-tech-solutions-inc-3717244969,2023-12-17,Casa Grande,United States,Associate,Hybrid,"Job ID-720257
Title- Data Analyst (Candidate must currently reside in the state of IN)
Location-Hybrid work in Indianapolis, IN 46204 (Candidate must currently reside in the state of IN)
Durtaion-7-8 Months contract+
Candidate must be paid $43/hr
Note
This position requires a passed fingerprint and back ground check
There will be additional background checks and security training once hired/on site.
This position is a hybrid work schedule, with at least two days a week on site. Resource will be issued equipment the first day, onsite.
Client will perform a Teams interview. Resource must have the ability to display themselves on video during the interview.
Resource will be responsible for the activities of Data Analyst for the State of Indiana's child support system.
Candidate must currently reside in the state of IN.
Job Duties
Design, develop, and format polished excel and tableau data visualizations (reports and dashboards) to support business requirements. Focus on Visualization Creation using data to perform reporting and direct analysis. Perform analysis that may be descriptive, diagnostic, predictive, or prescriptive. Responsible for maintaining and developing excel and tableau dashboards and reports, preparing data visualizations, and using data to forecast or guide business activity. Present data in a fashion that is easy to understand with proper documentation and user testing for successful adoption. Publish tableau workbooks to appropriate QA, production, and public servers. Meet project deadlines and requirements. Participate in and contribute to the CSB-IT Data Services Team. Performs other related duties as assigned.
Job Requirements
Experience with Analyzing data, identify trends, interpret results, and prepare excel and tableau reports and data visualizations for bureau leadership, county partners, and other stakeholders. Experience with developing, maintaining, and managing tableau reports and dashboards. Experience with creating Analytical, Time Series, Metrics, Rankings, Statistical, Christmas Tree, Tracking, and Expenditure reports. Experience with using excel data analysis functions, scripts, reports, and charts. Experience with writing SQL queries to get data from different databases like DB2 Z/OS; AWS Aurora and AWS RDS and Data Warehouse. Experience in working with Cross Technical, Functional and Business Teams. Capable to understand business requirements and develop reports based on requirements. Capable to work individually and work in a team environment
Skill
Experience using Excel Data analysis Functions, scripts, reports, and charts
Experience with Data Preparation for Reports and Dashboards
Business Intelligence experience, Data Warehouse and Reporting
SQL and PL/SQL
Experience with developing, maintaining, and managing Tableau Report/Dashboard using Tableau Desktop
Show more
Show less","Excel, Tableau, Data Visualization, Data Analysis, Reporting, Data Warehousing, SQL, PL/SQL, AWS Aurora, AWS RDS, Data Preparation, Business Intelligence","excel, tableau, data visualization, data analysis, reporting, data warehousing, sql, plsql, aws aurora, aws rds, data preparation, business intelligence","aws aurora, aws rds, business intelligence, data preparation, dataanalytics, datawarehouse, excel, plsql, reporting, sql, tableau, visualization"
data process analyst,Skiltrek,"Hartford, CT",https://www.linkedin.com/jobs/view/data-process-analyst-at-skiltrek-3785514319,2023-12-17,Enfield,United States,Mid senior,Hybrid,"Job Summary
Skiltrek has an immediate opening for a highly motivated Database Administration Senior Advisor to join their dynamic and growing team. All qualified candidates are encouraged to apply!
Responsible for daily review and resolution of duplicate/linkage tasks using the Initiate TM Inspector Tool.
Review all types of tasks based on the established workflow hierarchy (tasks: potential overlays, potential duplicates, potential linkages and review identifier).
Updates tasks as appropriate according to established procedures and guidelines, while meeting set expectations with metrics.
location: Hartford, Connecticut
job type: Contract
salary: $15 - 19 per hour
work hours: 8am to 5pm
education: Bachelors
Responsibilities
Responsible for daily review and resolution of duplicate/linkage tasks using the Initiate TM Inspector Tool.
Review all types of tasks based on the established workflow hierarchy (tasks: potential overlays, potential duplicates, potential linkages and review identifier).
Updates tasks as appropriate according to established procedures and guidelines, while meeting set expectations with metrics.
Provide timely and clear communication regarding production issues and business impacts, as needed.
Qualifications
Experience level: Manager
Education: Bachelors
Skills
Data Analysis
About Us
Skiltrek is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US.
At Skiltrek, we promise you the perfect opportunity of building technical excellence, understand business performance and nuances,
be abreast with the latest happenings in technology world and enjoy a satisfying work life balance.
Skiltrek is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender,
race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law.
Skiltrek is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Show more
Show less",Data Analysis,data analysis,dataanalytics
Senior Big Data Engineer,Integral Ad Science,"New York, NY",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-integral-ad-science-3743883334,2023-12-17,Rutherford,United States,Mid senior,Hybrid,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we’re looking for A
Senior Data Engineer
to join our team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!
What you’ll get to do
:
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Work on Big Data technologies such as Spark, Spark streaming, Kafka, NoSql, EMR & other AWS tech stack
Contribute to the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming & batch ETL and RESTful APIs
Provide leadership, work collaboratively, and be a mentor in an awesome team
You should apply if you have most of this
:
Bachelors or Masters in Computer Engineering, Computer Science, Electrical Engineering or related field
5+ years of recent hands-on in object oriented language (Java, Scala)
5+ years of experience designing and building data pipelines and data-intensive applications
Experience using Big Data frameworks (e.g.,Spark, Flink) , databases (e.g., NoSql, Delta Tables, Mysql / Postgres) for complex data assembly and transformation
Strong knowledge of collections, multi-threading, JVM memory model, etc.
In-depth understanding of algorithms, performance, scalability, and reliability in a Big Data setting
Solid understanding of OLTP and OLAP systems, database fundamentals
Solid knowledge of SQL
Experience in full software development, Agile, and CI/CD
Experience building production level systems in AWS cloud environment
What puts you over the top:
Experience with Spark streaming or Flink
Familiarity with messaging frameworks like Kafka and / or NoSql databases (Aerospike, Cassandra)
Orchestrating data pipelines using tools such as Airflow
New York Applicants: The salary range for this position is $100,466 - $172,228. Actual pay may vary based on experience or geographic location.
About Integral Ad Science
Integral Ad Science (IAS) is a leading global media measurement and optimization platform that delivers the industry’s most actionable data to drive superior results for the world’s largest advertisers, publishers, and media platforms. IAS’s software provides comprehensive and enriched data that ensures ads are seen by real people in safe and suitable environments, while improving return on ad spend for advertisers and yield for publishers. Our mission is to be the global benchmark for trust and transparency in digital media quality. For more information, visit integralads.com.
Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.
California Applicant Pre-Collection Notice:
We collect personal information (PI) from you in connection with your application for employment or engagement with IAS, including the following categories of PI: identifiers, personal records, commercial information, professional or employment or engagement information, non-public education records, and inferences drawn from your PI. We collect your PI for our purposes, including performing services and operations related to your potential employment or engagement. For additional details or if you have questions, contact us at compliance@integralads.com.
To learn more about us, please visit
http://integralads.com/
and
https://muse.cm/2t8eGlN
Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership.
Show more
Show less","Java, Scala, Spark, Spark streaming, Kafka, NoSql, EMR, AWS, Relational databases, RESTful APIs, Agile, CI/CD, Airflow, Cloud environment, SQL, OLTP, OLAP","java, scala, spark, spark streaming, kafka, nosql, emr, aws, relational databases, restful apis, agile, cicd, airflow, cloud environment, sql, oltp, olap","agile, airflow, aws, cicd, cloud environment, emr, java, kafka, nosql, olap, oltp, relational databases, restful apis, scala, spark, spark streaming, sql"
Healthcare Data Analyst,MetroPlusHealth,"New York, NY",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-metroplushealth-3690872265,2023-12-17,Rutherford,United States,Mid senior,Hybrid,"Empower. Unite. Care.
MetroPlusHealth
is committed to empowering New Yorkers by uniting communities through care. We believe that Health care is a right, not a privilege. If you have compassion and a collaborative spirit, work with us. You can come to work being proud of what you do every day.
About NYC Health + Hospitals
MetroPlus
Health
provides the highest quality healthcare services to residents of Bronx, Brooklyn, Manhattan, Queens and Staten Island through a comprehensive list of products, including, but not limited to, New York State Medicaid Managed Care, Medicare, Child Health Plus, Exchange, Partnership in Care, MetroPlus Gold, Essential Plan, etc. As a wholly-owned subsidiary of NYC Health + Hospitals, the largest public health system in the United States, MetroPlus
Health's
network includes over 27,000 primary care providers, specialists and participating clinics. For more than 30 years, MetroPlus
Health
has been committed to building strong relationships with its members and providers to enable New Yorkers to live their healthiest life.
Position Overview
The purpose of this position is to utilize MetroPlusHealth’s data to provide financial analysis related to the provision, payment, and reporting of health care services. The position is also responsible for research, collection, analysis, and presentation of the Plan’s utilization and financial data. Candidates interested must work with multiple departments within the Plan in an effort to coordinate analytics to enhance operational efficiencies.
Job Description
Prepare financial analyses on various aspects of the financial operations of the Plan including revenues, expenses, utilization, and membership.
Prepare financial analyses of medical services from large-shared databases utilizing queries and multiple-level extracts.
Participate in the production and quality review of monthly and regulatory reports, including but not limited to the Medicaid Managed Care Operating Report (MMCOR).
Prepare projection analyses on claims and utilization, including underlying trends and risk analysis.
Prepare documentation and policies and procedures of all reporting and analyses.
Review and update categorization logic for claims analysis.
Prepare various monthly accrual deliverables to the Accounting Department
Organize and manage large and varied data sets.
Analyze healthcare data to optimize business operations.
Convert data into usable information that is easy to understand.
Communicate findings using data visualization and detailed reports.
Create quarterly reporting package for executives.
Communicate monthly with Actuaries for data gathering, QA, communication, and completion of the Unpaid Claims Liability (UCL).
Collect, investigate, and analyze data to identify patterns, trends, insights, and solutions.
Special projects and other related duties as assigned.
Minimum Qualifications
Bachelor’s degree required
5+ years of relevant experience in business, health care, consulting, insurance, government, or related industry
Proficiency in Microsoft Excel, Access and Word is required
SQL Query skill is required.
Experience with statistical systems and data report writers.
Insurance operations experience preferred
Professional Competencies
Integrity and Trust
Customer Focus
Functional/Technical skills
Excellent verbal, written and mathematical skills.
Show more
Show less","Microsoft Excel, Microsoft Access, Microsoft Word, SQL, Data visualization, Data analysis, Healthcare data, Business operations, Data management, Reporting, Financial analysis, Medicare, Medicaid","microsoft excel, microsoft access, microsoft word, sql, data visualization, data analysis, healthcare data, business operations, data management, reporting, financial analysis, medicare, medicaid","business operations, data management, dataanalytics, financial analysis, healthcare data, medicaid, medicare, microsoft access, microsoft excel, microsoft word, reporting, sql, visualization"
Lead Database Engineer (Snowflake),A2Zxperts,"Jersey City, NJ",https://www.linkedin.com/jobs/view/lead-database-engineer-snowflake-at-a2zxperts-3584722425,2023-12-17,Rutherford,United States,Mid senior,Hybrid,"Our customer in Financial Services industry is looking for a Lead Database Engineer (Snowflake)
Title: Lead Database Engineer (Snowflake)
Locations: Primary: Jersey City, NJ / Tampa, FL / Dallas, TX | Secondary: McLean, VA
Skills: Snowflake, Cloud Data Platform, Distributed Database Administration, T-SQL, Data Warehouses, Data Marts, AWS CLI, AWS alerting/monitoring
Experience level: Mid-senior
Experience required: 5 Years
Education level: Bachelor's degree
Job function: Information Technology
Industry: Financial Services
Type: Full-time
Qualifications
Bachelor's degree in computer science, or related field or related experience
Experience with Snowflake Cloud Data Platform
3- 5 years of experience in distributed database Administration required
Demonstrated strong proficiency in T-SQL, database replication, backup
Experienced working with Data Warehouses and Data Marts
Strong knowledge of user access management, password management, and compliance requirements
Demonstrated proficiency in index design, query plan optimization, and analysis
Demonstrated knowledge and hands-on experience with AWS alerting/monitoring tools
Experience with AWS CLI and Networking
Experience in managing high transactional large databases
Ability to multi-task required and provide rapid support in production
Familiar with SDLC and Release Management
Must Have
Snowflake Cloud Data Platform
Distributed Database Administration
T-SQL, database replication, backup
Data Warehouses and Data Marts
Ability to manage high transactional large databases
For more details, feel free to reach out at Seema.c@a2zxperts.com
Show more
Show less","Snowflake Cloud Data Platform, Distributed Database Administration, TSQL, Data Warehouses, Data Marts, AWS CLI, AWS alertmonitoring, Database Replication, Backup, Index Design, Query Optimization, Release Management, SDLC","snowflake cloud data platform, distributed database administration, tsql, data warehouses, data marts, aws cli, aws alertmonitoring, database replication, backup, index design, query optimization, release management, sdlc","aws alertmonitoring, aws cli, backup, data marts, data warehouses, database replication, distributed database administration, index design, query optimization, release management, sdlc, snowflake cloud data platform, tsql"
"Lead Data Engineer, Data Productivity",SiriusXM,"New York, NY",https://www.linkedin.com/jobs/view/lead-data-engineer-data-productivity-at-siriusxm-3768755997,2023-12-17,Rutherford,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are. This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You'll Make An Impact
We are seeking a highly skilled and motivated Lead Data Productivity Engineer to join our dynamic team at SiriusXM. As a Lead Data Productivity Engineer, you will play a key role in designing, building, and maintaining the tools and services used by our data professionals in order to effectively drive the business in a data-driven manner. The ideal candidate will have a strong background in cloud technologies, data engineering, infrastructure as code, API design, and experience applying software engineering and DevOps best practices.
What You'll Do
Design, develop, and maintain tools and services that empower data professionals to streamline their workflows and enhance productivity.
Collaborate with cross-functional teams to understand data analysis, engineering, and modeling needs and translate them into effective and user-friendly solutions.
Implement best practices for optimizing data processing workflows, ensuring efficient utilization of resources, and minimizing latency in data-related tasks.
Identify and address bottlenecks in existing tools and services to improve overall system performance.
Integrate data productivity tools with existing data infrastructure and platforms, fostering seamless collaboration among teams.
Develop and implement automation solutions to streamline repetitive tasks and enhance the efficiency of data processes.
Create comprehensive documentation for tools and services, ensuring that users have access to clear and concise instructions.
Provide training and support to data professionals, enabling them to effectively leverage the tools and services developed.
Work closely with data scientists, engineers, and analysts to understand their requirements and challenges, fostering a collaborative and innovative environment.
Communicate project status, issues, and solutions effectively to stakeholders and team members.
What You’ll Need
Bachelor's degree in a relevant technical field (Computer Science, Information Technology, etc.), and 5+ years' career experience
Proven experience in software development, with a focus on tools and services for data professionals.
Proficiency in programming languages such as Python, Scala, or Java.
Experience with infrastructure as code tools such as CDK or Terraform.
Experience with big data technologies (e.g., Apache Spark, Hadoop) and data processing frameworks.
Knowledge of data storage solutions, database systems, and data warehousing.
Familiarity with machine learning frameworks and model development is a plus.
Excellent problem-solving skills and a proactive approach to addressing challenges.
Strong communication and collaboration skills.
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-11-87
Show more
Show less","Cloud technologies, Data engineering, Infrastructure as code, API design, Software engineering, DevOps, Python, Scala, Java, CDK, Terraform, Apache Spark, Hadoop, Data storage solutions, Database systems, Data warehousing, Machine learning, Problemsolving, Communication, Collaboration","cloud technologies, data engineering, infrastructure as code, api design, software engineering, devops, python, scala, java, cdk, terraform, apache spark, hadoop, data storage solutions, database systems, data warehousing, machine learning, problemsolving, communication, collaboration","apache spark, api design, cdk, cloud technologies, collaboration, communication, data engineering, data storage solutions, database systems, datawarehouse, devops, hadoop, infrastructure as code, java, machine learning, problemsolving, python, scala, software engineering, terraform"
"Data Architect:: 3 days on site in NYC, NY - Local Candidates only",Steneral Consulting,"New York, NY",https://www.linkedin.com/jobs/view/data-architect-3-days-on-site-in-nyc-ny-local-candidates-only-at-steneral-consulting-3661739912,2023-12-17,Rutherford,United States,Mid senior,Hybrid,"Hi,
Please find attached Job Description. If you are interested please do share with me your updated resume or call me on 3023356895
Job Title:-
Data Architect
Work Locatio
n:-
3 days on site in NYC, NY - Local Candidates only
Duration
12
+ Months
Work Authorization:-
Citizen, GC, GC-EAD
Interview Process:-
Video
Communication must be 10/10
Recent Financial Experience Needed
Detailed write up required - What they are doing on their current project and how they feel fit them self for this role.
Job Description
Seeking a Data Architect/Engineer to join the Application and Data Engineering department.
The candidate will be responsible for helping to drive, analyze, design, architect, and execute on a strategy to modernize the data platform across Non-Financial Risk.
This includes understanding the current data plane and business use cases that drive data decisions.
This all drives to the goal of efficient ways to enable enhanced data discovery, governance, metadata management, and integration that enables data to be sourced from hybrid on prem and cloud environments delivered over multiple possible mechanisms in an efficient method to the consumer under a Data Fabric/Mesh architecture
Key Responsibilities
Understand current data platform within NFRT
Work with development teams and business partners to understand data use cases.
Design and architect and execute on Data Fabric / Data Mesh solution
Evaluate tools, (external or internal) and aid in the development of tools to enable and support the Data platform
Design and aid in development of a data discovery, observability, governance platform.
Skills / Qualifications
Bachelor's degree in Computer Science, Software Engineering, Information Technology, or related field.
10+ years of work experience in Information Technology and 5+ years working as Data Architect or Data Engineer.
Experience with designing and implementing modern data platforms (Data Fabric, Data Mesh, Data Hubs, Data driven environments)
Experience in highly complex data environments with large data volumes including cloud / Hybrid implementations
Experience with the design of data catalogs / dictionaries driven by active metadata
Strong communication skills both verbal and written. Capable of collaborating effectively across a variety of IT and Business groups
Strong problem-solving skills. Ability to identify where focus is needed and bring clarity to business objectives, requirements, and priorities
Experience with Semantic DBs and Knowledge graphs
Experience with the establishment of Data Fabrics and Data Meshes
Experience with Collibra
Experience with one or more Cloudera, Apache Hive, Azure data lake, Snowflake
Experience with one or more of Databricks, Spark, Airflow, Kafka, Talend, informatica
Knowledge of one or more Postgres, DB2, MSSQL, MongoDB
Experience with Analytics and BI Tableau, Power BI
Experience with Data science and AIML Jupyter Notebook, Dataiku
Hi,
Hope you are doing great,
I am
Kumar Harshit,
a recruitment lead with
Steneral Consulting.
I have an urgent requirement mentioned below. If you find yourself comfortable with the requirement please reply back with your updated resume and I will get back to you or I would really appreciate if you can give me a call back at my contact number
302-549-3240.
Job Title:-
Data Architect
Location: NYC (Hybrid)
Duration: 12 Months
Job Description
Seeking a Data Architect/Engineer to join the Application and Data Engineering department. The candidate will be responsible for helping to drive, analyze, design, architect, and execute on a strategy to modernize the data platform across Non-Financial Risk. This includes understanding the current data plane and business use cases that drive data decisions. This all drives to the goal of efficient ways to enable enhanced data discovery, governance, metadata management, and integration that enables data to be sourced from hybrid on prem and cloud environments delivered over multiple possible mechanisms in an efficient method to the consumer under a Data Fabric/Mesh architecture
Key Responsibilities
Understand current data platform within NFRT
Work with development teams and business partners to understand data use cases.
Design and architect and execute on Data Fabric / Data Mesh solution
Evaluate tools, (external or internal) and aid in the development of tools to enable and support the Data platform
Design and aid in development of a data discovery, observability, governance platform.
Skills / Qualifications
Bachelor's degree in Computer Science, Software Engineering, Information Technology, or related field.
10+ years of work experience in Information Technology and 5+ years working as Data Architect or Data Engineer.
Experience with designing and implementing modern data platforms (i.e. Data Fabric, Data Mesh, Data Hubs, Data driven environments)
Experience in highly complex data environments with large data volumes including cloud / Hybrid implementations
Experience with the design of data catalogs / dictionaries driven by active metadata
Strong communication skills both verbal and written. Capable of collaborating effectively across a variety of IT and Business groups
Self-starter. Strong problem-solving skills. Ability to identify where focus is needed and bring clarity to business objectives, requirements, and priorities
Experience with Semantic DBs and Knowledge graphs
Experience with the establishment of Data Fabrics and Data Meshes
Experience with Collibra
Experience with one or more Cloudera, Apache Hive, Azure data lake, Snowflake
Experience with one or more of Databricks, Spark, Airflow, Kafka, Talend, informatica
Knowledge of one or more Postgres, DB2, MSSQL, mongoDB
Experience with Analytics and BI Tableau, Power BI
Experience with Data science and AIML Jupyter Notebook, Dataiku
Kumar Harshit
Associate Team Lead-Talent Acquisition -North America
Desk: 302-549-3240
kumar.harshit@steneral.com
Show more
Show less","Data Architecture, Data Engineering, Data Fabric, Data Mesh, Data Hubs, DataDriven Environments, Cloud Computing, Hybrid Implementations, Data Catalogs, Data Dictionaries, Data Governance, Data Discovery, Metadata Management, Data Integration, Semantic Databases, Knowledge Graphs, Collibra, Cloudera, Apache Hive, Azure Data Lake, Snowflake, Databricks, Apache Spark, Apache Airflow, Apache Kafka, Talend, Informatica, PostgreSQL, DB2, MSSQL, MongoDB, Tableau, Power BI, Jupyter Notebook, Dataiku","data architecture, data engineering, data fabric, data mesh, data hubs, datadriven environments, cloud computing, hybrid implementations, data catalogs, data dictionaries, data governance, data discovery, metadata management, data integration, semantic databases, knowledge graphs, collibra, cloudera, apache hive, azure data lake, snowflake, databricks, apache spark, apache airflow, apache kafka, talend, informatica, postgresql, db2, mssql, mongodb, tableau, power bi, jupyter notebook, dataiku","apache airflow, apache hive, apache kafka, apache spark, azure data lake, cloud computing, cloudera, collibra, data architecture, data catalogs, data dictionaries, data discovery, data engineering, data fabric, data governance, data hubs, data integration, data mesh, databricks, datadriven environments, dataiku, db2, hybrid implementations, informatica, jupyter notebook, knowledge graphs, metadata management, mongodb, mssql, postgresql, powerbi, semantic databases, snowflake, tableau, talend"
Senior Data Engineer,Saransh Inc,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-data-engineer-at-saransh-inc-3714182573,2023-12-17,Jasper,United States,Mid senior,Onsite,"Responsibilities & Qualifications
10+ Overall industry experience
7+ years' experience with building large-scale big data applications development
Bachelors in computer science or related field
Provide technical leadership in developing data solutions and building frameworks.
Expertise in solutions for processing large volumes of data, using data processing tools and Big Data platforms.
Experience building Data Lake, EDW and data applications using Azure, AWS and
Hands-on experience in cloud Data stack (preference is Azure)
Understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS, SQL experience
Hands-on experience with major programming/scripting languages like Java
Java experience with OOPS concepts, multithreading
It's nice to have experience deploying code on containers.
Conduct code reviews and strive for improvement in software engineering quality.
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience in successfully delivering big data projects using Kafka, Spark
Exposure working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus.
In depth knowledge of design principles and patterns
Show more
Show less","Big data development, Java, OOP concepts, Multithreading, Cloud Data stack, Azure, AWS, Spark, Kafka, NoSQL, Cassandra, HBase, DynamoDB, Elastic Search, PCI Data, Design principles, Design patterns","big data development, java, oop concepts, multithreading, cloud data stack, azure, aws, spark, kafka, nosql, cassandra, hbase, dynamodb, elastic search, pci data, design principles, design patterns","aws, azure, big data development, cassandra, cloud data stack, design patterns, design principles, dynamodb, elastic search, hbase, java, kafka, multithreading, nosql, oop concepts, pci data, spark"
Data Devops Engineer,Experfy,"Boston, MA",https://www.linkedin.com/jobs/view/data-devops-engineer-at-experfy-3590300673,2023-12-17,Jasper,United States,Mid senior,Onsite,"Opportunity Description
Job Purpose
As a senior software engineer you will work as an integral part of our Cloud Data
DevOps team within the Corporate Digital Data Architecture, Operations and
Governance Team. The ideal candidate has extensive DevOps knowledge and
experience and has previously been part of a DevOps team running an analytical
ecosystem in modern cloud-based big data systems in a fast-paced, agile
environment. The Data DevOps Engineer will focus on processing engine development as well as design, development, automation and optimization of its usage in each data platforms across the group.
Education Background:
Master's degree in Computer Science, or equivalent Experience and Knowledge:
4 or more years working in DevOps, software development
2 or more years' experience provisioning, operating, and managing AWS environments
Strong background in Linux/Unix administration and scripting
Strong background in Hadoop, Spark, Python, Java
Extensive experience with a public cloud provider, ideally Amazon Web Services
Strong understanding of Continuous Integration and Continuous Delivery and infra as code principles and practice
Ability to use a wide variety of open source technologies and cloud services
Research & investigative skills
Strong experience with SQL and NoSQL data stores
Software process automation with popular scripting languages (i.e. Python)
Experience developing code in at least one high-level programming language with code quality and code security at heart
Experience in automation and testing via scripting/programming
Understanding of Agile and other development processes and methodologies
Source, build/release, and configuration management in a continuous integration & delivery environment
Application performance analysis and monitoring
Knowledge of best-practice security and networking techniques for an Internet-facing system
Highly Desired, Skills Include:
Experience with automation and configuration management using Ansible, or an equivalent
Terraform, Docker, Openshift
Java development experience
Technology vendor management
Amazon Web Services certification highly desired
Requirements
Key responsabilities
Liaise in a geographically dispersed team across the globe
Participate in a continuous delivery pipeline to fully automate deployment of the highly available cloud data platforms that supports multiple teams/projects
Build tools for deployment, monitoring and operations. Troubleshoot and resolve issues in our development, test and production environments
Work with platform architects on software and system optimizations, helping to identify and remove potential performance bottlenecks
Stay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools
Understand, implement, and automate security controls, governance processes, and compliance validation
Design, manage, and maintain tools to automate operational processes
Show more
Show less","DevOps, Cloud Data, AWS, Linux/Unix, Hadoop, Spark, Python, Java, Continuous Integration, Continuous Delivery, Infrastructure as Code, Open Source Technologies, Cloud Services, SQL, NoSQL, Scripting Languages, Agile Development, Source Code Management, Application Performance Analysis, Monitoring, Security Techniques, Networking Techniques, Ansible, Terraform, Docker, Openshift, Java Development, Technology Vendor Management","devops, cloud data, aws, linuxunix, hadoop, spark, python, java, continuous integration, continuous delivery, infrastructure as code, open source technologies, cloud services, sql, nosql, scripting languages, agile development, source code management, application performance analysis, monitoring, security techniques, networking techniques, ansible, terraform, docker, openshift, java development, technology vendor management","agile development, ansible, application performance analysis, aws, cloud data, cloud services, continuous delivery, continuous integration, devops, docker, hadoop, infrastructure as code, java, java development, linuxunix, monitoring, networking techniques, nosql, open source technologies, openshift, python, scripting languages, security techniques, source code management, spark, sql, technology vendor management, terraform"
Data Engineer (Snowflake),VRK IT Vision Inc.,"Durham, NC",https://www.linkedin.com/jobs/view/data-engineer-snowflake-at-vrk-it-vision-inc-3645100458,2023-12-17,Jasper,United States,Mid senior,Onsite,"Data Engineer(Snowflake)
Hybrid Mode(5 Days in Month)
Westlake,TX/Durham,NC
Job Description
Hands on Snowflake Hands on OBIEE/OAS Oracle PL/SQL and Informatica ETL AWS Looking for 7-8+ years of experience (can be more junior)
Required Skills
Basic Qualification :
Additional Skills
Background Check :Yes
Show more
Show less","Snowflake, OBIEE/OAS, Oracle PL/SQL, Informatica ETL, AWS","snowflake, obieeoas, oracle plsql, informatica etl, aws","aws, informatica etl, obieeoas, oracle plsql, snowflake"
Data Engineer – Houston,Soho Square Solutions,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-%E2%80%93-houston-at-soho-square-solutions-3654419833,2023-12-17,Jasper,United States,Mid senior,Onsite,"Description
Role: Data Engineer
Required Skills: Java/Spark/Databricks/Kubernetes (+ AWS Cloud Exp)
Industry : Financial Services; FS - Banking & Capital Markets
Other Information: Intermediate (4-7 yrs exp)
Location: Houston, TX
Key Responsibilities
The job function of a Java Spark Databricks AWS data engineer involves working with data engineering technologies and platforms to design, develop, and maintain data solutions on the AWS cloud platform.
Here are some key responsibilities and tasks associated with this role:
Data Ingestion: Develop and implement processes to extract data from various sources, such as databases, APIs, and files, and load it into the data lake or data warehouse using Java, Spark, and AWS tools
Data Transformation: Perform data cleansing, validation, and transformation using Spark and Java programming, ensuring data quality and consistency. Apply business rules and data processing techniques to prepare the data for analysis and consumption
Data Pipeline Development: Design and build scalable data pipelines using AWS services like AWS Glue, AWS Data Pipeline, or Apache Airflow. Develop ETL (Extract, Transform, Load) processes to move and transform data between different systems and data stores
Data Modeling: Create and maintain data models and schemas, including dimensional and relational models, to support data storage and retrieval requirements. Optimize data structures for performance and efficiency.
Performance Optimization: Fine-tune Spark applications and data processing workflows to improve performance and reduce processing time. Optimize resource utilization, data partitioning, and data caching strategies.
Data Security and Governance: Implement data security and access controls to ensure data privacy and compliance with regulatory requirements. Apply data governance practices to manage metadata, data lineage, and data cataloging.
Monitoring and Troubleshooting: Monitor data pipelines and Spark jobs for performance, errors, and issues. Troubleshoot and resolve data-related problems, such as data quality issues or performance bottlenecks.
Collaboration and Documentation: Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand data requirements and deliver data solutions. Document data pipelines, processes, and system configurations.
Cloud Infrastructure Management: Configure and manage AWS services like Amazon EMR (Elastic MapReduce), Amazon S3 (Simple Storage Service), and AWS Glue for data processing, storage, and management. Monitor and optimize cloud resources for cost efficiency.
Continuous Improvement: Stay updated with emerging technologies, industry trends, and best practices related to data engineering and cloud computing. Continuously enhance skills and knowledge to improve data engineering processes and solutions.
Show more
Show less","Java, Apache Spark, Databricks, Kubernetes, AWS, Data Ingestion, Data Transformation, Data Pipeline Development, Data Modeling, Performance Optimization, Data Security, Data Governance, Monitoring, Troubleshooting, Collaboration, Documentation, Cloud Infrastructure Management, ETL (Extract Transform Load), Amazon EMR (Elastic MapReduce), Amazon S3 (Simple Storage Service), AWS Glue, AWS Data Pipeline, Apache Airflow","java, apache spark, databricks, kubernetes, aws, data ingestion, data transformation, data pipeline development, data modeling, performance optimization, data security, data governance, monitoring, troubleshooting, collaboration, documentation, cloud infrastructure management, etl extract transform load, amazon emr elastic mapreduce, amazon s3 simple storage service, aws glue, aws data pipeline, apache airflow","amazon emr elastic mapreduce, amazon s3 simple storage service, apache airflow, apache spark, aws, aws data pipeline, aws glue, cloud infrastructure management, collaboration, data governance, data ingestion, data pipeline development, data security, data transformation, databricks, datamodeling, documentation, etl extract transform load, java, kubernetes, monitoring, performance optimization, troubleshooting"
Data Engineer,People Tech Group Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3784968381,2023-12-17,Jasper,United States,Mid senior,Onsite,"Title: Lead Data Engineer
Duration: Fulltime
Location: Seattle, WA
Job Responsibilities:
• Need Senior Data engineer with 10+ years of experience in the field.
• With Informatica, Python, Bigdata, Strong on Advance SQL, AWS EMR, AWS Glue, redshift
• Nice to have: Big Data Experience (PySpark).
• Experience leading and influencing the data strategy of your team or organization.
• Experience in Advance SQL.
• Experience in Apache airflow.
• Experience with at least one massively parallel processing data technology such as Redshift, Spark or Hadoop based big data solution .
• Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, EC2, DynamoDB, Data-pipeline and other big data technologies .
• Coding proficiency in one modern programming language Python.
Nice to have:
• Experience with data modeling, data warehousing, Operations and building ETL pipelines.
Show more
Show less","Data Engineering, Python, Informatica, Big Data, SQL, AWS EMR, AWS Glue, Redshift, Apache Airflow, Hadoop, Data Warehousing, Analytics, Redshift, EC2, DynamoDB, PySpark, ETL","data engineering, python, informatica, big data, sql, aws emr, aws glue, redshift, apache airflow, hadoop, data warehousing, analytics, redshift, ec2, dynamodb, pyspark, etl","analytics, apache airflow, aws emr, aws glue, big data, data engineering, datawarehouse, dynamodb, ec2, etl, hadoop, informatica, python, redshift, spark, sql"
Senior Data Engineer,Databricks,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-databricks-3728714328,2023-12-17,Jasper,United States,Mid senior,Onsite,"GAQ223R68
While candidates in the listed locations are encouraged for this role, we are open to remote candidates in other locations.
At Databricks Information Technology, we are a product led organization transforming the way we work from how easy it is to use our IT services to the applications we develop that help us scale seamlessly in face of incredible growth.
You will influence technological decision-making for business teams future data, analysis, and reporting needs. The role supports the business’s daily operations inclusive of troubleshooting of data-intelligence warehouse environment and job monitoring. You will guide the business in identifying data needs and delivering mechanisms for acquiring and reporting such information and addressing the actual needs. You will gather and maintain best practices that can be adopted in big data stacking and sharing across the business. You will provide expertise to the business regarding data analysis, reporting, data warehousing, and business intelligence. You will report to the Director, Business Systems.
The Impact You Will Have
Design/Strategy: The Senior Data Engineer designs and supports the business’s database and table schemas for new and existing data sources for the data warehouse. Creates and supports the ETL in order to facilitate the accommodation of data into the warehouse. In this capacity, the Senior Data Engineer designs and develops systems for the maintenance of the business’s data warehouse, ETL processes, and business intelligence.
Collaboration: The role that the Senior Data Engineer plays is highly collaborative and, as such,works closely with data analysts, data scientists, and other data consumers within the business in an attempt to gather and populate data warehouse table structure, which is optimized for reporting. The Senior Data Engineer also works closely with other disciplines/departments and teams across the business in coming up with simple, functional, and elegant solutions that balance data needs across the business
Analytics: The Senior Data Engineer plays an analytical role in quickly and thoroughly analyzing business requirements for reporting and analysis and subsequently translating the emanating results into good technical data designs. In this capacity, the Senior Data Engineer establishes the documentation of reports, develops, and maintains technical specification documentation for all reports and processes.
What We Look For
5+ years of related experience with a Bachelor’s degree; or 3 years and a Master’s degree; or a PhD without experience; or equivalent work experience.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. In depth knowledge of Model and Design of DB schemas for read and write performance.
Extensive working knowledge of API or Stream based data extraction processes like Salesforce API and Bulk API is must.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with building data pipeline from various business applications like Salesforce, Marketo, NetSuite, Workday etc.
Experience with big data tools: Hadoop, Spark, Kafka, Spark & Kafka Streaming, Python, Scala, Talend etc.
Knowledge of BI Tools like Tableau, Looker etc
Benefits
Comprehensive health coverage including medical, dental, and vision
401(k) Plan
Equity awards
Flexible time off
Paid parental leave
Family Planning
Gym reimbursement
Annual personal development fund
Work headphones reimbursement
Employee Assistance Program (EAP)
Business travel accident insurance
Mental wellness resources
Pay Range Transparency
Databricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.
Zone 1 Pay Range
$115,400—$204,300 USD
About Databricks
Databricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.
Our Commitment to Diversity and Inclusion
At Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.
Compliance
If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.
Show more
Show less","Data Warehousing, Data Analysis, Business Intelligence, SQL, Relational Databases, ETL, Hadoop, Spark, Kafka, Python, Scala, Talend, Tableau, Looker","data warehousing, data analysis, business intelligence, sql, relational databases, etl, hadoop, spark, kafka, python, scala, talend, tableau, looker","business intelligence, dataanalytics, datawarehouse, etl, hadoop, kafka, looker, python, relational databases, scala, spark, sql, tableau, talend"
Data Engineer,"Resource Informatics Group, Inc","Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-resource-informatics-group-inc-3729305488,2023-12-17,Jasper,United States,Mid senior,Onsite,"Must have information during submission
Candidates Full Name, Mobile Numbers, Location, Work Authorization, Rate and LinkedIn profile
I can reach out to candidates faster if I don't have to go back and forth with you for details.
-Client Location - Chicago, IL - 2-3 days onsite - Local candidates only
-Rate - 60-70/hour Corp
-Duration - 6+ months
-Interview Process - Video interview + in-person interview
Must Have -
5+ experience hands-on experience in data engineering in Azure environment.
Expert in Python and SQL
Experience working with Databricks and Data Factory
Experience with Big Data preferably Apache Spark
Experience with loading data using any MPP architecture
Domain - Healthcare
Nice to Have - NA
Show more
Show less","Data Engineering, Azure, Python, SQL, Databricks, Data Factory, Apache Spark, MPP Architecture, Healthcare","data engineering, azure, python, sql, databricks, data factory, apache spark, mpp architecture, healthcare","apache spark, azure, data engineering, data factory, databricks, healthcare, mpp architecture, python, sql"
Senior Data Engineer,Eliassen Group,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-eliassen-group-3768718825,2023-12-17,Jasper,United States,Mid senior,Remote,"**100% Remote**
Our healthcare client is looking for a Senior Data Engineer to join their team.
Due to client requirement, applicants must be willing and able to work on a w2 basis. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.
Responsibilities of the Senior Data Engineer:
Spearhead the establishment of data engineering practices, standards, and processes
Design, develop, and maintain scalable and reliable data pipelines utilizing Microsoft Azure services
Create and maintain data models, ensuring data accuracy, consistency, and efficiency within the Microsoft Azure environment Optimize data structures for performance and ease of access
Collaborate with cross-functional teams to integrate data from various systems and sources
Establish data quality controls and monitoring processes using Microsoft Azure services
Continuously improve data pipeline and query performance within the Microsoft Azure framework
Implement and maintain data security and privacy best practices
Ensure data compliance with relevant regulations and standards across diverse geographical regions
Create and maintain clear and comprehensive documentation of data pipelines, data models, and processes
Provide guidance and mentorship to junior data engineers
Keep up-to-date with industry trends and emerging technologies in data engineering
Requirements of the Senior Data Engineer:
A Bachelor's degree in Computer Science or a related field and at least 10 years of experience in data engineering
Strong experience with ETL processes, data modeling, and data warehousing within Microsoft Azure
Proficiency in one or more programming languages commonly used in data engineering
Hands-on experience with Microsoft Azure and its associated data services
Please be advised- If anyone reaches out to you about an open position connected with Eliassen Group, please confirm that they have an Eliassen.com email address and never provide personal or financial information to anyone who is not clearly associated with Eliassen Group. If you have any indication of fraudulent activity, please contact InfoSec@eliassen.com.
Job ID: 381032
Show more
Show less","Data Engineering, Microsoft Azure, ETL, Data Modeling, Data Warehousing, Programming Languages, Data Security, Data Privacy, Data Quality, Data Pipelines, Data Structures, Data Compliance, Data Documentation, Mentorship, Industry Trends, Emerging Technologies","data engineering, microsoft azure, etl, data modeling, data warehousing, programming languages, data security, data privacy, data quality, data pipelines, data structures, data compliance, data documentation, mentorship, industry trends, emerging technologies","data compliance, data documentation, data engineering, data privacy, data quality, data security, data structures, datamodeling, datapipeline, datawarehouse, emerging technologies, etl, industry trends, mentorship, microsoft azure, programming languages"
Data Engineer,ASK Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-at-ask-consulting-3774797986,2023-12-17,Jasper,United States,Mid senior,Remote,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: Data Engineer
Location: Remote
Duration: 12 months
Pay Rate: $55/hr.
Job Description:
Required Skills:
Data Engineering
AI ML
GCP
Python and SQL
Great communication skills
Big Data
Roles And Responsibilities:
3+ years of professional experience as a data engineer
3+ years working with Python and SQL and GCP Cloud.
Experience with state of the art machine learning algorithms such as deep neural networks,support vector machines, boosting algorithms, random forest etc. preferred
Experience conducting advanced feature engineering and data dimension reduction in Big Data environment is preferred
Strong SQL skills in Big Data environment (Hive/ Impala etc.) a plus
Master's Degree in Computer Science & Data Science
Previous Company - Any Bank, Ecommerce
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Data Engineering, AI/ML, GCP, Python, SQL, Great communication skills, Big Data, Advanced feature engineering, Data dimension reduction, Hive, Impala, Master's Degree in Computer Science & Data Science","data engineering, aiml, gcp, python, sql, great communication skills, big data, advanced feature engineering, data dimension reduction, hive, impala, masters degree in computer science data science","advanced feature engineering, aiml, big data, data dimension reduction, data engineering, gcp, great communication skills, hive, impala, masters degree in computer science data science, python, sql"
Data Engineer,Revolution Technologies,United States,https://www.linkedin.com/jobs/view/data-engineer-at-revolution-technologies-3786512897,2023-12-17,Jasper,United States,Mid senior,Remote,"RevUp Your Career as a Data Engineer!!
Job Title:
Data Engineer
Job Location:
Remote
Summary
Revolution Technologies is hiring a data engineer for our leading biotechnology client! Launch your career to the next level in the technology industry with this top-rated employer! This role is a long term W2 Contract.
Key Skills and Experience Required
Expertise in SQL
Strong experience working with PowerBI
Proficient in Python or R
Job Description
Lead and participate in design sessions with Engineering teams, Data Scientists, Product Managers, business, and Information Technology (IT) stakeholders, that result in documentation for data processing, storage and delivery solutions.
Understand business capability needs and processes as they relate to IT solutions through partnering with Product Managers and business and functional IT stakeholders, and apply this knowledge to defining business problems that need to be solved Initiate and lead evaluation of new technologies including performing POCs and presenting results to others, with a goal of providing technical recommendations.
Help the team establish and improve processes and methodologies, like SCRUM or Kanban, and/or lead piloting new ones.
Implement data solutions according to design documentation using a variety of tools and programming languages, like AWS and GCP cloud solutions, Kafka, SQL and non-SQL databases, Python, Scala, Go etc., and follow teams established processes and methodologies;
Facilitate and participate in code reviews, retrospectives, functional and integration testing and other team activities focused on improving quality of delivery.
Provide reliable estimates for large scale projects.
Facilitate various cross team efforts, like Scrum of Scrums and Release Planning, focused on large scale roadmap alignments, sharing information, solving broad variety of problems, or improving processes.
Create and maintain design and code documentation in GitHub, Haystack, SharePoint and/or other repositories used by the team.
Qualifications
Bachelor's Degree required, Masters degree preferred
9 years of applicable experience, 6 years with a Masters degree
Strong level of experience building data models using R, Python or other statistical and/or mathematical programming packages.
Strong Power BI and Power app development skills.
Strong experience with engineering data intensive software using streaming and resource-based design principles. Technical expertise and advocate of software development best practices (Version Control, Code Documentation and Review, Cloud Based Sequence Analysis, Database Management)
Experience with at least one cloud native data warehouse database, BigQuery, Redshift, Snowflake etc.
Experience with Cloud native technologies for processing data at scale and delivering data pipelines including Kafka, Spark, AWS SQS, Lambda, Step functions, ECS, Fargate, Athena, BigQuery, GCP PubSub, Cloud functions, Cloud Run, Kubernetes.
Experience with DevOps methodologies including Infrastructure as Code concept.
Demonstrated experience with global multi-disciplinary teams and learning the science Creative, proactive, bold and out-of-box thinking.""
Why Revolution Technologies?
About the company
Revolution Technologies - Making a difference across the nation with premier consulting, staffing, and hiring services.
Revolution Technologies, founded in 1993, provides best-in-class services that make a positive difference in the lives of our clients
and
our talent. We provide strategic management consulting; ERP and EA consulting; staff augmentation, contract, and contingent staffing; contract-to-permanent and permanent placement; as well as payrolling, recruitment process outsourcing, and human resources advisement services.
Let’s talk about benefits
Revolution Technologies is proud to offer some of the best rates in the market. Revolution is also pleased to provide a comprehensive benefits package including medical, dental, vision, short term disability, access to a health savings account, tuition reimbursement, scholarship opportunities, 401k, life insurance, supplemental insurance, and paid time off.
Equal Opportunity Employer
Revolution Technologies, LLC is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.
Revolution Technologies: Turn to us!
Show more
Show less","SQL, PowerBI, Python, R, AWS, GCP, Kafka, SQL, NoSQL, Scala, Go, GitHub, Haystack, SharePoint, BigQuery, Redshift, Snowflake, Spark, AWS SQS, Lambda, Step functions, ECS, Fargate, Athena, GCP PubSub, Cloud functions, Cloud Run, Kubernetes, DevOps, Infrastructure as Code","sql, powerbi, python, r, aws, gcp, kafka, sql, nosql, scala, go, github, haystack, sharepoint, bigquery, redshift, snowflake, spark, aws sqs, lambda, step functions, ecs, fargate, athena, gcp pubsub, cloud functions, cloud run, kubernetes, devops, infrastructure as code","athena, aws, aws sqs, bigquery, cloud functions, cloud run, devops, ecs, fargate, gcp, gcp pubsub, github, go, haystack, infrastructure as code, kafka, kubernetes, lambda, nosql, powerbi, python, r, redshift, scala, sharepoint, snowflake, spark, sql, step functions"
AWS Data Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/aws-data-engineer-at-steneral-consulting-3734763302,2023-12-17,Jasper,United States,Mid senior,Remote,"Share only one candidate
LinkedIn is must
100% Remote.
Visa - GC/USC only
AWS Expereince must be very strong.
Position:
- AWS Data Engineer
Location: Chanhassen, MN - 100% Remote
Contract - 6 months contract to hire
This is a contract with a potential contract to hire. Would like someone with green card or US Citizen. Prefer they are local or near MN but will consider non local as this is 100% remote. Must be strong communicator.
This role is not fully approved yet but would like to see some resumes now as he is planning to hire for it but might take a few weeks on this so candidates should have some patience.
This person should have 5-7+ years or more of data engineering on AWS platform leveraging some of the many AWS services that exist in the ecosystem.
Should have AWS tooling exp- like Glue, Redshift, data analytics.
Should have AWS native services exp.
Any prior background around dev ops or ETL development before going into AWS is helpful but not at all required as that is not the focus of the job. We were just talking about what other experience could be useful.
Certification in AWS Data Analytics would be a plus.
Show more
Show less","AWS, Glue, Redshift, Data Analytics, ETL, Dev Ops","aws, glue, redshift, data analytics, etl, dev ops","aws, dataanalytics, dev ops, etl, glue, redshift"
"Senior Data Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Elizabeth, NJ",https://www.linkedin.com/jobs/view/senior-data-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783184605,2023-12-17,Manhattan,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Senior Data Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Elizabet-SeniorDataAnal.006
Show more
Show less","Generative AI, Python, JavaScript, JSON, R, OOP, Data Science, Technical Functionalities, Written and Verbal English Communication, Proactive Communication, Multiple Priorities Management, Stakeholder Management, EdTech, AI, Subject Matter Experts, Learning Science, Data Analytics, HighPerformance Coaching","generative ai, python, javascript, json, r, oop, data science, technical functionalities, written and verbal english communication, proactive communication, multiple priorities management, stakeholder management, edtech, ai, subject matter experts, learning science, data analytics, highperformance coaching","ai, data science, dataanalytics, edtech, generative ai, highperformance coaching, javascript, json, learning science, multiple priorities management, oop, proactive communication, python, r, stakeholder management, subject matter experts, technical functionalities, written and verbal english communication"
Sr. Operations Data Analyst (SAS Required) - Remote,EmblemHealth,"New York, NY",https://www.linkedin.com/jobs/view/sr-operations-data-analyst-sas-required-remote-at-emblemhealth-3700609212,2023-12-17,Manhattan,United States,Mid senior,Remote,"Summary Of Position
Manage portfolio of developed and scheduled reports for Claims, FRU, G&A, Provider and UM Data.
Interface with internal operations / external departmental customers to define requirements of analyses and reports.
Transform requirements into actionable, high-quality deliverables.
Perform periodic and ad-hoc operations data analysis to measure performance and conduct root cause analysis for Claims, FRU, G&A, Provider and UM data.
Compile, analyze and provide reporting that identifies and defines actionable information or recommends possible solutions for corrective actions.
Partner with other Operations areas as needed to provide technical and other support in the development, delivery, maintenance, and enhancement of analytical reports and analyses.
Collaborate with Operations Tower Leaders in identifying and recommending operational performance metrics; map metrics against targets and the company’s operational plans and tactical/strategic goals to ensure alignment and focus.
Serve as a liaison with peers in other departments to ensure data integrity.
Code and schedule reports using customer business requirements from Claims, FRU, G&A, Provider and UM data.
Principal Accountabilities
Conduct operational data analyses to identify root causes; develop actionable information (recommendations, conclusions, and possible solutions); produce reports to evaluate operational efficiencies and effectiveness.
Prepare dashboards and other management reports, soliciting information from business teams and serve as liaison for their submissions; ensure quality control; provide oversight to staff when necessary.
Identify and collect internal historical data; research and collect external benchmark data; devise more efficient and accurate approaches to vet and prepare metric reports; use sound reasoning and judgment for identifying and applying appropriate analytical approach.
Recommend and implement accuracy, efficiency, and productivity enhancements.
Maintain documentation library to promote efficient knowledge transfer of data collection strategies and data quality protocols.
Work with other areas as needed to ensure recommended solutions meet business requirements.
Manage multiple, simultaneous team-based projects along with other individually assigned projects.
Provide support in developing & expanding the scope of dashboards and other management reports for distribution to middle and upper management; organize and maintain report methodology documentation.
Communicate and collaborate with internal and external stakeholders as needed to support overall EmblemHealth objectives.
Perform other related tasks/projects as directed or required.
Education, Training, Licenses, Certifications
Bachelor’s Degree in Business, Data Management, or other related quantitative analysis field of study required
Relevant Work Experience, Knowledge, Skills, And Abilities
4 – 6+ years of relevant work experience including Data Analysis and reporting required
Business Intelligence Experience – Cognos or Tableau; proficiency with SAS required
Project management experience preferred
Proficient with MS Office (Word, Excel, Access, PowerPoint, Outlook, Teams, etc.) required
Experience working with large volumes of data required
Energy, drive and passion for End-to-End excellence and customer experience improvement required
Excellent collaborative skills and the ability to influence management decisions required
Strong problem solving and analytical skills that be applied across all types of business problems required
Strong communication skills (verbal, written, presentation, interpersonal, facilitation) with all audiences required
Additional Information
Requisition ID: 1000001321
Hiring Range: $63,000-$110,000
Show more
Show less","Data Analysis, SAS, Tableau, Cognos, MS Office, Report Generation, Data Management, Business Intelligence, Project Management, Root Cause Analysis, Quality Control, Problem Solving, Communication, Data Collection, Data Quality, Benchmarking","data analysis, sas, tableau, cognos, ms office, report generation, data management, business intelligence, project management, root cause analysis, quality control, problem solving, communication, data collection, data quality, benchmarking","benchmarking, business intelligence, cognos, communication, data collection, data management, data quality, dataanalytics, ms office, problem solving, project management, quality control, report generation, root cause analysis, sas, tableau"
Senior/Staff Data Analyst,Grow Therapy,"New York, NY",https://www.linkedin.com/jobs/view/senior-staff-data-analyst-at-grow-therapy-3764637719,2023-12-17,Manhattan,United States,Mid senior,Remote,"About us:
Grow Therapy is on a mission to serve as the trusted partner for therapists growing their practice, and patients accessing high-quality care. Powered by technology, we are a three-sided marketplace that empowers providers, augments insurance payors, and serves patients. Following the mass increase in depression and anxiety, the need for accessibility is more important than ever. To make our vision for mental healthcare a reality, we’re building a team of entrepreneurs and mission-driven go-getters. Since launching in February 2021, we’ve empowered over ten thousand therapists and hundreds of thousands of clients across the country and insurance landscape. We’ve raised over $100mm of funding from Transformation Capital, TCV, and SignalFire.
What
you'll be doing:
Develop data-driven business insights and work with cross-functional partners to find opportunities and recommend prioritization of product, growth, and optimization initiatives
Collaborate with data scientists and engineers to build and improve on the availability, integrity, accuracy, and reliability of data logging and data pipelines
Refine ambiguous questions and generate new hypotheses about the product and business through a deep understanding of the data, our customers, and our business
Build and enable others to build business critical dashboards by developing scalable datasets in close collaboration with stakeholders.
Design experiments across our multi-sided marketplace and interpret the results to draw detailed and impactful conclusions.
You’ll Be a Good Fit If:
You have 5+ years proven experience in using analytics to tackle complex business problems that cross multiple product/project areas and teams, ideally in a high-growth, fast-paced environment.
You have advanced SQL and basic Python expertise, intermediate understanding of experimental design (such as A/B experiments) and causal inference methods.
You are a wizard at building business critical dashboards/data visualizations (ideally within Looker, with strong LookML expertise)
You love data storytelling, creating a compelling narrative by extracting insights from data, and summarizing learnings / takeaways for executives.
You enjoy communicating effectively and building relationships with both technical and non-technical stakeholders.
The salary range for this role is 128k-170k
Benefits
The chance to drive impact within the mental healthcare landscape from day one
Comprehensive health insurance plans, including dental and vision
Our dedication to mental health guides our culture. Wellness benefits include (but are not limited to):
Flexible working hours and location (remote OR in-office, your choice!)
Generous PTO
Company-wide winter break
Mental health mornings (2 hours each week)
Team meditation
Wellness Stipend
In-office lunch and biweekly remote lunch on us!
Continuous learning opportunities
Competitive salary
The opportunity to help build a rapidly scaling start-up organization by taking strong ownership of your work, mentorship, and our unbounded leadership opportunities
Grow Therapy is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Show more
Show less","SQL, Python, Looker, LookML, Data analysis, Data visualization, Experimental design, Causal inference, Data storytelling, Data logging, Data pipelines, Dashboards, Hypothesis generation, Business insights, Prioritization","sql, python, looker, lookml, data analysis, data visualization, experimental design, causal inference, data storytelling, data logging, data pipelines, dashboards, hypothesis generation, business insights, prioritization","business insights, causal inference, dashboard, data logging, data storytelling, dataanalytics, datapipeline, experimental design, hypothesis generation, looker, lookml, prioritization, python, sql, visualization"
"Manager, Data Engineering",GlossGenius,"New York, NY",https://www.linkedin.com/jobs/view/manager-data-engineering-at-glossgenius-3782402254,2023-12-17,Manhattan,United States,Mid senior,Remote,"About GlossGenius
GlossGenius is building an ecosystem enabling entrepreneurs to succeed. We empower small business owners to focus on being creators, not admins, by offering a range of business management tools including booking and scheduling, marketing, analytics, payment processing and much more.
Tens of thousands of small business owners have chosen to rely on GlossGenius every day to run their entire set of business operations. With its powerful, intuitive platform, GlossGenius is some part a fintech company, some part an SMB software company, while its vibrant, distinguished brand makes it some part a consumer company.
About The Role
As the Data Engineering Manager, you will lead a team of data engineers responsible for building and maintaining the foundational infrastructure of our data platform. This is a hands-on role with a small team, where you will have the opportunity to provide technical leadership and serve as the bridge between our Data and Product Engineering organizations.
You will report to the Head of Data and can work out of our office in New York City or work remotely from anywhere in the continental US.
What You'll Do
Lead and grow a small team of Data Engineers, including career and performance management
Partner with the Analytics Engineering team build and maintain the roadmap for our data stack and infrastructure
Evolve our data stack as GlossGenius' Engineering organization migrates from a monolith to a service-oriented architecture
Facilitate the sharing of data from our warehouse for production and other commercial use cases
Be a champion of data privacy and quality
What We're Looking For
5+ years of experience as a software engineer, data engineer, or other data-related role; at least 2 years of management experience
Strong Python and SQL skills and knowledge of data modeling, ETL/ELT development principles, data warehousing concepts, and software engineering concepts
Willingness to make build vs. buy decisions for critical pieces of our data infrastructure
Experience managing a cloud data warehouse and a workload scheduling and orchestration framework, preferably Snowflake and Airflow
Familiarity with the other technologies in our data stack (including Fivetran, Stitch, Segment, dbt, Looker and Hex) preferred
Familiarity with languages and technologies used by our Engineering team (including Ruby, Kotlin, React, and React Native) preferred
A demonstrated self-starter with strong communication and project management skills
Benefits & Perks
Flexible PTO
Competitive health & dental insurance options, with premiums partially covered by GG
Fertility and adoption benefits via Carrot and Kindbody
Generous, fully-paid parental leave policy
401k benefit - employees are eligible to contribute starting day 1 of employment
Professional Development - employees receive a yearly stipend for approved learning and educational-related expenses
Pre-tax commuter benefits
Dependent Care FSA
Home office stipend
Team Bonding opportunities - including in person retreats and virtual events throughout the year
The starting base salary for this role in New York, California, and Washington is between $190,000-$220,000 + target equity + benefits. The base salary offered is dependent upon many factors including skills, experience, location, and education. The base pay range is subject to change and may be modified in the future.
In order to enter the NY office or participate in any in-person events, all employees must show proof of vaccination against COVID-19. For those individuals who are unable to be vaccinated, GlossGenius will engage in an interactive process to determine a reasonable accommodation.
At GlossGenius, we celebrate our differences and are committed to creating a workplace where all employees feel supported and empowered to do their best work. We believe this benefits not only our employees but our product, customers, and community as well. GlossGenius is proud to be an Equal Opportunity and Affirmative Action Employer.
Personal Information: Notice at Collection for Employees and Applicants
Show more
Show less","Python, SQL, Data modeling, ETL/ELT development principles, Data warehousing concepts, Software engineering concepts, Cloud data warehouse management, Workload scheduling and orchestration frameworks, Snowflake, Airflow, Fivetran, Stitch, Segment, dbt, Looker, Hex, Ruby, Kotlin, React, React Native, Communication skills, Project management skills","python, sql, data modeling, etlelt development principles, data warehousing concepts, software engineering concepts, cloud data warehouse management, workload scheduling and orchestration frameworks, snowflake, airflow, fivetran, stitch, segment, dbt, looker, hex, ruby, kotlin, react, react native, communication skills, project management skills","airflow, cloud data warehouse management, communication skills, data warehousing concepts, datamodeling, dbt, etlelt development principles, fivetran, hex, kotlin, looker, project management skills, python, react, react native, ruby, segment, snowflake, software engineering concepts, sql, stitch, workload scheduling and orchestration frameworks"
Principal Deep Learning Data Scientist,Burtch Works,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/principal-deep-learning-data-scientist-at-burtch-works-3785583792,2023-12-17,Manhattan,United States,Mid senior,Remote,"Our client, a leading equipment retailer looking for a Principal Deep Learning Data Scientist focused on Applied Machine Learning to join their growing team. This Data Scientist will work with advanced analytics to solve customer acquisition business problems and work with stakeholders to communicate insights from data. This is a great hands-on Data Science role with great benefits! Base pay can range from 140K- 215K depending on level plus bonus. Sponsorship is available.
Let's talk if you have:
MS (preferred) in Mathematics, Applied Analytics, Computer Science of Engineering
At least five years of Advanced Skills in deep neural networks and cloud computing solutions
Think and program with data on the fly in Python and SQL
MLOps/ML Engineering/deployment experience
Operationalized things through Docker, Github, GithubActions, Kubernetes, Airflow tech stack OR equivalent.
Experience building recommendation engines
Keywords
– Python, Spark SQL, Data Science, Applied Machine Learning, Deep Learning, LLM, Generative AI, NLP
Show more
Show less","Mathematics, Applied Analytics, Computer Science, Engineering, Deep Neural Networks, Cloud Computing, Python, SQL, MLOps, ML Engineering, Deployment, Docker, Github, GithubActions, Kubernetes, Airflow, Recommendation Engines, Data Science, Machine Learning, Deep Learning, Generative AI, Natural Language Processing","mathematics, applied analytics, computer science, engineering, deep neural networks, cloud computing, python, sql, mlops, ml engineering, deployment, docker, github, githubactions, kubernetes, airflow, recommendation engines, data science, machine learning, deep learning, generative ai, natural language processing","airflow, applied analytics, cloud computing, computer science, data science, deep learning, deep neural networks, deployment, docker, engineering, generative ai, github, githubactions, kubernetes, machine learning, mathematics, ml engineering, mlops, natural language processing, python, recommendation engines, sql"
Senior Data Engineer,TEKsystems,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-teksystems-3782479995,2023-12-17,Manhattan,United States,Mid senior,Remote,"Senior   Data Engineer
Quick Facts
100% Remote
EST Hours 9am-6pm
Rate: $84/hr Max
Top Skills
Databricks - 3+ years experience
Troubleshooting and optimization of data pipelines
There are a series of data sources across business units in different systems i.e. AWS, legacy systems, oracle etc. which they are trying to integrate into Databricks.
AWS – Current pipelines and data are in AWS so candidate needs to have experience with working in cloud AWS (Lambda, Redshift, Athena)
Python - Existing pipelines are in Python with some legacy pipelines in SQL, but main tool used is Python
Job Summary
Our client is seeking an experienced Cloud Data Engineer with significant DevOps and SysOps experience in AWS and Databricks.
The company is headquartered in NYC, but this role is envisioned as a remote position and will focus on providing engineering support for a variety of analytics, data architecture and data science endeavors. In this role you will primarily lead the design and evolution of the BI delta lake, while working closely with the rest of the BI team to build a flexible and robust data infrastructure.
You will co-develop and maintain an evolving number of data pipelines to enable advanced reporting capabilities across our Subscription, Publishing, Digital, Consumer Products, Corporate and Marketing teams.
This role will function as The Client’s primary subject matter expert on creating and maintaining efficient data pipelines to clean, conform and deliver large datasets, both structured and unstructured, to downstream destinations.
As the team’s ETL expert, there will be ample opportunities to conduct complex data wrangling/munging operations using a variety of AWS and other industry standards tools including, Python, Pandas, PySpark, Glue Studio, Lambdas, Advanced SQL, etc.
You will also work with APIs, create complex materialized views and scheduled queries, as well as develop error logging routines and associated alerts.
You can expect to work closely with the Enterprise Architect and Data Architect to design and deploy best practice data governance operations as well
Our team is small but capable, and looking for an intellectually curious engineer with excellent communication skills, who is interested in working very closely to co-invent solutions to complex problems.
Our department is also an environment that encourages constantly learning and cross-functional growth, so you can expect to explore and expand your knowledge in areas like supervised and unsupervised machine learning, graph databases, AI and applied statistics/mathematics.
Responsibilities
Develop batch and real-time data pipelines, and lead the integration of the Client's many 1st, 2nd, and 3rd party data sources, while working closely with other engineering services such as the personalization and testing teams.
Create data catalogs and validation routines to ensure quality and correctness of key operational datasets and metrics in real time.
Build integrations with organic and paid media platforms to effectively deliver data to support the optimization of various KPIs.
Partner with Data Architect to build data infrastructure that enables activation, attribution and segmentation capabilities across growth, retention and marketing objectives
Collaborate with lifecycle and product marketing teams to democratize insights that will drive subscriber engagement using data driven solutions.
Coach other engineers and BI team members on best practices and technical concepts of building large scale, robust and well governed data platforms.
Basic Qualifications
Excellent communicator and collaborator, able to apply technical acumen to drive business outcomes.
A natural team player with a willingness and desire to engage in cross training in a small team environment
4+ years in big data and/or data intensive projects in industry or academic/research settings·4+ years of deep experience developing in Python
Expert Level SQL developer – with an emphasis on Redshift but capable across multiple other transactional databases such as PostgreSQL, MySQL, Mongo dB, etc.
Significant experience developing with PySpark
Experience engineering big-data solutions using technologies like Redshift, Spark, S3, DynamoDB, Glue Studio and AWS SageMaker, Glue Studio, SageMaker Data Wrangler, etc.
Demonstrated understanding of data engineering tools and practices, including platforms like Airflow, Databricks, Snowflake, and Jenkins
Experience with deploying and running AWS-based data solutions and comfortable deploying tools such as Cloud Formation, AWS Glue, Kinesis, DynamoDB, Athena and Lambda
Demonstrated experience applying Master Data Management including metadata management, data lineage, and the principles of data governance
Ability to deliver technical solutions in the face of challenging data conditions.
Preferred Qualifications
Experience implementing marketing technology stacks including real-time messaging and attribution pipelines.
Experience leveraging DMPs (BlueKai) and CDPs (mParticle, Segment) to create deterministic user profiles that can be leveraged across a variety of applications.
Experience integrating with ML platforms and experimentation frameworks.
Familiarity with front-end development frameworks and experience in full stack development a plus.
Familiarity with binary data serialization formats such as Parquet, Avro, andThrift.
More Information
Candidate will be facilitating reporting and strategy, wrangling existing data sources
Role is to help clean with packaging and distribute existing data
Candidate will be collecting, cleaning and distributing data to different business units.
Data feeds include - marketing, transaction management, content metadata
Candidate will be helping to optimize and troubleshoot pipelines for e.g. Transactions, unsubscribe/subscribe digital subscriptions, user engagement with App, what people are reading/doing, how people are getting into the app etc.
Goal would be for this person to come in and free up Insights Manager to focus on other areas
Looking for candidate to have ability to take data and bring it up in dashboard - they use Tableau
Looking for candidate to create business insights and outcomes which they can advise rest of business to implement
Longer term vision is looking at getting new data in Data lake, mature Data lake, implement new features i.e. new catalogs, server less etc.
About TEKsystems
We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.
The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.
Show more
Show less","Databricks, Troubleshooting, Optimization, Data Pipelines, AWS, Redshift, Athena, Lambda, Python, SQL, PySpark, Glue Studio, Pandas, Cloud Formation, Kinesis, DynamoDB, Glue Studio, SageMaker, Snowflake, Jenkins, Apache Airflow, Data Management, Metadata Management, Data Lineage, Data Governance, RealTime Messaging, Attribution, BlueKai, mParticle, Segment, Parquet, Avro, Apache Thrift, Tableau","databricks, troubleshooting, optimization, data pipelines, aws, redshift, athena, lambda, python, sql, pyspark, glue studio, pandas, cloud formation, kinesis, dynamodb, glue studio, sagemaker, snowflake, jenkins, apache airflow, data management, metadata management, data lineage, data governance, realtime messaging, attribution, bluekai, mparticle, segment, parquet, avro, apache thrift, tableau","apache airflow, apache thrift, athena, attribution, avro, aws, bluekai, cloud formation, data governance, data lineage, data management, databricks, datapipeline, dynamodb, glue studio, jenkins, kinesis, lambda, metadata management, mparticle, optimization, pandas, parquet, python, realtime messaging, redshift, sagemaker, segment, snowflake, spark, sql, tableau, troubleshooting"
Data Engineer (Databricks),Lawrence Harvey,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-databricks-at-lawrence-harvey-3757675923,2023-12-17,Manhattan,United States,Mid senior,Hybrid,"**UNFORTUNATELY, AT THIS TIME APPLICANTS MUST BE ELIGIBLE TO WORK IN THE UNITED STATES WITHOUT SPOSNORSHIP. WE CANNOT PROVIDE ANY TYPE OF VISA SPONSORSHIP OR TRANSFER AT THIS TIME**
**THIS IS A FULL TIME OPPORTUNITY AND
NOT AVAILABLE ON C2C
**
Location
: NYC (Tuesday-Thursday in office)
This opportunity is with one of the world's leading entertainment/media companies worldwide across production, news, development, and marketing. They have a dynamic portfolio of DTC products which include streaming TV and more.
The DTC group is looking to expand the decision sciences team and is looking to add a Sr Data Engineer to join the team. This person will be responsible for creating a connected data ecosystem that unleashes the power of streaming data
Qualifications:
2-5 years experience
At least 1 year experience with
Databricks is required
Ability to design and build data pipelines to ingest data, integrate data from multiple data sources and create aggregated data sets for reporting needs
Hands-on experience working with Spark, Python, Pyspark, Scala
Experience in GCP, AWS or Azure
**UNFORTUNATELY, AT THIS TIME APPLICANTS MUST BE ELIGIBLE TO WORK IN THE UNITED STATES WITHOUT SPOSNORSHIP. WE CANNOT PROVIDE ANY TYPE OF VISA SPONSORSHIP OR TRANSFER AT THIS TIME**
**THIS IS A FULL TIME OPPORTUNITY AND
NOT AVAILABLE ON C2C
**
Show more
Show less","Databricks, Spark, Python, Pyspark, Scala, GCP, AWS, Azure","databricks, spark, python, pyspark, scala, gcp, aws, azure","aws, azure, databricks, gcp, python, scala, spark"
Data Analyst (Reward),Orion Group,"Aberdeen, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-reward-at-orion-group-3784574387,2023-12-17,Aberdeen, United Kingdom,Mid senior,Onsite,"Our O&G Operator client are seeking a Data Analyst focusing on the Reward function on a 12 month contract. This will be based full time in their Aberdeen offices.
Responsibilities:
• Executive Pay: Support the [Head of Reward] with delivery of the Executive Pay Strategy including production of the annual Directors Remuneration Report.
• Remuneration Committee: Support the [Head of Reward] with preparation for Remuneration Committee meetings, assisting with data collation and production of papers required for those committee meetings.
• Governance and Compliance: Support with equal pay audits and the production of regulatory reports.
• Annual Reward Cycle: Provide extensive support with the annual bonus and salary review process.
• Remuneration Benchmarking: Oversee the participation in market surveys; undertaking analysis of the compensation market. Support the [Head of Reward] in providing insight into compensation trends, market competitiveness and internal equity.
• Pension and Benefits: Support delivery of the Company-wide pension and benefits including overseeing the relevant consultants and reporting and compliance thereon.
• Variable pay: Support the [Head of Reward] with management of the annual bonus plan.
• Share schemes & Deferred Awards: Manage the relationship with share schemes consultants to deliver shares and other awards under Employee share schemes, Deferred Share Plans and Long-term Incentive Schemes.
• Job Evaluation: Support delivery of the organisational job evaluation and grading framework and advise HRBP’s and the business on job classification and reward structures for new roles.
• HR systems and Reporting: Manage dashboard and report production and analysis as required.
• Support payroll operations to ensure that these are aligned to regulations, ensuring compliance with all employment taxes.
Experience & Qualifications:
• Data management and reporting experience.
• Experience in delivering operational reward processes, including the annual compensation review.
• Relevant HR experience, including benchmarking and job evaluation methodologies.
• Ideally experience of pension and insured benefits management.
• Strong numerical and data analytics skills and insights experience, coupled with strong excel skills.
• Strong communication skills with experience with working across a wide range of stakeholders preferred.
• Project management skills are a strong plus.
• Knowledge of job evaluation methodologies (e.g. Hay, Radford, WTW or Mercer)
• Analytical and attention to detail, allowing sound judgment on complex issues.
• Creative thinker, pragmatic problem solver and focused.
• A team player who thrives in a challenging environment
Show more
Show less","Data analysis, Data management, Reporting, HR systems, Payroll, Excel, Communication, Project management, Job evaluation, Hay, Radford, WTW, Mercer","data analysis, data management, reporting, hr systems, payroll, excel, communication, project management, job evaluation, hay, radford, wtw, mercer","communication, data management, dataanalytics, excel, hay, hr systems, job evaluation, mercer, payroll, project management, radford, reporting, wtw"
Senior Data Product Engineer,Energy Jobline,"Aberdeen, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-product-engineer-at-energy-jobline-3773895119,2023-12-17,Aberdeen, United Kingdom,Mid senior,Onsite,"Our client is looking for a Senior Data Product Engineer for a contract position, located in Aberdeen.
Responsibilities
Support and deliver project activities related to Data Domain Data Products, supporting the overall data strategy and adoption of data platforms such as the Data Domain Platform (DDP) and Quantum.
Support the design, development and testing of data pipelines to make information and data available to key stakeholders and technical interfaces with a view to ensuring maximum efficiency and reliability.
Provide support for Data Products to rectify defects in a timely fashion as and when identified.
Ensure that good design and solution documentation on Data Product data lineage and transformations are maintained and kept up to date. Ensure each Data Product is catalogued in company systems (Purview, Data Galaxy).
Support the delivery of the Digital Twin systems data sets providing support as required to ensure tasks are delivered effectively.
Work on innovative use cases/projects to improve data quality and accessibility including for AI use
Develop and maintain ETL pipelines and workflows
Prepare and transform data for migration\consumption, implementing data quality checks
Create technical documentation
Build scripts and queries to ease/improve data management activity (Python, SQL). Manage GitHub libraries.
Engage with data domain owners to ensure mandatory scopes are planned and completed within defined timescales.
Identify, design and implement internal process improvements, automating manual processes and optimising data delivery
Work closely with the internal Data Governance and IS Teams to ensure compliance with best practice and alignment with strategy and processes.
Work with stakeholders to assist and provide input with data-related requirements to enhance business performance and support their data needs.
Promote the use of the Data Platforms and tooling for Data Transparency. Communicate progress to key stakeholders.
Provides the engineering expertise to data domains and consuming teams on how to create and use data products.
Provide guidance and mentoring for colleagues and drive upskilling and awareness in data understanding, interpretation and insights
Requirements
At least a BSc degree in Engineering domain, Data management or Computer Science disciplines with 10 years professional experience or 5+ years work experience in an oil Exploration & Production environment
Technical skills and a good understanding of data modelling, management, and analysis techniques. Experience of working within a Power BI based reporting environment.
Excellent knowledge of ETL tools and scripting: Python, SQL, Great Expectations
Experience of cloud platforms, features, and capabilities (Microsoft Azure components) specifically Azure Data Factory \ Azure Functions
Python, SQL scripting; working experience in large-scale data wrangling with relational databases and/or Spark.
Good level of knowledge of data governance and control tools (cataloguing, lineage, Purview, quality, etc.).
Behavioral skills: self-motivated, autonomy, open-mindedness, adaptability, achievement orientation, team player
Show more
Show less","Data Product Engineering, Data Domain Data Products, Data Domain Platform (DDP), Quantum, Data Pipelines, ETL Pipelines, Data Quality Checks, Python, SQL, GitHub, Data Governance, Data Cataloguing, Data Lineage, Purview, Data Galaxy, Digital Twin Systems, AI, Power BI, Microsoft Azure, Azure Data Factory, Azure Functions, Spark, Relational Databases, Data Governance Tools, Data Control Tools, Data Cataloguing Tools, Data Lineage Tools, Data Quality Tools","data product engineering, data domain data products, data domain platform ddp, quantum, data pipelines, etl pipelines, data quality checks, python, sql, github, data governance, data cataloguing, data lineage, purview, data galaxy, digital twin systems, ai, power bi, microsoft azure, azure data factory, azure functions, spark, relational databases, data governance tools, data control tools, data cataloguing tools, data lineage tools, data quality tools","ai, azure data factory, azure functions, data cataloguing, data cataloguing tools, data control tools, data domain data products, data domain platform ddp, data galaxy, data governance, data governance tools, data lineage, data lineage tools, data product engineering, data quality checks, data quality tools, datapipeline, digital twin systems, etl pipelines, github, microsoft azure, powerbi, purview, python, quantum, relational databases, spark, sql"
Senior Data Engineer (Aberdeen-based),Aize,"Aberdeen, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-aberdeen-based-at-aize-3731403501,2023-12-17,Aberdeen, United Kingdom,Mid senior,Hybrid,"Description
What you tell your friends you do
“As a senior data engineer, I help build the data platform on which our product sits ”
What You Will Really Be Doing
🚀 Continuously improve the streamlined pipeline for contextualizing, validating and ingesting the customer data that drives our SaaS product (and working to integrate the data into a stable and extensible pipeline).
🎉 Work with the product teams to help them understand how they can use available data for building new user experiences.
💡 Learn from domain experts on real world engineering data and use this knowledge to create data quality analyses that builds confidence in our customers.
🛠 Identify opportunities for and creating self-service tools that make data onboarding easier.
How You Will Be Doing This
You will be a part of an agile and multidisciplinary team, bringing your own unique skill set to the table and collaborating with kind colleagues to accomplish your goals. You prioritize your work with the team and its product owner, weighing both the business and technical value of each task.
You don’t do things just because they were always done that way. You don’t ignore the lessons learned, however - even those learned by others. There’s no template for what we do. Your tools will be familiar, but as an Aizer you’ll be doing lots of firsts. So you need a data engineer’s brain and the heart of an explorer.
Where you will be doing this
You are expected to be located in
Aberdeen, Scotland
as we continue to build our teams. Thanks to our flexible working approach though you can blend work and leisure, essentially having a ""workation."" Consider it a hybrid role😉 Escape to the breathtaking Scottish Highlands and Islands for a weekend, unwind and catch some rays on a European beach, or fly off to see your folks in your home country. Amid all the bustling and hustling, remember to spend quality time with your loved ones! While we support your preferred work style, be sure to harmonize & integrate that with the needs of your team and colleagues while keeping your main hub in Aberdeen.
The Team
Day to day, you will have the opportunity to work as part of a driven and creative team of architects, developers and other data engineers developing tools to make data onboarding and access easier over time.
Skills & Requirements
✔️ Problem-Solving:
Extraordinary ability to take on open-ended problems in unstructured environments.
✔️ Adaptability:
Adaptive and introspective; willing to learn, teach, lead, and follow.
✔️ Minimum 5 Years of Experience:
Working as a data engineer or in a relevant field.
✔️ English Proficiency:
Even though we are developing a tool to collaborate digitally, it’s still evidently important to communicate verbally, therefore we require a professional speaking proficiency in English.
✔️ Python Proficiency:
When it comes to tech stack, we require proficiency in Python and experience delivering production-grade data pipelines.
Other Tech Skills
Experience developing and/or deploying services on a cloud platform and working with containers (e.g., Docker)
Understanding of APIs and experience using them
Apache Airflow, Apache Beam, Apache NiFi, Apache Spark, Apache Kafka
SQL databases, document databases, graph databases
SQL
Databricks
Azure
Gitlab or equivalent
Travel:
Ability to travel abroad twice a year for networking purposes.
Passion
: Most importantly, we are looking for you who have a passion for using new technology to help solve complex problems.
We Offer
Money.
We chip in on your pension and keep you covered with medical insurance.
You are provided with a high spec HP Laptop or MacBook Pro.
Take part in tech talks, lunch and learns, team activities and awesome parties.
Modern office with onsite nursery, gym, café, and restaurant along with subsidised lunches.
Lots of things you can learn through our skilled sparring partners.
We like to inspire your passion, giving you the opportunity to attend industry events.
We also have amazing perks such as contributions to your mobile phone plan and broadband for a flexible working from a home pattern.
Show more
Show less","Data engineering, Data pipeline, Data quality, Selfservice tools, Agile, Multidisciplinary, Python, Cloud platform, Containers, APIs, Apache Airflow, Apache Beam, Apache NiFi, Apache Spark, Apache Kafka, SQL, Databricks, Azure, Gitlab","data engineering, data pipeline, data quality, selfservice tools, agile, multidisciplinary, python, cloud platform, containers, apis, apache airflow, apache beam, apache nifi, apache spark, apache kafka, sql, databricks, azure, gitlab","agile, apache airflow, apache beam, apache kafka, apache nifi, apache spark, apis, azure, cloud platform, containers, data engineering, data pipeline, data quality, databricks, gitlab, multidisciplinary, python, selfservice tools, sql"
Mulesoft Data Integration Developer,First PREMIER Bank/PREMIER Bankcard,"Sioux Falls, SD",https://www.linkedin.com/jobs/view/mulesoft-data-integration-developer-at-first-premier-bank-premier-bankcard-3756650506,2023-12-17,South Dakota,United States,Associate,Onsite,"Location:
Sioux Falls, South Dakota
Shift:
Monday – Friday 8:00 a.m. – 5:00 p.m.
Job Schedule:
Full-Time
Company:
PREMIER Bankcard
About the Role
This role will work on developing and supporting back-end API’s through the PREMIER MuleSoft integration platform. Although pre-dominantly a Monday to Friday role, a successful candidate needs to accommodate some flexible working hours as needed.
Job Duties and Responsibilities
Responsible for the creation or modification of MuleSoft API’s and/or Digital development of high complexity and scope.
Responsible to identify, resolve, and follow through on problems that arise with daily functions. Enforce attention to detail and consistent follow-up to ensure successful completion without degradation of quality.
Document changes to new MuleSoft API’s and/or Digital development including integrations by completing the projects form template and submitting to Software Development management as required in established internal and external audit requirements as defined in the SDLC.
Work closely with project leaders and users to develop workflows and specific functionality for the application.
Assist higher level developer in the development of MuleSoft API’s and/or Digital development of a more complex nature.
Research and resolve Help Desk Tickets that are assigned by Software Development management with potential assistance from a supervisor.
Applies systems analysis techniques and procedures, including consulting with users, to determine hardware, software, or system functional specifications.
Design, develop, modify, document, analyze, create, and test computer systems or programs, including prototypes, based on and related to user or system design specifications.
Skills and Qualifications
Bachelor’s degree or equivalent combination of education, training, and/or experience.
Understanding of the complexity of PREMIER’s internal applications and possess a basic understanding of the external applications that Software Development supports.
3-4 years of progressively responsible experience in a MuleSoft development environment and/or Digital development environment.
Application programming experience – Net/C# experience is desired.
Understanding of API data integration and micro services best practices.
Understanding of RDMS systems – SQL Server experience is desired.
Understanding of message queue techniques – RabbitMQ experience is desired.
Understanding of API testing tools – Ready API or Postman experience is desired.
Experience using log aggregation tools for performance monitoring and error detection.
Competitive Benefits Package
Full medical benefits when working 20+ hours per week
Traditional and High Deductible health plan options available
FREE dental and vision coverage
Generous Paid Time Off plans
401(k) – dollar-for-dollar match up to 5% of total compensation
Special discounts and offers for events at the Denny Sanford PREMIER Center
PREMIER Wellness Program
Paid Community Volunteer Hours – PREMIER averages 30,000 hours per year
Fun Employee Parties
Our Culture
Emphasis on personal success, respect, health, wellness, fun and giving back
Employees are rewarded, valued, and celebrated for hard work
Various Career advancement opportunities and growth
Appreciation is shown through concerts, outdoor bashes, cash, car giveaways and more
Show more
Show less","MuleSoft, API, Digital development, Software Development, Net/C#, SQL Server, RabbitMQ, Ready API, Postman, Log aggregation tools, RDMS","mulesoft, api, digital development, software development, netc, sql server, rabbitmq, ready api, postman, log aggregation tools, rdms","api, digital development, log aggregation tools, mulesoft, netc, postman, rabbitmq, rdms, ready api, software development, sql server"
Senior Marketing Data Analyst,First International Bank & Trust,"Sioux Falls, SD",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-first-international-bank-trust-3760588677,2023-12-17,South Dakota,United States,Mid senior,Onsite,"We are seeking a highly analytical and detail-oriented Senior Marketing Data Analyst to join our marketing team. In this role, you will champion Salesforce Marketing Cloud, serve as a SME (Subject Matter Expert) on our customer databases, lead generation strategies, measure ROI on campaigns, research, and supervise our current Marketing Data Analyst. The ideal candidate will have a strong background in data analysis and marketing analytics with an emphasis on Salesforce Marketing Cloud and Google Analytics.
This position is In-Person and will have the employee primarily located in one of our Branches.
Key Responsibilities
Champion Salesforce Marketing Cloud including but not limited to:
Recommend and implement marketing automation to streamline/align sales efforts and increase effectiveness.
Collaborate with SMEs to architect prospect engagement activities.
Share outcomes of efforts through reporting and suggest enhancements for better results.
Serve as SME of FIBT’s customer database and share insights with business line leaders and operations teams to help identify opportunities and drive effective strategies for engagement and growth.
Lead effective strategies for customer engagement and cross-selling; includes targeted digital marketing campaigns and SEO/SEM practices; includes collaboration with third parties.
Measure ROI of online and offline advertising campaigns including, but not limited to, reporting of KPIs such as leads, conversion rates, website traffic and social media engagement.
Conduct competitive research and analyze benchmarking data, including consumer behavioral insights.
Supervise marketing data team member(s)
Qualifications
Bachelor degree in Marketing or Business Administration; a minimum of 7 years of marketing analyst experience, or equivalent combination of education and experience is required.
Salesforce Marketing Cloud experience highly preferred.
Google Analytics experience is highly preferred.
In-depth understanding of SEM campaign strategies and SEO practices and experience with PPC campaigns.
First International Bank and Trust is a family-owned full-service Community Bank with locations in ND, MN, SD, and AZ. We show our employees we care by providing competitive benefits and training and growth opportunities. Here are the things we offer within our full-time benefits package:
Health, Dental & Vision
401K Retirement Plan with Employer Match
Paid Parental Leave
Life and Disability Insurance
Generous PTO and Holiday pay
If you are interested in learning more, you can apply or if you have questions before applying you can reach out directly at TalentAcquisition@FIBT.com.
Equal Opportunity Employer
Show more
Show less","Salesforce Marketing Cloud, Data Analysis, Marketing Analytics, Google Analytics, Reporting, Database Management, Competitive Research, SEO, SEM, PPC Campaigns, Lead Generation, ROI Measurement, Customer Engagement, CrossSelling, Digital Marketing, Social Media Engagement","salesforce marketing cloud, data analysis, marketing analytics, google analytics, reporting, database management, competitive research, seo, sem, ppc campaigns, lead generation, roi measurement, customer engagement, crossselling, digital marketing, social media engagement","competitive research, crossselling, customer engagement, dataanalytics, database management, digital marketing, google analytics, lead generation, marketing analytics, ppc campaigns, reporting, roi measurement, salesforce marketing cloud, sem, seo, social media engagement"
Remote Sensing Data Scientist,"KBR, Inc.",Greater Sioux Falls Area,https://www.linkedin.com/jobs/view/remote-sensing-data-scientist-at-kbr-inc-3764350565,2023-12-17,South Dakota,United States,Mid senior,Hybrid,"Belong. Connect. Grow. with KBR!
Around here, we define the future.
But we at KBR we share one goal: to improve the world responsibly as a company of innovators, thinkers, creators, explorers, volunteers, and dreamers.
Delivering Solutions, Changing the World. We deliver science, technology, and engineering solutions to governments and companies around the world. - https://www.kbr.com/en
KBR’s Science and Space division is seeking a Remote Sensing Data Scientist to support our Technical Support Services Contract (TSSC) with the U.S. Geological Survey (USGS) at the Earth Resources Observation and Science (EROS) Center (http://eros.usgs.gov/), located near Sioux Falls, SD (http://www.siouxfalls.org).
Are you a data scientist professional who is looking to flex your AI/ML skills to develop cutting edge geospatial products that can make a difference in the world? How would you like to leverage over 50 years of continuous Earth monitoring products, afforded by Landsat and other satellite missions, to study trends in change and create high-level geospatial science products to monitor, assess, and project land cover change? If you’re interests are piqued, keep reading!
This position is aligned with the Science Division of the TSSC and will support the USGS EROS Land Cover Program on the Land Cover Next Project. The LCNext project develops foundational capabilities and geospatial products for integrating land cover and land cover change science within the Integrated Science and Applications Branch (ISAB) at the Earth Resources Observation and Science (EROS) Center. The main objective is to design, develop, and implement an operational capability for monitoring, assessing, and projecting land cover, land cover change, and impervious surface and to use this framework to support scientific research for understanding a changing Earth. Land cover and land use data are high priority data themes within the USGS. These data are often combined with other sources of relevant scientific data to provide a robust suite of geospatial data that allow informed decisions by managers and the public. Land cover map data (and derived products) require repeated production, updating and improvement, to provide the most relevant land cover and land cover change information for the Nation. The work includes coordination with a team of exceptionally skilled remote sensing scientists, data scientists, systems/software engineers, and communications specialists, as well as collaboration with our USGS customers at the EROS Center.
Read on to see if you’re a match.
If you think you have what it takes and are interested in becoming an integral part of our team, we’d love to hear from you!
RESPONSIBILITIES:
Achieve a thorough understanding of land cover and land cover change mapping in terms of project-level and scientific/land management goals, the historical and current state of the field, and fundamental domain-specific challenges and limitations
Leverage machine learning/deep learning frameworks to improve upon existing supervised multiclass classification and regression workflows
Implement workflows at scale within an AWS cloud environment or through submission to a high-performance compute cluster
Support the generation and evaluation of geospatial data products
Write organized, scalable, and well-documented code
Interact with software engineers for code reviews and output evaluation and verification
Support the automation of workflows for other areas of the project
Author technical documentation, support or lead scientific manuscripts and conference abstracts, and prepare/deliver oral presentations on progress and outcomes
PREFERRED SKILLSET:
Technical Skills:
Knowledge of remote sensing principles, satellite imaging systems, and land cover science
Experience using the Python/Julia programming languages for data analysis, data science, and geospatial analysis
Experience with Python libraries such as numpy, pandas, scikit-learn, gdal, rasterio, and tensorflow
Knowledge of statistical approaches to data sampling, analysis, and validation techniques
Experience building machine learning/deep learning workflows
Familiarity with cloud-based and/or high-performance computing environments, parallel and distributed computing
Familiarity with using version control systems (e.g., Git)
Experience with GIS software, such as QGIS, ERDAS Imagine, and ArcGIS
Soft Skills:
Able to collaborate effectively in a hybrid/remote work environment
Self-motivated and willing to tackle open-ended problems
Well-organized and able to track and report progress toward goals
Effectively able to communicate problems, request clarification, and express opinions
Able to convey technical topics at various levels of detail to different audiences
REQUIREMENTS:
Education:
Master’s degree in data science, geography, remote sensing, mathematics, statistics, or related field is preferred. BA/BS in data science, geography, remotes sensing, mathematics statistics or related field required.
Years of Experience:
BA/BS +8 years of relevant work experience or MA/MS +6 years of relevant work experience
Three years of continuous residency in the U.S.
This position requires the ability to obtain and maintain a national agency check and background investigation after hire to obtain credentials for facility access and user accounts.
WORK LOCATION:
This position supports a coordinated team across multiple locations and offers either remote work status or a hybrid remote/onsite schedule; some travel may be required.
STANDARD KBR REQUIREMENTS:
All of KBR’s employee’s must be team players, able to communicate effectively with internal and external customers, able to follow safety and quality policies, able to follow company programs, processes, procedures, practices, requirements, goals, and objectives. In addition, they must also be organized, with good time management skills.
Experience and/or Education in lieu of these qualifications will be reviewed for applicability to meet these requirements.
KBR Benefits​
KBR offers a selection of competitive lifestyle benefits which could include 401K plan with company match, medical, dental, vision, life insurance, AD&D, flexible spending account, disability, paid time off, or flexible work schedule. We support career advancement through professional training and development.​
KBR partners with several other companies to fulfill its requirements as a government contractor. The selected subcontracting companies align their benefits as closely as possible to those above.
Inclusion and Diversity at KBR​
At KBR, we are passionate about our people, sustainability, and our Zero Harm culture. These inform all that we do and are at the heart of our commitment to, and ongoing journey toward being a more inclusive and diverse company. That commitment is central to our team of team’s philosophy and fosters an environment of real collaboration across cultures and locations. Our individual differences and perspectives bring enhanced value to our teams and help us develop solutions for the most challenging problems. We understand that by embracing those differences and working together, we are more innovative, more resilient, and safer. We Deliver – Together. ​
Show more
Show less","Remote Sensing, Data Science, Geospatial Analysis, Machine Learning, Deep Learning, Python, Julia, Numpy, Pandas, ScikitLearn, GDAL, Rasterio, Tensorflow, Statistical Analysis, Version Control Systems, QGIS, ERDAS Imagine, ArcGIS, Cloud Computing, HighPerformance Computing, Parallel and Distributed Computing, GIS Software","remote sensing, data science, geospatial analysis, machine learning, deep learning, python, julia, numpy, pandas, scikitlearn, gdal, rasterio, tensorflow, statistical analysis, version control systems, qgis, erdas imagine, arcgis, cloud computing, highperformance computing, parallel and distributed computing, gis software","arcgis, cloud computing, data science, deep learning, erdas imagine, gdal, geospatial analysis, gis software, highperformance computing, julia, machine learning, numpy, pandas, parallel and distributed computing, python, qgis, rasterio, remote sensing, scikitlearn, statistical analysis, tensorflow, version control systems"
IT Business Intelligence & Data Analyst III,ClickJobs.io,"Fayetteville, NC",https://www.linkedin.com/jobs/view/it-business-intelligence-data-analyst-iii-at-clickjobs-io-3788845438,2023-12-17,Fayetteville,United States,Mid senior,Onsite,"Position Title:: IT Business Intelligence & Data Analyst III Working Title:: IT Business Intelligence & Data Analyst III Position Number:: 004410 Full-Time Or Part-Time:: Full Time Time Limited Position:: No Number of Vacancies:: 1 Department:: Systems & Procedures - Bus & Fin Posting Number:: 0401644 SHRA/EHRA:: EHRA (Non-Faculty) Job Category:: Professional Hiring Range:: Commensurate with Education and Experience Overall Position Competency:: Flat Rate Description of primary duties and responsibilities:: Primary Purpose of the Organizational Unit: Systems and Analysis department is comprised of two units within the Division of Business and Finance: Business Systems provides systems assessment, development, integration, project management, and maintenance for the Business and Finance units and their customers throughout the university. Special projects are often directed by senior management to improve the efficiency and effectiveness of administrative operations related to financial processes.The Business Analysis unit uses data models, reports, visualizations, and other analytical solutions to find order in disparate data sets and support university decision-making. Special Projects are often directed by senior management to make inferential and/or predictive analyses to further the impact of the university. Primary Purpose of the Position: The Business Intelligence Analyst is part of a new team that will craft comprehensive analytical products designed to assist divisional leadership in satisfying the goals and objectives of the division and University. These analytics will inform strategies as they relate to performance and process efficacy. Description of Work Continued:: 45% – Descriptive/Inferential Analytics and Visualizations: This position will work with divisional stakeholders to gather, document, and understand operational requirements to create a reporting architecture to supply divisional departments with access to required data and/or visualizations to aid in departmental and/or divisional decision-making. This will include the creation and maintenance of divisional and departmental dashboards, key performance indicators, and other reports.This position will be responsible for ad-hoc reporting requests related to their assigned knowledge domain. 20% – Predictive Analytics: This position will utilize various software programs (SAS 9.4, SAS Enterprise Miner, SAS Studio, JMP) or other scripting languages with statistical capacities to predict future financial capacities given historical and current relationships.Determine the best software solution and format for divisional leadership to explore, examine scenarios, and digest the resulting data to better instruct strategic decision-making.Provide ongoing maintenance and enhancements of models to stay current with changes in the operating environment. 20% – Divisional Education and Program Maintenance: Tailor instruction to a diverse audience within the Division to enable end-users to successfully navigate, use, and integrate the division’s software offerings into their current processes.Be a subject matter expert on the division’s reporting system(s), how it integrates with the University’s Enterprise Resource Planning solution and/or 3rd party software add-ons, and how to resolve end-user problems or errors.Test software enhancements or upgrades prior to their introduction into the production environment. 15% – Divisional Consulting: The department is frequently called upon to provide directed internal consulting services for the Business and Finance division to continuously improve and optimize divisional processes. This position will be part of a cross-functional team when their domain is within the purview of the project. The position will aid the consulting team in documenting the issue(s) under investigation, documenting and investigating current resource limitations, and comprehensively understanding the root cause(s) of the issue(s) via numerous methods (end-user interviews, process mapping, statistical analyses, etc.), and supply recommendations to mitigate and/or eliminate causes and optimize the process. Minimum Education And Experience Requirements:: Graduate of an accredited university with a BA with 4 to 6 years experience or an MS in a related field Proficient in the use of a range of data and information technology systems Proficient data analytics skills and data mining The desired candidate should have experience with the following environments, languages, and tools: Oracle, SQL, PLSQL, XML/JSON, HTML, data visualization, real-time system integrations, and business intelligence applications such as R, Python, SAS, Argos or equivalent programming languages/applications. A demonstrated technical understanding of the business processes mandated by administrative systems and their interfaces. Knowledge of principles and concepts behind applications analysis/design, project planning, resource estimates, thorough testing and debugging, implementation, and documentation. Preferred Qualifications:: Five years’ experience with Ellucian Banner or similar financial system products. Notice to Applicants: Please make sure that the work history listed on your application is identical to the work history listed on your resume. The application must be filled out completely, Do Not Use the phrase “see resume.” Please list at least three (3) professional references to include a current supervisor. References will only be contact if you are selected for the position and with your permission. Open Until Filled:: Yes Additional Information for Applicants: All new employees are required to have listed credentials/degrees verified prior to employment. Transcripts should be provided for all earned degrees and/or the degree which is being used to satisfy credential/qualification requirements. Transcript requests are the responsibility of the candidate. EEO Statement:: This position is subject to the successful completion of an employment background check. An employment background check includes a criminal background check, employment verification, reference checks, license verification (if applicable) and credit history check (if applicable). Fayetteville State University is committed to equality of educational opportunity and does not discriminate against applicants, students, or employees based on race, religion, color, national origin, sex, age, disabling condition, political affiliation or sexual orientation. Moreover, Fayetteville State University values diversity and actively seeks to recruit talented students, faculty, and staff from diverse backgrounds. Veteran's Statement: Fayetteville State University is a VEVRAA Federal Contractor and seeks priority referrals of protected veterans for our openings. Quick Link:: https://jobs.uncfsu.edu/postings/24975
Show more
Show less","Data Analytics, Data Mining, Oracle, SQL, PLSQL, XML/JSON, HTML, Data Visualization, Realtime System Integrations, Business Intelligence, R, Python, SAS, Argos, Ellucian Banner, Reporting, Dashboards, Key Performance Indicators, Predictive Analytics, SAS 9.4, SAS Enterprise Miner, SAS Studio, JMP","data analytics, data mining, oracle, sql, plsql, xmljson, html, data visualization, realtime system integrations, business intelligence, r, python, sas, argos, ellucian banner, reporting, dashboards, key performance indicators, predictive analytics, sas 94, sas enterprise miner, sas studio, jmp","argos, business intelligence, dashboard, data mining, dataanalytics, ellucian banner, html, jmp, key performance indicators, oracle, plsql, predictive analytics, python, r, realtime system integrations, reporting, sas, sas 94, sas enterprise miner, sas studio, sql, visualization, xmljson"
Data Operations Analyst,Open Systems Technologies,"Greenwich, CT",https://www.linkedin.com/jobs/view/data-operations-analyst-at-open-systems-technologies-3725514241,2023-12-17,Elwood,United States,Mid senior,Hybrid,"A financial firm is looking for a
Data Operations Analyst
to join their team in Greenwich, CT.
Compensation: $170,000-175,000
Responsibilities
Assist in reviewing processes and technologies to improve data accuracy, timeliness, and efficiency.
Regularly interact with users of financial data to identify, access, and query series of internal and 3rd party vendors to ensure data accuracy.
When new data services are needed, this person will help evaluate potential data sources.
Qualifications
BA/BS with strong academic background
8+ years experience in a similar role at an investment firm or a financial data-related role at another type of firm (bank, financial services firm, auditor, data vendor, govt agency, or other), with a solid understanding of data quality management, data architecture, and business intelligence
Extensive experience with Security Master, historical datasets, and diverse financial products and market data
Knowledge of SQL is a plus
Experience working with Bloomberg, Reuters, Refinitiv, DataStream, DTCC, MSCI, and/or FactSet terminals and Excel add-ins
Requires strong communication skills, proven ability to plan and execute projects, communicating goals and progress as well as any problems
Strong problem-solving and organizational skills are a must, along with meticulous attention to detail and a goal-oriented work approach in a fast-paced work environment
Show more
Show less","Data Operations Analysis, Data Accuracy, Data Timeliness, Data Efficiency, Data Quality Management, Data Architecture, Business Intelligence, Security Master, Historical Datasets, Financial Products, Market Data, SQL, Bloomberg, Reuters, Refinitiv, DataStream, DTCC, MSCI, FactSet, Excel Addins, Communication Skills, Project Planning, Project Execution, Goal Setting, Progress Communication, Problem Solving, Organizational Skills, Attention to Detail, GoalOriented Work Approach, FastPaced Work Environment","data operations analysis, data accuracy, data timeliness, data efficiency, data quality management, data architecture, business intelligence, security master, historical datasets, financial products, market data, sql, bloomberg, reuters, refinitiv, datastream, dtcc, msci, factset, excel addins, communication skills, project planning, project execution, goal setting, progress communication, problem solving, organizational skills, attention to detail, goaloriented work approach, fastpaced work environment","attention to detail, bloomberg, business intelligence, communication skills, data accuracy, data architecture, data efficiency, data operations analysis, data quality management, data timeliness, datastream, dtcc, excel addins, factset, fastpaced work environment, financial products, goal setting, goaloriented work approach, historical datasets, market data, msci, organizational skills, problem solving, progress communication, project execution, project planning, refinitiv, reuters, security master, sql"
Senior Developer - Data Platform,MYOB,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-developer-data-platform-at-myob-3766381053,2023-12-17,Sunbury, Australia,Mid senior,Hybrid,"We’re MYOB, a business management platform designed to unleash the potential of businesses across Australia and New Zealand! As the #originalstartup, our roots are in finance and accounting software, but today we are so much more. We help our Aussie and Kiwi customers unleash their full potential, giving them the tools to Start, Survive and Succeed all in the one place.
We’re always on the hunt for those who bring a different perspective, diversity of thought and the drive to make our culture even better. Take your career to a new dimension at MYOB.
About The Team
Our Tech team are true heroes. They’ve taken our legacy success and transformed it into an extraordinary SaaS platform to support SME and Enterprise businesses in ways they never knew they needed. By joining the Tech team, you’ll be an integral part of building our all-in-one business management platform.
The Opportunity
MYOB is on an overarching strategic mission to be a data-led organisation. Our Data Platform has been built with a data-mesh model design at its core, with a strong focus on empowering data producers and consumers across the entire business with the right mentality, skills and tools to build great data products.
You will be a strong Software Engineer first and foremost, with hands-on design, programming and operations of the data platform being your primary focus. You will collaborate deeply with your team to engineer a high-quality platform that is performant, maintainable, extensible, recoverable and easy to use and operate. You will provide the tools that enable MYOB to process batch and stream data at scale and develop data products such as APIs to be maximised by our commercial products and customers.
The Experience You Will Bring
Good understanding of practices and principles such as TDD, Clean Code and SOLID
Experience applying DevOps and CI/CD practices
Knowledge of microservice and event-driven architectures
Experience in Python or other programming languages with a willingness to use and master Python
Experience working with SQL and NoSQL engines
Interest in data, analytics and customer data platforms
Growth mindset and drive to continuously improve personal and team processes, performance and culture
Pragmatism to balance short term delivery focus with longer term objectives
Bonus Points For
Familiarity with ETL processes and handling large volumes of data efficiently
Knowledge of modern data tooling for warehouses, catalogues, transformation, ETL pipelines and analytics such as Snowflake, DBT and Airflow
Our Culture & Benefits
Our values have stood the test of time. If you want to work and collaborate where opinions are valued, and your ideas can make a difference, come to a place where Your Work Matters.
🎥 See what it's like to work at MYOB and what we're all about.
🎯 Do your best work in a flexible work environment, right down to financial assistance to set up your home office…it’s called Flexperience, and it’s designed by you and your team!
🎯 Our partnership with Smiling Mind helps support the wellbeing of our team members and customers
🎯 Drive your own learning via conferences, in-house training, LinkedIn Learning, study assistance and a strong focus on leaders creating a learning environment
🎯 A multitude of leave options including up to an additional four weeks of purchased leave, generous parental leave, domestic violence leave, transgender leave, volunteer leave, study leave, plus more!
🎯 Communities built around ‘Wellness’, ‘Belonging’ and the ‘Planet’ where you can make a meaningful contribution
🎯 Access to best-in-class discounts and vouchers from leading retailers, and a lot more.
We are proud to be a Circle Back Initiative Employer and we commit to responding to every applicant.
MYOB are an equal opportunity employer and we champion diversity. Don’t meet every single requirement of this role? Still apply! Research tells us that that women and underrepresented groups are less likely to apply unless they meet every single requirement. At MYOB we believe that the right hire is someone who makes an addition to our culture, rather than someone who fits in and conforms to our status quo. Moving to ‘Culture Add’ means adding team members who not only value MYOBs standards and workplace culture, but also bring an aspect of diversity that positively contributes to MYOB. So, if you’re excited about this role, or about MYOB, we’d still love to hear from you!
Show more
Show less","Python, SQL, NoSQL, TDD, Clean Code, SOLID, DevOps, CI/CD, Microservices, Eventdriven architectures, Data warehousing, Data catalogs, Data transformation, ETL pipelines, Data analytics, Snowflake, DBT, Airflow","python, sql, nosql, tdd, clean code, solid, devops, cicd, microservices, eventdriven architectures, data warehousing, data catalogs, data transformation, etl pipelines, data analytics, snowflake, dbt, airflow","airflow, cicd, clean code, data catalogs, data transformation, dataanalytics, datawarehouse, dbt, devops, etl pipelines, eventdriven architectures, microservices, nosql, python, snowflake, solid, sql, tdd"
Data Analyst,Australian Energy Market Operator (AEMO),Greater Melbourne Area,https://au.linkedin.com/jobs/view/data-analyst-at-australian-energy-market-operator-aemo-3784515632,2023-12-17,Sunbury, Australia,Mid senior,Hybrid,"Department : Operations
Division : Market Management
Reference number : 10993
AEMO at the Heart of Energy
We are the Australian Energy Market Operator (AEMO), committed to designing and operating a sustainable energy system that delivers safe, reliable, and affordable electricity and gas. Our mission includes facilitating the transition to a net-zero energy system by 2050, working collaboratively with industry partners to achieve 100% renewable generation capability by 2025. We have the once-in-a-lifetime opportunity to co-design the future of our energy systems, and our core values revolve around Character, Commitment, and Connection. Join us as we contribute to this significant mission in the energy sector.
About The Role
We are looking for a full-time Operational Analyst to join our Gas Settlements team on a 12-month fixed-term contract in our Melbourne office.
This role combines operational and analytical activities relating to market participant transactions in the wholesale gas markets. Whether you are looking for first analyst opportunity or have some prior experience, your desire to learn and bring value to the team is important.
In return, you will receive broad exposure to how our markets work and gain a deeper understanding of their intricacies and complexities. You will have the opportunity to develop and excel rapidly in a friendly, diverse, and successful team who are driven to succeed.
What You Will Be Doing
Your focus is to perform operational and analytical activities for Gas Settlements team. You will be at the forefront of investigating, analysing and resolving data and system issues in a timely manner
Assist in the impact analysis on systems and internal business processes when rule or procedure changes are identified to ensure ongoing compliance with settlement systems and processes.
Be involved in market audits through provision of data and explaining settlements processes
Be involved in improving the settlement systems
Develop and execute user acceptance test scripts
Maintain business process documentation
What You Will Bring To The Role
Tertiary qualifications in a related field such as business, commerce, computing, engineering or other numerate disciplines
Ideally some experience in the development and use of data analysis tools such as Microsoft Excel and SQL is highly desirable
Ability to investigate, analyse and resolve data issues
High attention to detail, coupled with an ability to meet deadlines and prioritise multiple tasks in a dynamic environment
Dedicated to self-improvement and an effective team player
In Return Some Of Our Benefits To You
Flexible working: work from home, part time, job share, hybrid options, and additional leave options
Professional development via projects, industry networks, job rotation, study assistance and more.
Give back with up to 4 days of volunteering leave per year.
Reward your hard work with annual performance bonuses (subject to eligibility).
Embrace a healthier you with our wellness program, discounted health insurance, gym perks and our comprehensive Employee Assistance Program (EAP).
About Our Process
AEMO values diversity and inclusivity in the workplace, welcoming applications from all backgrounds without regard to age, disability, gender, sexual orientation, parental status, race, religion and our First Nation peoples . We are dedicated to accommodating applicants' needs during the application or interview process, simply let the us know by emailing us at talent@aemo.com.au
If you would like to know more about working at AEMO , please check out our careers page for more information. Please note, applications for this role will close on
Wednesday 3 January
and all applications will be considered after this date.
For more information on the Australian Energy Market Operator (AEMO) please refer to www.aemo.com.au or visit one of our social media channels below.
Show more
Show less","Data Analysis, MS Excel, SQL, Business Process Documentation, Business Analysis, Market Auditing, User Acceptance Testing","data analysis, ms excel, sql, business process documentation, business analysis, market auditing, user acceptance testing","business analysis, business process documentation, dataanalytics, market auditing, ms excel, sql, user acceptance testing"
ORSA / Data Scientist (23-050),CTI,"Aiea, HI",https://www.linkedin.com/jobs/view/orsa-data-scientist-23-050-at-cti-3787510044,2023-12-17,Hawaii,United States,Associate,Onsite,"As an
Operations Research and Systems Analyst (ORSA)/Data Scientist
you will support a Department of Defense (DoD) client as part of a contractor Test and Evaluation capability. You’ll be assisting client decision makers in solving complex problems by producing the critical analysis of data and reasoning towards DoD products, equipment, or systems in accordance with DoD and contractual requirements. You’ll be part of a team reviewing and evaluating complex test specifications, test results, test trends and produce qualitative and quantitative analysis by developing and applying probability models, statistical inference, simulations, and data optimization of DoD project test results.
As an ORSA, you’ll be required to provide these results with clear and precise recommendations.
Develop, design, formulate, execute, coordinate, and document a broad range of analyses involving current and projected conditions and problems pertaining to DoD projects
Assist Test Plan development to focus on desired data collected for analysis; design data collection plans; organize data reduction and analysis activities
Participate in Working-level Integrated Project Teams (WIPT) and guide the WIPT activities with ORSA driven input. and support the organization’s requirements development process
Assist in identifying critical operational issues and developing test objectives to address effectiveness and suitability; determine Measures of Effectiveness and Measures of Performance
Apply Scientific Test and Analysis Techniques (STAT) to test systems effectively and efficiently for significant statistical results
Consult and provide technical support to USG client leadership
Develop and maintain working relationships with government agencies and commercial entities to enable better test, evaluation and analysis of relevant technologies and processes
Recommend and implement modeling and simulation tools and software to maximize efficient use of resources
This position is required to be onsite and located at Camp Smith in Hawaii. Relocation assistance is available to eligible canddiates.
Requirements
Necessary Skills and Experience
Bachelor’s degree in mathematics, statistics, economics, or related field
Minimum of 5 years of experience with design of experiment, statistics, data analysis techniques or equivalent education
Experience and knowledge of major combat Modeling and Simulation methods and systems such as AFSIM
Must have an active U.S. government Top Secret security clearance and be SCI eligible. U.S. citizenship is required to obtain a security clearance
Beneficial Skills And Experience
Excellent technical/analytical writing skills, with a strong ability to use standard Microsoft office products, such as Word, Excel, Outlook, SharePoint, and Access
Minimum 3 years of experience with DoD Test and Evaluation projects involving test planning and concept development, test design, data collection and test analysis supporting operational test and evaluation (OT&E).Compliance with training, education, and experience meeting Defense Acquisition Workforce Improvement Act (DAWIA) Test and Evaluation Level II
Self-motivated and able to work independently
Excellent communication and interpersonal skills
Ablity to travel for short durations worldwide in support of test events and operations
Benefits
CTI is a rapidly growing company offering the following:
Medical, dental and vision insurance
H.S.A. (partially funded by CTI) and Flex Spending
Company-paid life insurance/AD&D and disability insurance
Optional supplemental life, critical illness, hospital indemnity and accident insurances
Paid vacation, sick leave and holidays
401k plan with Safe Harbor contribution
Tuition reimbursement/professional training options
Employee Assistance Program
Travel Assistance
Financial Planning Assistance
Voluntary Pre-Paid Legal
Flexible schedules with telecommuting options
Service awards program
CTI is an Equal Opportunity employer and shall abide by the requirements of 41 CFR 60-1.4(a), 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. Moreover, these regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.
Show more
Show less","Operations Research, Systems Analysis, Data Science, Probability Models, Statistical Inference, Simulations, Data Optimization, Test Plan Development, Data Collection, Data Reduction, Data Analysis, Integrated Project Teams, Modeling and Simulation, Microsoft Office, Word, Excel, Outlook, SharePoint, Access, Test and Evaluation, DoD Test and Evaluation, Test Planning, Concept Development, Test Design, Data Collection, Test Analysis, Operational Test and Evaluation, Defense Acquisition Workforce Improvement Act, DAWIA, Travel","operations research, systems analysis, data science, probability models, statistical inference, simulations, data optimization, test plan development, data collection, data reduction, data analysis, integrated project teams, modeling and simulation, microsoft office, word, excel, outlook, sharepoint, access, test and evaluation, dod test and evaluation, test planning, concept development, test design, data collection, test analysis, operational test and evaluation, defense acquisition workforce improvement act, dawia, travel","access, concept development, data collection, data optimization, data reduction, data science, dataanalytics, dawia, defense acquisition workforce improvement act, dod test and evaluation, excel, integrated project teams, microsoft office, modeling and simulation, operational test and evaluation, operations research, outlook, probability models, sharepoint, simulations, statistical inference, systems analysis, test analysis, test and evaluation, test design, test plan development, test planning, travel, word"
Senior Data Analyst,AlohaCare,"Honolulu, HI",https://www.linkedin.com/jobs/view/senior-data-analyst-at-alohacare-3779294790,2023-12-17,Hawaii,United States,Mid senior,Onsite,"Apply on line at
http://www.alohacare.org/Careers/Default.aspx
The Company
AlohaCare is a local, non-profit health plan serving the Medicaid and Medicare dual eligible population. We provide comprehensive managed care to qualifying health plan members through well-established partnerships with quality health care providers and community-governed health centers. Our mission is to serve individuals and communities in the true spirit of aloha by ensuring and advocating access to quality health care for all. This is accomplished with emphasis on prevention and primary care through community health centers that founded us and continue to guide us as well as with others that share our commitment. As Hawaii’s third-largest health plan, AlohaCare offers comprehensive prevention, primary and specialty care coverage in order to successfully build
a healthy Hawaii
.
The Culture
AlohaCare employees share a passion for helping Hawaii’s most underserved communities. This passion for helping and caring for others is internalized and applied to our employees through a supportive and positive work environment, healthy work/life balance, continuous communication, and a generous benefits package.
AlohaCare’s leadership empowers and engages its employees through frequent diversity, recognition, community, and educational events and programs. AlohaCare has a strong commitment to support Hawaii’s families and reinforces a healthy work/home balance for its employees. Because AlohaCare values honesty, respect, and trust with both our internal and external customers, we encourage open-door, two-way communication through daily interactions, employee events and quarterly all-staff meetings. AlohaCare’s comprehensive benefits package includes low cost medical, dental, drug and vision insurance, PTO program, 401k employer contribution, referral bonus and pretax transportation and parking program.
These employee-focused efforts contribute to a friendly, team-oriented culture which is positively reflected into the communities we serve.
Position Summary
Responsible for supporting the use and understanding of available data within AlohaCare. Works to maintain and improve data captured in organization’s databases. Conducts ad hoc analyses, prepares reports, and presents information to support data-driven decision making.
Primary Job Duties And Responsibilities
Identifies data requirements by partnering with managers and other stakeholders.
Designs and develops SQL queries, extracts data from databases and analyzes the data using a variety of analytical techniques. Prepares accurate reports and presents findings.
Partners with managers in the methodological design and analysis of health outcomes studies. Assists in the development of reports to satisfy contractual reporting requirements, track and trend health plan performance, effect and facilitate ongoing performance monitoring of program(s).
Performs utilization analysis utilizing data from different sources and reports results.
Creates and maintains supporting project documentation (SQL, data request, data sets) and ensures distribution of documentation to appropriate staff as required.
Provides data and analytical support for department and organizational initiatives.
Assists with standardized and automated report development.
Mentors other Data Analysts in the use of available statistical and analytical tools (e.g. SQL, Excel, Stata, reporting tools) as appropriate, including training sessions and support.
Assign and monitor requests submitted through Data Request application.
Serve as a liaison between the Data Analysts and other departments in need of complex analyses or evaluation project.
Performs other duties as assigned.
Responsible to maintain AlohaCare’s confidential information in accordance with AlohaCare policies, and state and federal laws, rules, and regulations regarding confidentiality. Employees have access to AlohaCare data based on the data classification assigned to this job title.
Required Competencies & Qualifications
Bachelor’s Degree with emphasis in computer science, information systems, mathematics, statistics, public health, health sciences, data analysis, accounting, or related field.
4+ years professional training or work experience which demonstrates the ability to perform the duties required for the position.
Experience using SQL, Excel, other statistical/analytical tools and understanding of logic operators and conditional statements.
Knowledge of relational databases and structures.
Strong problem solving and reasoning skills.
Experience with computer systems and software programs such as R /SAS/SPSS or equivalent statistical package.
Ability and enthusiasm to collaborate on complex analytic project requirements and results with a broad range of stakeholders.
Excellent interpersonal skills and ability to work with cross functional teams.
Ability to work independently with minimal supervision.
Achieves results, builds trust, communicate effectively, customer and quality focused.
Ability to multi-task, organize, adapt to changing priorities, and manage a diversity of high priority projects in a fast-paced environment.
Requires operation of general office equipment to include PC, fax/copy machine and ACD Mitel Phones.
Advanced skill using Microsoft Programs; Word, Excel, PowerPoint, and Outlook
Preferred Competencies & Qualifications
SSRS, QlikView, Power BI or other business intelligence experience.
Health care industry experience within managed care, hospital, medical office or equivalent preferred.
Knowledge of health research methodology, biostatistics, and medical terminology.
Physical Demands/Working Conditions
Inside working conditions.
No environmental hazards.
Sedentary Work: Exerting up to 20 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull, or otherwise move objects. Sedentary work involves sitting most of the time but may involve walking or standing for brief periods of time. Jobs are sedentary if walking and standing are required only occasionally, and all other sedentary criteria are met.
Salary range: $60,000 - $80,000 annually
AlohaCare is committed to providing equal employment opportunity to all applicants in accordance with sound practices and federal and state laws. Our policy prohibits discrimination and harassment because of race, color, religion, sex (including gender identity or expression), pregnancy, age, national origin, ancestry, marital status, arrest and court record), disability, genetic information, sexual orientation, domestic or sexual violence victim status, credit history, citizenship status, military/veterans’ status, or other characteristics protected under applicable state and federal laws, regulations, and/or executive orders.
Show more
Show less","SQL, Excel, Statistical Analysis, Relational Databases, R, SAS, SPSS, Data Analysis, Microsoft Office, SSRS, QlikView, Power BI, Business Intelligence, Health Research Methodology, Biostatistics, Medical Terminology","sql, excel, statistical analysis, relational databases, r, sas, spss, data analysis, microsoft office, ssrs, qlikview, power bi, business intelligence, health research methodology, biostatistics, medical terminology","biostatistics, business intelligence, dataanalytics, excel, health research methodology, medical terminology, microsoft office, powerbi, qlikview, r, relational databases, sas, spss, sql, ssrs, statistical analysis"
Staff Data Engineer,Recruiting from Scratch,"Honolulu, HI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398133,2023-12-17,Hawaii,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Honolulu, HI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828474,2023-12-17,Hawaii,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Continuous Delivery, Models, APIs, Data Governance, Data Products, Data Quality, Reliability, Security, Scalability, ETL, S3, Snowflake, Kafka, Spark, Python, SQL, Distributed Databases, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, Legal Compliance, Data Classification, Retention","data engineering, tdd, continuous delivery, models, apis, data governance, data products, data quality, reliability, security, scalability, etl, s3, snowflake, kafka, spark, python, sql, distributed databases, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, legal compliance, data classification, retention","agile engineering practices, apis, automated testing, continuous delivery, continuous integration, data classification, data engineering, data governance, data products, data quality, data warehouses, deployment, dimensional data modeling, distributed databases, etl, kafka, legal compliance, models, pair programming, python, reliability, retention, s3, scalability, schema design, security, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Honolulu, HI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392538,2023-12-17,Hawaii,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Data Products, Data Governance, Data Security, Scalability, Data Ownership, Data Representation, Data Storage, R&D, Prototyping, Big Data Architectures, ETL, S3, Snowflake, Kafka, Spark, Python, Relational Databases, SQL, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Data Management, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, data products, data governance, data security, scalability, data ownership, data representation, data storage, rd, prototyping, big data architectures, etl, s3, snowflake, kafka, spark, python, relational databases, sql, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data management, data classification, data retention","agile engineering practices, automated testing, automation, big data architectures, continuous delivery, continuous integration, data classification, data engineering, data governance, data management, data ownership, data products, data representation, data retention, data security, data storage, data warehouses, deployment, dimensional data modeling, etl, etl pipelines, kafka, pair programming, prototyping, python, rd, realtime streaming technologies, relational databases, s3, scalability, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Data Scientist (TS/SCI),Motion Recruitment,"Honolulu, HI",https://www.linkedin.com/jobs/view/data-scientist-ts-sci-at-motion-recruitment-3725137009,2023-12-17,Hawaii,United States,Mid senior,Onsite,"A leading innovator for the defense is seeking a Data Scientist to join their team. The client is seeking an experienced professional with strong analytical and problem-solving skills, and is eager to lead projects supporting the Department of Defense (DoD). This role requires candidates to hold at least a TS/SCI clearance.
Responsibilities
Data curation and modeling
Deployment and implementation
Collaborate with external team for user requirements
Qualifications
Degree completion in Science, Math, or Engineering
Python
SQL
MLOps (Docker / Kubernetes)
AWS
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
LI#-DP1
Posted By:
Derek Progin
Show more
Show less","Data Curation, Data Modeling, Deployment, Implementation, User Requirements, Python, SQL, MLOps, Docker, Kubernetes, AWS","data curation, data modeling, deployment, implementation, user requirements, python, sql, mlops, docker, kubernetes, aws","aws, data curation, datamodeling, deployment, docker, implementation, kubernetes, mlops, python, sql, user requirements"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Honolulu, HI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707758,2023-12-17,Hawaii,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming","data engineering, machine learning, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming","airflow, aws, azure, bash, data engineering, docker, dynamodb, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Medicare Stars Business Data Analyst,HMSA,"Honolulu, HI",https://www.linkedin.com/jobs/view/medicare-stars-business-data-analyst-at-hmsa-3769103016,2023-12-17,Hawaii,United States,Mid senior,Hybrid,"Job Summary
Hybrid Work Environment - Must reside in Hawaii **
Serves as a high-level business data analyst to inform initiatives and strategy within the Medicare Stars team. Works independently to monitor Medicare Star Ratings program performance at the plan and measure level and manages all aspects of Stars performance data: identification, synthesis of disparate sources, analysis, management, reporting and visualization. Supports efforts with internal and external partners by providing relevant and actionable data.
Minimum Qualifications
Bachelor's degree and four years of related work experience; or equivalent combination of education and related work experience.
Demonstrated knowledge in identifying issues, collecting data, and analyzing and interpreting data.
Intermediate working knowledge of Microsoft Office applications including Word, PowerPoint and Outlook.
Advanced working knowledge of Microsoft Excel.
Intermediate working knowledge of MicroStrategy, SQL, SAS, Access, R or other business intelligence language/tools.
Intermediate working knowledge of Tableau or other data visualization tools.
Duties And Responsibilities
Research and Analysis:
Proactively develops, implements and/or enhances methods to extract complex data sets from available sources (e.g., HMSA data repositories, HEDIS software, external party extracts, etc.).
Integrates data and uses advanced analytical methods/tools to identify trends, issues, root causes and focused population segments. Translates data into actionable visualizations using tables, graphs, charts, written reports and presentations.
Provides insights and recommendations to internal and external partners to support Stars performance improvement initiatives.
Applies understanding of standard quality measures (e.g., HEDIS, CAHPS, HOS) to identify additional strategic improvement levers (e.g., supplemental data, benefit alignment) that may impact Stars performance.
Ensures Stars measure data is complete and accurate by implementing effective controls and validation activities.
Cross-Functional Collaboration
Participates in various projects and cross-departmental teams that support the Stars program. Prioritizes Stars performance data needs across efforts.
Serves as the Stars performance data subject matter expert in various workgroups and efforts (e.g., data strategies, HEDIS performance, provider performance platforms such as Coreo, etc.). Writes business requirements and test scripts, performs unit and system testing.
Coordinates Stars performance data activities with related quality team activities (e.g., HEDIS data team, Payment Transformation quality team, FQHC/RHC quality team).Coordinates Stars improvement activities with the HCC risk adjustment team to ensure integration and alignment of quality goals with HCC risk adjustment objectives.
Represents HMSA in Blues plan and/or other industry work groups to compare best practices for improving Stars performance data.
Stars Performance Monitoring
Maintains a thorough understanding of the Medicare Part C & D Star Ratings Technical Notes. Monitor proposed changes, proactively identify potential risks to Stars performance, as well as associated mitigation strategies.
Gathers data to create and publish the Stars dashboard and related metrics on a monthly basis. Proactively identifies and supports acquisition of new data sources.
Distributes actionable data to facilitate Stars performance monitoring against current targets and year-end projections.
Works with internal and external teams to proactively identify and implement Stars dashboard enhancements.
Validates CMS Stars plan performance reports (e.g., First Plan Preview, Westat MCAHPS reports) against internal data.
Other Duties/Functions
Performs all other miscellaneous responsibilities and duties as assigned or directed.
Show more
Show less","Microsoft Office, Microsoft Excel, MicroStrategy, SQL, SAS, Access, R, Tableau, Data visualization tools, Data extraction, Data analysis, Data interpretation, Data presentation, Business intelligence, HEDIS, CAHPS, HOS, Medicare Part C & D Star Ratings Technical Notes","microsoft office, microsoft excel, microstrategy, sql, sas, access, r, tableau, data visualization tools, data extraction, data analysis, data interpretation, data presentation, business intelligence, hedis, cahps, hos, medicare part c d star ratings technical notes","access, business intelligence, cahps, data extraction, data interpretation, data presentation, data visualization tools, dataanalytics, hedis, hos, medicare part c d star ratings technical notes, microsoft excel, microsoft office, microstrategy, r, sas, sql, tableau"
Medicare Stars Senior Business Data Analyst,HMSA,"Honolulu, HI",https://www.linkedin.com/jobs/view/medicare-stars-senior-business-data-analyst-at-hmsa-3768895286,2023-12-17,Hawaii,United States,Mid senior,Hybrid,"Job Summary
Hybrid Work Environment - Must reside in Hawaii **
Serves as a lead business data analyst to inform initiatives and strategy within the Medicare Stars team. Works independently to monitor Medicare Star Ratings program performance at the plan and measure level and manages all aspects of Stars performance data: identification, synthesis of disparate sources, analysis, management, reporting and visualization. Supports efforts with internal and external partners by providing relevant and actionable data.
Minimum Qualifications
Bachelor's degree and five years of related work experience; or equivalent combination of education and related work experience.
Demonstrated knowledge in identifying issues, collecting data, and analyzing and interpreting data.
Intermediate working knowledge of Microsoft Office applications including Word, PowerPoint and Outlook.
Advanced working knowledge of Microsoft Excel.
Advanced working knowledge of MicroStrategy, SQL, SAS, Access, R or other business intelligence language/tools.
Intermediate working knowledge of Tableau or other data visualization tools.
Duties And Responsibilities
Research and Analysis:
Proactively develops, implements and/or enhances methods to extract complex data sets from available sources (e.g., HMSA data repositories, HEDIS software, external party extracts, etc.).
Integrates data and uses advanced analytical methods/tools to identify business-critical trends, issues, root causes and focused population segments.Translates data into actionable visualizations using tables, graphs, charts, written reports and presentations.
Works closely with management to develop short and long-term departmental objectives.
Synthesizes data insights and stakeholder input to provide recommendations to internal and external partners to support Stars performance improvement initiatives.
Applies expert understanding of standard quality measures (e.g., HEDIS, CAHPS, HOS) to identify additional strategic improvement levers (e.g., supplemental data, benefit alignment) that may impact Stars performance.
Ensures Stars measure data is complete and accurate by implementing effective controls and validation activities.
Cross-Functional Collaboration
Participates in and contributes to various projects and cross-departmental teams that support the Stars program. Prioritizes Stars performance data needs across efforts.
Serves as the lead Stars performance data subject matter expert in various workgroups and efforts (e.g., data strategies, HEDIS performance, provider performance platforms such as Coreo, etc.). Writes business requirements and test scripts, performs unit and system testing.
Coordinates Stars performance data activities with related quality team activities (e.g., HEDIS data team, Payment Transformation quality team, FQHC/RHC quality team). Coordinates Stars improvement activities with the HCC risk adjustment team to ensure integration and alignment of quality goals with HCC risk adjustment objectives.
Represents HMSA in Blues plan and/or other industry work groups to compare best practices for improving Stars performance data.
Stars Performance Monitoring
Maintains a high-level understanding of the Medicare Part C & D Star Ratings Technical Notes. Monitor proposed changes, proactively identify potential risks to Stars performance, as well as associated mitigation strategies.
Gathers data to create and publish the Stars dashboard and related metrics on a monthly basis. Proactively identifies and supports acquisition of new data sources.
Distributes actionable data to facilitate Stars performance monitoring against current targets and year-end projections.
Works with internal and external teams to proactively identify and implement Stars dashboard enhancements.
Validates CMS Stars plan performance reports (e.g., First Plan Preview, Westat MCAHPS reports) against internal data.
Other Duties/Functions
Performs all other miscellaneous responsibilities and duties as assigned or directed.
Show more
Show less","Medicare Star Ratings program performance, Data analysis, Data visualization, Data management, SQL, SAS, Access, R, Tableau, Microsoft Office, Microsoft Excel, MicroStrategy, Business Intelligence, HEDIS, CAHPS, HOS, Coreo, CMS Stars Plan Performance Reports, Data Dashboard, Data Validation, Data Acquisition","medicare star ratings program performance, data analysis, data visualization, data management, sql, sas, access, r, tableau, microsoft office, microsoft excel, microstrategy, business intelligence, hedis, cahps, hos, coreo, cms stars plan performance reports, data dashboard, data validation, data acquisition","access, business intelligence, cahps, cms stars plan performance reports, coreo, data acquisition, data dashboard, data management, data validation, dataanalytics, hedis, hos, medicare star ratings program performance, microsoft excel, microsoft office, microstrategy, r, sas, sql, tableau, visualization"
Lead Data Governance Analyst,Burtch Works,Greater Chicago Area,https://www.linkedin.com/jobs/view/lead-data-governance-analyst-at-burtch-works-3764329851,2023-12-17,Palatine,United States,Mid senior,Remote,"Looking for a great full-remote opportunity in a highly stable industry? Our client, a healthcare system non-profit, is seeking a Lead Data Governance Quality Analyst. In this role you will own the data governance strategy from development to implementation, manage metadata repositories, design and manage a data governance framework, and select and train data stewards for the organization.
Requirements:
Bachelor’s degree in business, healthcare, data, or other relevant field
SQL expertise a must
Demonstrated experience implementing data governance framework
Demonstrated experience in hands-on data analysis
Prior experience in healthcare a huge plus!
This is a fully remote position! Sponsorship is not available at this time. Compensation ranges from $120k-140k depending upon experience.
Keywords: data quality, repositories, lineage, dictionaries, catalogs, powerbi, azure, aws, gcp, python, excel, snowflake, tableau, oracle, ibm, SAP
Show more
Show less","Data governance, Data quality, Data analysis, Metadata repositories, Data governance framework, Data stewardship, SQL, Python, PowerBI, Azure, AWS, GCP, Snowflake, Tableau, Oracle, IBM, SAP, Excel","data governance, data quality, data analysis, metadata repositories, data governance framework, data stewardship, sql, python, powerbi, azure, aws, gcp, snowflake, tableau, oracle, ibm, sap, excel","aws, azure, data governance, data governance framework, data quality, data stewardship, dataanalytics, excel, gcp, ibm, metadata repositories, oracle, powerbi, python, sap, snowflake, sql, tableau"
Global Market Leading Energy Firm - Staff Data Engineer,Xcede,Greater Chicago Area,https://www.linkedin.com/jobs/view/global-market-leading-energy-firm-staff-data-engineer-at-xcede-3606679693,2023-12-17,Palatine,United States,Mid senior,Hybrid,"Global Market Leading Energy Firm - Staff Data Engineer
My client is a Global Market Leading Energy Firm leading the global energy transition. They offer the opportunity to work with cutting-edge technologies with excellent professional growth. They have a great reputation across industry & offer an excellent work environment.
The role is paying $180,000 - $210,000 base plus a bonus & stock.
NOTE: The role is flexible with 3 days in the Chicago office and 2 days working from home - this is non-negotiable.
·
MUST
have a Masters's degree or a relevant bachelor's degree - with a
high GPA from college or university.
·
Hands-on experience
designing, planning, productionizing & maintaining reliable and scalable data infrastructure and data products in complex environments.
·
Experience
designing and implementing large-scale distributed systems.
·
Experience
with different query languages
·
Demonstrable
coding expertise in one or more object-oriented programming languages.
Show more
Show less","Data Infrastructure, Data Products, Distributed Systems, Query Languages, ObjectOriented Programming","data infrastructure, data products, distributed systems, query languages, objectoriented programming","data infrastructure, data products, distributed systems, objectoriented programming, query languages"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744394289,2023-12-17,Torrington,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data classification, Data retention","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehouses, etl, data classification, data retention","airflow, continuous integration, data classification, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Information Technology Analyst IV (Data Engineer),Solano County,"Fairfield, CA",https://www.linkedin.com/jobs/view/information-technology-analyst-iv-data-engineer-at-solano-county-3751779772,2023-12-17,Antioch,United States,Associate,Onsite,"Salary:
$54.42 - $66.14/hour; $9,432.38 - $11,465.12/month; $113,188.57 - $137,581.41/year
Introduction
Enjoy great benefits, job security and contribute to your community at Solano County!
Department of Information Technology
The Department of Information Technology (DoIT) at Solano County provides customer-oriented and convenient access to information and services through the use of technology; anytime - anywhere. The County strives for a cost-effective use of technology, with interactive exchange and sharing of data within departments, with constituents, with other government organizations and business partners.
Find out more about the Department of Information Technology by clicking on the following link: Department of Information Technology
THE POSITION
Information Technology Analyst IV
(Data Engineer)
As an Information Technology Analyst IV (Data Engineer) you will apply advanced levels of specialized and technical analytical skills and knowledge while serving as team leader/project manager for designated major systems. You will perform the most difficult tasks within the Information Technology Analyst series, including providing advanced technical support to system users. You will provide lead direction to software development, network/systems administration, service desk, or security and database systems, other Information Technology Analysts, and/or consultants on assigned systems design, or infrastructure and maintenance projects. Supervision will be from our Information Technology Analyst (Principal) as well as our Information Technology Manager. You may also receive functional direction from the Assistant Director and/or Chief Information Officer.
The Data Management Team!
As an Information Technology Analyst IV (Data Engineer), you will be a prominent member of our Data Management team supporting the Central IT Department of the County of Solano, two (2) data centers, and eighteen (18) departments with over 3,000 employees across multiple industries, ultimately providing service to over 447,643 constituents.
As a member of our Data Management Team, you will primarily support our Health and Social Services Department which includes Public Health, Social and Behavioral Health Services, and has the highest demand for data use and reporting requirements. You will profile and use data sources to build a data model to generate clean and accurate data for report writers and data users across the County. You will also work with data users to identify and anticipate reporting and data needs, extract data from the source systems, profile and load it into a model, and generate reports and analytics.
Education And Experience Requirements
Education
Equivalent to an Associate’s degree, preferably in information technology, or a closely related field.
Experience
Depending upon assignment, five (5) years of experience performing progressively responsible software development, or infrastructure management duties and functions.
Note
: A Bachelor’s degree from an accredited college or university, preferably in information technology, management information systems may be substituted for two years of experience.
Note:
Additional experience may substitute on a year for year basis for the educational
requirement.
Please click on the following link to access the job description:
Information Technology Analyst IV
The Ideal Candidate
The ideal candidate is an excellent communicator; they are collaborative, motivating, entrepreneurial, and comfortable working autonomously. They will form meaningful partnerships within the Department and with stakeholders throughout the County. Our ideal candidate embraces change and looks for opportunities for improvement. They enjoy examining data, visualizing, and creating a story, and can project practical applications for the data in multiple scenarios.
Desired Advanced Knowledge And Experience
Modern Data warehouse
Azure Data Lake
Azure Synapse
Data Modeling with Kimball Methodology
Facts and Dimensions
ETL, SSIS, Data Factory
SQL, Python, R, SA
Power BI
DevOps
Purview
Dynamics CRM
Desired Certifications
Azure Data Engineering
Azure Solution Architect
DAMA certified (CDMP)
Power BI Data Analyst
BENEFITS/ WHAT'S IN IT FOR YOU?
Benefits Summary
Solano County offers a cafeteria-style medical package with health benefits, offered through CalPERS. The County contribution for family coverage is $1,900.58 per month for 2023. The County offers a cash back provision for those who choose employee-only or who waive medical insurance coverage. The County may offer a supplemental contribution for employees enrolled in Employee plus Two or More coverage.
Dental and vision insurances for the employee and eligible dependents are paid 100% by the County.
Solano County participates in CalPERS retirement and contributes to Social Security.
The County observes twelve (12) full day fixed and two (2) half day fixed paid holidays per year. Additionally, employees in this bargaining unit receive two (2) floating paid holiday(s) per year. Vacation is accrued at approximately ten (10) days per year. Sick leave accrues at approximately 12 days per year. Effective July 1 of each year, 80 hours of administrative leave is granted.
Employees are eligible to receive an additional 2.5% longevity pay, per level, after the completion of continuous service at 10, 15, 20, 25, 30 and 35 years.
Please click on the following link to access the benefits summary:
Benefits Summary
Learning and Development Culture
Solano County Is Committed To “Invest In And For The Future” By Providing Training Resources To Encourage Employee Professional Development And Growth Within Our Organization. While Employed With Solano County, Employees Have The Opportunity To Pursue Their Career Goals, Interests, And Develop The Competencies On The Solano County Leadership Development Model By Participating In The Following Programs:
Tuition Reimbursement Program
Annual Education Fair
County Mentoring Program
Leadership Academy
Supervisory Trainings
Skill Development Trainings
Self-paced learning opportunities
SELECTION PROCESS
12/01/2023 - Deadline to submit application along with educational documents.
Based on the information provided in the application documents, the qualified applicants may be invited for further examination and will either be pre-scheduled by the Department of Human Resources or be invited to self-schedule. All applicants meeting the minimum qualifications are not guaranteed advancement through any subsequent phase of the examination. Depending upon the number of applications received, the selection process may consist of an initial application screening, a mandatory information meeting, a supplemental questionnaire assessment, a written and/or practical exam, an oral board exam, or any combination listed. Responses to supplemental questions may be used as screening and testing mechanisms and will be used to assess an applicant’s ability to advance in the process; as such, responses to supplemental questions should be treated as test examination responses. Information contained herein does not constitute either an expressed or implied contract.
A minimum score of 70% is required to continue in the selection process, unless otherwise announced. All potential new hires and employees considered for promotion to management, confidential positions or unrepresented positions will be subject to a background and reference check after contingent job offer is accepted. These provisions are subject to change.
RETIREES - Solano County invites all to apply for positions; however pursuant to Government Code Section 21221(h) and 21224, hiring restrictions may apply to California Public Sector Pension Plan Retirees.
How To Apply
Please click on ""Apply Online"" at the bottom of this posting.
Applications must be submitted through the JobAps system. Paper copies of applications are not accepted. All additional application materials as requested in the job announcement (degree/transcripts, certificates, DD-214 if applicable, ADA Accommodation Request) must be submitted by fax to (707) 784-3424, or by email to recruitment@solanocounty.com and are due by the application review date. Be sure to include the recruitment title (Information Technology Analyst IV) and the recruitment number (23-364060-07) in your email or fax.
Previously submitted application materials (i.e. copies of diploma and/or transcripts, etc.) for prior recruitments will not be applied for this recruitment but must be re-submitted for this recruitment. Any further questions can be directed to the Department of Human Resources at (707) 784-6170, business hours are Monday-Friday, 8:00 a.m.-5:00 p.m. EOE/AA.
Please note that all dates/times listed in the job announcement are Pacific Time.
Document Submittal Requirements
EDUCATION DOCUMENTS MAY BE REQUIRED
All candidates qualifying for the position under the education requirement must submit a copy of their official/unofficial transcripts (verifying the courses and units completed) or degree (verifying date, degree and area of specialization conferred) by the application submittal deadline. Candidates who fail to submit their transcripts by the application submittal deadline will be disqualified from the recruitment.
PLEASE NOTE THE FOLLOWING: Candidates who attended a college or university that is accredited by a foreign or non-U.S. accrediting agency must have their educational units evaluated by an educational evaluation service. The result must be submitted to the Human Resources Department no later than the close of the recruitment. Please contact the local college or university to learn where this service can be obtained.
How To Submit Your Documents
In addition to uploading attachments when applying online, candidates may submit documents by fax to (707) 784-3424, or by email to recruitment@solanocounty.com. Be sure to include the recruitment title (Information Technology Analyst IV) and the recruitment number (23-364060-07) in your email or fax.
SUPPLEMENTAL QUESTIONNAIRE
The Supplemental Questionnaire will be used to determine applicants’ qualifications for this position and assess an applicant’s ability to advance in the recruitment process; therefore, applicants are encouraged to answer all questions thoroughly and completely. Omitted information will not be considered or assumed. Applicants who have no experience in a specific area are recommended to state ""no experience in this area"" instead of leaving the space blank.
Please note that the experience in your answers must be reflected in your employment history.
I understand that I must fill out all sections of this application thoroughly and completely. In addition, I understand that stating ""see resume"" in any areas of this application will be considered an incomplete application.
Yes No
How many years of full-time experience do you have performing progressively responsible software development or infrastructure management duties and functions?
No experience
Less than 1 year
2 to 3 years
4 to 5 years
6+ years
What is the highest level of education you have completed?
High School Diploma
Associate Degree
Bachelor's Degree
Master's Degree
How many years of experience do you have working with cloud platforms (Azure, AWS, etc.)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with data platform management systems (Microsoft, Informatica, Redshift, Snowflake)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with Electronic Health Record Systems (Avatar, NextGen, Epic)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with Content Management Systems (Hyland, Documentum)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with Statistical/Data analytics tools (R, Python, SAS)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with CRM Systems (Dynamics365, Saleforce)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have working with advanced Report and BI Development (Power BI, SQL, Crystal Reports, Tableau, Qlik)?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with IT Project Management?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Data Governance?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Data Warehousing?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Reference and Master Data?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Metadata?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Data Quality?
No experience
1-2 years
3-5 years
6+ years
How many years of experience do you have with Data Security?
No experience
1-2 years
3-5 years
6+ years
Please indicate which of the following certifications you have achieved (select all that apply):
Azure Fundamentals Azure Data Fundamentals Azure AI Fundamentals Power BI Data Analyst Azure Data Scientist Azure Solution Architect Azure Data Engineer DAMA certification (CDMP) I don't have any of the above certifications
I understand I must submit a copy of my Associates degree or transcripts if qualifying under the education requirement for this position.
Yes No
Will you be submitting proof of education? If so, how will you be submitting proof of education (transcripts, copy of degree, etc.)?
Attached to this application
Via email at recruitment@solanocounty.com
I will not be submitting educational documents
Supervision Received and Exercised
Supervision is received from a Principal Information Technology Analyst or an Information Technology Manager. The incumbent may receive functional direction from the Assistant Director and/or Chief Information Officer. Incumbents in this classification may function as a project manager and/or team leader over technical and professional staff.
VETERANS PREFERENCE POINTS
To be eligible, applicant must have served at least 181 consecutive days of active duty in the Armed Forces of the United States and have received either an honorable discharge or a general discharge under honorable conditions. A COPY OF THE DD 214, SHOWING DISCHARGE TYPE (GENERALLY COPY 4), MUST BE RECEIVED IN THE HUMAN RESOURCES DEPARTMENT BY THE APPLICATION SUBMITTAL DEADLINE. Applicants who have a service connected disability must also submit a recent award letter from the VA stating they are receiving disability benefits for service connected reasons.
Veteran applicants for initial County employment with an honorable or general under conditions discharge shall receive five (5) points added to their combined score. Disabled veterans rated at not less than 30% disability shall have ten (10) points added to their combined score. Veteran’s preference points will only be added to passing scores in competitive open examinations.
Americans With Disabilities Act
It is the policy of Solano County that all employment decisions and personnel policies will be applied equally to all County employees and applicants and be based upon the needs of County service, job related merit, and ability to perform the job.
APPLICANTS WITH DISABILITIES: Qualified individuals with a disability, who are able to perform the essential functions of the job, with or without reasonable accommodation, and need an accommodation during any phase of the recruitment/testing/examination process (as detailed in the “Selection Process”), must complete the following form: Request for Testing Accommodation by Applicants with Disabilities Form
This form must be received in the Human Resources Department by the final filing date of the recruitment. Applicants will be contacted to discuss the specifics of the request.
SOLANO COUNTY
Click here to take a video tour of Solano County
OUR COMMUNITY
Solano County is the ideal place to live, learn, work and play... The America's Promise Alliance has named Solano County as one of the 100 Best Communities for Young People for six straight years - the only California community with that distinction.
Live -
Solano County as well as cities within the County have ranked in the top 15 hottest markets across the country and within the Bay Area due to prime location and affordability.
Learn
- Higher education abounds! Within the County, education choices include: Solano Community College, CSU Maritime Academy, Brandman University, and Touro University. Bordering our County is the renowned University of California Davis.
Work
- The blend of agriculture, corporate business and pleasant lifestyle enhance the attraction of Solano County. Blessed with a thriving agricultural economy, the county is also home to biotechnology and other growth industries.
Play
- Situated midway between San Francisco and Sacramento-the State capitol, Solano County is home to rolling hillsides, waterfronts and fertile farmland. County residents can enjoy day trips to the San Francisco Bay area, Lake Tahoe region and the Napa and Sonoma Valleys.
County Population (2019): 447,643
The provisions of this bulletin do not constitute an expressed or implied contract. Any provision contained in this bulletin may be modified or revoked without notice.
SOLANO COUNTY IS AN EQUAL EMPLOYMENT OPPORTUNITY EMPLOYER
Closing Date/Time: 12/15/2023 5:00:00 PM
Show more
Show less","Azure Data Lake, Azure Synapse, Data Modeling, Kimball Methodology, ETL, SSIS, Data Factory, SQL, Python, R, SA, Power BI, DevOps, Purview, Dynamics CRM, Azure Data Engineering, Azure Solution Architect, DAMA certified (CDMP), Power BI Data Analyst","azure data lake, azure synapse, data modeling, kimball methodology, etl, ssis, data factory, sql, python, r, sa, power bi, devops, purview, dynamics crm, azure data engineering, azure solution architect, dama certified cdmp, power bi data analyst","azure data engineering, azure data lake, azure solution architect, azure synapse, dama certified cdmp, data factory, datamodeling, devops, dynamics crm, etl, kimball methodology, power bi data analyst, powerbi, purview, python, r, sa, sql, ssis"
Future Opportunity- Data Engineering Consultant,Avanade,"Concord, CA",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781779637,2023-12-17,Antioch,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Databricks, SQL technologies, Azure Synapse, Data security, Data integrity, Data handling, Data understanding, Data analysis, Data interpretation, Entity extraction, Relationship extraction, Database indexing, Data pipelines, Data streams, System integration","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, databricks, sql technologies, azure synapse, data security, data integrity, data handling, data understanding, data analysis, data interpretation, entity extraction, relationship extraction, database indexing, data pipelines, data streams, system integration","azure databricks, azure synapse, data handling, data integrity, data interpretation, data security, data streams, data understanding, dataanalytics, database indexing, databricks, datapipeline, entity extraction, microsoft fabricsynapse, powerbi, purview, python, relationship extraction, spark, sql technologies, system integration, tsql"
Senior Cloud Data Engineer,BDO USA,"San Ramon, CA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467838,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Data Lake Medallion Architecture, Batch/Streaming Data Ingestion, AI Algorithms/Machine Learning, Cloud Data Analytics Solutions, SQL, DDL, DML, Views, Functions/Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Git, DevOps, Linux, Tableau, Microsoft Fabric, Power BI, Azure Analysis Services, UiPath, Alteryx, Computer Vision, .Net, Qlik, Azure Data Factory, RedShift, AWS, Synapse, IoT, RPA, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS, Snowflake, Athena, Data Pipeline, Glue, Azure","data analytics, business intelligence, data warehousing, data modeling, semantic model definition, star schema construction, data lake medallion architecture, batchstreaming data ingestion, ai algorithmsmachine learning, cloud data analytics solutions, sql, ddl, dml, views, functionsstored procedures, performance tuning, c, python, java, scala, git, devops, linux, tableau, microsoft fabric, power bi, azure analysis services, uipath, alteryx, computer vision, net, qlik, azure data factory, redshift, aws, synapse, iot, rpa, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs, snowflake, athena, data pipeline, glue, azure","ai algorithmsmachine learning, alteryx, athena, aws, azure, azure analysis services, azure data factory, batchstreaming data ingestion, bicep, business intelligence, c, cloud data analytics solutions, computer vision, data lake medallion architecture, data ops, data pipeline, dataanalytics, datamodeling, datawarehouse, dbt, ddl, delta, devops, dml, functionsstored procedures, git, glue, iot, java, linux, microsoft fabric, net, pandas, performance tuning, powerbi, purview, python, qlik, redshift, rpa, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, synapse, tableau, terraform, uipath, views"
Senior Product Developer | Automotive Data Analytics,Epicor,"Dublin, CA",https://www.linkedin.com/jobs/view/senior-product-developer-automotive-data-analytics-at-epicor-3779497529,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Requirements
Description & Requirements
Senior Product Developer
The Senior Product Developer is responsible for designing, coding, testing, and maintaining company products by participating in all phases of the development process. The Product Developer, Sr works with other areas of the business to provide recommendation s and mentors other team members.
What You Will Be Doing:
Writes, refactors, and tests new and existing applications/f ramework components.
Actively participates in designing and reviewing core functionality.
Designs and delivers formal or informal training, mentorship, and supports other developers.
Develops, analyzes and maintains tools that support and automate processes for hardware or software product release.
Looks to constantly improve coding standards and provide recommendation s.
Creates and provide estimates of work.
Proactively delivers work on time with good quality.
What you will likely bring:
3-5 years of experience within Oracle Databases
3-5 years of experience using Tableau or other BI Tools
3-5 years experience using MySQL or SQL Server
Ability to propose and implement new ideas.
Ability to lead a team to meet deadlines (technical and time management)
Ability to lead and mentor others.
Analytical thinking and creative problem-solving skills.
Thought leader and change agent.
What Could Set You Apart:
5+ years of applicable experience and demonstrated success/knowle dge.
2+ years of specialized/in dustry experience.
Bachelor’s degree (or equivalent experience).
About Epicor
At Epicor we know that success comes from working together. Everyone has a role to play, and it’s the essential partnerships across our company that are crucial to our customers’ success and our growth as a business.
We’re truly a team. Working in close partnership, we bring wide-ranging talents together in powerful collaborations. We think innovatively, share our knowledge generously, and constantly learn from our colleagues. We’re proud of the success we achieve every day, but we never stop challenging ourselves and encouraging each other. Together, we go further and imagine an even brighter future.
Whatever your career journey, we’ll help you find the right path. Through our training courses, mentorship, and continuous support, you’ll get everything you need to thrive. At Epicor, your success is our success. And that success really matters, because we’re the essential partners for the world’s most essential businesses—the hardworking companies who make, move, and sell the things the world needs.
Equal Opportunities and Accommodations Statement
Epicor is committed to creating a workplace and global community where inclusion is valued; where you bring the whole and real you—that’s who we’re interested in. If you have interest in this or any role- but your experience doesn’t match every qualification of the job description, that’s okay- consider applying regardless.
We are an equal-opportunity employer.
Show more
Show less","MySQL, Oracle Databases, SQL Server, Tableau, BI Tools, Agile Methodology, Coding Standards, Application Development, Testing, Training, Mentorship, Problem Solving, Analytical Thinking, Leadership, Teamwork, Communication, Change Management","mysql, oracle databases, sql server, tableau, bi tools, agile methodology, coding standards, application development, testing, training, mentorship, problem solving, analytical thinking, leadership, teamwork, communication, change management","agile methodology, analytical thinking, application development, bi tools, change management, coding standards, communication, leadership, mentorship, mysql, oracle databases, problem solving, sql server, tableau, teamwork, testing, training"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Antioch, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709544,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Distributed Systems, Data Extraction, Data Ingestion, Data Processing, Airflow, KubeFlow, Pipeline Tools, NLP, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, DynamoDB, ETL, Kafka, Storm, Spark Streaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention, 401K, Bonus, Equity Programs, GenderAffirming Offerings, Included Health, HRT Stipends, Flexible Vacation, Cell Phone Stipend, Internet Stipend, Wellness Stipend, Food Stipend, HomeOffice Setup Stipend, CompanySponsored Events","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, distributed systems, data extraction, data ingestion, data processing, airflow, kubeflow, pipeline tools, nlp, large language models, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, kafka, storm, spark streaming, applied machine learning, data management tools, data classification, data retention, 401k, bonus, equity programs, genderaffirming offerings, included health, hrt stipends, flexible vacation, cell phone stipend, internet stipend, wellness stipend, food stipend, homeoffice setup stipend, companysponsored events","401k, airflow, applied machine learning, aws, azure, bash, bonus, cell phone stipend, companysponsored events, data classification, data cleaning, data engineering, data extraction, data ingestion, data management tools, data mining, data normalization, data processing, data retention, datamodeling, distributed systems, docker, dynamodb, equity programs, etl, flexible vacation, food stipend, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, hrt stipends, included health, internet stipend, java, kafka, kubeflow, kubernetes, large language models, machine learning, nlp, nosql databases, pandas, pipeline tools, python, r, relational databases, snowflake, spark, spark streaming, sql, statistical analysis, storm, visualization, wellness stipend"
"Staff Software Engineer, Data & Connectivity",Ridgeline,"San Ramon, CA",https://www.linkedin.com/jobs/view/staff-software-engineer-data-connectivity-at-ridgeline-3600374099,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Ridgeline is
the
industry cloud platform for investment management – the single source of truth with data integrated in real-time across the front, middle, and back office. We are the hub of operations for investment management firms, which necessitates instantaneous delivery of complex data and integrations across ancillary services that allow investment managers to get their jobs done.
As a Ridgeline Staff Software Engineer, Data & Connectivity, you’ll work closely with the Engineering and Product Management teams to define and execute the vision to build multiple data transformation and data streaming services as a core part of the Ridgeline system. You will have the unique opportunity to build high-quality, creative business applications in a fast-moving, progressive work environment. You’ll think outside the box and add your own genius, passion, and interests to the software development lifecycle, leaving a legacy in an industry primed for change.
What will you do?
Guide technology decisions for data pipelines that consume financial data and tie the Ridgeline platform into the investment management ecosystem
Deliver a data network that connects hundreds of financial institutions for data aggregation, reporting, analytics, and automated fund reconciliation
Have end-to-end responsibility for technical facets and flow of data in a complex system, including the product, infrastructure, security, and quality
Impact a developing tech stack based on modern front-end frameworks and cost-efficient utilization of AWS back-end services
Contribute business insight, design skills, and technical expertise to a team where design, strategy, and engineering collaborate closely
Be involved in the entire software development process, from requirements and design reviews through the implementation of a new product
Identify emerging technologies, champion them, and incorporate them into our best-of-breed products
Participate in the creation and construction of developer-based automation that leads to scalable, high-quality applications customers will depend on to run their businesses
Think creatively, own problems, seek solutions, and communicate clearly along the way
Contribute to a collaborative environment deeply rooted in learning, teaching, and transparency
Demonstrate a strong dedication to code quality, automation, and operational excellence using unit/integration/component tests and workflows
Desired Skills And Experience
7+ years in a software engineering position or similar function, with a history of architecting and designing new products and technologies
A degree in Computer Science, Information Science, or a related discipline
Mastery of data structures, algorithms, and architectural patterns
Expertise in one or more of the following languages: Python, Java, Kotlin
Working knowledge or expertise in JavaScript, TypeScript, HTML, CSS, React
Experience using frameworks such as Hibernate and Spring
Strong background in designing data models for both relational and NoSQL databases
Designing scalable, durable, and highly available microservices
Practiced at performance monitoring, identifying issues, and relieving bottlenecks
Willingness to learn about new technologies while simultaneously developing expertise in a business domain/problem space
Ability to focus on short-term deliverables while maintaining a big-picture perspective
An aptitude for problem-solving
Ability to communicate effectively with colleagues at all levels and all audiences
A serious interest in having fun at work
Nice-to-Haves
Experience building responsive Web interfaces using frameworks such as React and Redux
In-depth experience with database engines such as MySQL, Postgres, Oracle, DynamoDB, MongoDB, Redis
Understanding of building back-end infrastructure using AWS offerings such as Aurora, Lambda, S3, Aurora, Lambda, S3, ECS, Fargate, ElastiCache, and API Gateway
Experience with agile development methodologies
Background in design, economics, or business
Involvement in writing ETL or integrating with products such as Workato.
About Ridgeline
Ridgeline is the industry cloud platform for investment management. It was founded in 2017 by visionary entrepreneur Dave Duffield (co-founder of both PeopleSoft and Workday) to address the unique technology challenges of an industry in need of new thinking. We are building a modern platform in the public cloud, purpose-built for the investment management industry to empower business like never before.
Headquartered in Lake Tahoe with offices in Reno, Manhattan, and the Bay Area, Ridgeline is proud to have built a fast-growing, people-first company that has been recognized by Fast Company as a “Best Workplace for Innovators,” by LinkedIn as a “Top U.S. Startup,” and by The Software Report as a “Top 100 Software Company.”
Ridgeline is proud to be a community-minded, discrimination-free equal opportunity workplace.
Ridgeline processes the information you submit in connection with your application in accordance with the Ridgeline Applicant Privacy Statement. Please review the Ridgeline Applicant Privacy Statement in full to understand our privacy practices and contact us with any questions.
Compensation And Benefits
[For New York and California Based Only]
The typical starting salary range for new hires in this role is listed below. In select locations (including, the San Francisco Bay Area, CA, and the New York City Metro Area), an alternate range may apply as specified below.
The typical starting salary range for this role is: $165,000-$200,000.
The typical starting salary range for this role in the select locations listed above is: $175,000-$215,000.
Final compensation amounts are determined by multiple factors, including candidate experience and expertise, and may vary from the amount listed above.
As an employee at Ridgeline, you’ll have many opportunities for advancement in your career and can make a true impact on the product.
In addition to the base salary, 100% of Ridgeline employees can participate in our Company Stock Plan subject to the applicable Stock Option Agreement. We also offer rich benefits that reflect the kind of organization we want to be: one in which our employees feel valued and are inspired to bring their best selves to work. These include unlimited vacation, educational and wellness reimbursements, and $0 cost employee insurance plans. Please check out our Careers page for a more comprehensive overview of our perks and benefits.
Show more
Show less","Data Pipelines, Data Streaming Services, Modern FrontEnd Frameworks, AWS BackEnd Services, Agile Development Methodologies, Unit Testing, Integration Testing, Component Testing, Workflows, Data Structures, Algorithms, Architectural Patterns, Python, Java, Kotlin, JavaScript, TypeScript, HTML, CSS, React, Hibernate, Spring, Relational Databases, NoSQL Databases, Microservices, Performance Monitoring, Problem Solving, Communication Skills, Collaboration Skills, Learning, Teaching, Transparency, Code Quality, Automation, Operational Excellence, MySQL, Postgres, Oracle, DynamoDB, MongoDB, Redis, Aurora, Lambda, S3, ECS, Fargate, ElastiCache, API Gateway, ETL, Workato, React, Redux","data pipelines, data streaming services, modern frontend frameworks, aws backend services, agile development methodologies, unit testing, integration testing, component testing, workflows, data structures, algorithms, architectural patterns, python, java, kotlin, javascript, typescript, html, css, react, hibernate, spring, relational databases, nosql databases, microservices, performance monitoring, problem solving, communication skills, collaboration skills, learning, teaching, transparency, code quality, automation, operational excellence, mysql, postgres, oracle, dynamodb, mongodb, redis, aurora, lambda, s3, ecs, fargate, elasticache, api gateway, etl, workato, react, redux","agile development methodologies, algorithms, api gateway, architectural patterns, aurora, automation, aws backend services, code quality, collaboration skills, communication skills, component testing, css, data streaming services, data structures, datapipeline, dynamodb, ecs, elasticache, etl, fargate, hibernate, html, integration testing, java, javascript, kotlin, lambda, learning, microservices, modern frontend frameworks, mongodb, mysql, nosql databases, operational excellence, oracle, performance monitoring, postgres, problem solving, python, react, redis, redux, relational databases, s3, spring, teaching, transparency, typescript, unit testing, workato, workflows"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Antioch, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091304,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, ML Data Engine, ML Data OPs, Data Pre/Post Processing Pipelines, ML/DL Models, Statistical Analysis, Visualization, Pandas, R, Data Extraction, Data Ingestion, Data Normalization, Data Processing, Airflow, KubeFlow, Pipeline Tools, NLP, Git, Python, Java, Bash, SQL, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, DynamoDB, ETL Pipelines, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention, LGBTQ Social Networking, QueersInclusive Benefits, GenderAffirming Coverage, Included Health, HRT Stipends, Monthly Cell Phone/Internet/Wellness Stipends, Home Office Setup Stipend","data engineer, ml data engine, ml data ops, data prepost processing pipelines, mldl models, statistical analysis, visualization, pandas, r, data extraction, data ingestion, data normalization, data processing, airflow, kubeflow, pipeline tools, nlp, git, python, java, bash, sql, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl pipelines, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention, lgbtq social networking, queersinclusive benefits, genderaffirming coverage, included health, hrt stipends, monthly cell phoneinternetwellness stipends, home office setup stipend","airflow, applied machine learning, aws, azure, bash, data classification, data extraction, data ingestion, data management tools, data normalization, data prepost processing pipelines, data processing, data retention, dataengineering, docker, dynamodb, etl pipelines, gcp, genderaffirming coverage, git, helm, home office setup stipend, hrt stipends, included health, java, kafka, kubeflow, kubernetes, lgbtq social networking, ml data engine, ml data ops, mldl models, monthly cell phoneinternetwellness stipends, nlp, nosql databases, pandas, pipeline tools, python, queersinclusive benefits, r, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Dublin, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712040,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, ML data pipelines, Data engineering, Data science, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Text data processing, NLP, Large language models, Statistical analysis, Data visualization, Pandas, R, Distributed systems, Microservices","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, ml data pipelines, data engineering, data science, machine learning, data mining, data cleaning, data normalization, data modeling, text data processing, nlp, large language models, statistical analysis, data visualization, pandas, r, distributed systems, microservices","airflow, aws, azure, bash, data cleaning, data engineering, data mining, data normalization, data science, datamodeling, distributed systems, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, large language models, machine learning, microservices, ml data pipelines, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, text data processing, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Ramon, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773089712,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","python, java, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, aws, azure, data classification, data management, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Ramon, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711074,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management tools, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Dublin, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087758,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, ML, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming","data engineer, ml, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming","airflow, aws, azure, bash, dataengineering, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, ml, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Concord, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087757,2023-12-17,Antioch,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML data ops, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, DynamoDB, Kafka, Storm, Machine learning, Data mining, Data cleaning, Data normalizing, Data modeling, ETL pipelines, Conversational AI APIs, Recommender systems, Distributed systems, Microservices, Streamprocessing systems","data engineering, ml data ops, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, dynamodb, kafka, storm, machine learning, data mining, data cleaning, data normalizing, data modeling, etl pipelines, conversational ai apis, recommender systems, distributed systems, microservices, streamprocessing systems","airflow, bash, conversational ai apis, data cleaning, data engineering, data mining, data normalizing, datamodeling, distributed systems, docker, dynamodb, etl pipelines, git, helm, java, kafka, kubernetes, machine learning, microservices, ml data ops, python, recommender systems, snowflake, spark, sql, storm, streamprocessing systems"
Sr Data Automation & Application Developer,Michael Page,"Nuevo Laredo, Tamaulipas, Mexico",https://mx.linkedin.com/jobs/view/sr-data-automation-application-developer-at-michael-page-3768797141,2023-12-17,Laredo,United States,Mid senior,Onsite,"+ 5 years as a business intelligence developer in the manufacturing industry|Knowledge of Power BI, Power Apps, Power Query, Power Pivot, etc
About Our Client
Leading global design, engineering and manufacturing company that sells highest quality components and systems used in heating, air conditioning, cooking, etc.
Job Description
Recognize organizational needs in the context of BI and create data models to transform raw data into relevant insights and provide business intelligence solutions. * Use Power BI for interactive reports and visual reports. * Analyze the data and display it in reports to aid decision-making. * Work with customers to understand their needs and develop solutions that help achieve the organization's goals. * Use Power BI to run DAX queries and functions. * Create charts and data documentation with explanations of algorithms, parameters, models, and relationships. * Collaborate with teams to analyze current procedures for defining and building new systems. * Communicate needs with client teams and interns successfully. * Maintain and support existing BI solutions. * Design and implement data collection applications. * Train end users in the use of data integration tools. * *Manages and participates in the full development lifecycle, including requirements analysis and design. * * Serve as a technical guide in development projects. Draft technical specifications based on conceptual design and established business requirements. * Support, maintenance, and documentation of software functionality. * Identify and evaluate new technologies for implementation. * Analyze code to find causes of errors and revise programs as needed. * Manages and participates in software design meetings and analyzes user needs to determine technical requirements. * Collaborate with the end user to prototype, refine, test, and debug programs to meet needs. * Act as a subject matter expert for one or more technologies and take the initiative to learn new ones. * Research, create proofs of concept, and introduce new technologies to the team. * Actively learns and adopts team-defined technologies and tools. * Mentors others to accelerate their professional growth and encourages them to participate
The Successful Applicant
5 years as a business intelligence developer, data analyst in the manufacturing industry or developing applications in industry 4.0 environments
Job-Specific Knowledge:
Administration, database, and mastery in data analysis. * Advanced experience in using BI tools and systems such as Power BI / ERP Systems. * Microsoft BI stack compression. * Knowledge of Power BI, Power Apps, Power Query, Power Pivot, and Power Automate tools and technologies. * MS Office 365 / SharePoint development / Power Platform Specialist * PowerShell scripting
Other Requirements:
Knowledge of database management systems:
Online Analytical Processing (OLAP)
ETL (Extract, Transform, and Load) system.Business Intelligence Technologies:
Microsoft Power Apps
Ability to run DAX queries in Power BI DesktopSkills:
Analytical Thinking
Communication
Problem-solving skills
Initiative and innovationLanguages:
Fluent conversational English
What's On Offer
Savings fund
17 day aguinaldo
30% prima vacacional
Life insurance
Major medical insurance
Contact: Fátima Elizondo
Quote job ref: JN-112023-6259411
Mostrar más
Mostrar menos","Power BI, Power Apps, Power Query, Power Pivot, Power Automate, DAX, ETL, OLAP, ERP, SharePoint, PowerShell, Microsoft Office 365, Database management systems","power bi, power apps, power query, power pivot, power automate, dax, etl, olap, erp, sharepoint, powershell, microsoft office 365, database management systems","database management systems, dax, erp, etl, microsoft office 365, olap, power apps, power automate, power pivot, power query, powerbi, powershell, sharepoint"
Master Data Analyst,Kforce Inc,"Ballwin, MO",https://www.linkedin.com/jobs/view/master-data-analyst-at-kforce-inc-3777072399,2023-12-17,Cahokia,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is looking for a Master Data Analyst in Ballwin, MO. Summary: We are seeking a tech-savvy and motivated Master Data Analyst to join our Master Data Team. Reporting to the Master Data Team Lead, the successful candidate will be responsible for ensuring the accuracy, consistency, and optimization of our master data sets. This role offers an excellent opportunity for growth within the organization and to contribute to our data management strategies. Responsibilities:
Master Data Analyst will collaborate with various teams to understand data requirements and ensure the accuracy and completeness of master data sets
Conduct data entry, validation, and maintenance tasks to uphold consistency and integrity of master data
Assist in identifying and resolving data quality issues and discrepancies
Contribute to the development and implementation of data governance policies and best practices
As a Master Data Analyst, you will generate reports and provide data analysis to support decision-making processes
Participate in data-related projects and initiatives aimed at enhancing data quality and operational efficiency
Stay updated on industry best practices and emerging trends in data management
Requirements
Bachelor's degree in Computer Science, Information Systems, or business related field
1-3 years of experience in data analysis, preferably in master data mgmt. or a related field
Tech-savvy individual with basic understanding of enterprise resource planning (ERP) systems (e.g., IQMS, Oracle), database management systems (e.g., SQL) and data visualization tools (e.g., Power BI)
Strong analytical skills with proficiency in data analysis tools and techniques
Excellent communication and collaboration skills to work effectively within a team environment
Detail-oriented with a commitment to maintaining data accuracy and integrity
Eagerness to learn and grow within the organization
Ability to adapt to changing priorities and handle multiple tasks concurrently
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $35 - $50 per hour
Show more
Show less","Data Analysis, Master Data Management, Data Governance, Data Visualization, Enterprise Resource Planning (ERP), Database Management Systems (DBMS), SQL, Power BI, Data Quality, Data Integrity, Communication, Collaboration, DetailOriented, ProblemSolving, Adaptability, Multitasking","data analysis, master data management, data governance, data visualization, enterprise resource planning erp, database management systems dbms, sql, power bi, data quality, data integrity, communication, collaboration, detailoriented, problemsolving, adaptability, multitasking","adaptability, collaboration, communication, data governance, data integrity, data quality, dataanalytics, database management systems dbms, detailoriented, enterprise resource planning erp, master data management, multitasking, powerbi, problemsolving, sql, visualization"
SAP Data Modeler/Analyst,TekIntegral,United States,https://www.linkedin.com/jobs/view/sap-data-modeler-analyst-at-tekintegral-3696906967,2023-12-17,Cahokia,United States,Associate,Remote,"Location: Remote
Duration: 6+ Months
Rate: $ 58 / hr C2C
Visa: USC GC
Required Qualifications
Minimum of a Bachelor's degree
5+ years' experience
Knowledge of ERP [SAP/JDE] database/tables [Sales and Distribution /Material Management modules]
Multiple years of work experience in S/4 HANA data analytics
Multiple years of work experience with S/4 HANA operational report suite
Experience in SQL [legacy Hadoop analysis] + Data bricks/Synapse and Azure stack [minimum ability to set up and do basic querying]
Mastery of project management methodology, tools, and templates (including program/project planning, schedule development, scope management, and cost management)
Working knowledge of Tableau/Power BI would be a nice to have
Project Planning and experience writing user stories in JIRA
Experience in testing and test methodologies
Ability to learn new functions and technologies fast learner
Proven ability to work in a global matrix, multi-functional organization
Advanced communication and collaborative skills
Desired Qualifications
Existing Client experience
Experience in supply chain
Understanding of the Scrum process
Superior reporting skills
Project management certification (PMI/CAPM or PMP, PRINCE2)
Show more
Show less","ERP, SAP, JDE, SQL, Data Bricks, Synapse, Azure Stack, Tableau, Power BI, Jira, Scrum, PMP, CAPM, PRINCE2","erp, sap, jde, sql, data bricks, synapse, azure stack, tableau, power bi, jira, scrum, pmp, capm, prince2","azure stack, capm, data bricks, erp, jde, jira, pmp, powerbi, prince2, sap, scrum, sql, synapse, tableau"
Data Business Analyst,"Anveta, Inc",United States,https://www.linkedin.com/jobs/view/data-business-analyst-at-anveta-inc-3697346096,2023-12-17,Cahokia,United States,Associate,Remote,"Please read the below mentioned detail carefully before applying for this opportunity
:
C2C job opportunity
OPT
/
CPT / H1B to please excuse for this role
Note
: Resources local to
Raleigh
,
NC
or
from an easily commutable distance to the given location
or candidate from adjoining states who are willing to go onsite at Raleigh
,
NC
to be ONSITE from the Day
-
1 of the project with there families
(no cross country moves)
are only
requested to apply
.
Position: Data Business Analyst
Location: Remote @ Raleigh
,
NC
Required Experience: 5
-
10 Years
Duration: 6+ months contract
Ideal Start Date: ASAP
Our client aims to establish an enterprise data model with a primary goal of standardizing shared data entities and attributes and a secondary goal of standardizing database management systems and tools. Related transactional databases and data warehouses will be hosted in Azure, generally using Microsoft Dataverse and SQL Server. Technologies in use include Oracle, DB2, MySQL, and SQL Server, as well as desktop-based applications such as MS Access and MS Excel.
Job Responsibilities
Provide analysis and leadership on data analysis projects across the enterprise.
Provide insight to other data analysts, data architects, and application developers related to creating an enterprise data model that meets the business and technical requirements.
Provide subject matter expertise related to Microsoft Dataverse, SQL Server, Azure Data Factory, and SSIS
.
Mandatory To Have Skills
3 Years - Outstanding interpersonal and communication skills and experience collaborating and influencing across all levels of the organization.
3 Years - Experience with Azure Data Factory as an ETL tool
3 Years - Bachelor's Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) with a focus on data analysis, data structure
3 Years - Experience With MS Dataverse Is Required
3 Years - Experience with MS SSIS
3 Years - Proven track record of solving real world problems using data. Excellent critical thinking skills is a must have for this role.
3 Years - Understanding of different database platforms: Oracle, SQL Server (On-Prem / Azure), DB2, and MS Access. cc
3 Years - Excellent communication skills including written, verbal, and technology illustrations.
3 Years - Understanding of Data Warehousing and data mining.
3 Years - Understanding of modeling strategies (dimensional, snowflake, relational, unstructured).
3 Years - Experience designing and delivering data mapping specifications for large reporting platforms. Writing and maintaining business rules using SQL logic
3 Years - Demonstrated experience in writing ad-hoc SQL statements
3 Years - Strong interest in playing a technical data steward role across our business and technology partners to understand and detail our data, appropriate
3 Years - Proven experience with ERD/Data Modelling tools (Toad or others).
3 Years - Experience in executing projects in an Agile / Hybrid environment
3 Years - Demonstrated technical ability in learning new technologies and adapting to frequent changes in the technical environment
3 Years - Experience with data reporting tools (Power BI, others)
If you or anyone in your network are interested, please send the relevant resume to: abhishek@anveta.com OR asad@anveta.com.
Please include few times when you would be able to speak via Video Call!
Thanks!
Show more
Show less","Data analysis, Data structure, Communication, Collaboration, Azure Data Factory, SQL Server, Microsoft Dataverse, MS Access, MS Excel, Oracle, DB2, MySQL, ETL, Data modeling, Data warehousing, Data mining, SQL logic, ERD, Agile, Power BI","data analysis, data structure, communication, collaboration, azure data factory, sql server, microsoft dataverse, ms access, ms excel, oracle, db2, mysql, etl, data modeling, data warehousing, data mining, sql logic, erd, agile, power bi","agile, azure data factory, collaboration, communication, data mining, data structure, dataanalytics, datamodeling, datawarehouse, db2, erd, etl, microsoft dataverse, ms access, ms excel, mysql, oracle, powerbi, sql logic, sql server"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Nevada, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739298794,2023-12-17,Cahokia,United States,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino"
Data Protection Analyst,"Software Guidance & Assistance, Inc. (SGA, Inc.)",United States,https://www.linkedin.com/jobs/view/data-protection-analyst-at-software-guidance-assistance-inc-sga-inc-3780625683,2023-12-17,Cahokia,United States,Associate,Remote,"Software Guidance & Assistance, Inc., (SGA), is searching for an
Data Protection Analyst
for a
CONTRACT
assignment with one of our premier
Financial
clients in
Jersey CITY, NJ(remote)
.
This position serves as the subject matter technical expert to provide support for the design, configuration, and operation of my clients data protection program.
Responsibilities
:
Develop and Implement Data Protection Policies:
Create and update data protection policies and procedures.
Work closely with legal and compliance teams to align policies with applicable laws.
Policy and Procedure development
Work with internal Communications Team to produce communications about upcoming changes and improvements to the Data Protection Control set
Generate and review User Policies, Data Protection response procedures, and desktop standards
Provide Training and Awareness:
Identify risky user behavior and make recommendations for user risk mitigation
Review and improve upon both mandated and supplemental training for users
Incident Response and Management:
Develop and implement incident response plans for data breaches.
Assist in investigations into data security incidents and recommend corrective actions.
Produce and maintain a robust metrics dataset around Data Protection trends and threats
Leverage analytics tools such as PowerBI and Splunk to identify data risks
Continue to mature metrics capabilities
Conduct Data Protection Impact Assessments (DPIAs):
Assess the impact of data processing activities on privacy.
Provide recommendations to mitigate risks and ensure compliance with relevant regulations.
Required
Skills
:
3 years of experience in data protection and privacy roles.
3 years of experience in administration and engineering of Microsoft 365 exchange, Compliance Center data policies, Powershell, or Windows 2016+ Systems
Technical experience administering data protection polices in Microsoft Purview, including DLP and data classification policies
Technical experience using the Varonis suite
Data Analysis experience using tools such as PowerBI, Splunk, Hadoop, Snowflake
Knowledge and experience with NIST controls applicable to data protection
In-depth knowledge of data protection regulations (e.g. GDPR, GLBA, PCI, CCPA)
(Preferred) Experience conducting Data Privacy risk assessments
Strong communication and interpersonal skills; Ability to demonstrate technical capabilities to both technical and non-technical audiences
Pertinent certifications, e.g. Microsoft SC400, CISSP, Microsoft AZ900
SGA is a technology and resource solutions provider driven to stand out. We are a women-owned business. Our mission: to solve big IT problems with a more personal, boutique approach. Each year, we match consultants like you to more than 1,000 engagements. When we say let's work better together, we mean it. You'll join a diverse team built on these core values: customer service, employee development, and quality and integrity in everything we do. Be yourself, love what you do and find your passion at work. Please find us at
https://sgainc.com/  .
SGA is an Equal Opportunity Employer and does not discriminate on the basis of Race, Color, Sex, Sexual Orientation, Gender Identity, Religion, National Origin, Disability, Veteran Status, Age, Marital Status, Pregnancy, Genetic Information, or Other Legally Protected Status. We are committed to providing access, equal opportunity, and reasonable accommodation for individuals with disabilities in employment, and our services, programs, and activities. Please visit our company
EEO page
to request an accommodation or assistance regarding our policy.
Show more
Show less","Data Protection, Microsoft 365 Exchange, Compliance Center data policies, PowerShell, Windows 2016+ Systems, Microsoft Purview, DLP, Varonis Suite, PowerBI, Splunk, Hadoop, Snowflake, NIST controls, Data protection regulations (GDPR GLBA PCI CCPA), Data Privacy risk assessments, Microsoft SC400, CISSP, Microsoft AZ900","data protection, microsoft 365 exchange, compliance center data policies, powershell, windows 2016 systems, microsoft purview, dlp, varonis suite, powerbi, splunk, hadoop, snowflake, nist controls, data protection regulations gdpr glba pci ccpa, data privacy risk assessments, microsoft sc400, cissp, microsoft az900","cissp, compliance center data policies, data privacy risk assessments, data protection, data protection regulations gdpr glba pci ccpa, dlp, hadoop, microsoft 365 exchange, microsoft az900, microsoft purview, microsoft sc400, nist controls, powerbi, powershell, snowflake, splunk, varonis suite, windows 2016 systems"
Remote work - Need Sr. Data Mapping Analyst - Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/remote-work-need-sr-data-mapping-analyst-remote-at-steneral-consulting-3770658751,2023-12-17,Cahokia,United States,Associate,Remote,"Can be fully remote, but not California.
Must be US Citizen or Green Card and willing to convert down the road
Must Haves
Healthcare Data
Visualization experience\
SQL
SAS
Python
Data Mapping
Data mart logic
Title:
Senior Data Mapping Analyst (685)
GET YEARS OF EXPERIENCE with the following AND LIST LEVEL OF PROFFICIENCY (Beginner, Intermediate, Advanced)
SQL
SAS
Python
Data Mapping
Data mart logic
Sr Data Mapping Analyst
Looking for a senior data resource to assist with Case Management Platform transition project (Moving from Jiva to MHK). This team will need to ""determine the mapping of data fields from the old to new system, as well as determine definition mapping, gap analysis of fields/data/definitions, and other activities relating to moving to a new system.""
There is currently a database called the Member Engagement Datamart built on Jiva, which might need to be rebuilt entirely. This person will be very hands on, pulling data, analyzing, and figuring out the logic.
They should be a curious and deeply analytical individual. They should be capable of working without much direction, and able to speak up and ask if they don't know something.
They need SQL, SAS, and Python would be helpful. This is a business-side role. There are technical partners to do the building. This person needs to be more of the ""logic"" person than the builder (e.g. how to redo the logic to get the same output.)
Core skills are mapping data, understanding gap analysis, taking apart what is in place, looking at new definitions, and comparing data sets.
Skillsets to evaluate: SQL, SAS, Python, Data Mapping, Data mart logic
Please make experience with visualization and/or health data clear in the write up, if not obvious from resume. If they have worked on a similar project in the past, please highlight it. They will be trying to recreate all the same reports and if they have a better way of pulling the data, they should recommend it.
Show more
Show less","SQL, SAS, Python, Data mapping, Data mart logic, Healthcare data, Visualization, Gap analysis, Data analysis, Logic","sql, sas, python, data mapping, data mart logic, healthcare data, visualization, gap analysis, data analysis, logic","data mapping, data mart logic, dataanalytics, gap analysis, healthcare data, logic, python, sas, sql, visualization"
Senior Business Data Analyst,Lorven Technologies Inc.,United States,https://www.linkedin.com/jobs/view/senior-business-data-analyst-at-lorven-technologies-inc-3741154147,2023-12-17,Cahokia,United States,Associate,Remote,"Job Title: Senior Business Data Analyst (PST Time)
Job Location: Remote Position
Duration: 12 months
Job Description
Required Skills and Experience:
Must Commercial Banking exp
DWH concepts, SQL Queries, Data modelling, Data Warehousing and Data profiling
Bonus Skills
Applications like Fiserv, and AFS, Scripting
Show more
Show less","Commercial Banking, DWH concepts, SQL Queries, Data modelling, Data Warehousing, Data profiling, Fiserv, AFS, Scripting","commercial banking, dwh concepts, sql queries, data modelling, data warehousing, data profiling, fiserv, afs, scripting","afs, commercial banking, data modelling, data profiling, datawarehouse, dwh concepts, fiserv, scripting, sql queries"
Real Estate Data Analyst,"APN Software Services, Inc.",United States,https://www.linkedin.com/jobs/view/real-estate-data-analyst-at-apn-software-services-inc-3742883269,2023-12-17,Cahokia,United States,Associate,Remote,"Job title: Data Analyst
Duration: 3 Month
Location: Washington (Remote)
Description
Position Summary -This remote work-from-home position plays a supporting role on the Real Estate Services team. The portfolio analyst shall be responsible for research and data analysis specific to real estate across more than 950 sites and over 9 million square feet. Owns, leases and has donated facilities of office, industrial and retail properties. This position is well suited for an analytical, detail oriented and experienced professional in corporate real estate. The ideal candidate must have experience in corporate real estate databases with an emphasis on Lease Administration. This position will play a key role in analysis providing key data points for delivery to several Real Estate related systems.
Key Duties
Responsible for real estate data analysis and delivery
Review of lease-related critical dates for distribution
Create and validate renewal projects
Administer and respond to Real Estate Team Email Box
Assist with lease administration tasks
Performs other duties as assigned
Qualifications
3-5 years relevant work experience, with at least 3 years in commercial/corporate real estate/lease administration
Bachelor’s degree from a four-year college or university with Accounting, Finance or Real Estate emphasis. Equivalent work experience may be sufficient
Advanced Excel skills
Knowledge of CoStar Real Estate Manager (portfolio and projects modules) a plus
Flexibility
APN Software services Inc. is an
equal opportunity employer
. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”
Show more
Show less","Data analysis, Real estate, Lease administration, Advanced Excel, CoStar Real Estate Manager, APN Software services Inc.","data analysis, real estate, lease administration, advanced excel, costar real estate manager, apn software services inc","advanced excel, apn software services inc, costar real estate manager, dataanalytics, lease administration, real estate"
Data Engineer Co-Op,Bayer,"Creve Coeur, MO",https://www.linkedin.com/jobs/view/data-engineer-co-op-at-bayer-3690877394,2023-12-17,Cahokia,United States,Mid senior,Onsite,"Data Engineer Co Op
Your Tasks And Responsibilities
The primary responsibilities of this role, data engineer Co-op, are to:
Collaborate daily with a team of Data Engineers and Product Managers to build and support scalable data solutions;
Partner with other teams, including application developers, data scientists and business colleagues, to identify solution needs;
Author code to implement new solutions or add new features to existing solutions;
Develop any technical documentation needed to accurately represent application design and code;
Gain understanding of the business operations and functions for solutions owned within the team;
Seek opportunities to discover new and better solutions;
Effectively communicate within team and with direct lead;
Take the initiative to do what needs to be done without being asked
Required Qualifications
Candidate must be currently enrolled in a Bachelor’s or Master’s degree program in any of the following: Computer Science, Computer Engineering, MIS, or related field;
Upon completion of the co-op term, student must return to school to complete studies
Preferred Qualifications
Experience or education in the following areas: SDLC (software development lifecycle), Object Oriented or Functional Programming languages (ex: JavaScript, Swift, Java, C#, Scala, Ruby, Go) or languages such as C/C++
Strong written and verbal communication skills;
Full time availability 40 hours a week, M-F. This is a HYBRID role based out of our Creve Couer office
YOUR APPLICATION
Bayer offers a wide variety of competitive compensation and benefits programs. If you meet the requirements of this unique opportunity, and want to impact our mission Science for a better life, we encourage you to apply now. Be part of something bigger. Be you. Be Bayer.
To all recruitment agencies: Bayer does not accept unsolicited third party resumes.
Bayer is an Equal Opportunity Employer/Disabled/Veterans
Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below.
Bayer is an E-Verify Employer.
Location:
United States : Missouri : Creve Coeur
Division:
Enabling Functions
Reference Code:
799317
Contact Us
Email:
hrop_usa@bayer.com
Show more
Show less","Data Engineering, SDLC (Software Development Lifecycle), Object Oriented Programming, Functional Programming, Java, C#, Scala, Ruby, Go, C/C++, JavaScript, Swift","data engineering, sdlc software development lifecycle, object oriented programming, functional programming, java, c, scala, ruby, go, cc, javascript, swift","c, cc, data engineering, functional programming, go, java, javascript, object oriented programming, ruby, scala, sdlc software development lifecycle, swift"
Senior Data Engineer,Bayer,"Chesterfield, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bayer-3785832331,2023-12-17,Cahokia,United States,Mid senior,Onsite,"Senior Data Engineer for Chesterfield, MO to lead design & implementation of data processing, storage and delivery solutions; define strategies & engineering guidelines for major data platforms; identify data solutions to meet business capability needs & processes; participate in code reviews, testing & retrospectives; create & maintain design and code documentation; develop project estimates; collaborate with cross-functional IT stakeholders to align roadmaps, delivery dates & integration activities; ensure internal standards are met & drive improvements to internal processes; evaluate new technologies & languages for company use; lead conversations with product management stakeholders; mentor & coach junior team members. Requires Master’s in C.S., I.T., M.I.S., Computer or Software Engineering or closely-related field & 3 yrs experiencein IT-related position(s): engineering data-intensive software using streaming & resource-based design principles; working with NoSQL databases, including Google BigQuery; using object-oriented and/or functional programming languages, including Java, Python, Scala and/or Go; modeling & developing data architecture; designing logical & physical models for datasets; working with relational databases, including Postgres, MySQL and/or Oracle; modeling large datasets in distributed databases; working with Platform-as-a-Service software, including Cloud Foundry and/or Kubernetes; using Spark and/or Kafka for stream processing; applying machine learning methodologies to develop data solutions; using Cloud technologies, including AWS & Google Cloud Platform; and modeling orchestration using Apache AirFlow. Will also accept Bachelor’s in said fields & 5 yrs progressive post-Bachelor’s stated experience. Telecommuting permitted from home office location within reasonable commuting distance of Chesterfield, MO up to 2 days per week. Mail resume to Cascinda Fischbeck, Bayer Research and Development Services LLC, 800 N. Lindbergh Blvd. E2NE, St. Louis, MO 63167 or email resume to careers_us@bayer.com. Include reference code below with resume.
Bayer Research & Development Services LLC is an Equal Opportunity Employer/Disabled/Veterans
Bayer Research & Development Services LLC is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below.
If you meet the requirements of this unique opportunity, and want to impact our mission Science for a better life, we encourage you to apply now. Job postings will remain open for a minimum of ten business days and are subject to immediate closure thereafter without additional notice.
Division:
Crop Science
Reference Code
806556
Functional Area:
APD
Location:
Chesterfield, MO
Employment Type:
Regular
Position Grade:
Contact Us
Address Telephone E-Mail
Creve Coeur, MO
careers_us@bayer.com
63167
Show more
Show less","Data Engineering, Data Processing, Data Storage, Data Delivery, Data Platforms, Data Architecture, Data Solutions, Business Capability Needs, Code Reviews, Testing, Retrospectives, Design Documentation, Project Estimates, CrossFunctional Collaboration, Roadmaps, Delivery Dates, Integration Activities, Internal Standards, Process Improvement, Technology Evaluation, Product Management, Mentoring, Coaching, Java, Python, Scala, Go, NoSQL Databases, Google BigQuery, ObjectOriented Programming, Functional Programming, Logical Models, Physical Models, Datasets, Relational Databases, Postgres, MySQL, Oracle, Distributed Databases, PlatformasaService Software, Cloud Foundry, Kubernetes, Spark, Kafka, Stream Processing, Machine Learning, Cloud Technologies, AWS, Google Cloud Platform, Apache AirFlow","data engineering, data processing, data storage, data delivery, data platforms, data architecture, data solutions, business capability needs, code reviews, testing, retrospectives, design documentation, project estimates, crossfunctional collaboration, roadmaps, delivery dates, integration activities, internal standards, process improvement, technology evaluation, product management, mentoring, coaching, java, python, scala, go, nosql databases, google bigquery, objectoriented programming, functional programming, logical models, physical models, datasets, relational databases, postgres, mysql, oracle, distributed databases, platformasaservice software, cloud foundry, kubernetes, spark, kafka, stream processing, machine learning, cloud technologies, aws, google cloud platform, apache airflow","apache airflow, aws, business capability needs, cloud foundry, cloud technologies, coaching, code reviews, crossfunctional collaboration, data architecture, data delivery, data engineering, data platforms, data processing, data solutions, data storage, datasets, delivery dates, design documentation, distributed databases, functional programming, go, google bigquery, google cloud platform, integration activities, internal standards, java, kafka, kubernetes, logical models, machine learning, mentoring, mysql, nosql databases, objectoriented programming, oracle, physical models, platformasaservice software, postgres, process improvement, product management, project estimates, python, relational databases, retrospectives, roadmaps, scala, spark, stream processing, technology evaluation, testing"
Senior Data Engineer,TekWissen ®,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tekwissen-%C2%AE-3782890075,2023-12-17,Cahokia,United States,Mid senior,Onsite,"Overview:
Tekwissen Group, is a workforce management provider throughout the USA and many other countries in the world. This client is a German multinational Pharmaceutical and biotechnology company and one of the largest pharmaceutical companies in the world, headquartered in Leverkusen, and areas of business include pharmaceuticals; consumer healthcare products, agricultural chemicals, seeds and biotechnology products.
Job Title: Senior Data Engineer
Location: St Louis, MO, 63146
Duration: 12 Months
Job Type: Contract
Work Type: Remote
Job Description:
What you will do is why you should join us:
Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets
Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate
Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact
Explore relevant technology stacks to find the best fit for each dataset
Pursue opportunities to present our work at relevant technical conferences
Project your talent into relevant projects. Strength of ideas trumps position on an org chart
If you share our values, you should have:
At least 7 years' experience in software engineering
At least 2 years' experience with Go
Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach
Experience with stream processing using Apache Kafka
A level of comfort with Unit Testing and Test Driven Development methodologies
Familiarity with creating and maintaining containerized application deployments with a platform like Docker
A proven ability to build and maintain cloud-based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform
Experience data modelling for large scale databases, either relational or NoSQL
Bonus points for:
Experience with protocol buffers and gRPC
Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes
Experience working with scientific datasets, or a background in the application of quantitative science to business problems
Bioinformatics experience, especially large-scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation
TekWissen Group is an equal opportunity employer supporting workforce diversity.
Show more
Show less","Software Engineering, Go, RESTful APIs, Apache Kafka, Unit Testing, Test Driven Development, Docker, AWS, Azure, Google Cloud Platform, Data Modelling, Protocol buffers, gRPC, Google Cloud Dataflow, Google Kubernetes Engine, Kubernetes, Bioinformatics, Variant Annotation, Genotype Phenotype Correlation","software engineering, go, restful apis, apache kafka, unit testing, test driven development, docker, aws, azure, google cloud platform, data modelling, protocol buffers, grpc, google cloud dataflow, google kubernetes engine, kubernetes, bioinformatics, variant annotation, genotype phenotype correlation","apache kafka, aws, azure, bioinformatics, data modelling, docker, genotype phenotype correlation, go, google cloud dataflow, google cloud platform, google kubernetes engine, grpc, kubernetes, protocol buffers, restful apis, software engineering, test driven development, unit testing, variant annotation"
Lead Mechanical Engineer (Mission Critical/Data Center),WSP in the U.S.,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-mechanical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3778949027,2023-12-17,Cahokia,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a member of WSP USA
, is currently initiating a search for a
Lead Mechanical Engineer
located out of our
St. Louis, MO office.
As a Mechanical Engineer with us, you will design complex cooling and HVAC systems including air distribution systems, chiller plants, and alternative energy solutions for mission critical facilities.
Your Impact
Independently support the project team during design and construction stages of projects
Design air distribution, hydronic and automated temperature controls systems
Integrate complex mechanical engineering requirements into facility designs
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Collaborate and coordinate with internal project discipline team members and external vendors and manufacturers
Communicate mechanical engineering concepts and decisions to clients and stakeholders
Interact regularly with clients, which includes maintaining current relationships and developing new relationships
Mentor and train junior engineers
Track and coordinate all mechanical disciplines: HVAC, Energy, Controls, Fire Protection, Fire Alarm, Plumbing, Fuel Oil Storage / Management / Distribution
Provide oversight of all aspects of mechanical design, review systems, drawings prior to issuance
Perform Computational Fluid Dynamic (CFD) evaluations for existing and new facilities, both internal and external
Select and schedule major equipment
Develop project specifications
Survey and evaluate existing conditions
Perform construction administration tasks
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client, and construction team members. Candidate should be willing to travel to client sites.
Required Qualifications
Bachelor’s degree in Mechanical Engineering or Architectural Engineering with mechanical building systems emphasis
7+ years of experience in designing mechanical systems for the high performing, commercial, industrial or mission critical/data center buildings
EIT or Registered Professional Engineer (PE), if eligible
Excellent interpersonal skills, teamwork, and written and verbal communication skills
Proficiency with applicable software including AutoCAD, Revit, Trane Trace and Pipeflow
Knowledge of building, mechanical and energy codes
Preferred Qualifications:
Experience with the analysis and modeling of interior and exterior Computational Fluid Dynamics of airflow
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 20%
Job Status: Regular
Employee Type: Full
Primary Location: ST LOUIS - N BROADWAY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-MO-St Louis
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Mechanical Engineering, Architectural Engineering, HVAC, Energy, MEP, AutoCAD, Revit, CFD, Building Information Modeling, Trace, Pipeflow","mechanical engineering, architectural engineering, hvac, energy, mep, autocad, revit, cfd, building information modeling, trace, pipeflow","architectural engineering, autocad, building information modeling, cfd, energy, hvac, mechanical engineering, mep, pipeflow, revit, trace"
Big Data Developer,TekIntegral,"Maryland Heights, MO",https://www.linkedin.com/jobs/view/big-data-developer-at-tekintegral-3665656721,2023-12-17,Cahokia,United States,Mid senior,Hybrid,"Big Data Developer
Kforce has a client in Maryland Heights, MO that is seeking a Mid-Level - Java, Kafka Spark Hadoop Developer with expertise in Big Data and Streaming technologies to join our dynamic team.
Pay Rate: $55/hr C2C
Location:
13736 Riverport Dr, Maryland Heights, MO 63043
3 days onsite 2 days remote
Summary
In this role, you will contribute to cutting-edge Big Data and Streaming projects, working with a talented group of professionals in a challenging and rewarding environment. As a Developer, you will be responsible for designing, developing, and maintaining robust and scalable solutions utilizing Java, Scala, Kafka, Hadoop, Spark, Hive, HDFS, HBase, and NoSQL databases. You will work closely with cross-functional teams to implement data processing and analytics solutions that drive actionable insights from large-scale datasets.
Responsibilities
Design, develop, and deploy high-performance, scalable, and fault-tolerant applications using Java and Scala
Develop and maintain real-time data processing pipelines using Kafka, Spark Streaming, and other streaming technologies
Implement data ingestion, storage, and retrieval processes utilizing Hadoop, HDFS, Hive, HBase, and NoSQL databases; Translate them into efficient data processing workflows
Optimize data processing and analytics algorithms for performance, scalability, and reliability
Troubleshoot and resolve issues related to data processing, data quality, and performance bottlenecks
Conduct code reviews, ensure code quality, and adhere to best practices in software development
Stay updated with emerging trends and technologies in Big Data, Streaming, and NoSQL domains, and evaluate their applicability to enhance existing systems
Skills
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field
Proven experience of 3 to 5 years as a Java/Scala Developer working on Big Data and Streaming projects
Experience with version control systems like Git and proficiency in Agile/Scrum methodologies
Hands-on experience with Hadoop ecosystem components such as HDFS, Hive, and HBase
In-depth knowledge of Apache Kafka, including experience with producer/consumer APIs, partitioning, message delivery semantics, and offset management
Familiarity with NoSQL databases like MongoDB, Cassandra, or Redis, and their integration with Big Data frameworks
Understanding of distributed computing principles, data partitioning, and fault tolerance
Strong programming skills in Java and/or Scala, with a solid understanding of object-oriented and functional programming concepts
Strong problem-solving skills and the ability to optimize algorithms and data processing workflows
Proficiency in Apache Spark for batch and stream processing, including Spark Core, Spark Streaming, and Spark SQL
Excellent teamwork and communication skills, with the ability to collaborate effectively with cross-functional teams
Show more
Show less","Java, Scala, Kafka, Hadoop, Spark, Spark Streaming, Hive, HDFS, HBase, NoSQL, MongoDB, Cassandra, Redis, Git, Agile, Scrum, Objectoriented programming, Functional programming, Distributed computing, Data partitioning, Fault tolerance, Data processing, Data analytics, Teamwork, Communication","java, scala, kafka, hadoop, spark, spark streaming, hive, hdfs, hbase, nosql, mongodb, cassandra, redis, git, agile, scrum, objectoriented programming, functional programming, distributed computing, data partitioning, fault tolerance, data processing, data analytics, teamwork, communication","agile, cassandra, communication, data partitioning, data processing, dataanalytics, distributed computing, fault tolerance, functional programming, git, hadoop, hbase, hdfs, hive, java, kafka, mongodb, nosql, objectoriented programming, redis, scala, scrum, spark, spark streaming, teamwork"
Lead Electrical Engineer (Mission Critical/Data Center),WSP in the U.S.,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-electrical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3757438469,2023-12-17,Cahokia,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
What You’ll Do:
kW Mission Critical Engineering
is currently initiating a search for a
Lead Electrical Engineer
that can be located for our
kW Tempe, Arizona office or our St. Louis, MO office.
As a Lead Electrical Engineer with us, you will design complex power and other building systems including generator plants, medium voltage distribution, uninterruptible power systems, lighting, fire alarm, and grounding while leading projects and a team of electrical engineers.
Your Impact
Produce high quality technical and professional deliverables for projects and proposals
Apply deep knowledge of engineering techniques across multiple technical functions
Utilize advanced analytical and design techniques to solve technical problems
Exemplify well-developed advanced experience in electrical discipline
Lead the development of initial electrical system concepts
Present complex technical solutions to clients
Manage and coordinate project teams and projects
Develop work plans to address technical issues within project time and budget
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend and lead client meetings
Manage and mentor junior staff
Collaborate and coordinate with internal project discipline team members, equipment vendors and manufacturers
Perform project management activities including writing proposals, establishing budgets, and managing client interactions
Coordinate activities concerned with technical development, scheduling, and resolving engineering design issues
Coordinate the activities of technical staff from project award through project completion
Design complex and large electrical medium voltage and low voltage distribution systems and electrical building systems (i.e. general power, lighting, grounding, etc.)
Survey and evaluation of existing conditions
Develop project specifications
Perform construction administration
Develop and maintain client relationships
Contribute and interact with team, develop and manage high quality technical and professional deliverables on projects and proposals
Participate in local professional organization (attend meetings/lectures), i.e., poster sessions, participate in conference panel
Exercise responsible and ethical decision-making regarding company funds, resources and conduct and adhere to WSP""s Code of Conduct and related policies and procedures
Proven track record of upholding workplace safety and ability to abide by WSP""s health, safety and drug/alcohol and harassment policies
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client and construction team members. Candidate will have previous experience as a lead project electrical engineer capable of directing the project team.
Required Qualifications
Bachelor’s degree in Electrical Engineering or Architectural Engineering with electrical building systems emphasis
7+ years of experience in designing electrical systems for the high performing, commercial, industrial or mission critical/data center buildings
Registered Professional Engineer (PE)
Experience mentoring and training others in field
Strong verbal and written communication skills
Ability to interact well with others as well as develop and contribute to high quality technical and professional deliverables on projects and proposals
Strong working knowledge of electrical systems and codes
Attention to detail, highly organized, self-starter
Participate in conference programs including panels, lectures, poster sessions, papers and presentations
Preferred Qualifications:
Enhancing credentials (LEED, Uptime ATD, etc.) preferred
Experience with the analysis and modeling of short circuit coordination and arc flash studies
Mission Critical/Data Center experience
Experience with international projects and knowledge of international codes and standards
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 10%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-MO-Creve Coeur, US-MO-St Louis
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, Building Information Modeling, Electrical Engineering, Architectural Engineering, LEED, Uptime ATD, Project Management, Construction Management, Telecommunications, Security, Fire Protection, Controls, Power Systems, Medium Voltage, Low Voltage, Arc Flash Studies, Short Circuit Coordination, AutoCAD","revit, building information modeling, electrical engineering, architectural engineering, leed, uptime atd, project management, construction management, telecommunications, security, fire protection, controls, power systems, medium voltage, low voltage, arc flash studies, short circuit coordination, autocad","arc flash studies, architectural engineering, autocad, building information modeling, construction management, controls, electrical engineering, fire protection, leed, low voltage, medium voltage, power systems, project management, revit, security, short circuit coordination, telecommunications, uptime atd"
Power BI Data Analyst,Propper International,"St Charles, MO",https://www.linkedin.com/jobs/view/power-bi-data-analyst-at-propper-international-3763854966,2023-12-17,Cahokia,United States,Mid senior,Hybrid,"Job Title:
Power BI Data Analyst:
Propper International is looking for a forward-thinking and innovative Power BI Data Analyst to provide business insights that drive improvements. The Power BI Data Analyst will play a key role by capturing, processing, and transforming datasets into clear summaries to fuel data-driven business decisions and solutions. The Data analyst will be heavily involved in the integration of different data sources, the use of ETL, SQL databases, and Power BI to successfully deliver BI solutions on a periodic basis.
Responsibilities
Work closely with business stakeholders to understand their requirements and use multiple data sources and Power BI to deliver data-driven insights and informed decisions.
Use Power BI to create customized reports, dashboards, and KPIs to monitor key performance indicators.
Provide live Power BI data through development of dynamic reporting, dashboarding, drill-down capabilities, and visualization solutions.
Develop processes for data mining, data modeling, and data production.
Create test plans and scenarios to ensure quality and requirement traceability for final deliverable.
Provide Power BI training on various levels of expertise to companywide users and executives as needed.
Implement data security measures to protect sensitive information and ensure compliance with data protection regulations.
Actively identify solutions to enhance the existing Business Intelligence infrastructure, processes, and technology.
Perform other duties as assigned.
Requirements (Must Have)
4-year bachelor's degree in IT or related fields
5+ years in Data Analysis using Microsoft Business Intelligence Stack (Power BI, SSAS, SSRS, SSIS, Data Factory, Power Query, MDX, DAX, etc.)
Solid knowledge of SQL Data Warehousing and database fundamentals such as multidimensional database and relational database design.
Strong programming skills in SQL. Knowledge of Python is a plus.
Proficiency with data science techniques and in manipulating data through data cleansing, data transformation, and data modeling.
Strong analytical and problem-solving skills.
Solid project management skills using Agile methodologies to meet project deadlines, budgets, and business requirements.
Good communication and presentation skills
Excellent Customer Service Orientation is a MUST.
Position Details
On-Site, Monday thru Friday 8am-5pm
1099 Contract position. Contract duration is 6 months to 12 months, with possibility of direct hire
Hourly range of $39.00/hr to $43.00/hr depending upon experience.
Who Is Propper
We got our start in 1967 with a contract for the U.S. Navy, manufacturing the iconic ""Dixie Cup"" hat worn by U.S. sailors. Over the decades, we've supplied more than 120 million garments to the U.S. Department of Defense, law enforcement agencies, and the public safety community.
You may not have heard of us, but you've definitely seen our work.
Our heritage informs how we operate. We are a retail brand, but we still think like a contract manufacturer. Contracts aren't won by selling a particular lifestyle, telling unique stories, or appealing to emotion. They are won with features, quality, and demonstrable value. Contracts are won on the concrete.
This mentality drives every aspect of our production. Make it Work. Make it Last. Make it Real. Or don't make it at all.
Equal opportunity Employer
Show more
Show less","Power BI, SQL, Data Analysis, Data Mining, Data Modeling, Data Transformation, Data Visualization, Dashboards, KPIs, Python, Agile, Project Management, SSAS, SSRS, SSIS, Data Factory, Power Query, MDX, DAX","power bi, sql, data analysis, data mining, data modeling, data transformation, data visualization, dashboards, kpis, python, agile, project management, ssas, ssrs, ssis, data factory, power query, mdx, dax","agile, dashboard, data factory, data mining, data transformation, dataanalytics, datamodeling, dax, kpis, mdx, power query, powerbi, project management, python, sql, ssas, ssis, ssrs, visualization"
SENIOR MARKETING DATA ANALYST,Ameren,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-ameren-3778739779,2023-12-17,Cahokia,United States,Mid senior,Hybrid,"About Us
Ameren is a leader in the energy industry, and our transformation toward more clean, renewable energy is also transforming other industries and infrastructure in our communities. As a regional company serving local customers, we not only serve our communities, we're a part of them. This isn't just a job. At Ameren, we invest in you, so you can power the quality of life you want.
Diversity, Equity & Inclusion is one of the core values that guides us in everything we do. We are committed to building a skilled and diverse workforce that brings diverse perspectives to every area of our business.
Our benefits include:
Medical coverage on date of hire
100% employer paid cash balance pension plan
401(k) with company match fully vested on date of hire
Minimum of 15 days paid vacation and 12 paid holidays
Paid parental leave and family caregiver leave
About Ameren Services (B&CS)
Ameren Services provides administrative support and services to Ameren Corporation and its operating companies, subsidiaries and affiliates. Ameren Services includes a wide range of skill sets and roles, from finance and legal experts to digital and cyber specialists, plus those charged with ensuring environmental compliance and operational safety. Together, we help execute a strategy that enables Ameren to deliver superior long-term value to customers, shareholders and the environment.
About The Position
The Sr Marketing Data Analyst is:
Responsible for integrating data from our various marketing campaigns, formatting the data into an easy-to-analyze visual format, working independently to uncover insights, and communicating those insights and recommendations to communications strategists, business partners and leaders.
Works independently to design and develop methods, processes, and systems to consolidate and analyze marketing automation and lead generation data to generate actionable insights.
Works independently to solve complex business problems using data analysis.
Key responsibilities include:
Collaborate with communications strategists and/or other stakeholders to identify and document data requirements and measurement strategies based on campaign goals and objectives prior to the launch of marketing and/or communications campaigns.
Pull marketing engagement data from various platforms to integrate, analyze and interpret results for marketing tactics or channels associated with a campaign, including web, email, paid advertising (e.g., digital display ads, CTV, streaming radio, and paid search), direct mail, energy statement messaging, YouTube and social media.
Automate and visualize the data, leveraging tools like Looker Studio or Power BI to provide a holistic picture of campaign performance and provide recommendations for optimizations.
Review all the variables that impact our marketing and campaign results, including creative, copy, CTAs, targeting, timing and distribution channels, to identify and draw conclusions regarding performance, trends and insights.
Make ongoing and real-time data-driven recommendations and identify opportunities for optimizations and A/B or multivariate tests.
Create easy-to-understand reports and templates that effectively display key data points and tell a story in PowerPoint or via a visualization tool for working teams, marketing and leadership teams.
Independently and effectively work and communicate with communications strategists and leadership to deliver or present customer and campaign insights and optimizations.
Partner effectively with other departments to fully define analytics requirements, provide engineering, data validations and modeling.
Develop innovative and efficient solutions for leveraging APIs for data integration, processing and automation and assist in the ongoing maintenance of reporting and analytics applications.
Mentor and provide ongoing support for less experienced team members.
Other duties as assigned.
Qualifications
Bachelor’s degree required, preferably in mathematics, statistics, computer science, marketing, or business.
5+ years of relevant experience
Master’s and/or Ph.D. in a technical discipline e.g., engineering, mathematics, statistics, finance/economics, etc.) may replace a maximum of 2 years of experience.
Career path level depends on applicant experience and credentials
In addition to the above qualifications, the successful candidate will demonstrate:
Strong knowledge of data visualization and relational databases.
Strong knowledge of marketing platforms including marketing automation software (e.g., Salesforce Marketing Cloud or HubSpot), website analytics and tagging systems (e.g., GA4, GTM and Big Query), digital marketing platforms (e.g., Google AdWords, Campaign Manager 360 and AWS Pinpoint), social media platforms (e.g., Sprinklr, Facebook, Twitter and LinkedIn) and visualization tools (e.g., Looker Studio, Power BI etc.).
Writing queries (SQL), and programming (R, Python, VBA, or SAS) is not required but would be a plus.
Strong analytical skills to address business problems.
Strong knowledge with Microsoft Office Products – specifically Microsoft Excel.
Ability to manage multiple priorities and to clarify priorities, scope and requirement in ambiguous situations.
Additional Information
Ameren’s selection process includes a series of interviews and may include a leadership assessment process. Specific details will be provided to qualified candidates.
If end date is listed, the posting will come down at 12:00 am on that date:
Tuesday December 19, 2023
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, ethnicity, age, disability, genetic information, military service or status, pregnancy, marital status, sexual orientation, gender identity or expression, or any other class, trait, or status protected by law.
Show more
Show less","Data visualization, Relational databases, Marketing automation software, Website analytics, Tagging systems, Digital marketing platforms, Social media platforms, Visualization tools, Data analysis, SQL, R, Python, VBA, SAS, Microsoft Office Products, Microsoft Excel, Statistics, Computer science, Marketing, Business, Engineering, Mathematics, Finance, Economics, Salesforce Marketing Cloud, HubSpot, GA4, GTM, Big Query, Google AdWords, Campaign Manager 360, AWS Pinpoint, Sprinklr, Facebook, Twitter, LinkedIn, Looker Studio, Power BI","data visualization, relational databases, marketing automation software, website analytics, tagging systems, digital marketing platforms, social media platforms, visualization tools, data analysis, sql, r, python, vba, sas, microsoft office products, microsoft excel, statistics, computer science, marketing, business, engineering, mathematics, finance, economics, salesforce marketing cloud, hubspot, ga4, gtm, big query, google adwords, campaign manager 360, aws pinpoint, sprinklr, facebook, twitter, linkedin, looker studio, power bi","aws pinpoint, big query, business, campaign manager 360, computer science, dataanalytics, digital marketing platforms, economics, engineering, facebook, finance, ga4, google adwords, gtm, hubspot, linkedin, looker studio, marketing, marketing automation software, mathematics, microsoft excel, microsoft office products, powerbi, python, r, relational databases, salesforce marketing cloud, sas, social media platforms, sprinklr, sql, statistics, tagging systems, twitter, vba, visualization, visualization tools, website analytics"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Santa Monica, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091257,2023-12-17,Simi Valley,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Pandas, R, Airflow, KubeFlow, NLP, PySpark, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Legal Compliance, Data Management Tools, Data Classification, Retention, 401K","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, pandas, r, airflow, kubeflow, nlp, pyspark, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management tools, data classification, retention, 401k","401k, airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, nlp, pandas, python, r, retention, snowflake, spark, sparkstreaming, sql, storm"
"Data Engineering Lead, Translational Genomics",Roche,"South San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineering-lead-translational-genomics-at-roche-3612292759,2023-12-17,Alameda,United States,Associate,Onsite,"The Position
If you are a big data engineer and want to work on something that truly can change the world, this job is for you. Biology is approaching an inflection where we can directly leverage data to understand the cellular basis of human diseases and from this generate therapeutics that can treat these diseases. Our Translational Genomics initiative is spearheading this effort and bringing together data from human genetics, functional genomics, molecular biology, disease model engineering, and tissue and cellular profiling. We need a Data Engineering Lead to help us create a next-generation data engine that scalably and rigorously ingests and transforms data generated from this initiative so they are ready for machine-driven analysis. The Data Engineering Lead will act as an architect and engineering manager tasked to oversee the construction and operation of this data engine. This data engine will be used to help assemble an exabyte scale connected and computable data universe composed of high value internally and externally generated data and results that we can build our data science efforts on top of. Your efforts will therefore directly enable computational discovery of disease targets and from these potentially life saving therapies.
A person hired in this position will
Manage a team that will architect and deliver a next generation data engine that enables scalable, flexible, and rigorous data transformations using modern data management practices.
Help architect and deliver data infrastructure that will enable machines to crawl and compute on and across all our data.
Work with a cross functional team of scientists and engineers to design and deliver these solutions.
Exert influence across the informatics organization via presentations and collaborations
Successful Candidates Will Meet The Following Requirements
You have a BS in a computational discipline with 12 years of work experience or a Masters with 7 years of experience.
7+ years experience architecting and developing scalable pipelines, frameworks and platforms to power data science efforts in distributed cloud environments, 5 of which are on AWS.
Multiple years of experience leading a distributed team of engineers to deliver solutions.
Practical understanding of the data management practices required to power rigorous data science and enable advanced analytics like AI & ML.
Exceptional communication skills
Experience leading projects focused on omics data.
Hands-on experience working with the following technologies, frameworks, and languages: Java, Scala, Python, Spark, Airflow, RabbitMQ, Spring.
What To Expect From Us
A highly collaborative and dynamic research environment where we aim to advance the rate of scientific discovery using purposefully built solutions
Access to large multimodal omic datasets focused on disease biology, samples and compute resources
Access to state-of-the-art technologies and pioneering research
Participation in seminar series featuring academic and industry scientists
Campus-like lifestyle with a healthy work-life balance
Mentored opportunities to further develop professional skills
#gCS
Who We Are
A member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. We are a research-driven biotechnology company, whose medical innovations for cancer and other serious illnesses make a difference for patients across the globe. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset & are dedicated to remaining a great place to work.
Genentech is an equal opportunity employer & prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, disability, marital & veteran status. For more information about equal employment opportunity, visit our Genentech Careers page.
The expected salary range for this position based on the primary location of California is $148,800-$276,400. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided below.
Benefits
#gREDinformatics
Who we are
A member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. Genentech has multiple therapies on the market for cancer & other serious illnesses. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset & are dedicated to remaining a great place to work.
Diversity and Inclusion (D&I) are critical to the success of our company and our impact on society. We believe that by championing diversity of background, thought and experience, we can foster a sense of belonging and provide an environment where every employee feels valued, included, and able to contribute their best for the patients we serve. We’re focused on attracting, retaining, developing and advancing our people to their full potential by rewarding bold ways of thinking and integrating inclusive behaviors into every aspect of our work.
Genentech is an equal opportunity employer, and we embrace the increasingly diverse world around us. Genentech prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin or ancestry, age, disability, marital status and veteran status. For more information about equal employment opportunity, visit our Genentech Careers Homepage.
If you have a disability and need an accommodation in relation to the online application process, please contact us by completing this form Accommodations for Applicants.
Roche is an equal opportunity employer and strictly prohibits unlawful discrimination based upon an individual's race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, mental/physical disability, medical condition, marital status, veteran status, or any other characteristic protected by law.
Show more
Show less","Java, Scala, Python, Spark, Airflow, RabbitMQ, Spring, AWS, Apache Airflow, Data Management, Data Engineering, Data Science, Analytics, Machine Learning, Genomics, Omics Data, Bioinformatics, Computational Biology, Distributed Systems, Cloud Computing","java, scala, python, spark, airflow, rabbitmq, spring, aws, apache airflow, data management, data engineering, data science, analytics, machine learning, genomics, omics data, bioinformatics, computational biology, distributed systems, cloud computing","airflow, analytics, apache airflow, aws, bioinformatics, cloud computing, computational biology, data engineering, data management, data science, distributed systems, genomics, java, machine learning, omics data, python, rabbitmq, scala, spark, spring"
Safety Security Data Analyst,San Mateo County Transit District,"San Mateo, CA",https://www.linkedin.com/jobs/view/safety-security-data-analyst-at-san-mateo-county-transit-district-3763561857,2023-12-17,Alameda,United States,Associate,Onsite,"The San Mateo County Transit District serves nearly 100,000 customers each weekday on its SamTrans buses, Redi-Wheel paratransit vehicles, Caltrain commuter rail cars and shuttles, as well as a robust capital program. The Transit District, which is in the heart of the San Francisco Bay Area, also is the managing agency for the San Mateo County Transportation Authority. Staff enjoys a dynamic organization that fosters personal development and professional advancement of its staff. The Transit District’s core values include integrity, customer focus, respect, quality, teamwork, leadership and accountability. Excellent benefits are provided.
DIVISION
Rail
EMPLOYMENT TYPE
Non-exempt/non-safety sensitive
APPLICATION DEADLINE
Continuous Recruitiment Until Filled
Job Summary
The Safety & Security Data Analyst reports to the Deputy Director Safety and Security and will work closely with the Chief Safety Officer, Rail. The Safety & Security Data Analyst will work 50% with District Safety and Security and 50% with Rail Safety and will be responsible for collecting data, assessing, reporting, and providing insights on the programs operated by the San Mateo County Transit District (SamTrans) and Peninsula Corridor Joint Powers Board (Caltrain).
Minimum Qualifications
Sufficient experience, tr aining and/or education to demonstrate the knowledge and ability to successfully perform the essential functions of the position. In lieu of a degree, work-related experience that demonstrates the skills and experience necessary to perform this role will be accepted. Development of the required knowledge and abilities is typically obtained through but not limited to:
Bachelor’s degree in mathematics, Computer Science, Information Management, Statistics, or closely related field.
Two years of full-time working experience in a data analysis role using MS, Excel, R, Python, etc.
Must possess advanced-level MS Excel skills.
Must be able to communicate effectively orally and in writing.
Preferred Qualifications
Experience with project management
Essential Functions And Duties
Process and analyze safety related data collected from vendors and the Metropolitan Transportation Commission (MTC).
Analyze safety related information to discover trends and patterns. Develop and create a repository of safety information.
Create data models to monitor the performance and quality of the safety program to address business compliance.
Recommend solutions to business challenges.
Lead and/or participate in safety related working groups for SamTrans and Caltrain.
Example Of Duties
Participate and/or develop the requirements for databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.
Interpret data, analyze results using statistical techniques to provide ongoing reports.
Present the data models and trend/patterns analysis to internal and external stakeholders.
Analyze data and make operational recommendations from key safety and performance data (accidents, incidents, near misses, lag & lead indicators).
Perform all job duties and responsibilities in a safe manner to protect oneself, fellow employees, and the public from injury or harm. Promote safety awareness and follow safety procedures in an effort to reduce or eliminate accidents.
Perform other duties as assigned.
How To Apply
To apply, please visit the https://www.caltrain.com/about-caltrain/jobs . Complete an online employment application and supplemental questionnaire. Open until filled. A resume will not be accepted in lieu of the application and supplemental questionnaire (If required). Incomplete application will not be considered.
The Human Resources Department will make reasonable efforts in the recruitment/examination process to accommodate applicants with disabilities upon request. If you have a need for accommodation, please contact the Human Resources Department at (650) 508-6308.
SamTrans celebrates diversity and is committed to creating an inclusive, and welcoming workplace environment. We are an Affirmative Action/Equal Opportunity Employer. Minorities, Women, Persons with Disabilities and Veterans are encouraged to apply.
Selection Process May Include
The process may include written and skills test assessments or supplemental questions and will require a panel interview. Only those candidates who are the most qualified will continue in the selection process. Meeting the minimum qualifications does not guarantee an invitation to continue in the process.
PAY RANGE
$1,414 - $2,120 weekly ($73,494 - $110,241 estimated annually)
Current Employment Benefits
For further Benefits details please go to: https://www.caltrain.com/about-caltrain/jobs/employee-benefits
Holidays: Seven (7) paid holidays, plus up to four (4) floating holidays per year
Paid Time Off: Up to 26 days per year
Cafeteria Plan: Medical, dental, vision care, group life insurance, and more
Transportation: Free bus transportation for employees and qualified dependents
Retirement: Social Security and California Public Employees Retirement Systems (CalPERS)
Classic Members - 2% @ 60 benefit formula, 3-year average of highest compensation
New Members - 2% @ 62 benefit formula, 3-year average of highest compensation
Show more
Show less","MS Excel, R, Python, SQL, Statistics, Data analysis, Data modeling, Data visualization, Business intelligence, Project management, Safety, Rail transportation, Public transportation, Data collection, Data interpretation, Data mining, Reporting, Communication, Teamwork, Problemsolving, Critical thinking, Analytical skills, Attention to detail","ms excel, r, python, sql, statistics, data analysis, data modeling, data visualization, business intelligence, project management, safety, rail transportation, public transportation, data collection, data interpretation, data mining, reporting, communication, teamwork, problemsolving, critical thinking, analytical skills, attention to detail","analytical skills, attention to detail, business intelligence, communication, critical thinking, data collection, data interpretation, data mining, dataanalytics, datamodeling, ms excel, problemsolving, project management, public transportation, python, r, rail transportation, reporting, safety, sql, statistics, teamwork, visualization"
Senior Data Engineer,Pulley,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-pulley-3780322712,2023-12-17,Alameda,United States,Mid senior,Onsite,"🚀 About Pulley
Pulley's mission is to make it easier for anyone to start a company. We believe that more startups should exist, and that founder-led companies are more successful in the long term. With Pulley’s cap table management tools, companies can better understand and optimize their equity for the long term. Starting a company is hard enough. Managing equity shouldn’t be.
We’re a high-performing team looking for passionate, execution-focused, self-starters to help us build equity management tools for founders. Pulley is growing quickly with over 1600 customers including unicorns like Clubhouse, Birdies, Coda, and Bitwise - all within our first year of product launch. Our trajectory is fueled by top investors like Stripe, General Catalyst, Caffeinated Capital, 8vc, Elad Gil, among other great angels. All of our growth has been organic, and we’re growing the team to meet the demand.
🌟 Who We're Looking For
We are looking for a
Data Engineer
to own the full stack of data from insights to logging. As a Data Engineer working across Pulley, you would build automation and analytical tools to accelerate the operational efficiency of Pulley. You would also define best practices for data architecture, quality, and privacy.
🛠 Responsibilities
Design, build and maintain new data structures and reliable ETL processes to unlock analysis and insights
Identify opportunities to build automated reporting and develop self-service analytics tools
Collaborate with data scientists, engineers, product managers, and other business functions to translate analytical needs and business principles into data requirements
Implement and manage controls to ensure data privacy, security, and compliance
👟 Fit
You are a fast learner - You can readily learn most technologies as you go
You understand the business implications beyond the data
You have a strong work ethic and willingness to do whatever it takes to deliver results
You thrive in bringing structure to highly ambiguous environments
You have experience taking initiatives or products from idea to launch
🙌 Qualifications
3+ years experience
Experience with current data technologies
Preferred Experience: React, Go, Python, GCP, SQL performance tuning, anomaly/outlier detection
💚 Benefits
Generous health insurance
Unlimited vacations
✨
Our Culture
About
TL&DR - Pulley is not a good fit for everyone, and that’s OK. This is a bit about the culture of Pulley. We need people who aren’t just interested in working at a startup; we need people who are excited about building a great company. In addition to function-specific skills, here are some traits that enable our team members to thrive:
Ego-less Learner -- Every day at Pulley, we are trying things that we’ve never done before - sometimes they work; sometimes they don’t. To be successful here, you need to embrace that feeling of ‘being a beginner’ and be willing to accept feedback in stride.
Less Talk; More Action -- Everyone rolls up our sleeves at Pulley. If your strength is being the ‘idea gal or guy’ but you don’t create your own slides or write your own code, then we are not a great fit.
Prioritize ruthlessly - Our view is that being a 10x engineer is not about writing more code; it’s about making the right decision on what to build. We’re hiring for people who can make the right strategic decisions on where to spend their time
We’re also inspired by the culture work done at other companies and plan to develop our own playbook here too. One of the concepts that resonate most with us is the importance of stunning colleagues.
If you like the sound of our environment and you’re passionate about joining a team like the one we’ve described, we'd love to talk!
Annual Salary Range
$105,000—$180,000 USD
Show more
Show less","Data Engineering, ETL, Data Analysis, Data Privacy, Data Security, Data Compliance, GCP, Python, Go, React, SQL","data engineering, etl, data analysis, data privacy, data security, data compliance, gcp, python, go, react, sql","data compliance, data engineering, data privacy, data security, dataanalytics, etl, gcp, go, python, react, sql"
Founding Data Engineer,Ciro,"San Francisco, CA",https://www.linkedin.com/jobs/view/founding-data-engineer-at-ciro-3736362252,2023-12-17,Alameda,United States,Mid senior,Onsite,"We are hiring for this role in San Francisco (this is not a remote position)
Ciro processes billions of data points from dozens of public and proprietary sources to deliver near real-time updates to sales teams about their prospects. As a Founding Data Engineer at Ciro, you will design and lead Ciro’s data infrastructure. You will see all parts of the business and shape Ciro’s culture from the earliest stages.
You Will
Build high-performance data processing pipelines leveraging Python data engineering libraries, BigQuery, and PostgreSQL.
Own and manage Ciro’s data pipeline infrastructure, including scheduling/orchestration, data movement, and deployment.
Work closely with the founders and early customers to set product and technical direction for the company.
Play a lead role in defining Ciro’s engineering culture as we grow.
Sample Projects You Might Work On
Design and build a machine learning system to classify a business’s industry and the services it provides based on the text of its website.
Design a system to automatically deploy and orchestrate Python data processing scripts on Google Cloud.
Design a system to regularly synchronize Ciro’s data indexes — Postgres, BigQuery, and Typesense (similar to Elasticsearch).
Some Of The Skills We’re Excited About
5+ years of software backend/data engineering experience, including familiarity with data modeling, ETL, schema and system design, Git-based workflow, and documentation.
Sound knowledge about database concepts such as transactions, indexing, and concurrency.
Productive, resourceful, and effective at problem-solving.
Ability to balance effective execution with a high bar for engineering excellence.
Preferred: Experience using modern deployment technologies like Docker and at least one major Cloud provider (AWS/GCP/Azure).
Preferred: High degree of SQL proficiency and knowledge of modern data stack technologies like BigQuery/Snowflake and dbt.
Show more
Show less","Python, BigQuery, PostgreSQL, SQL, Machine learning, Docker, Google Cloud, Typesense, Git, ETL, Data modeling, Schema design, System design, AWS, GCP, Azure, Snowflake, dbt","python, bigquery, postgresql, sql, machine learning, docker, google cloud, typesense, git, etl, data modeling, schema design, system design, aws, gcp, azure, snowflake, dbt","aws, azure, bigquery, datamodeling, dbt, docker, etl, gcp, git, google cloud, machine learning, postgresql, python, schema design, snowflake, sql, system design, typesense"
"Staff Data Engineer, Fleet Analytics",Tesla,"Palo Alto, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-fleet-analytics-at-tesla-3737835058,2023-12-17,Alameda,United States,Mid senior,Onsite,"What To Expect
Data is deeply embedded in the product and engineering culture at Tesla. We rely on data – lots of it – to improve autopilot, to optimize hardware designs, to proactively detect faults, and to optimize load on the electrical grid. We collect data from each of our cars, Superchargers, and energy storage devices to make these products better and our customers safer.
We're the Fleet Analytics team, a central team that helps many teams leverage the data we collect. We help engineers through direct support by doing data analysis for them and through applications and tools so they can self-serve those analyses in the future. To do so, we leverage our internal data platform built on top of AWS, S3, Spark, Trino using open source data science tools such as Jupyter notebooks, Pandas, Bokeh, Superset, and Airflow. Our work has a direct impact on Tesla's product, and enables the work of hundreds of engineers across disciplines throughout the company.
We're looking for a talented staff engineer to develop applications which leverage our wealth of device data. These applications will span the full breadth of data engineering, data analysis, and data science activities, taking a first-principles approach to problem solving to inform future hardware and firmware designs, as well as ensuring that our existing vehicles, chargers, and energy devices continue to perform to Tesla's exacting standards. Applications will include full-stack web applications, software frameworks to enable efficient training and modeling, and complex systems which drive action. You will be responsible for bringing these applications from concept to production in collaboration with other members of Fleet Analytics, as well as providing ongoing maintenance and community support. In addition, you will occasionally partner with a team focusing on another discipline (e.g., mechanical engineering, electrical engineering, or firmware engineering), joining a critical project to help them define product requirements, optimize control algorithms, or otherwise improve the product quality.
What You'll Do
Work with stakeholders to develop and maintain complex software systems which elevate the use of device data at Tesla
Provide guidance to Tesla's data engineering / data science community regarding best practices
Work with engineers to drive usage of applications and tools
Write reproducible data analysis over petabytes of data using cutting-edge open source technologies
Summarize and clearly communicate data analysis assumptions and results
Build data pipelines to optimize the efficiency and accuracy of analysis work across the company
Design and implement metrics, applications and tools that will enable engineers by allowing them to self-serve their data insights
Write clean and tested code that can be maintained and extended by other software engineers
What You'll Bring
7+ Years of Software Development experience in a related field
Strong proficiency in Python, SQL
Experience with data processing engines like Apache Spark
Experience with data science tools such as Pandas, Numpy, R, Matlab, Octave
Experience building data pipelines, web applications, and machine learning models in a professional environment
Strong foundation in statistics
Experience building data visualizations
Strong verbal and written communication skills
Strong problem-solving skills to help refine problem statements and figure out how to solve them with the available data and from first principles
Nice To Have
Strong proficiency in Scala
Understanding of distributed computing, i.e. how HDFS, Spark and Presto work
Experience with devops tools - e.g., Linux, Ansible, Docker, Kubernetes
Experience with complex hardware systems
Experience with continuous integration and continuous development
Benefits
Compensation and Benefits
Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire:
Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction
Family-building, fertility, adoption and surrogacy benefits
Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution
Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA
Healthcare and Dependent Care Flexible Spending Accounts (FSA)
LGBTQ+ care concierge services
401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits
Company paid Basic Life, AD&D, short-term and long-term disability insurance
Employee Assistance Program
Sick and Vacation time (Flex time for salary positions), and Paid Holidays
Back-up childcare and parenting support resources
Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance
Weight Loss and Tobacco Cessation Programs
Tesla Babies program
Commuter benefits
Employee discounts and perks program
Expected Compensation
$104,000 - $348,000/annual salary + cash and stock awards + benefits
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
Tesla
Show more
Show less","Python, SQL, Apache Spark, Pandas, Numpy, R, Matlab, Octave, Machine learning, Statistics, Data visualizations, Web applications, Data pipelines, Scala, Distributed computing, HDFS, Presto, Linux, Ansible, Docker, Kubernetes, Jupyter notebooks, Bokeh, Superset, Airflow, DevOps","python, sql, apache spark, pandas, numpy, r, matlab, octave, machine learning, statistics, data visualizations, web applications, data pipelines, scala, distributed computing, hdfs, presto, linux, ansible, docker, kubernetes, jupyter notebooks, bokeh, superset, airflow, devops","airflow, ansible, apache spark, bokeh, data visualizations, datapipeline, devops, distributed computing, docker, hdfs, jupyter notebooks, kubernetes, linux, machine learning, matlab, numpy, octave, pandas, presto, python, r, scala, sql, statistics, superset, web applications"
Senior Data Engineer,Atlassian,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3736868905,2023-12-17,Alameda,United States,Mid senior,Onsite,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.
Responsibilities
A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.
Qualifications
On your first day, we'll expect you to have:
BS in Computer Science or equivalent experience with 5+ years as Data Engineer or similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
We’d Be Super Excited If You Have
Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $163,300 - $217,700
Zone B: $147,000 - $196,000
Zone C: $135,600 - $180,700
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, Java, Data pipelines, Spark, Airflow, AWS, Redshift, Athena, EMR, Apache, Hive, Kafka, SQL, Kappa architecture, Agile, TDD, CICD, Financial systems, People systems, Relational databases","python, java, data pipelines, spark, airflow, aws, redshift, athena, emr, apache, hive, kafka, sql, kappa architecture, agile, tdd, cicd, financial systems, people systems, relational databases","agile, airflow, apache, athena, aws, cicd, datapipeline, emr, financial systems, hive, java, kafka, kappa architecture, people systems, python, redshift, relational databases, spark, sql, tdd"
"Software Engineer, Data Warehouse Analytics",Stripe,"San Francisco, CA",https://www.linkedin.com/jobs/view/software-engineer-data-warehouse-analytics-at-stripe-3785922521,2023-12-17,Alameda,United States,Mid senior,Onsite,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
The team is responsible for offering data warehouse analytics capabilities for Stripe and the stack is supported by a collection of internally developed services and several popular open source technologies like Trino/Presto, Hive Metastore, etc. The systems we own support all of the data warehouse workloads initiated by both services and by individual Stripes across the company to support various business analytics use cases. We process hundreds of thousands of queries on petabytes of data per day and form the key data serving layer for one of the largest financial data lakes in the world. The team is distributed across the USA.
What you’ll do
You shall be a key contributor to the evolution of our platform and how Stripe analyzes its vast and ever growing data warehouse, to power ever evolving use-cases around analytics, reporting, fraud, AI/ML training, etc at scale. As a Software Engineer, you’ll be empowered to make decisions with a significant impact on Stripe while making our systems reliable, secure, and a delight to use.
Responsibilities
Scope and lead large technical projects with enormous impact for Engineers within Stripe
Build and maintain the infrastructure which powers the core of Stripe.
Directly contribute to core interface design and write code.
Work closely with the open source community to identify opportunities for adopting new open source features as well contribute back to the OSS.
Plan for the growth of Stripe’s infrastructure. Unblock, support and communicate with internal partners to achieve results.
Ensure operational excellence and enable a highly available, reliable and secure Data Warehouse Analytics platform.
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
BS or MS in Computer Science or equivalent field and interest in Data.
2-7 years of professional experience writing high quality production level code or software programs.
Have experience with distributed SQL query engines like Trino.
Experience developing, maintaining and debugging distributed systems built with open source tools.
Experience building infrastructure as a product centered around user needs.
Experience optimizing the end to end performance of distributed systems.
Experience with scaling distributed systems in a rapidly moving environment.
Preferred Qualifications
Experience coding in Java, Scala, Golang.
Experience working with distributed SQL query engine space.
Familiarity designing APIs or building developer platforms.
Familiarity with cloud-based environments such as AWS.
Pay and benefits
The annual US base salary range for this role is $172,800 - $233,800. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","SQL, Java, Scala, Golang, Hadoop, Hive, Trino, Presto, AWS, Distributed systems, Open source tools, API design, Developer platforms, Cloudbased environments, Data warehousing, Analytics, Reporting, Fraud detection, AI/ML training","sql, java, scala, golang, hadoop, hive, trino, presto, aws, distributed systems, open source tools, api design, developer platforms, cloudbased environments, data warehousing, analytics, reporting, fraud detection, aiml training","aiml training, analytics, api design, aws, cloudbased environments, datawarehouse, developer platforms, distributed systems, fraud detection, golang, hadoop, hive, java, open source tools, presto, reporting, scala, sql, trino"
"Sr. Data Engineer, Business Analytics",Tesla,"Fremont, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-business-analytics-at-tesla-3767481464,2023-12-17,Alameda,United States,Mid senior,Onsite,"What To Expect
Tesla’s Business Analytics team mission is to convert data into actionable reporting, insights, and automation. We are looking to hire a Sr. Data Engineer dedicated to design & maintain our team’s data pipelines and manage our internal database. We are committed to hiring the world's best and brightest people who can sit at the intersection of multiple of these disciplines and are passionate about continuing to develop their skills further. We believe that small agile teams using modern development stacks and comprised of diverse members with complementary skill sets can have an outsized impact in driving better decisions and automating core business processes.
What You'll Do
Design, improve, and manage the main data and infrastructure powering our business modeling, including databases, data pipelines, and servers
Partner with our data scientists and analysts to design and build effective data pipelines required for our modelling
Partner with our analysts to design optimized SQL queries, enabling scalable reporting with a growing data volume
Coordinate with the IT BI team to validate and push optimized query logic to our upstream DataMart
Build and maintain local ETLs for our specific data science models and ad-hoc reporting
Partner with the IT BI team to enable data sharing process flow between our internal database and our enterprise data warehouse, and/or any other local databases
Think strategically about building a scalable system with a growing data volume and Identify process gaps/areas for optimization & automation
What You'll Bring
Mastery of relational databases, SQL, and ETL tools
Bachelor’s degree in Computer Science or similar
5+ years of experience in data architecture or related field
Proven track record in building, deploying, and optimizing complex data pipelines (batch and streaming)
Deep understanding of data architecture best practices and normalization techniques
Solid experience with Unix/Shell, Python, and big data technologies (Apache Spark, Kafka)
Expertise in designing distributed APIs, web services, and using orchestration tools (Docker, Apache Airflow, CI/CD)
Benefits
Compensation and Benefits
Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire:
Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction
Family-building, fertility, adoption and surrogacy benefits
Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution
Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA
Healthcare and Dependent Care Flexible Spending Accounts (FSA)
LGBTQ+ care concierge services
401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits
Company paid Basic Life, AD&D, short-term and long-term disability insurance
Employee Assistance Program
Sick and Vacation time (Flex time for salary positions), and Paid Holidays
Back-up childcare and parenting support resources
Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance
Weight Loss and Tobacco Cessation Programs
Tesla Babies program
Commuter benefits
Employee discounts and perks program
Expected Compensation
$80,000 - $258,000/annual salary + cash and stock awards + benefits
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
Tesla
Show more
Show less","Data Engineering, Data Architecture, SQL, ETL, Data Pipelines, Data Modeling, Business Analytics, Data Science, Python, Unix/Shell, Apache Spark, Kafka, Big Data Technologies, Distributed APIs, Web Services, Orchestration Tools, Docker, Apache Airflow, CI/CD, Relational Databases, Normalization Techniques, Compensation and Benefits","data engineering, data architecture, sql, etl, data pipelines, data modeling, business analytics, data science, python, unixshell, apache spark, kafka, big data technologies, distributed apis, web services, orchestration tools, docker, apache airflow, cicd, relational databases, normalization techniques, compensation and benefits","apache airflow, apache spark, big data technologies, business analytics, cicd, compensation and benefits, data architecture, data engineering, data science, datamodeling, datapipeline, distributed apis, docker, etl, kafka, normalization techniques, orchestration tools, python, relational databases, sql, unixshell, web services"
Senior Data Engineer,Rippling,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-rippling-3747286394,2023-12-17,Alameda,United States,Mid senior,Onsite,"About Rippling
Rippling is the first way for businesses to manage all of their HR & IT—payroll, benefits, computers, apps, and more—in one unified workforce platform.
By connecting every business system to one source of truth for employee data, businesses can automate all of the manual work they normally need to do to make employee changes. Take onboarding, for example. With Rippling, you can just click a button and set up a new employees’ payroll, health insurance, work computer, and third-party apps—like Slack, Zoom, and Office 365—all within 90 seconds.
Based in San Francisco, CA, Rippling has raised $1.2B from the world's top investors—including Kleiner Perkins, Founders Fund, Sequoia, Bedrock, and Greenoaks—and was named one of America's best startup employers by Forbes (#12 out of 500)
About The Role
We are looking for a hands-on senior engineer to play a key role in Rippling’s data team. As a senior engineering resource on the data engineering team, you will be leading the design and development of systems that will enable analytics, experimentation, and user-facing features. You will be closely involved with multiple data adjacent teams and stakeholders, alongside helping junior talent in the team learn and grow.
The Data Engineering Team at Rippling is a combination of warehousing and data platform engineering, supporting a variety of orgs across the company (Data Science, Marketing, Bizops, Revops, Finance to name a few). Here’s an idea of some of the initiatives we’re working on:
A realtime, central data lake to operationalize warehouse data.
Creating a metrics layer to make reporting more efficient and accurate.
Building a catalog to make our data assets searchable and easy to discover.
Making our globally distributed data stack compliant and scalable
What You'll Do
Help architect, build, and scale our data pipelines from our OLTP database, other internal systems and third party tools to our warehouse (Snowflake)
Leverage data technologies like Kafka, Presto, Flink, Airflow, Mongo, Snowflake and Spark
Support reporting, data science, operations and machine learning functions
Create data platforms, data lakes, and data ingestion systems that work at scale
Define and enforce data quality checks and audits for code warehousing datasets
Define and support internal SLAs for core data sets
Qualifications
5+ Years experience in Data and Software Engineering
Expertise in writing complex data transforms in SQL and Python
Knowledge of data warehousing concepts around building custom ETL integrations, building data infrastructure (SCD,CDC,Snapshots,indexing,partitioning)
Knowledge on Data Security and Governance (nice to have)
Experience in analytics, dimensional modeling, and ETL optimization preferred
BS/BA in a technical field such as Computer Science or Mathematics preferred
Additional Information
Rippling is an equal opportunity employer. We are committed to building a diverse and inclusive workforce and do not discriminate based on race, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, veteran or military status, or any other legally protected characteristics, Rippling is committed to providing reasonable accommodations for candidates with disabilities who need assistance during the hiring process. To request a reasonable accommodation, please email accomodations@rippling.com
Rippling highly values having employees working in-office to foster a collaborative work environment and company culture. For office-based employees (employees who live within a 40 mile radius of a Rippling office), Rippling considers working in the office, at least three days a week under current policy, to be an essential function of the employee's role.
This role will receive a competitive salary + benefits + equity. The salary for US-based employees will be aligned with one of the ranges below based on location; see which tier applies to your location here.
A variety of factors are considered when determining someone’s compensation–including a candidate’s professional background, experience, and location. Final offer amounts may vary from the amounts listed below.
The pay range for this role is:
144,000 - 237,000 USD per year(Tier 1)
Show more
Show less","SQL, Python, Data warehousing, ETL integrations, Data infrastructure, SCD, CDC, Snapshots, Indexing, Partitioning, Data security, Data governance, Analytics, Dimensional modeling, ETL optimization, Kafka, Presto, Flink, Airflow, Mongo, Snowflake, Spark","sql, python, data warehousing, etl integrations, data infrastructure, scd, cdc, snapshots, indexing, partitioning, data security, data governance, analytics, dimensional modeling, etl optimization, kafka, presto, flink, airflow, mongo, snowflake, spark","airflow, analytics, cdc, data governance, data infrastructure, data security, datawarehouse, dimensional modeling, etl integrations, etl optimization, flink, indexing, kafka, mongo, partitioning, presto, python, scd, snapshots, snowflake, spark, sql"
Founding Data Engineer (Spark/Python/Elastic),Fintool.com [YC],"San Francisco, CA",https://www.linkedin.com/jobs/view/founding-data-engineer-spark-python-elastic-at-fintool-com-yc-3727393593,2023-12-17,Alameda,United States,Mid senior,Onsite,"Apply here
About
Fintool is a financial copilot for institutional investors. It’s ChatGPT on top of financial documents, starting with SEC filings. Fintool is engineered to discover financial insights beyond the reach of timely human analysis or search software. It helps from summarizing long 10-K to finding new investment opportunities.
Fintool is backed by Y Combinator as well as entrepreneurs such as the co-founders of Datadog, Vercel, HuggingFace or domain experts from OpenAI to Deepmind.
Team
Nicolas Bustamante: spent 7 years building one of the largest AI-driven legal search engines (Bloomberg for lawyers). Nicolas hired nearly 200 people, secured millions of dollars in debt and equity funding, and the profitable business was successfully acquired by Summit Partners, a $43B billion growth equity fund, for $x00M+
Edouard Godrey: worked for nine years at Apple, leading teams of data scientists and engineers. He worked on Apple Search (Spotlight) and Apple Pay, maintaining big data pipelines and deploying cutting-edge AI models. He received the 2019 Apple Pay Innovation Award for outstanding contributions and fresh insights.
Our philosophy
Small team: small in-person teams outperform large and well-funded companies. When people visit our office, they should be surprised by how few people we are.
Ship code: we avoid meetings, PM jargon to release early, release often, and listen to customers.
In-person: we believe high-performing teams do their best work, build long-term relationships, and have the most fun in person.
Company Values
Clone and improve the best: we're not about reinventing the wheel but about enhancing proven success. We are shameless cloners who stand on the shoulders of giants. We draw inspiration and then create differentiation because distinctiveness drives dominance.
Release early, release often, and listen to your customers: speed matters in business, so we push better-than-perfect updates for customers asap. Mastery comes from repeated experiments and learning from mistakes rather than putting in a set number of hours. It’s 10,000 iterations, not 10,000 hours.
Warren Buffett: We model our personal and professional ethos on the principles he exemplifies. Upholding integrity, valuing honesty, practicing frugality, championing lifelong learning, embracing humility, extending generosity, applying rationality, and demonstrating patience. Every day, we strive to mirror these Buffett-inspired virtues.
Job Desc
We are building real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier.
Tech stack: Spark/Databricks, Python, Elastic, Postgres and LLM. Knowing React, Next.js, and TypeScript is a plus.
Experience: 5+ years of deploying production code at a company with a large infrastructure.
Location: San Francisco
Contract: Full-time
Apply here
Show more
Show less","Spark/Databricks, Python, Elastic, Postgres, LLM, React, Next.js, TypeScript, Unstructured data, Data engineering, Machine learning, Data pipelines, Financial documents, SQL, Data science","sparkdatabricks, python, elastic, postgres, llm, react, nextjs, typescript, unstructured data, data engineering, machine learning, data pipelines, financial documents, sql, data science","data engineering, data science, datapipeline, elastic, financial documents, llm, machine learning, nextjs, postgres, python, react, sparkdatabricks, sql, typescript, unstructured data"
Senior Data Engineer,The Trade Desk,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-the-trade-desk-3781943001,2023-12-17,Alameda,United States,Mid senior,Onsite,"The Trade Desk is a global technology company with a mission to create a better, more open internet for everyone through principled, intelligent advertising. Handling over 1 trillion queries per day, our platform operates at an unprecedented scale. We have also built something even stronger and more valuable: an award-winning culture based on trust, ownership, empathy, and collaboration. We value the unique experiences and perspectives that each person brings to The Trade Desk, and we are committed to fostering inclusive spaces where everyone can bring their authentic selves to work every day.
Do you have a passion for solving hard problems at scale? Are you eager to join a dynamic, globally- connected team where your contributions will make a meaningful difference in building a better media ecosystem? Come and see why Fortune magazine consistently ranks The Trade Desk among the best small- to medium-sized workplaces globally.
What we do:
Our data engineers are end-to-end owners who have the opportunity to participate in many aspects of designing, building, and delivering data-focused products for our stakeholders.
We believe in building end to end modeling pipelines that have a core focus on meeting core KPI’s and being able to validate these KPI’s. We work very closely with data scientists to evaluate modelling solutions up front from both a runtime, accuracy, and reproducibility perspective. Additionally, we prioritize frequent collaboration with data science, product, and other engineering teams to ensure that pipelines and tooling we build are meaningful, scalable, and pleasant to use.
Our system performs every day, 24/7, serving global traffic. We build a distributed system in a highly collaborative environment, utilizing a broad range of technologies. Our data engineers work on finding solutions to algorithmic, optimization, and scale challenges in everything we do.
What you’ll do:
You will work with data scientists, ML pipelines, data processing automation, data processing pipelines, model deployments and experimentation configuration, data quality, data warehousing, data privacy and governance – to name a few.
You will solve petabyte-scale data challenges across large-scale distributed systems coordinating thousands of servers in cloud and physical data centers around the world.
Senior Engineers contribute to more than our product – they build up our team. Through a combination of mentoring, technical leadership, and/or direct management of small teams, they make others better and raise the bar for those around them.
Our Senior Data Engineers are end-to-end owners. You will participate actively in all aspects of designing, building, and delivering data products for our clients.
Who you are:
You have a sustained track record of making significant, self-directed, and end-to-end contributions to large, impactful data engineering projects. You think beyond just the task at hand to deeply understand the ‘why’ behind what you are doing.
You understand engineering fundamentals. On our scale, many off-the-shelf techniques and existing technologies (open source and enterprise) do not work. You can work from first principles to evaluate solutions and adapt them to a unique environment.
You have experience in data engineering.
You have experience working with data scientists and understand and empathize with their needs and comfort levels.
You can think through model lifecycles end to end. This covers evaluating model feasibility - both runtime performance and from an accuracy POV; thinking of validation up front and challenging if the validation processes are sound; and model pipeline from training to deployments to setting up experiments.
You have worked with data modeling, configured AB test experimentation, and have experience working in technologies like SQL, ETL, and technologies similar to Spark, Airflow, SQL, data warehousing, etc...
You have experience with application deployments using tools analogous to Docker, Kubernetes, etc…
Bonus points for if you have experience working with deep learning models such as Tensorflow, Pytorch etc...
Experience with high level languages such as C#, Java, Python, Scala
You are a broadly skilled engineer, accustomed to developing web services. You have experience building always-on systems, working across a variety of technologies and service layers.
You have a product-focused mindset. You have the passion and ability to contribute to the process of discovering what will delight our clients and push forward one of the world’s largest and most influential industries toward a vision of openness, transparency, and evidence-based decision-making.
You work with confidence and without ego. Our engineers have deep knowledge and exercise a high degree of leadership in their daily work. You have strong opinions that are weakly held, defensible ideas, and advocate for what you believe is right. You are also adept at identifying and evaluating trade-offs, willing to be proven wrong, and quick to walk through fire to support your fellow teammates.
You value, seek out, and foster diversity. We are a global team from many diverse backgrounds, with different experiences and perspectives. To complement this team, you will welcome ideas that are different from your own and be skilled at finding and building from common ground.
You are a creative thinker, not bound by “the way things have always been done”.
What you know is less important than how well you learn and innovate. We don't need engineers who know all the answers; we need engineers who can invent the answers no one has thought of yet, to the questions yet to be asked. What and how you can contribute is what’s important to us. Our consideration is not limited by the kind of education you have or the specific technologies you have experience with. Variety of technical challenge is one of the best things about working at The Trade Desk as an engineer, but we do not expect you to know every technology we use when you start. What we care about is that you can learn quickly and solve complex problems using the best tools for the job. Our culture runs much deeper than just having fun together (though, we do that well too...) – the people we want on our team are trust-builders, generous givers, scrappy problem solvers, and gritty pursuers of excellence. Does this sound like you? If so, we welcome your application and the chance to meet you.
The Trade Desk does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of The Trade Desk. The Trade Desk is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.
[LA JOBS ONLY]
The Trade Desk will consider qualified applicants with criminal histories for employment in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring, Ordinance No. 184652.
[SF JOBS ONLY]
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
NY, CO, CA, and WA residents only:
In accordance with NY, CO, CA, and WA law, the range provided is The Trade Desk's reasonable estimate of the base compensation for this role. The actual amount may differ based on non-discriminatory factors such as experience, knowledge, skills, abilities, and location. All employees may be eligible to become The Trade Desk shareholders through eligibility for stock-based compensation grants, which are awarded to employees based on company and individual performance. The Trade Desk also offers other compensation depending on the role such as sales-based incentives and commissions. Plus, expected benefits for this role include comprehensive healthcare (medical, dental, and vision) with premiums paid in full for employees and dependents, retirement benefits such as a 401k plan and company match, short and long-term disability coverage, basic life insurance, well-being benefits, reimbursement for certain tuition expenses, parental leave, sick time of 1 hour per 30 hours worked, vacation time for full-time employees up to 120 hours thru the first year and 160 hours thereafter, and around 13 paid holidays per year. Employees can also purchase The Trade Desk stock at a discount through The Trade Desk’s Employee Stock Purchase Plan.
The Trade Desk also offers a competitive benefits package. Click here to learn more.
Note:
Interns are not eligible for variable incentive awards such as stock-based compensation, retirement plan, vacation, tuition reimbursement or parental leave
At the Trade Desk, Base Salary is one part of our competitive total compensation and benefits package and is determined using a salary range. The base salary range for this role is
$121,500—$222,800 USD
Show more
Show less","Data engineering, Data modeling, Data processing automation, Data processing pipelines, Data quality, Data warehousing, Data privacy, Data governance, Machine learning (ML) pipelines, Model deployments, Experimentation configuration, Cloud computing, Physical data centers, Tensorflow, Pytorch, Docker, Kubernetes, SQL, ETL, Spark, Airflow, C#, Java, Python, Scala","data engineering, data modeling, data processing automation, data processing pipelines, data quality, data warehousing, data privacy, data governance, machine learning ml pipelines, model deployments, experimentation configuration, cloud computing, physical data centers, tensorflow, pytorch, docker, kubernetes, sql, etl, spark, airflow, c, java, python, scala","airflow, c, cloud computing, data engineering, data governance, data privacy, data processing automation, data processing pipelines, data quality, datamodeling, datawarehouse, docker, etl, experimentation configuration, java, kubernetes, machine learning ml pipelines, model deployments, physical data centers, python, pytorch, scala, spark, sql, tensorflow"
Data Engineer III,Crystal Equation Corporation,"Burlingame, CA",https://www.linkedin.com/jobs/view/data-engineer-iii-at-crystal-equation-corporation-3781069066,2023-12-17,Alameda,United States,Mid senior,Onsite,"Job Description
Pay range is $67 - $84 per hour with full benefits available, including paid time off, medical/dental/vision/life insurance, 401K, parental leave, and more. Our compensation reflects the cost of labor across several US geographic markets. Pay is based on several factors including market location and may vary depending on job-related knowledge, skills, and experience.
THE PROMISES WE MAKE:
At Crystal Equation, we empower people and advance technology initiatives by building trust. Your recruiter will prep you for the interview, obtain feedback, guide you through any necessary paperwork and provide everything you need for a successful start. We will serve to empower you along the way and provide the path for your professional journey.
Data Engineer III
Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
Show more
Show less","Data Engineering, ETL, Data Management, Predictive Modeling, Scalability, Enterprise Data, Team Collaboration, Communication, Data Architecture, Computer Science, Computer Engineering","data engineering, etl, data management, predictive modeling, scalability, enterprise data, team collaboration, communication, data architecture, computer science, computer engineering","communication, computer engineering, computer science, data architecture, data engineering, data management, enterprise data, etl, predictive modeling, scalability, team collaboration"
Sr. Azure Data Engineer,First Soft Solutions LLC,"Fremont, CA",https://www.linkedin.com/jobs/view/sr-azure-data-engineer-at-first-soft-solutions-llc-3751137038,2023-12-17,Alameda,United States,Mid senior,Onsite,"We are actively hiring for
Senior
Azure
Data
Engineer
/ Data Modeler
Location: Fremont CA ( Remote)
Type: 12+ months
Client will consider the folks from PST/MST/CST only. ( Need ID proof)
Must Have
Enterprise Data modeling / Design
Azure SQL Data Warehouse (Synapse)
Azure synapse serverless pool
T-SQL ( Hands on experience , writing quires , building stored procs, performance optimization , etc..)
ADF / Any Enterprise ETL Tool
ADLS / any cloud storage
Should be able to understand the requirements and convert them into technical specifications , also able to work independently with minimal or no supervision.
Nice To Have
NoSQL databases ( preferred Cosmos)
Any scripting language ( Python preferred)
Denodo / PowerBI
Databricks
Show more
Show less","Azure, Data Modeling, Data Warehousing, SQL, TSQL, ADF, ETL, Cloud Storage, NoSQL, Python, Denodo, PowerBI, Databricks","azure, data modeling, data warehousing, sql, tsql, adf, etl, cloud storage, nosql, python, denodo, powerbi, databricks","adf, azure, cloud storage, databricks, datamodeling, datawarehouse, denodo, etl, nosql, powerbi, python, sql, tsql"
"Sr. Data Engineer, Automation and Analytics",Tesla,"Palo Alto, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-automation-and-analytics-at-tesla-3737835190,2023-12-17,Alameda,United States,Mid senior,Onsite,"What To Expect
The electrical component team in Supply Chain is looking for a highly skilled and experienced senior Data Engineer to support our Supply Chain Organization. You will plan effective data storage, security, sharing and publishing within the organization, including our Global Supply Managers as well as Design Engineer on a regular basis. You will have the responsibility to maintain large supply chain datasets and formulate applicable data-driven solutions which can effectively utilized by the global supply management team. It is essential that you can think strategically, connecting the dots in the bigger picture, as well as being comfortable in the details of the deliverables. A candidate for this role needs to be a self-motivated for data analytics and process raw, unstructured data using batch and real time processing frameworks.
What You'll Do
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
Drive problem solving and continuous improvement initiatives to improve supply chain operations processes
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
Think strategically and considering Tesla’s global supply chain structure
Build and maintain purchasing database on a regular basis
Make data and reporting updates as needed to ensure accuracy
Develop data-based solutions to complex purchasing challenges and be able to present relevant information in non-specialist forum
Identify process gaps and areas for optimization and automation
Own system requirements, and participate in the software development process from design to user acceptance testing
What You'll Bring
5+ years of related experience with a Bachelor's degree; or 3 years and a Master's degree; or a PhD without experience; or equivalent work experience
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. In depth knowledge of Model and Design of DB schemas for read and write performance.
Highly skilled in programming (R, Python). Apache Airflow is a plus
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Experience creating Full-Stack UI is a plus
Work experience in material planning, supply chain, purchasing, or manufacturing is a plus
Strong organizational skills and the ability to prioritize and manage multiple projects simultaneously with complex and demanding deadlines in a dynamic environment
Exceptional communications, strategic thinking, and interpersonal skills
Proven ability to set expectations, manage to deadlines and hold individuals and teams accountable to critical milestones
Knowledge of BI Tools like Tableau
Benefits
Compensation and Benefits
Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire:
Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction
Family-building, fertility, adoption and surrogacy benefits
Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution
Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA
Healthcare and Dependent Care Flexible Spending Accounts (FSA)
LGBTQ+ care concierge services
401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits
Company paid Basic Life, AD&D, short-term and long-term disability insurance
Employee Assistance Program
Sick and Vacation time (Flex time for salary positions), and Paid Holidays
Back-up childcare and parenting support resources
Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance
Weight Loss and Tobacco Cessation Programs
Tesla Babies program
Commuter benefits
Employee discounts and perks program
Expected Compensation
$80,000 - $258,000/annual salary + cash and stock awards + benefits
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
Tesla
Show more
Show less","SQL, Relational databases, Data pipelines, Data sets, Database schemas, R, Python, Apache Airflow, FullStack UI, Material planning, Supply chain, Purchasing, Manufacturing, Tableau, BI Tools, Aetna PPO, HSA plans, Dental plans, Vision plans, 401(k), Employee Stock Purchase Plans, Basic Life insurance, AD&D insurance, Shortterm disability insurance, Longterm disability insurance, Employee Assistance Program, Sick time, Vacation time, Paid Holidays","sql, relational databases, data pipelines, data sets, database schemas, r, python, apache airflow, fullstack ui, material planning, supply chain, purchasing, manufacturing, tableau, bi tools, aetna ppo, hsa plans, dental plans, vision plans, 401k, employee stock purchase plans, basic life insurance, add insurance, shortterm disability insurance, longterm disability insurance, employee assistance program, sick time, vacation time, paid holidays","401k, add insurance, aetna ppo, apache airflow, basic life insurance, bi tools, data sets, database schemas, datapipeline, dental plans, employee assistance program, employee stock purchase plans, fullstack ui, hsa plans, longterm disability insurance, manufacturing, material planning, paid holidays, purchasing, python, r, relational databases, shortterm disability insurance, sick time, sql, supply chain, tableau, vacation time, vision plans"
Senior Software Development Engineer - Big Data,"HireIO, Inc.","San Francisco, CA",https://www.linkedin.com/jobs/view/senior-software-development-engineer-big-data-at-hireio-inc-3739546051,2023-12-17,Alameda,United States,Mid senior,Onsite,"Team Introduction
The Data Platform team works on building data infrastructures and data products to support business engineering teams.
As a Software Development Engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.
Responsibilities - What You'll Do
Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis)
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Requirements
BS/MS from a quantitative field of study (CS, STEM, etc)
Experience in API, backend, and data services development
Experience in Big Data stack(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.)
Experience with ETL (Extraction, Transformation & Loading) or ELT, and architecting data systems
Ability to ship code in Java, Python and SQL
Solid communication and collaboration skills
Show more
Show less","Software Development, Data Platforms, Data Infrastructure, Data Products, Data Transformations, Data Analysis, Big Data Systems, Hadoop, MapReduce, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink, ETL, ELT, Java, Python, SQL","software development, data platforms, data infrastructure, data products, data transformations, data analysis, big data systems, hadoop, mapreduce, hive, spark, metastore, presto, flume, kafka, clickhouse, flink, etl, elt, java, python, sql","big data systems, clickhouse, data infrastructure, data platforms, data products, data transformations, dataanalytics, elt, etl, flink, flume, hadoop, hive, java, kafka, mapreduce, metastore, presto, python, software development, spark, sql"
REMOTE DATA ENGINEER,AppleOne Employment Services,"San Francisco, CA",https://www.linkedin.com/jobs/view/remote-data-engineer-at-appleone-employment-services-3780618983,2023-12-17,Alameda,United States,Mid senior,Remote,"Full Benefits: Dental, Health and Vision
401k Matching
PTO and Extended Leave
Great Pay up to $130K
Design, build, maintain, and optimize large scale data processing, and reporting modules on cloud and on-premise using combination of PostgreSQL, Elasticsearch databases, Spark, Data Lake and Event Driven solutions. Solid experience designing and developing high performance systems using big data processing & querying tools including PostgreSQL.
The right candidate will need to have professional knowledge of developing ETL pipelines which involves Postgres, SQL queries, pgPL/SQL, python scripts, Spark, Data Lake & Airflow. It is preferred to understand multi-language text data match processes (Transliteration, Translation and Fuzzy Match).
Design, develop and maintain high performance complex data processing & querying logic on RDBMS and Data Lake.
Participate in various requirements sessions with internal teammates and convert business / functional requirements into technical solutions, along with supporting materials such as use cases, designs, flowcharts, models, specifications, and reports.
Provide technical support to requirements, development, and testing.
Work directly with Project Management, Business Intelligence and Data Engineering to develop end-user training and reference materials.
Draft standards and procedures affecting data analysis and process modeling, as well as large scale data design & processing, maintenance, and management.
Support initiatives related to scalability and performance.
Conduct performance tuning reviews for improvements and prepare written reports of findings, including recommendations.
Have experience working with GitLab and being able to build and implement CI/CD pipelines for continues development on different stages (dev, QA, production).
Basic Qualifications
8+ years database developer experience on Postgres or Oracle. Including SQL, stored procedures using pgpl/sql or pl/sql programing languages
5+ years doing ETL processes. It will be a big plus doing it in a cloud environment specifically on AWS.
4+ years data engineering experience specifically using Spark, scala or pyspark
4+ python development
4+ years development experience on AWS environment(RDS, Lambda, Step functions, Kinesis, Athena, …)
4+ years working in an Object-Oriented programming language(Java, C, C++, …).
Must have Linux/Unix OS working experience.
Must have containerized development experience.
Must have CI/CD automated deployment experience.
Must have experience with performance tuning and optimization in a PostgreSQL or Oracle.
It is a plus to have Elasticsearch index build and querying experience.
It is a plus to have Databricks working experience.
It is plus to understand how web-based applications work.
Confident and proactive self-starter, skilled in taking initiative, assessing requirements, coming up with plans, and taking the lead in making plans reality.
Advanced knowledge of decision and intelligence systems concepts working with a big data/data lake, and database structures such as normalization, datamarts, fact tables, dimensions, views, indexing, and partitions.
#1091
AppleOne is proud to be an Equal Opportunity Employer. We believe in people, and we are committed to working with people of all backgrounds and connecting them with clients and companies who share our goals of diversity and inclusiveness. All qualified applicants will receive consideration for employment without regard to race, religion, ancestry, color, national origin, age, gender identity or expression, genetic information, marital status, medical condition, disability, protected veteran status, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable federal, state, or local laws.
For temporary assignments lasting 13 weeks or longer, the Company is pleased to offer major medical, dental, vision, 401k and any statutory sick pay where required.
If you believe you need a reasonable accommodation due to a disability or health condition in order to search for a job opening or to apply for a position, please contact your staffing representative who will reach out to our HR team.
AppleOne participates in the E-Verify program in certain locations as required by law. Learn more about the E-Verify program. https://e-verify.uscis.gov/web/media/resourcesContents/E-Verify_Participation_Poster_ES.pdf]]>
Show more
Show less","PostgreSQL, Elasticsearch, Spark, Data Lake, Airflow, ETL, SQL, pgPL/SQL, Python, Scala, Pyspark, Java, C, C++, Linux/Unix, GitLab, CI/CD, AWS, RDS, Lambda, Step functions, Kinesis, Athena, Databricks, Normalization, Datamarts, Fact tables, Dimensions, Views, Indexing, Partitions","postgresql, elasticsearch, spark, data lake, airflow, etl, sql, pgplsql, python, scala, pyspark, java, c, c, linuxunix, gitlab, cicd, aws, rds, lambda, step functions, kinesis, athena, databricks, normalization, datamarts, fact tables, dimensions, views, indexing, partitions","airflow, athena, aws, c, cicd, data lake, databricks, datamarts, dimensions, elasticsearch, etl, fact tables, gitlab, indexing, java, kinesis, lambda, linuxunix, normalization, partitions, pgplsql, postgresql, python, rds, scala, spark, sql, step functions, views"
Senior Data Engineer,Freestar,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-freestar-3100530052,2023-12-17,Alameda,United States,Mid senior,Remote,"About Freestar
Freestar engineer teams develop cutting-edge monetization solutions for websites and mobile apps. By combining industry-leading technology, data, and massive scale, we enable busy site owners and mobile app publishers to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. The result is publishers having more time to do what they do best: create content and focus on their apps.
Senior Engineer (Remote)
We’re looking for a Senior Engineer for our Data Platform Team - the product that drives our core business. As a Senior Engineer at Freestar, you'll be one of the main contributors to a talented team of developers to innovate features for our product that separate us from our competitors.
What You'll Be Doing
Run points on whole projects by recommending, prototype, build and debug data infrastructures on Google Cloud Platform (GCP) with other principal and senior engineers within the team, and to mentor less experienced Data Engineers.
Demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
Participate in early-stage conversation with our product development team about product / features.
Improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.
Skills And Experience You'll Need To Have
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub).
4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS.
4+ years of hands-on experience using SQL to perform complex data manipulation
3+ years of experience modeling data warehouses
3+ years of experience building data pipelines, CI/CD pipelines, and fit for purpose data stores
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Experience writing software in one or more languages such as Python, Java, Scala, etc.
Experience with systems monitoring/alerting, capacity planning and performance tuning
Enjoy analyzing and organizing rapidly-changing business data to support product and business solutions
Nice to have Qualifications: *
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
Why You Want To Work With Us
Full-Time, Salaried Position
Working remotely
Generous Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Show more
Show less","Data warehouse modernization, Architectures, ETL/ELT pipelines, Reporting/analytic tools, Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub, SQL, Python, Java, Scala, Systems monitoring/alerting, Capacity planning, Performance tuning, CloudSQL, Spanner, Cloud Storage, Dataflow, Dataproc, BigQuery, Dataprep, Composer, IoT architectures, Realtime data streaming pipelines, Machine learning models, Statistical tools, Ad tech, Media landscape","data warehouse modernization, architectures, etlelt pipelines, reportinganalytic tools, beam, airflow, hadoop, spark, hive, kafka, pubsub, sql, python, java, scala, systems monitoringalerting, capacity planning, performance tuning, cloudsql, spanner, cloud storage, dataflow, dataproc, bigquery, dataprep, composer, iot architectures, realtime data streaming pipelines, machine learning models, statistical tools, ad tech, media landscape","ad tech, airflow, architectures, beam, bigquery, capacity planning, cloud storage, cloudsql, composer, data warehouse modernization, dataflow, dataprep, dataproc, etlelt pipelines, hadoop, hive, iot architectures, java, kafka, machine learning models, media landscape, performance tuning, pubsub, python, realtime data streaming pipelines, reportinganalytic tools, scala, spanner, spark, sql, statistical tools, systems monitoringalerting"
Senior Marketing Data Engineer,Samsara,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-marketing-data-engineer-at-samsara-3741492822,2023-12-17,Alameda,United States,Mid senior,Remote,"Who We Are
Samsara (NYSE: IOT) is the pioneer of the Connected Operations™ Cloud, which is a platform that enables organizations that depend on physical operations to harness Internet of Things (IoT) data to develop actionable insights and improve their operations. At Samsara, we are helping improve the safety, efficiency and sustainability of the physical operations that power our global economy. Representing more than 40% of global GDP, these industries are the infrastructure of our planet, including agriculture, construction, field services, transportation, and manufacturing — and we are excited to help digitally transform their operations at scale.
Working at Samsara means you’ll help define the future of physical operations and be on a team that’s shaping an exciting array of product solutions, including Video-Based Safety, Vehicle Telematics, Apps and Driver Workflows, Equipment Monitoring, and Site Visibility. As part of a recently public company, you’ll have the autonomy and support to make an impact as we build for the long term.
Recent awards we’ve won include:
Glassdoor's Highest-Rated Tech Companies for Culture and Values 2023
Great Place To Work Certified™ 2023
Best Place to Work by Built In 2023
Financial Times The Americas’ Fastest Growing Companies 2023
Deloitte Fast 500 Companies
We see a profound opportunity for data to improve the safety, efficiency, and sustainability of operations, and hope you consider joining us on this exciting journey.
About the team:
Data and Analytics is a critical team within Marketing. Our mission is to enable revenue performance by providing marketing and sales teams with the insights, tools, infrastructure and consultation to make data driven judgements. We are a scrappy and growing team that loves all things data! The team will be composed of data engineers, analytics managers and data scientists. We are passionate about leveraging world class data and analytics to deliver a great customer experience.
Our team promotes an agile, collaborative, supportive environment where diverse thinking, innovative design, and experimentation is welcomed and encouraged.
You should apply if:
You want to impact the industries that run our world: Your efforts will result in real-world impact—helping to keep the lights on, get food into grocery stores, reduce emissions, and most importantly, ensure workers return home safely.
You want to build for scale: With over 2.3 million IoT devices deployed to our global customers, you will work on a range of new and mature technologies driving scalable innovation for customers across industries driving the world's physical operations.
You are a team player: Working on our partners requires a mix of independent effort and collaboration. Motivated by our mission, we’re all racing toward our connected operations vision, and we intend to win—together.
You are a life-long learner: We have ambitious goals. Every Samsarian has a growth mindset as we work with a wide range of technologies, challenges, and customers that push us to learn on the go.
Click here
to learn about what we value at Samsara.
In this role, you will:
Develop and maintain marketing databases, datasets, pipelines and Samsara’s Customer Data Platform (CDP) to enable advanced segmentation, targeting, automation and analytics.
Manage critical data pipelines to enable our growth initiatives and advanced analytics. Manage the SLAs for those data pipelines and constantly improve efficiency and data quality.
Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with data mart and CDP environments.
Develop and improve the current data architecture, data quality, monitoring and data availability.
Identify data needs from broad stakeholders, understand requirements for metrics and analysis, and build efficient and scalable data products to enable a data-driven marketing approach.
Write sophisticated yet optimized data transformations in SQL/Python to generate data products consumed by customer systems and Analytics, Marketing Operations, Sales Operations teams.
Champion, role model, and embed Samsara’s cultural principles (Obsess Over the Customer, Build for the Long Term, Growth Mindset) as we scale globally and across new offices
Minimum requirements for the role:
5+ years of working experience in a growth, software or data engineering role
Excellent SQL and Python knowledge with strong hands-on data modeling
Experience with data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
Hands-on experience working with modern data technologies stack such as Databricks, Google Big Query, Redshift, RDS, Snowflake or similar solutions
Experience with development lifecycle tools such as Github, TFS, etc.
Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets.
Familiarity with customer, marketing and/or web data
Experience integrating data from core Sales and Marketing platforms (e.g. Marketing Automation, CRM, and web analytics)
Self-starter, motivated, responsible, innovative and technology-driven candidate who performs well both unsupervised and as a team member
A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences
An ideal candidate also has:
Knowledge of Marketo, Salesforce.com and Google Analytics
Experience working with CDPs such as Segment, Blueshift, Lytics or Adobe Real-time CDP
Experience with data visualization tools and packages (e.g. Looker, Domo, Tableau, MixPanel)
Familiarity with Marketing Technologies (MarTech stacks)
Experience coding with Scala, R or Pandas
Data Science, machine learning or predictive analytics experience
Samsara’s Compensation Philosophy
: Samsara’s compensation program is designed to deliver total compensation (based on role, level, and geography) that is above market. We do this through our base salary + bonus/variable + restricted stock unit awards (RSUs). A new hire RSU award is awarded at the time of hire, and additional RSU refresh grants may be awarded annually.
We pay for performance, and top performers are eligible to receive above target equity refresh awards which allow employees to achieve higher market positioning.
The range of annual base salary for full-time employees for this position is below. Please note that base pay offered may vary depending on factors including your city of residence, job-related knowledge, skills, and experience.
$122,400—$180,000 USD
At Samsara, we welcome everyone regardless of their background, race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, etc. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact.
Accommodations
Samsara is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email accessibleinterviewing@samsara.com or click here if you require any reasonable accommodations throughout the recruiting process.
Benefits
Full time employees receive an above market total compensation package along with employee-led remote and flexible working, health benefits, Samsara for Good charity fund, and much, much more. Take a look at our Benefits site to learn more.
Flexible Working
At Samsara, we have adopted a flexible way of working, enabling teams and individuals to do their best work, regardless of where they’re based. We value in-person collaboration and know a change of scenery and quiet space to work is welcomed from time to time, but also appreciate that the world of work has changed. Our offices remain open for those who prefer to collaborate or work in-office, but we also encourage fully remote applicants. As most roles are not required to be in the office, we are able to hire remotely where Samsara has an established presence. If a role is required to be in a certain location and candidates do not have work authorization for that location, Samsara will conduct an immigration assessment. If the role is not required to be in a specific location, Samsara will move forward with the remote location that works best for the business. All offers of employment are contingent upon an individual’s ability to secure and maintain the legal right to work at the company.
Please note: Samsara does not accept agency resumes and is not responsible for any fees related to unsolicited resumes. Please do not forward resumes to Samsara employees.
Show more
Show less","SQL, Python, Data modeling, Data warehouse architectures, ETL/ELT tools, Reporting/analytic tools, Databricks, Google Big Query, Redshift, RDS, Snowflake, GitHub, TFS, Marketo, Salesforce.com, Google Analytics, Segment, Blueshift, Lytics, Adobe Realtime CDP, Looker, Domo, Tableau, MixPanel, Scala, R, Pandas, Data Visualization, Machine learning, Predictive analytics","sql, python, data modeling, data warehouse architectures, etlelt tools, reportinganalytic tools, databricks, google big query, redshift, rds, snowflake, github, tfs, marketo, salesforcecom, google analytics, segment, blueshift, lytics, adobe realtime cdp, looker, domo, tableau, mixpanel, scala, r, pandas, data visualization, machine learning, predictive analytics","adobe realtime cdp, blueshift, data warehouse architectures, databricks, datamodeling, domo, etlelt tools, github, google analytics, google big query, looker, lytics, machine learning, marketo, mixpanel, pandas, predictive analytics, python, r, rds, redshift, reportinganalytic tools, salesforcecom, scala, segment, snowflake, sql, tableau, tfs, visualization"
"SR. Scala Engineer, Database Engineering",Experfy,"San Francisco, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590298790,2023-12-17,Alameda,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, SQL, RPC, Data warehousing, Query optimization, Fault tolerance, Opensource, Enterprisescale, Distributed systems, Scalability, Software engineering, Data platforms, Team leadership, Web3, Blockchain, Data management, Scripting, Test automation","scala, apache spark, apache arrow, sql, rpc, data warehousing, query optimization, fault tolerance, opensource, enterprisescale, distributed systems, scalability, software engineering, data platforms, team leadership, web3, blockchain, data management, scripting, test automation","apache arrow, apache spark, blockchain, data management, data platforms, datawarehouse, distributed systems, enterprisescale, fault tolerance, opensource, query optimization, rpc, scala, scalability, scripting, software engineering, sql, team leadership, test automation, web3"
"Data Engineer / Spark, Hadoop",Motion Recruitment,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-spark-hadoop-at-motion-recruitment-3762362756,2023-12-17,Alameda,United States,Mid senior,Remote,"A top financial company is looking to hire a Data Engineer with experience in Spark, Hadoop, Hive
Basic Qualifications (Required Skills & Experience)
5+ years of experience,
Python Spark Hive, Hadoop ETL work SQL GCP +
Other Qualifications & Desired Competencies
Fully Remote
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
401(k)
Posted By:
Julie Bennett
Show more
Show less","Python, Spark, Hadoop, Hive, ETL, SQL, GCP","python, spark, hadoop, hive, etl, sql, gcp","etl, gcp, hadoop, hive, python, spark, sql"
Senior Data Infrastructure Engineer,Trendpop,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-infrastructure-engineer-at-trendpop-3619298435,2023-12-17,Alameda,United States,Mid senior,Remote,"Trendpop is looking for a Senior Data Infrastructure Engineer to help us reverse engineer the algorithms that power virality on social media. Trendpop Core, our large-scale data infrastructure platform, ingests millions of social media posts and processes multiple terabytes per day to surface valuable insights for our customers. This person will work closely with our CTO to build the next phase of Trendpop Core infrastructure to support additional data sources, scale to the next order of magnitude, and glean smarter insights using cutting edge machine learning techniques. This person should be excited to work in a tiny but effective engineering team at a fast-paced startup. They will be given broad autonomy to tackle the toughest infrastructure problems we face every day!
Show more
Show less","Data Infrastructure, Data Engineering, Largescale Data Platform, Data Ingestion, Data Processing, Machine Learning, Cutting Edge Machine Learning Techniques, Social Media, Data Sources","data infrastructure, data engineering, largescale data platform, data ingestion, data processing, machine learning, cutting edge machine learning techniques, social media, data sources","cutting edge machine learning techniques, data engineering, data infrastructure, data ingestion, data processing, data sources, largescale data platform, machine learning, social media"
Senior Data Engineer / Remote,Motion Recruitment,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-motion-recruitment-3754627944,2023-12-17,Alameda,United States,Mid senior,Remote,"Our client is changing the way we can find our friends and family from around the world. Founded in 2008, they have millions of users per month visiting their platform for various reasons.
They are looking for a Senior Data Engineer to join their Data Ops team to come in and make an impact right away. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, Airflow, AWS, and have a computer Science Degree. If this is you APPLY NOW!
Basic Qualifications (Required Skills & Experience)
6-8 years of experience
Python – 5 years of experience
Spark – 5 years (Prefer Pyspark)
AWS – 2+ years of experience
Airflow – 2+ years of experience
EMR
ETL work
SQL
Technology degree is important as well! (ex: CS degree)
Other Qualifications & Desired Competencies
Fully Remote!!!
Equity and Bonuses involved
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k)
Posted By:
Casey Ryan
Show more
Show less","Python, PySpark, Airflow, AWS, EMR, ETL, SQL, Computer Science degree","python, pyspark, airflow, aws, emr, etl, sql, computer science degree","airflow, aws, computer science degree, emr, etl, python, spark, sql"
Senior/First Data Engineer,Sleeper,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-first-data-engineer-at-sleeper-3737470211,2023-12-17,Alameda,United States,Mid senior,Remote,"Position Summary
The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.
You will be the first hire in this function.
Location
SF Bay Area, NYC, or Remote
Key Responsibilities
ETL & Backend Development:
Design and optimize ETL pipelines.
Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.
Data Architecture:
Design scalable and efficient data models for Cassandra and ScyllaDB.
Ensure data integrity, quality, and security.
Data Science Support:
Collaborate with data scientists, providing them with clean and reliable datasets.
Assist in implementing and scaling data science models.
Innovation & Research:
Stay abreast of latest technologies.
Recommend technical improvements for data processing and storage.
Qualifications
Required
Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field.
5+ years of experience in backend development, with a strong focus on data engineering.
Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.
Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).
Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).
Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.
Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).
Soft Skills: Exceptional problem-solving skills.
Strong communication for technical and non-technical discussions.
Nice-to-have
Experience with cloud platforms like AWS, GCP, or Azure.
Exceptional communication skills, both verbal and written.
Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).
Benefits
Competitive salary ($150,000-$225,000/year) and stock options
Comprehensive health, dental, and vision plans
401(k)
Flexible working hours and remote work options
Regular team building events and activities
Show more
Show less","Python, Java, Scala, Elixir, Apache Kafka, Apache Airflow, SQL, NoSQL, Cassandra, ScyllaDB, Redshift, BigQuery, Snowflake, AWS, GCP, Azure, Pandas, Scikitlearn, TensorFlow, PyTorch, Data warehousing, Data science, Machine learning, Cloud platforms, Distributed systems","python, java, scala, elixir, apache kafka, apache airflow, sql, nosql, cassandra, scylladb, redshift, bigquery, snowflake, aws, gcp, azure, pandas, scikitlearn, tensorflow, pytorch, data warehousing, data science, machine learning, cloud platforms, distributed systems","apache airflow, apache kafka, aws, azure, bigquery, cassandra, cloud platforms, data science, datawarehouse, distributed systems, elixir, gcp, java, machine learning, nosql, pandas, python, pytorch, redshift, scala, scikitlearn, scylladb, snowflake, sql, tensorflow"
Experienced Data Engineer - Data Engineering,Plaid,"San Francisco, CA",https://www.linkedin.com/jobs/view/experienced-data-engineer-data-engineering-at-plaid-3755356727,2023-12-17,Alameda,United States,Mid senior,Hybrid,"We believe that the way people interact with their finances will drastically improve in the next few years. We’re dedicated to empowering this transformation by building the tools and experiences that thousands of developers use to create their own products. Plaid powers the tools millions of people rely on to live a healthier financial life. We work with thousands of companies like Venmo, SoFi, several of the Fortune 500, and many of the largest banks to make it easy for people to connect their financial accounts to the apps and services they want to use. Plaid’s network covers 12,000 financial institutions across the US, Canada, UK and Europe. Founded in 2013, the company is headquartered in San Francisco with offices in New York, Washington D.C., London and Amsterdam.
Making data-driven decisions is key to Plaid's culture. To support that, we need to scale our data systems while maintaining correct and complete data. We provide tooling and guidance to teams across engineering, product, and business and help them explore our data quickly and safely to get the data insights they need, which ultimately helps Plaid serve our customers more effectively. In addition, Plaid will not be successful if we can't move quickly. We build the data systems and tools that enable everyone at Plaid to be data-driven, making analytics easy, obvious, and proactive across the company.
Data Engineers heavily leverage SQL and Python to build data workflows that integrate with our Golang and Typescript applications. We use tools like DBT, Airflow, Redshift, ElasticSearch, Atlan, and Retool to orchestrate data pipelines and define workflows. We work with engineers, product managers, business intelligence, data analysts, and many other teams to build Plaid's data strategy and a data-first mindset. Our engineering culture is IC-driven -- we favor bottom-up ideation and empowerment of our incredibly talented team. We are looking for engineers who are motivated by creating impact for our consumers and customers, growing together as a team, shipping the MVP, and leaving things better than we found them.
You will be in a high impact role that will drive and define data standards and data culture across Plaid. You will have the opportunity to transition into the Tech Lead role on the team which will come with higher visibility and more stakeholder collaboration. You will collaborate with and have strong and cross functional partnerships with literally all teams at Plaid from Engineering to Product to Marketing/Finance. You will be responsible for mentoring junior engineers on the team and being closely involved in their design and implementations. You will be able to provide guidance and build the technical culture on the team in the form of design reviews and PRs.
Responsibilities
Defining the long-term technical roadmap for building a data-driven culture at Plaid.
Focus on data quality and privacy.
Leading key data engineering projects that drive collaboration across the company.
Mentoring engineers, operations, and data analysts on best practices for data organization and query performance.
Advocating for adopting industry tools and practices at the right time.
Owning core SQL and python data pipelines that power our data lake and data warehouse.
Well-documented data with defined dataset quality, uptime, and usefulness.
Qualifications
6+ years of dedicated data engineering experience, solving complex data pipelines issues at scale.
You value SQL as a flexible and extensible tool, and are comfortable with modern SQL data orchestration tools like DBT, Mode, Hightouch, and Airflow.
Experience working both real-time systems like Kinesis, Kafka, Flink, and batch data pipelines in Redshift, Presto, and Data Lakes.
You appreciate the importance of schema design, and can evolve an analytics schema on top of unstructured data.
You are excited to try out new technologies. You like to produce proof-of-concepts that balance technical advancement and user experience and adoption.
You like to get deep in the weeds to manage, deploy, and improve low level data infrastructure.
You are empathetic working with stakeholders. You listen to them, ask the right questions, and collaboratively come up with the best solutions for their needs.
You are a champion for data privacy and integrity, and always act in the best interest of consumers.
$187,200 - $280,800 a year
Target base Salary for this role is $187,200- $280,800 per year. Additional compensation in the form(s) of equity and/or commission are dependent on the position offered. Plaid provides a comprehensive benefit plan, including medical, dental, vision, and 401(k). Pay is based on factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience and skillset, and location. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
Our mission at Plaid is to unlock financial freedom for everyone. To support that mission, we seek to build a diverse team of driven individuals who care deeply about making the financial ecosystem more equitable. We recognize that strong qualifications can come from both prior work experiences and lived experiences. We encourage you to apply to a role even if your experience doesn't fully match the job description. We are always looking for team members that will bring something unique to Plaid!
Plaid is proud to be an equal opportunity employer and values diversity at our company. We do not discriminate based on race, color, national origin, ethnicity, religion or religious belief, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, military or veteran status, disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local laws. Plaid is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance with your application or interviews due to a disability, please let us know at accommodations@plaid.com.
Please review our Candidate Privacy Notice here .
Show more
Show less","SQL, Python, Golang, TypeScript, DBT, Airflow, Redshift, ElasticSearch, Atlan, Retool, Kinesis, Kafka, Flink, Presto, Data Lakes, Data orchestration tools, Schema design, Data privacy, Data integrity, Data quality, Data pipelines, Data engineering, Data governance, Data management, Data analysis, Data architecture, Tech Lead","sql, python, golang, typescript, dbt, airflow, redshift, elasticsearch, atlan, retool, kinesis, kafka, flink, presto, data lakes, data orchestration tools, schema design, data privacy, data integrity, data quality, data pipelines, data engineering, data governance, data management, data analysis, data architecture, tech lead","airflow, atlan, data architecture, data engineering, data governance, data integrity, data lakes, data management, data orchestration tools, data privacy, data quality, dataanalytics, datapipeline, dbt, elasticsearch, flink, golang, kafka, kinesis, presto, python, redshift, retool, schema design, sql, tech lead, typescript"
Senior Data Engineer,Mastercard,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mastercard-3782409403,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Data Engineer
Have you ever wondered how do you see offers on your credit card based on your past purchase? If you wanted to find the answer do join our team.
We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our platform - as Data Engineering is the eyes through which they see the product.
Data Engineer Responsibilities
Lead day to day system development and maintenance activities of the team to meet service level agreements and create solutions with high level of innovation, cost effectiveness, high quality and faster time to market.
Support code versioning, and code deployments for Data Pipelines.
Ensure test coverage for unit testing and support integration and performance testing.
Contribute ideas to help ensure that required standards and processes are in place.
Research and evaluate current and upcoming technologies and frameworks.
Build data expertise and own data quality for your areas.
Minimum Qualifications
Extensive Java/Scala development experience.
Extensive experience with SQL.
Strong experience with Spark Processing engine.
Strong experience with Big data tools / technologies (Hive, Impala, OOZIE, Airflow, NIFI)
Strong experience with Data Modeling.
Experience in Kafka and PCF (Pivotal Cloud Foundry)
Experience analyzing data to discover opportunities and address gaps.
Experience working with cloud or on-prem Big Data platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
San Francisco, California: $138,000 - $221,000 USD
Show more
Show less","Java, Scala, SQL, Spark, Hive, Impala, Oozie, Airflow, NiFi, Data Modeling, Kafka, Pivotal Cloud Foundry, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata","java, scala, sql, spark, hive, impala, oozie, airflow, nifi, data modeling, kafka, pivotal cloud foundry, aws redshift, google bigquery, azure data warehouse, netezza, teradata","airflow, aws redshift, azure data warehouse, datamodeling, google bigquery, hive, impala, java, kafka, netezza, nifi, oozie, pivotal cloud foundry, scala, spark, sql, teradata"
Senior Data Engineer,Zoox,"Foster City, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zoox-2938046886,2023-12-17,Alameda,United States,Mid senior,Hybrid,"The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.
You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.
Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.
About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.
Follow us on LinkedIn
About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.
Follow us on LinkedIn
A Final Note
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.
Show more
Show less","Data Warehousing, Python, Scala, SQL, Airflow, Apache Spark, Hadoop, HDFS, HBase, Apache Kafka, Kinesis, C++, DataOps, Robotics, Machine Learning","data warehousing, python, scala, sql, airflow, apache spark, hadoop, hdfs, hbase, apache kafka, kinesis, c, dataops, robotics, machine learning","airflow, apache kafka, apache spark, c, dataops, datawarehouse, hadoop, hbase, hdfs, kinesis, machine learning, python, robotics, scala, sql"
Data Engineer,"Consultant Specialists, Inc. (CSI)","South San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-consultant-specialists-inc-csi-3782265785,2023-12-17,Alameda,United States,Mid senior,Hybrid,"ROCGJP00024750 Data Engineer II
W2 ONLY.
NO C2C. NO 3RD PARTIES.
LOCAL ONLY - Hybrid schedule. Will work onsite in South San Francisco of 3 days a week.
Desired Skills:
Experience in Data Visualization tools such as Tableau, Power BI etc. as it relates to data surfacing.
Previous experience with Informatica, Talend tools
Exposure to Data Science Technologies and Capabilities
The candidate will join our existing Information Management Office (IMO) team to build and deploy data solutions for clinical trial operations, data science/predictive capabilities, and analytics/business intelligence tools. These platforms are built on AWS Cloud with S3, Glue, Talend, Redshift, Snowflake, Tableau, Qlik, and Alation.
Serves as the IMO data engineer rep on ECD programs with minimum oversight.
Assist in communicating technical concepts to business stakeholders as well as communicate any gaps to the technical team.
Understand how work fits into the larger project and identify problems with requirements and communicate to IMO leadership.
Participates in roadmap and strategy development discussions. Provide input on project estimations, specifications and any on-going issues that may negatively impact the project deliverables
Partner closely with project managers, technology and business teams to evaluate and provide engineering solutions to their needs. Work closely with other Infra/DevOps, data engineers, data analysts and Data Scientists in the team for delivering high quality solutions.
Lead solution architecture with minimal supervision
Provide L1 & L2 support for all data engineering tickets by maintaining the agreed upon SLAs, RTOs and uptime goals. Engage senior cloud engineers/leads and vendor support teams in an event of escalation.
Required skills
3 years+ Experience with Data Engineering in Cloud Data Solutions (AWS preferred)
3 years+ Experience building Data Platforms, data lakes, modern data warehouses architectures and Self-service Business Intelligence solutions
Expertise in designing efficient Data Models, optimizing existing Data Marts, developing and deploying Data structures based on those Data Models
Expertise in designing and implementing Data security to ensure the compliance of all the data assets and analytical applications
3 years+ Experience in SQL, Relational databases
3 years+ Extensive experience with data processing and ETL/ELT techniques
2 years+ Experience developing and supporting scalable data pipelines using technologies such as Kafka, Spark, Airflow to support Batch and streaming data efficiently
2 years+ Experience with Snowflake Data Cloud
3 years+ Python programming experience.
Experience with high performance distributed data computing.
Experience with good software development, automation practices, including collaborative development using DevOps pipelines.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Excellent communication, advanced English reading, writing, listening and speaking skills.
Show more
Show less","Tableau, Power BI, Informatica, Talend, Data Science, AWS Cloud, S3, Glue, Redshift, Snowflake, Qlik, Alation, Kafka, Spark, Airflow, SQL, Python, DevOps","tableau, power bi, informatica, talend, data science, aws cloud, s3, glue, redshift, snowflake, qlik, alation, kafka, spark, airflow, sql, python, devops","airflow, alation, aws cloud, data science, devops, glue, informatica, kafka, powerbi, python, qlik, redshift, s3, snowflake, spark, sql, tableau, talend"
"Software Engineer, Data Tools & Experimentation Platform",Asana,"San Francisco, CA",https://www.linkedin.com/jobs/view/software-engineer-data-tools-experimentation-platform-at-asana-3770944181,2023-12-17,Alameda,United States,Mid senior,Hybrid,"The Data Infrastructure organization owns the infrastructure, services, and pipelines that transport and process data from all of Asana’s product surfaces, providing data to stakeholders in Data Science, Business, and Product teams. Our platform is used for everything from making ship / no-ship decisions to powering intelligent in-product features.
The Data Tools & Experimentation Platform team splits our time across two areas: analytical data tools, and the Experimentation framework, used to run product A/B tests.
We’re a small but mighty team that covers a lot of surface area. From distributed data processing with Apache Spark to full stack web development on the Experimentation UI, to building and maintaining Kubernetes applications, you’ll have the opportunity to wear many hats and create outsized impact.
This role might be for you if you’re agile, scrappy, and interested in learning and mastering a variety of technologies.
This role is based in our San Francisco office with an office-centric hybrid schedule. Along with most Asanas, you’ll work from this office in person on Mondays, Tuesdays, and Thursdays. Most Asanas have the option to work from home on Wednesdays and Fridays. If you're interviewing for this role, your Talent Acquisition Partner will share more about the in-office requirements.
What You’ll Achieve
Work on all parts of Asana’s in-house experimentation platform, from the statistical backend (Apache Spark / Scala) to the user interface (React / Typescript)
Improve our tools for data analytics, visualization, governance, and lineage (e.g. Databricks, Redash, Amundsen, Marquez, etc.)
Build and manage infrastructure using Kubernetes and Terraform
Ensure data availability by participating in an on-call rotation for our team’s services
Develop clean, beautiful code and leave it better than you found it
About You
2 or more years of experience working within large, well-maintained codebases
You’re enthusiastic about working on a broad range of technical challenges, from distributed data processing to full-stack web development
You take pride in writing clean code and building performant infrastructure
Experience with Python or Scala
At Asana, we're committed to building teams that include a variety of backgrounds, perspectives, and skills, as this is critical to helping us achieve our mission. If you're interested in this role and don't meet every listed requirement, we still encourage you to apply.
What We’ll Offer
Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit.
For this role, the estimated base salary range is between $171,000 - $209,000. The actual base salary will vary based on various factors, including market and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base salary range for this role may be modified.
In addition to base salary, your compensation package may include additional components such as equity, sales incentive pay (for most sales roles), and benefits. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role.
We strive to provide equitable and competitive benefits packages that support our employees worldwide and include:
Mental health, wellness & fitness benefits
Career coaching & support
Inclusive family building benefits
Long-term savings or retirement plans
In-office culinary options to cater to your dietary preferences
These are just some of the benefits we offer, and benefits may vary based on role, country, and local regulations. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role.
About Us
Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor’s and Inc.’s Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture. With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong.
We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations.
Show more
Show less","Apache Spark, Scala, Databricks, Redash, Amundsen, Marquez, Kubernetes, Terraform, React, Typescript, Python","apache spark, scala, databricks, redash, amundsen, marquez, kubernetes, terraform, react, typescript, python","amundsen, apache spark, databricks, kubernetes, marquez, python, react, redash, scala, terraform, typescript"
"Senior Data Engineer - Azure, ADF",BayOne Solutions,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-azure-adf-at-bayone-solutions-3758950171,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Senior Data engineers with 10+ years of experience.
This is a 100% remote role, PST hours
3-4 rounds of Interviews (tech screening, including real-time coding).
Skill Set:
Big Data:
Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark
Cloud
: Azure, ADF
System Design
: API Design, GraphQL, Event-driven architecture, Design Patterns
Microservice
: Core Java 8 or above, SpringBoot, Azure Functions.
Front end
( Nice to Have )
:
JavaScripting, React
Show more
Show less","Scala, Databricks, Spark SQL, Spark Streaming, Python, PySpark, Azure, ADF, API Design, GraphQL, Eventdriven architecture, Design Patterns, Core Java 8, SpringBoot, Azure Functions, JavaScript, React","scala, databricks, spark sql, spark streaming, python, pyspark, azure, adf, api design, graphql, eventdriven architecture, design patterns, core java 8, springboot, azure functions, javascript, react","adf, api design, azure, azure functions, core java 8, databricks, design patterns, eventdriven architecture, graphql, javascript, python, react, scala, spark, spark sql, spark streaming, springboot"
Big Data Developer,Sigmaways Inc,"San Francisco, CA",https://www.linkedin.com/jobs/view/big-data-developer-at-sigmaways-inc-3784231335,2023-12-17,Alameda,United States,Mid senior,Hybrid,"We are looking for a Big Data Developer who will contribute to high-quality technology solutions that address business needs by developing data applications for the customer business lines. You will contribute to the development and ongoing maintenance of a number of strategic data initiatives and data and analytic applications.
Responsibilities:
Hands-on development role focused on creating big data and analytics solutions.
Coding of mission-critical components.
A strong Big data developer/coder who can write database queries and also tune those queries to perform optimally.
Analyze business and functional requirements and contribute to the overall solution.
Participate in design reviews, and provide input to the design recommendations.
Participate in project planning sessions with project managers, business analysts, and team members.
Required skills:
BS or MS in Computer Science or equivalent experience.
Software development experience with solid working experience in Big Data technologies.
Knowledge of the architecture and internals of technologies in the Hadoop ecosystem.
Experience designing and implementing large, scalable distributed systems.
Must have solid expertise and hands-on experience in Spark, Scala and SQL.
S
olid understanding of the Hadoop Ecosystem ( HDFS, Yarn, MapReduce, Spark, Hive, Impala )
and should be able to mentor and lead junior team members.
Good understanding of database technologies, including SQL and NoSQL databases.
Ability to debug and promptly resolve production issues.
Proficiency with advanced object-oriented programming.
Excellent problem-solving and analytical skills.
Excellent written and oral communication skills.
Show more
Show less","Hadoop, SQL, Spark, Scala, NoSQL, Hive, Yarn, MapReduce, Impala, HDFS, OOP","hadoop, sql, spark, scala, nosql, hive, yarn, mapreduce, impala, hdfs, oop","hadoop, hdfs, hive, impala, mapreduce, nosql, oop, scala, spark, sql, yarn"
Data Engineer,Underdog.io,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-underdog-io-3783490007,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Underdog.io is a modern recruiting platform. One of our hiring partners is looking to hire a Data Engineer. In this role, you'll work on tools, features, and applications that are critical to the company's needs with opportunities to switch teams and projects as you and the business grow and evolve. The ideal candidate will be versatile, display leadership qualities, and be enthusiastic to take on new problems as the company continues to push technology forward.
The company is looking to hire someone with (1) a Bachelor’s degree or equivalent practical experience, (2) experience with software development in one or more programming languages, and (3) experience with data structures or algorithms in either an academic or industry setting. The ideal candidate should also have experience building and developing large-scale infrastructure, accessible technologies, distributed systems or networks, and/or have experience with compute technologies.
On any given day, you will (1) write product or system development code, (2) participate in or lead design reviews with peers and stakeholders to decide amongst available technologies, (3) review code developed by other developers and provide feedback to ensure best practices, (4) contribute to existing documentation and adapt content based on product/program updates and user feedback, and (5) triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on the system.
The company uses popular languages, frameworks, and tools, including JavaScript, Python, Golang, Scala, Ruby on Rails, SQL, AWS, Kubernetes, and Docker.
The Underdog.io team is available if you have any questions or would like to discuss other potential roles.
Show more
Show less","Data Engineering, Team Leadership, Software Development, Data Structures, Algorithms, Infrastructure Development, Accessible Technologies, Distributed Systems, Networking, Compute Technologies, Product Development, System Development, Design Reviews, Code Review, Documentation, Product/Program Updates, User Feedback, Product/System Issue Triage, Debugging, Tracking, Issue Resolution, JavaScript, Python, Golang, Scala, Ruby on Rails, SQL, AWS, Kubernetes, Docker","data engineering, team leadership, software development, data structures, algorithms, infrastructure development, accessible technologies, distributed systems, networking, compute technologies, product development, system development, design reviews, code review, documentation, productprogram updates, user feedback, productsystem issue triage, debugging, tracking, issue resolution, javascript, python, golang, scala, ruby on rails, sql, aws, kubernetes, docker","accessible technologies, algorithms, aws, code review, compute technologies, data engineering, data structures, debugging, design reviews, distributed systems, docker, documentation, golang, infrastructure development, issue resolution, javascript, kubernetes, networking, product development, productprogram updates, productsystem issue triage, python, ruby on rails, scala, software development, sql, system development, team leadership, tracking, user feedback"
Senior Data Engineer,FLYR,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-flyr-3748818034,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Our Vision
FLYR is focused on the relentless application of advanced and intuitive technologies that help transportation leaders unlock their ultimate potential.
FLYR is a technology company that is purpose-built for the travel industry. Leveraging deep learning, an advanced form of AI, FLYR is helping airlines, cargo, and hospitality businesses around the globe elevate their results. With FLYR, businesses are able to improve revenue performance and modernize the e-commerce experience through accurate forecasting, automation, and analytics.
Flight Itinerary (About The Role)
Our data-driven, deep-learning-based methodology enables any airline to truly maximize their revenue and accurately forecast outcomes, even under unprecedented conditions such as COVID-19. To achieve this vision, we’re looking for a talented Senior Data Engineer to join our leading-edge data engineering customer implementation team. This team delights new customers throughout the onboarding process, applying innovative, best-in-class tools and workflows to build performant, reliable data pipelines. The Data Engineer will play a key role in ensuring our customers see rapid time-to-value, and will build creative technical solutions for new and complex problems.
What Your Journey Will Look Like (Responsibilities)
Complete complex customer ingest and data warehousing projects
Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability
Engage with customers from data discovery, to ETL development, to data QA/QC metrics determination and delivery SLAs
Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability
Learn our customers’ business needs and apply business acumen to ensure the success of highly technical projects
Collaborate closely with FLYR's data platform team to define tool requirements to support onboarding and ingest of new data sources
Defining platform data validation test suites
Provide mentorship and training to newer crew members
What To Pack For This Trip (Qualifications)
Quantitative work/education background (computer science or equivalent)
Ability to ship production-quality Python code
5+ years of hands-on experience with cloud computing services, Airflow and BigQuery
5+ years of experience building and operating data transformation pipelines
Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools
Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL
Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science and Data Platform teams to define product and platform capabilities to improve customer onboarding
Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders
First-Class Amenities
Equity in Series C startup with high growth potential
Comprehensive healthcare plans (Choice of PPO & HMO available)
Generous PTO policy and flexible working arrangements
401K with company match
Free breakfast/Lunch (in-office)
100% paid Parental Leave for 12 weeks
Annual educational fund
Compensation: The salary range for this role is commensurate with experience and is as follows:
$119,000 — $167,000 USD
Our Commitment to Equality
Here at FLYR, we’re committed to growing with intention, having our teams better reflect the world around us. We strive to create an environment of inclusion and even more importantly, belonging, where psychological safety, empathy, and human connection are at the center of our leadership principles. Not only does this enable us to create better products and have a better work environment, it’s good for the bottom line and it’s the right thing to do.
FLYR provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender identity, sex, sexual orientation, national origin, age, physical or mental disability, genetics, marital or veteran status. In addition to federal law requirements, FLYR complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company operates.
Privacy Policy
All applicants, including those based in California or the EU, are encouraged to review our Privacy and Cookie Policy .
Show more
Show less","Data Engineering, Data Warehousing, Data Transformation, Data Management, Data pipelines, SQL, Python, Airflow, BigQuery, Cloud Computing, ETL, BI Visualization, Product Management, Stakeholder Management, Communication, Problem Solving","data engineering, data warehousing, data transformation, data management, data pipelines, sql, python, airflow, bigquery, cloud computing, etl, bi visualization, product management, stakeholder management, communication, problem solving","airflow, bi visualization, bigquery, cloud computing, communication, data engineering, data management, data transformation, datapipeline, datawarehouse, etl, problem solving, product management, python, sql, stakeholder management"
Senior Data Engineer,Kikoff,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kikoff-3777314655,2023-12-17,Alameda,United States,Mid senior,Hybrid,"We are looking for an experienced Data Engineer to join our growing team. The ideal candidate will have a strong background in data pipeline development, data quality control, and data infrastructure for machine learning models. Additionally, the candidate should have a strong understanding of marketing data and business sense. In this role, you will be responsible for designing, building, and maintaining data pipelines, ensuring data quality and accuracy, and supporting machine learning models.
Responsibilities
Design and build data pipelines to collect, process, and store large amounts of data from multiple sources
Implement data quality control processes to ensure accuracy and completeness of data
Develop and maintain data infrastructure to support machine learning models
Collaborate with data scientists and machine learning engineers to understand data requirements for models
Work with business teams to understand marketing data and business requirements
Monitor and maintain the performance of data pipelines and infrastructure
Participate in the design and implementation of data storage and retrieval systems
Stay up-to-date with emerging technologies and tools in the data engineering field
Requirements
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
At least 5 years of experience in data engineering
Strong experience with data pipeline development and data quality control
Hands-on experience with data infrastructure for machine learning models
Knowledge of data storage and retrieval systems such as Hadoop, Spark, and NoSQL databases
Proficiency in programming languages such as Python and SQL
Understanding of marketing data and business sense is a plus
Strong problem-solving and analytical skillsExcellent communication and collaboration skills
What We Offer
This is a consumer fintech startup, and you will be working with serial entrepreneurs who have built strong consumer brands and innovative products. We are backed by some of Silicon Valley’s top VCs, including GGV capital, Lightspeed Ventures, Female Founders Fund, and others. We value extreme ownership, clear communication, a strong sense of craftsmanship, and the desire to create lasting work and work relationships. Yes, you can build an exciting business AND have real-life real-customer impact.
💰 US salary range for this full-time position consists of base + equity + benefits
Our Salary Ranges Are Determined By Role And Experience
140k - 190k base + equity
🏥 Medical, dental, and vision coverage - Kikoff covers the full cost of health insurance for the employee!
🤑 Stock options
📈 Access to 401k plan
🏝 In addition to 10 annual company holidays, we offer Summer Fridays
✈️ Generous unlimited vacation policy to help you recharge
Additionally, we host regular team building events to help you get to know the Kikoff team! Our last virtual cooking event had everyone's mouth on fire, but cooled down by home-mixed cocktails.
Show more
Show less","Data Pipeline Development, Data Quality Control, Data Infrastructure, Machine Learning Models, Marketing Data, Business Sense, Data Pipelines, Data Scientists, Machine Learning Engineers, Data Storage, Data Retrieval Systems, Hadoop, Spark, NoSQL Databases, Python, SQL, ProblemSolving, Analytical Skills, Communication, Collaboration","data pipeline development, data quality control, data infrastructure, machine learning models, marketing data, business sense, data pipelines, data scientists, machine learning engineers, data storage, data retrieval systems, hadoop, spark, nosql databases, python, sql, problemsolving, analytical skills, communication, collaboration","analytical skills, business sense, collaboration, communication, data infrastructure, data pipeline development, data quality control, data retrieval systems, data scientists, data storage, datapipeline, hadoop, machine learning engineers, machine learning models, marketing data, nosql databases, problemsolving, python, spark, sql"
SENIOR DATABASE ENGINEER CASSANDRA,Software Technology Inc.,"Concord, CA",https://www.linkedin.com/jobs/view/senior-database-engineer-cassandra-at-software-technology-inc-3672232647,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Hi
Hope you are doing well,
Urgent Opening For
SENIOR
DATABASE
ENGINEER (CASSANDRA)
role, if you are interested and a good fit, feel free to reach me directly at ksuresh@stiorg.com or +1(609) 998-3431.
Role :
SENIOR DATABASE ENGINEER (CASSANDRA)
Type
: Full Time / Contract
Location
: Only Locals CA, Hybrid Role, Concord, CA
Your Roles
Leads the design planning, impact analysis, administration, implementation and maintenance of the organization's application Cassandra databases. Consults with and advises senior management and multiple clients on high impact data or database management issues, influencing strategic direction.
Handles and leads various large-scale or highly complex data/database management activities including one or more of the following: designing highly complex logical and/or physical database data model; large volume data transformation and migration; capacity planning; developing database design policies, procedures and standards; and security requirements identification, analysis and development.
Provides mentoring, guidance and general oversight to lesser experienced staff in a variety of database design, performance tuning and/or administration activities.
Candidate Will Be Required To
Lead or participate in Cassandra database management activities including designing highly complex logical and physical databases
Installation and administration of Cassandra databases
Documentation of guides for production database administrators
Defining standards for installation, deployment, security, authentication and authorization, management policies and best practices
Defining and implementing backup and restoration strategies
Defining and implementing monitoring and alarming strategies
Perform the planning, research, design, implementation, maintenance, and control of server class databases
Consult with and advise management and multiple clients on high impact data or database management issues, influencing strategic direction
Minimum Qualifications
4+ years of experience with implementing and administrating Cassandra database
Experience with Data tax Cassandra
Experience with Data tax Ops Centre backup/restoration and monitoring/alarm implementation
Experience with Ansible automation tool or equivalent
4+ years of Shell or Python or Perl experience
Preferred Skills
Proven experience with Mongo dB or other SQL and NoSQL databases a plus
Experience with Agile methodology
Demonstrated experience with UNIX and Shell Scripting
Demonstrated experience in designing for high volume OLTP applications.
Demonstrated experience in Change Management and SDLC
Experience with Version Control System such as Git
Experience with Issue and Tracking software such as Jira
Regards ,
Suresh Reddy.k
Technical Recruiter
Software Technology Inc.
Email: ksuresh@stiorg.com
609-998-3431
Show more
Show less","Cassandra, SQL, NoSQL, MongoDB, DataStax Cassandra, DataStax Ops Centre, Ansible, Shell, Python, Perl, UNIX, Shell Scripting, Agile methodology, Version Control System, Git, Issue Tracking, Jira","cassandra, sql, nosql, mongodb, datastax cassandra, datastax ops centre, ansible, shell, python, perl, unix, shell scripting, agile methodology, version control system, git, issue tracking, jira","agile methodology, ansible, cassandra, datastax cassandra, datastax ops centre, git, issue tracking, jira, mongodb, nosql, perl, python, shell, shell scripting, sql, unix, version control system"
"Staff, Data Engineer, API/ML Infrastructure (Store No. 8 | Health & Wellness)",Store No. 8,"San Bruno, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-api-ml-infrastructure-store-no-8-health-wellness-at-store-no-8-3782298064,2023-12-17,Alameda,United States,Mid senior,Hybrid,"About Store No 8
We are the incubation arm of Walmart. Store No 8 was formed in 2017 as part of Walmart's larger innovation mission to shape the future of commerce. We pursue big ideas and take risks by stepping outside of Walmart's core business to focus on leapfrog capabilities across conversational commerce, mixed reality, in-store digitization, and more. Our ultimate goal: fuel Walmart's core business, create new operational efficiencies, and unlock amazing experiences for our customers in the long-term.
About the Role:
We are the Health & Wellness venture at Store No 8, looking for an experienced Data Engineer to join our Data Engineering team! You will collaborate with the data science and engineering team to design, build, scalable infrastructure and API that enables analytics, modeling, and app development. You will work with product team to translate requirements into solutions.
This is a rare opportunity, moving at the speed of a start-up, with the backing and in house data (lots of data) of the Fortune 1 company.
What you'll do:
Improve our data infrastructure to meet business needs and balance between agility, reliability, and scalability, and move us from pilot and scale.
Build and optimize API backend infrastructure and ML model serving infrastructure.
Build and optimize high-quality foundational datasets and the relevant data pipelines.
Inform and make architectural decisions, establish best practices, develop lean scalable processes, and contribute to the overall direction of the data organization.
What you’ll bring:
Expert knowledge of Python (or Java) and SQL, with experience building complex, scalable cloud-based systems and data pipelines.
Experience with backend REST API design and building Highly scalable Microservices to integrate with several external cloud based applications.
Experience in building event driven Applications with Kafka, Google pub-sub and event Horizon.
Expert knowledge in NoSQL and SQL databases(Cosmos, Casandra, MongoDB, postGres) Design and Data models.
Ability to proactively identifying opportunities to improve ETL and pipeline performance.
Ability to navigate environments where problems/requirements are not well-defined (and evolve quickly).
Excellent communication skills with ability to explain complex technical concepts in easy-to-understand ways.
Last but not least - a humble collaborative can-do attitude and natural curiosity.
A plus if you have:
Passion for Health & Wellness.
Experience with modern data storage and processing technologies (i.e. BigQuery, GCS, Dataproc, Airflow, etc).
Experience owning and proactively improving data models, implementing best practices, and influencing change of existing architect.
Experience designing data architecture to power a variety of uses cases, including machine learning, data analytics and visualization.
Experience with MLOps systems, including data pipelines and production-level machine learning (ML) infrastructure to support large-scale data workflow from data collection, ingestion, storage, preparation, to data query/analytics.
Experience with healthcare data.
Perks and Benefits
Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, options for hybrid and flexible schedules, and much more.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance.
Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Show more
Show less","Python, Java, SQL, REST API, Microservices, Kafka, Pubsub, Event Horizon, NoSQL, SQL, Cosmos, Cassandra, MongoDB, Postgres, BigQuery, GCS, Dataproc, Airflow, Machine Learning, Data Analytics, Data Visualization, MLOps, Data Pipelines, Healthcare Data","python, java, sql, rest api, microservices, kafka, pubsub, event horizon, nosql, sql, cosmos, cassandra, mongodb, postgres, bigquery, gcs, dataproc, airflow, machine learning, data analytics, data visualization, mlops, data pipelines, healthcare data","airflow, bigquery, cassandra, cosmos, dataanalytics, datapipeline, dataproc, event horizon, gcs, healthcare data, java, kafka, machine learning, microservices, mlops, mongodb, nosql, postgres, pubsub, python, rest api, sql, visualization"
"Senior Data Engineer, Productivity Engineer",SiriusXM,"Oakland, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-productivity-engineer-at-siriusxm-3770198254,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
We are seeking a highly skilled and motivated Senior Data Engineer, Productivity Engineer to join our dynamic team at SiriusXM. As a Data Productivity Engineer, you will play a key role in designing, building, and maintaining the tools and services used by our data professionals in order to effectively drive the business in a data-driven manner. The ideal candidate will have a strong background in cloud technologies, data engineering, infrastructure as code, API design, and experience applying software engineering and DevOps best practices.
What You’ll Do
Design, develop, and maintain tools and services that empower data professionals to streamline their workflows and enhance productivity.
Collaborate with cross-functional teams to understand data analysis, engineering, and modeling needs and translate them into effective and user-friendly solutions.
Implement best practices for optimizing data processing workflows, ensuring efficient utilization of resources, and minimizing latency in data-related tasks.
Identify and address bottlenecks in existing tools and services to improve overall system performance.
Integrate data productivity tools with existing data infrastructure and platforms, fostering seamless collaboration among teams.
Develop and implement automation solutions to streamline repetitive tasks and enhance the efficiency of data processes.
Create comprehensive documentation for tools and services, ensuring that users have access to clear and concise instructions.
Provide training and support to data professionals, enabling them to effectively leverage the tools and services developed.
Work closely with data scientists, engineers, and analysts to understand their requirements and challenges, fostering a collaborative and innovative environment.
Communicate project status, issues, and solutions effectively to stakeholders and team members.
What You’ll Need
Bachelor's degree in a relevant technical field (Computer Science, Information Technology, etc.).
5+ years’ experience in software development, with a focus on tools and services for data professionals.
Proficiency in programming languages such as Python, Scala, or Java.
Experience with infrastructure as code tools such as CDK or Terraform.
Experience with big data technologies (e.g., Apache Spark, Hadoop) and data processing frameworks.
Knowledge of data storage solutions, database systems, and data warehousing.
Familiarity with machine learning frameworks and model development is a plus.
Excellent problem-solving skills and a proactive approach to addressing challenges.
Strong communication and collaboration skills.
Must have legal right to work in the U.S.
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-11-101
Show more
Show less","Python, Scala, Java, CDK, Terraform, Apache Spark, Hadoop, Machine Learning, Problem Solving, Communication, Collaboration","python, scala, java, cdk, terraform, apache spark, hadoop, machine learning, problem solving, communication, collaboration","apache spark, cdk, collaboration, communication, hadoop, java, machine learning, problem solving, python, scala, terraform"
Staff Database Engineer,BlackLine,"Pleasanton, CA",https://www.linkedin.com/jobs/view/staff-database-engineer-at-blackline-3783318024,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Get to Know Us:
It's fun to work in a company where people truly believe in what they're doing!
At Blackline, we're committed to bringing passion and customer focus to the business of enterprise applications.
Since being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance.
Being a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers.
Work, Play and Grow at BlackLine!
Make Your Mark:
As a Staff Database Engineer at BlackLine, you will play a vital role in the performance, delivery, stability, and security of the databases we use, while continually driving forward improvements and optimizations at the database layer. As a member of the database team, you will be involved in the planning, development, and maintenance of the database, including troubleshooting issues, collaborating with other teams to define and build new features, optimize existing features, and collaborate in order to drive growth, improve controls and processes, and reduce overhead and complexity. In this role, we are looking for people who are team players, passionate about their areas of expertise, and constantly striving to learn and improve, not just in the sense of their own skills, but also in growing with peers whom they work with day-to-day. If you are someone who strives for excellence in all that they do, including helping those in your team, and someone who wants to ultimately deliver the best value and success you can, then we want to talk to you.
You'll Get To:
Participate in cross-functional teams and build relationships across the organization.
Build high-performance, massively-scalable, always-available Cloud-based systems.
Ensure data integrity and quality in database systems.
Maintain standard policies for database development activities.
Provide database solutions based on technical documents and business requirements.
Provide technical assistance to resolve all database issues related to performance, capacity and access.
Analyze issues holistically, from the application tier through the database, down to the storage.
Database Engineer will ensure that all deadlines are met and that quality is of the highest level throughout the Software Development Life Cycle.
What You'll Bring:
8 + years experience SQL 2008, 2012, 2014, 2017
4 or more years working in high-transaction environments is required
Advanced working knowledge of different index types and how they are used: columnstore, full-text, filtered, indexes with include columns
Advanced Execution Plan understanding
SSIS/SSRS experience is a strong plus
Experience with performance tuning and optimization, using native monitoring and troubleshooting tools and techniques, including complex queries as well as procedure and indexing strategies
Excellent written and verbal communication
Adaptable team-player with a focus on results and value delivery
Able to organize and plan work independently
Lead various technology POCs
Advance understanding of OLTP vs OLAP environments
Working knowledge of relational database internals (locking, consistency, serialization, recovery paths)
We’re Even More Excited If You Have:
NoSQL is a plus
Azure, GCP or AWS are a plus
MCTS, MCITP, and/or MVP certifications are a plus
Thrive at BlackLine Because You Are Joining:
A technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the world's most trusted name in Finance Automation!
A culture that is kind, open, and accepting. It's a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives.
A culture where BlackLiner's continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity.
BlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws.
BlackLine recognizes that the ways we work and the workplace itself has shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.
Salary Range:
USD $139,800.00 - USD $215,800.00
Pay Transparency Statement:
Placement within this range depends upon several factors, including the applicant’s prior relevant job experience, skill set, and geographic location. In addition to base pay, BlackLine also offers short-term and long-term incentive programs, based on eligibility, along with a robust offering of benefit and wellness plans.
Show more
Show less","SQL, Database systems, Data integrity, Data quality, SSIS, SSRS, Performance tuning, Optimization, Complex queries, Procedure indexing strategies, NoSQL, Azure, GCP, AWS, OLTP, OLAP, MCTS, MCITP, MVP","sql, database systems, data integrity, data quality, ssis, ssrs, performance tuning, optimization, complex queries, procedure indexing strategies, nosql, azure, gcp, aws, oltp, olap, mcts, mcitp, mvp","aws, azure, complex queries, data integrity, data quality, database systems, gcp, mcitp, mcts, mvp, nosql, olap, oltp, optimization, performance tuning, procedure indexing strategies, sql, ssis, ssrs"
Senior Software Engineer (Data),Harnham,"Palo Alto, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-at-harnham-3782390806,2023-12-17,Alameda,United States,Mid senior,Hybrid,"SENIOR SOFTWARE ENGINEER (DATA)
SOUTH BAY - HYBRID
$200,000 - $250,000 + BENEFITS
This is a very exciting opportunity for a Senior Software Engineer to join an organization that is using AI to help companies make smarter decisions at scale in manufacturing. This person should have experience building out scalable data pipelines at an enterprise level, while also being experienced in building data systems from scratch. This role lends itself to someone with ambition and technical skill that is comfortable in a startup environment.
THE COMPANY
This Senior Engineer will be joining a business that is one of the most technologically advanced in their industry. Their technology in AI and ML has been invaluable in their market, and are a difference maker for the companies that are making use of their technology. This company has a huge amount of room to grow, and joining at this time is a fantastic opportunity for a motivated candidate.
THE ROLE
Batch and stream data processing at a high volume
Storing and warehousing data effectively and at scale
Building out large parts of the data ingestion and collection product
Keeping data clean and configuring the entire data pipeline before deploying into AWS
YOUR SKILLS & EXPERIENCE
Experience with modern all-purpose languages, Python preferred
Kafka or other stream processing tool
MongoDB and SQL databases
Experience with data engineering tools such as Redshift, Athena, Airflow
HOW TO APPLY
Please register your interest by sending your resume to Evan Bachteal via the apply link on this page.
Show more
Show less","Python, Kafka, MongoDB, SQL, Redshift, Athena, Airflow, AWS","python, kafka, mongodb, sql, redshift, athena, airflow, aws","airflow, athena, aws, kafka, mongodb, python, redshift, sql"
Sr. Data Systems Engineer,Vaxcyte,"San Carlos, CA",https://www.linkedin.com/jobs/view/sr-data-systems-engineer-at-vaxcyte-3737458628,2023-12-17,Alameda,United States,Mid senior,Hybrid,"Company Profile:
Vaxcyte, Inc. (Nasdaq: PCVX)
is a vaccine innovation company engineering high-fidelity vaccines to protect humankind from the consequences of bacterial diseases. The Company is developing broad-spectrum conjugate and novel protein vaccines to prevent or treat bacterial infectious diseases. Vaxcyte’s lead candidate, VAX-24, is a 24-valent, broad-spectrum, carrier-sparing pneumococcal conjugate vaccine (PCV) being developed for the prevention of invasive pneumococcal disease (IPD). The Company is re-engineering the way highly complex immunizations are made through modern synthetic techniques, including advanced chemistry and our exclusively licensed XpressCFTM cell-free protein synthesis platform. Unlike conventional cell-based approaches, the Company’s system for producing difficult-to-make proteins and antigens is intended to accelerate its ability to efficiently create and deliver high-fidelity vaccines with enhanced immunological benefits. Vaxcyte’s pipeline also includes VAX-31, a 31-valent PCV candidate; VAX-A1, a prophylactic vaccine candidate designed to prevent Group A Strep infections; VAX-PG, a therapeutic vaccine candidate designed to slow or stop the progression of periodontal disease; and VAX-GI, a vaccine program designed to prevent Shigella. The Company is driven to eradicate or treat invasive bacterial infections, which have serious and costly health consequences when left unchecked. For more information, visit www.vaxcyte.com .
Vaxcyte, headquartered in San Carlos, CA, went public in June 2020 and currently has a team of approximately 180 employees and anticipates continued, significant growth. Following equity offerings in October 2022 and April 2023, which generated over $1.1 billion in net proceeds, the Company’s balance sheet is further strengthened to advance its pipeline of novel vaccines, including VAX-24. These financings followed positive data readouts from Vaxcyte’s Phase 1/2 proof-of-concept study evaluating VAX-24 in adults aged 18-64 and Phase 2 study in adults 65 and older. The Company believes these results support a best-in-class potential for VAX-24, which was designed to replace the current standard-of-care in adults and children. VAX-24 is being investigated for the prevention of IPD, which can be most serious for infants, young children, older adults and those with immune deficiencies or certain chronic health conditions. Given the global impact of pneumococcal disease remains significant, the public health community continues to advocate for vaccines that can offer broader protection to prevent IPD. Vaxcyte’s PCV franchise, consisting of VAX-24 and VAX-31, is designed specifically to address this need and has the potential to deliver the broadest protection for this very serious disease. We believe that our PCVs could receive regulatory approval based on successful completion of clinical studies utilizing well-defined surrogate immune endpoints, consistent with how other PCVs have obtained regulatory approval in the past, rather than requiring clinical field efficacy studies.
Summary:
We are seeking an experienced and detail-oriented Data Systems Engineer with knowledge of System administration and DevOps with on premise and cloud-based platforms to join our IT team. In this role you will implement and administer Windows Domain and Active Directory, compute, storage, database, networking and security implementation and management. Strong knowledge in domain controllers, group policies, Active Directory, cloud services, databases, and web app deployment are essential for this position.
Infrastructure Management:
Provision, configure, and maintain cloud and VMWare resources, including virtual machines, databases, storage, WebApps and networking components.
Monitor and optimize cloud infrastructure for performance, cost, and security.
Implement and maintain backup and disaster recovery solutions.
Ensure cloud resources are compliant with security policies and best practices.
System Administration:
Administer and maintain Windows-based servers and systems.
Configure and manage domain controllers and Active Directory services.
Implement and manage group policies to ensure security and compliance.
Troubleshoot and resolve issues related to Windows infrastructure.
Implement security updates and patches for Windows environments.
Database Administration:
Possess knowledge in configuring, administering, and optimizing relational databases hosted on cloud.
Understand the principles of database availability, security, and performance.
Collaborate with development teams to support database-related tasks.
Web Application Deployment in Azure:
Deploy web applications on cloud.
Configure and manage web app settings, scaling, and monitoring.
Ensure high availability and performance of web applications.
DevOps Integration (Basic):
Collaborate with development teams to support basic CI/CD pipelines on cloud platforms.
Assist in automation and scripting tasks to streamline administration processes.
Provide basic support for containerization technologies like Docker and Kubernetes.
Documentation and Training:
Maintain detailed documentation of infrastructure, systems, databases, Python web applications, and administration processes.
Provide training and support to internal teams regarding system administration, database management, and web app deployment.
Requirements:
Bachelor's degree in computer science, information technology, or a related field (or equivalent workexperience).
8 -12 years of experience in Systems Administration.
Proven experience as a Windows Administrator with knowledge in domain controllers, group policies, Active Directory.
Strong understanding of Azure services, networking, and security.
Expertise in Windows Server administration and troubleshooting.
Experience administering on premise VMWare infrastructure.
Proficiency in scripting and automation using PowerShell and/or other relevant tools.
Knowledge in PostgreSQL administration, including database setup, optimization, and maintenance.
Experience in deploying Python web applications in Azure.
Familiarity with DevOps practices, including CI/CD concepts and automation.
Excellent problem-solving and troubleshooting skills.
Strong communication and collaboration abilities.
Azure certifications (e.g., Azure Administrator, Azure Solutions Architect) and relevant Python web app deployment experience are highly desired.
All Vaxcyte employees require vaccination against COVID-19.
Reports to:
Senior Director, CMC IT Systems
Location:
San Carlos, CA
Compensation:
The compensation package will be competitive and includes comprehensive benefits and an equity component.
Salary Range:
$171,000 – $185,000
Send resumes to:
careers@vaxcyte.com
Vaxcyte, Inc.
825 Industrial Road, Suite 300
San Carlos, CA 94070
We are an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.
Show more
Show less","Windows Domain, Active Directory, Cloud services, Databases, Web app deployment, Azure, VMWare, Docker, Kubernetes, PostgreSQL, Python, DevOps, CI/CD, PowerShell, SQL","windows domain, active directory, cloud services, databases, web app deployment, azure, vmware, docker, kubernetes, postgresql, python, devops, cicd, powershell, sql","active directory, azure, cicd, cloud services, databases, devops, docker, kubernetes, postgresql, powershell, python, sql, vmware, web app deployment, windows domain"
"Member of Technical Staff, Data Engineer",Inflection AI,"Palo Alto, CA",https://www.linkedin.com/jobs/view/member-of-technical-staff-data-engineer-at-inflection-ai-3593512326,2023-12-17,Alameda,United States,Mid senior,Hybrid,"About The Role
You are an experienced data practitioner who will work closely with AI and infrastructure staff to improve our AI through better data. You will be responsible for collecting AI training data, optimizing how we use that data, and analyzing data to inform product direction. Data is key to Inflection’s success, so your work will have a massive impact and will directly improve our models and product.
You are expected to have experience building large scale data processing pipelines, using tools like PySpark, Beam, or Flink. Expertise in ML is not necessary, but a familiarity with Machine Learning and NLP and a willingness to learn more on the job are important. A track record of adapting to new domains and a desire to use data to improve products are strong indicators for success in this role.
You may have relevant experience as a Data Scientist, ML Engineer, Data Engineer or Software Engineer, but will be expected to apply your data skills to every step of building our large language model product.
Employee Pay Disclosures
At Inflection AI, we aim to attract and retain the best employees and compensate them in a way that appropriately and fairly values their individual contributions to the company. The pay range for this position in California, is estimated to fall in the base range of approximately $150,000 - $250,000. This estimate can vary based on the factors described above, so the actual starting annual base salary may be above or below this range.
About Inflection
We are a small, friendly and multi-disciplinary AI studio creating a personal AI for everyone. Our first AI is called Pi, for personal intelligence, a supportive and empathetic conversational AI. Our studio is made up of the world's leading AI developers, creative designers, writers and innovators working together in a deeply multidisciplinary style to create a brand new class of digital experiences.
Where We Work
We work in a hybrid model. During the week we’re in the office on a flexible basis, often a few days per week. We are currently hiring in Palo Alto and London, although we do sometimes make exceptions.
We work on a 6 week cycle. Every 7th week, we convene together in person as a company to brainstorm, bond and build.
How We Work
We value excellence and ownership.
Our organizational structure focuses on individual responsibilities rather than management hierarchies. Everyone is expected to lead by doing. We are big believers in the unreasonable effectiveness of highly talented Individual Contributors who are given all the resources, space and ownership to move fast and deliver outstanding results.
Teamwork and generosity are at our core.
Our culture celebrates positive challenges, asking questions, learning and actively supporting one another. This mentality of shared respect and purposeful teamwork is key to our success. We equally value all technical and non-technical contributions.
Constructive disagreement is essential.
We appreciate when team members challenge assumptions, put forward new ideas, or encourage us to move faster or slower. Openness, honesty and kindness make us great.
Feedback is our ground truth.
We have a tight feedback loop between the user experience and our AI creation process. Quantitative and qualitative data drives our priorities. This goes for internal culture too. Everyone has ownership and visibility into key decisions and progress.
Writing creates accountability.
Whether on internal communication tools or in team memos, we are strong communicators with a special focus on the written word.
We deeply value time to reset outside of work.
We encourage one another to constantly take time to recharge and always focus on maintaining a healthy work-life balance.
Engineering at Inflection
We are a vertically integrated AI studio. This means that our entire technology stack – from large foundational model pre-training to the user interface – is built in-house, with each of the components co-optimized to deliver the best AI experiences. We have built one of the most advanced large language models in the world, based on multiple novel and proprietary innovations.
We believe in scale as the engine of progress in AI, and we are building one of the largest supercomputers in the world to develop and deploy the new generation of AIs.
We wear multiple hats and don’t distinguish between engineering and research. We continuously explore and exploit, creating new and perfecting existing techniques and solutions. User feedback is our North Star.
Our Benefits
We offer generous benefits to ensure a positive, safe, inclusive and inspiring work environment for all Inflectioneers.
Physical office space in Palo Alto and London, along with reimbursements for work from home expenses and co-working spaces
Unlimited paid time off
Parental leave and flexibility for all parents and caregivers
Generous medical, dental and vision plans for US employees
Compliance with country-specific benefits for non-US employees
Visa sponsorship for new hires
Avenues for personal growth such as coaching, conference attendance, or specific trainings
Diversity & Inclusion
We are building personal AIs that we hope will serve everyone. We are deeply committed to representing the full extent of the human experience inside our AI Studio. This means that everyone from any walk of life is welcome if you have the right skills. We populate diverse candidate pools for all open roles.
Show more
Show less","PySpark, Beam, Flink, Machine Learning, NLP, Data Science, ML Engineering, Data Engineering, Software Engineering, Large Language Model, Supercomputer, AI Studio, Generative AI, Conversational AI","pyspark, beam, flink, machine learning, nlp, data science, ml engineering, data engineering, software engineering, large language model, supercomputer, ai studio, generative ai, conversational ai","ai studio, beam, conversational ai, data engineering, data science, flink, generative ai, large language model, machine learning, ml engineering, nlp, software engineering, spark, supercomputer"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Trenton, NJ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086803,2023-12-17,Hillsborough,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, NLP, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Compliance, Data Management, Data Retention","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, nlp, large language models, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data compliance, data management, data retention","airflow, applied machine learning, aws, azure, bash, data cleaning, data compliance, data engineering, data management, data mining, data normalization, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Software Engineer (Oracle Database Expertise),iconectiv,"Bridgewater, NJ",https://www.linkedin.com/jobs/view/software-engineer-oracle-database-expertise-at-iconectiv-3762940739,2023-12-17,Hillsborough,United States,Mid senior,Hybrid,"Responsibilities:
Responsible for developing software that anticipates and supports customer business needs, iconectiv business objectives, and keeps ahead of the competition. Specifically, responsible for the design, development and testing of software programs in accordance with iconectiv procedures. Provide troubleshooting support to customers as needed. Participates in the review of requirements and Product Test planning and execution. Support the production of necessary Learning Support materials. As necessary, responsible for/participates in overall architecture design, third party software evaluation and computing platform selection. As necessary, participates on teams involved in cross product solutions and interfaces to other products. May be responsible for application database support which encompasses build, development and test environment support along with packaging/shipping software.
Required Qualifications
:
Demonstrated experience delivering software systems on aggressive schedules with high quality.
Oracle RDBMS and JDBC experience a MUST
Agile/Lean methodology
Systems design
Java
Software configuration tools (source control, defect tracking, etc.)
Large scale performance applications
Excellent teaming and collaboration skills
SQL Developer
Schema design, Data modeling, and optimization
Tuning, indexing, partitioning, query optimization
Stored procedures, triggers, transaction management
B.S. in Computer Science or equivalent with 5+ years of experience or 8+ years of experience for a SR. Developer
US Citizenship required
Additional Skills desired:
Experience with latest SW development tools such as Eclipse IDE
Database Administration
Oracle RAC
Data Warehousing
AWS
Process monitoring and logging experience
Messaging experience, e.g. JBoss messaging, Oracle AQ
Database migration
Distributed database
MyBatis
J2EE EJB experience
JBoss application server experience
Reporting 3PP experience (e.g., JasperSoft)
Billing Systems experience
Show more
Show less","Java, SQL, Oracle RDBMS, JDBC, Agile/Lean methodology, Software configuration tools, Schema design, Data modeling, Tuning, Query optimization, Stored procedures, Triggers, Transaction management, Eclipse IDE, Oracle RAC, Data Warehousing, AWS, Process monitoring, Logging, Messaging, JBoss messaging, Oracle AQ, Database migration, Distributed database, MyBatis, J2EE, EJB, JBoss application server, JasperSoft, Billing Systems","java, sql, oracle rdbms, jdbc, agilelean methodology, software configuration tools, schema design, data modeling, tuning, query optimization, stored procedures, triggers, transaction management, eclipse ide, oracle rac, data warehousing, aws, process monitoring, logging, messaging, jboss messaging, oracle aq, database migration, distributed database, mybatis, j2ee, ejb, jboss application server, jaspersoft, billing systems","agilelean methodology, aws, billing systems, database migration, datamodeling, datawarehouse, distributed database, eclipse ide, ejb, j2ee, jaspersoft, java, jboss application server, jboss messaging, jdbc, logging, messaging, mybatis, oracle aq, oracle rac, oracle rdbms, process monitoring, query optimization, schema design, software configuration tools, sql, stored procedures, transaction management, triggers, tuning"
Data Transformation Lead - Global Data Platform,Chubb,"Whitehouse Station, NJ",https://www.linkedin.com/jobs/view/data-transformation-lead-global-data-platform-at-chubb-3756332855,2023-12-17,Hillsborough,United States,Mid senior,Hybrid,"Job Description
This position will support our Global Data Platforms within the Global Data Organization. Responsibilities include the following:
Knowledge of Agile values, principles and practices; certified Scrum Master, Release Train Engineer, and/or Agile Practitioners are preferred
Manage/oversee the planning and delivery for multiple squads to deliver on quarterly commitments
Facilitate agile ceremonies including Quarterly Planning, Iteration Planning, Backlog Refinement, Stand-ups, Demos, and Retrospectives
Mitigate risks, issues, and impediments to help the teams/organization be successful
Support adoption of Agile processes and Agile ways of working in the organization
Manages squad/domain synchronization, process efficiency, and continuous improvement processes
Teach, mentor, and improve Agile principles and practices across the organization
Build a culture that promotes transparency, inspection, and adaptation
Embrace, champion, and adapt to change and transformation in alignment with business goals and overall strategic direction
Leverage Agile metrics / reporting to identify team opportunities and strengths for continuous improvement purposes
Populate scorecards and status reports as needed to provide timely and accurate progress on initiatives
Onboard resources and assist with transition planning to minimize team disruption / impacts as team members join
Review timesheets to oversee/govern both internal and external spend to ensure alignment with the budget
Effectively communicate at all levels of management as needed
Experience working with Jira as an Agile work management tool is preferred
Qualifications
Agile and Jira experience and expertise required
Experience leading/overseeing multiple teams at once
People management experience
Demonstrated experience in leading change and/or transformation
Excellent communication and facilitation skills
Proficiency in Microsoft suite - PowerPoint, Excel, MS Teams, Sharepoint, and Word
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","Agile, Scrum Master, Release Train Engineer, Agile Practitioner, Jira, Quarterly Planning, Iteration Planning, Backlog Refinement, Standups, Demos, Retrospectives, Microsoft Suite, PowerPoint, Excel, Sharepoint, Word","agile, scrum master, release train engineer, agile practitioner, jira, quarterly planning, iteration planning, backlog refinement, standups, demos, retrospectives, microsoft suite, powerpoint, excel, sharepoint, word","agile, agile practitioner, backlog refinement, demos, excel, iteration planning, jira, microsoft suite, powerpoint, quarterly planning, release train engineer, retrospectives, scrum master, sharepoint, standups, word"
"Chubb Associate, Reporting & Data Analyst, Personal Risk Services",Chubb,"Whitehouse Station, NJ",https://www.linkedin.com/jobs/view/chubb-associate-reporting-data-analyst-personal-risk-services-at-chubb-3756333931,2023-12-17,Hillsborough,United States,Mid senior,Hybrid,"Job Description
The Business Solutions team provides critical business information to key areas of Personal Risk Services (PRS). The team is also responsible for the provisioning, stewardship, and validation of PRS and corporate data sources. This position works closely with business areas to provide reporting and analysis for PRS leadership. The strategic focus of this position is the mastery of the PRS data ecosystem and the continuous improvement of PRS reporting processes, efficiencies, and underlying data.
Key Responsibilities
Reporting & Analytics
Develop a mastery of PRS, Corporate, and third-party data sources to support senior management decision making on PRS strategic initiatives
Collaborate with business partners in Finance, Product, Sales & Distribution, and Underwriting to understand business challenges and leverage PRS data to provide key insights and analysis
Streamline critical reporting processes providing stakeholders in areas such as General Counsel and Catastrophe Modeling with timely and accurate data and respond to urgent requests for information during an event response
Apply and refine best practices on data analysis, querying, process automation and reporting with emphasis on continuous process improvement and rapid turnaround
Data Stewardship and Administration
Govern data and reporting capabilities, validate production sources, and drive testing initiatives for projects creating or impacting PRS data
Perform root cause analysis: Identify and communicate anomalous data, determine the underlying issue, quantify the impact, and propose options for resolution
Perform daily administrative tasks such as resolution of data rejects, client stewardship, production reporting deployment, and reporting security access
Competencies
Experience with business intelligence tools (Qlik Sense, Microsoft Power Query and Power BI, SQL, Python, Databricks)
Results driven with highly effective analytical/problem solving and research skills
Ability to maintain progress on multiple tasks, adjust quickly to meet business priorities, and respond to urgent management requests
Knowledge of personal lines insurance skills including an understanding of sales, pricing and underwriting is preferred
Ability to communicate effectively between highly technical staff in IT and analytics and end-user business partners
Education And Experience
Bachelor’s degree or equivalent business experience in business, math, computer science, or data management
Minimum 3 years business intelligence / data mining experience
Chubb strives to offer a diverse and inclusive and rewarding work environment. Teamwork and mutual respect are central to how Chubb operates, and we believe the best solutions draw upon diverse perspectives, experiences and skills. We operate in such a way where everyone, regardless of their singular background has the opportunity to contribute to our collective success.
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","Qlik Sense, Microsoft Power Query, Power BI, SQL, Python, Databricks, Business intelligence, Data mining, Data analysis, Querying, Process automation, Reporting, Data stewardship, Data administration, Data governance, Root cause analysis, Data rejects, Client stewardship, Production reporting, Reporting security, Analytical skills, Problem solving skills, Research skills, Multitasking, Adaptability, Communication skills, Technical writing, Presentation skills, Personal lines insurance, Sales, Pricing, Underwriting, Bachelor's degree, Business experience, Computer science, Data management","qlik sense, microsoft power query, power bi, sql, python, databricks, business intelligence, data mining, data analysis, querying, process automation, reporting, data stewardship, data administration, data governance, root cause analysis, data rejects, client stewardship, production reporting, reporting security, analytical skills, problem solving skills, research skills, multitasking, adaptability, communication skills, technical writing, presentation skills, personal lines insurance, sales, pricing, underwriting, bachelors degree, business experience, computer science, data management","adaptability, analytical skills, bachelors degree, business experience, business intelligence, client stewardship, communication skills, computer science, data administration, data governance, data management, data mining, data rejects, data stewardship, dataanalytics, databricks, microsoft power query, multitasking, personal lines insurance, powerbi, presentation skills, pricing, problem solving skills, process automation, production reporting, python, qlik sense, querying, reporting, reporting security, research skills, root cause analysis, sales, sql, technical writing, underwriting"
NJATCSU Master Data Management Analyst #: 19-03286,HireTalent - Diversity Staffing & Recruiting Firm,"Trenton, NJ",https://www.linkedin.com/jobs/view/njatcsu-master-data-management-analyst-%23-19-03286-at-hiretalent-diversity-staffing-recruiting-firm-3764242924,2023-12-17,Hillsborough,United States,Mid senior,Hybrid,"The MDM will work with business and technical teams to properly define, document and align business requirements with technical solutions.
Work with business and technical teams to properly define, document and align business requirements with technical solutions.
Ability to gather business requirements, design solution with technical teams, and write functional specification documents.
Work with IT to secure, manage and create value from data management tools.
Ability to lead requirements gathering, future state design and fit-gap analysis workshops with business stakeholders.
Ability to reconcile, enhance and recommend business process alignment with MDM and governance requirements.
Ability to clearly articulate problems and proposed options and solutions and apply judgement in implementing application engineering, methodologies, processes and practices to ensure security, resilience, maintainability and quality of MDM solutions.
Assist business teams, via training or other means, with understanding, MDM and associated processes.
Ensure changes and enhancements within the application are properly tested before deploying to production.
Perform root cause analysis to identify system/data defects and determine proper course of action.
Plan and execute data cleanup efforts required for all master data elements to comply with global, company-wide master data management definition, process and ownership guidelines.
Review current MDM data models to understand the as-is environment.
Proactively identify risk, issues and problems on programs/projects.
Provide guidance on project and new development work, ensuring adherence to master data standards and strategy.
Work with database/MDM team to identify, prioritize and schedule changes needed within the MDM application experience with data quality reporting and cleanup.
Comprehensive understanding of customer domain as well as business rules and workflows within an MDM application.
Able to work independently and to prioritize and handle multiple duties/projects at one time.
Strong proactive problem solving skills, decision-making, and analytical skills required.
Solid understand of master data domains required
Excellent interpersonal, verbal and written communication skills required
A minimum of 5 years of experience as a functional/system analyst with enterprise MDM solutions, including experience working with industry leading master data tools
Experience with the IBM InfoSphere MDM is preferred
High level of proficiency in MS Excel/PowerPoint and strong attention to detail required
PREFERRED EDUCATION: 4 year college degree in Information Systems preferred.
Show more
Show less","Business Requirements Gathering, Solution Design, Functional Specification Writing, Data Management Tools, Requirements Gathering, Future State Design, FitGap Analysis Workshops, Business Process Alignment, MDM and Governance Requirements, Problem Solving, Application Engineering, Methodologies, Processes, Practices, MDM Solutions, Data Quality Reporting, Data Cleanup, Master Data Domains, Business Rules, Workflows, Proactive Problem Solving, DecisionMaking, Analytical Skills, Master Data Domains, Interpersonal Skills, Verbal Communication, Written Communication, Enterprise MDM Solutions, Industry Leading Master Data Tools, IBM InfoSphere MDM, MS Excel, PowerPoint, Attention to Detail, Information Systems","business requirements gathering, solution design, functional specification writing, data management tools, requirements gathering, future state design, fitgap analysis workshops, business process alignment, mdm and governance requirements, problem solving, application engineering, methodologies, processes, practices, mdm solutions, data quality reporting, data cleanup, master data domains, business rules, workflows, proactive problem solving, decisionmaking, analytical skills, master data domains, interpersonal skills, verbal communication, written communication, enterprise mdm solutions, industry leading master data tools, ibm infosphere mdm, ms excel, powerpoint, attention to detail, information systems","analytical skills, application engineering, attention to detail, business process alignment, business requirements gathering, business rules, data cleanup, data management tools, data quality reporting, decisionmaking, enterprise mdm solutions, fitgap analysis workshops, functional specification writing, future state design, ibm infosphere mdm, industry leading master data tools, information systems, interpersonal skills, master data domains, mdm and governance requirements, mdm solutions, methodologies, ms excel, powerpoint, practices, proactive problem solving, problem solving, processes, requirements gathering, solution design, verbal communication, workflows, written communication"
BigData Engineer,"Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/bigdata-engineer-at-conch-technologies-inc-3748604700,2023-12-17,Andalusia,United States,Mid senior,Onsite,"HI,
Greetings from Conch Technologies Inc
Position: BigData Engineer ( 7 position open )
Location: Phoenix, AZ ( Onsite Daya 1 )
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL and PySpark) Solid Data warehousing concepts
Knowledge of Financial reporting ecosystem will be a plus .5+ years of experience within Data Engineering/ Data Warehousing using Big Data technologies will be a add-on Expert on Distributed ecosystems Hands-on experience with programming using Core Java or Python/Scala.
Expert on Hadoop and Spark Architecture and its working principle Hands-on experience on writing and understanding complex SQL(Hive/Py Spark-dataframes), optimizing joins while processing huge amounts of data .
Experience In UNIX Shell Scripting .
Ability to design and develop optimized Data pipelines for batch and real time data processing Should have experience in analysis, design, development, testing, and implementation of system applications Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows.
Preferred Qualifications: Knowledge of cloud platforms like
GCP/AWS, building Microservices and scalable solutions, will be an advantage 1 + years of experience in designing and building solutions using Kafka streams or queues Experience with GitHub/Bitbucket and leveraging CI/CD pipelines .Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have Excellent technical and analytical aptitude Good communication skills .Excellent Project management skills. Results driven
--
With Regards,
Nagesh G
Mobile:
408-381-5645
Desk:
901-313-3066
Email: nagesh@conchtech.com
Web:
www.conchtech.com
Show more
Show less","Engineering, Java, SQL, Data Warehousing, Core Java, HBase, MapReduce, GCP, Python, Linux, Scala, HDFS, MySQL, Scala, Hive, PySpark, Pig, Kafka, Spark, Oozie, Structured Streaming, Machine Learning, AWS, GitHub, Hadoop, Big Data, NoSQL, Spark SQL, Microservices, HBase, UNIX, MongoDB, Couchbase","engineering, java, sql, data warehousing, core java, hbase, mapreduce, gcp, python, linux, scala, hdfs, mysql, scala, hive, pyspark, pig, kafka, spark, oozie, structured streaming, machine learning, aws, github, hadoop, big data, nosql, spark sql, microservices, hbase, unix, mongodb, couchbase","aws, big data, core java, couchbase, datawarehouse, engineering, gcp, github, hadoop, hbase, hdfs, hive, java, kafka, linux, machine learning, mapreduce, microservices, mongodb, mysql, nosql, oozie, pig, python, scala, spark, spark sql, sql, structured streaming, unix"
Senior Python Data Engineer,Synechron,"New York, United States",https://www.linkedin.com/jobs/view/senior-python-data-engineer-at-synechron-3784870084,2023-12-17,Crookston,United States,Mid senior,Onsite,"Summary :
We are looking for strong Python Developer with strong background in data engineering and integration experience.
Duties :
Integration Engineer responsible for daily support and project based development of credit risk management systems.
ETL developers are responsible for designing and creating the data warehouse and all related extraction, transformation and load of data functions.
This is an opportunity to gain experience in risk management processing using new technologies.
You are :
5 years of full-time development experience using Python.
Experience building data piplines using Azure Data Factory and Databricks.
Experience with Python application frameworks (Django, Flask, Pyramid, Tornado).
Experience with Python testing and code analysis tools (Pytest, Pylint).
Strong SQL skills.
Familiarity with SSIS.
Strong troubleshooting skills.
On-point communication skills.
Education :
Bachelor’s degree in computer science or finance.
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 45 offices in 19 countries and the possibility to work abroad
Laptop and a mobile phone
10 days of paid annual leave (plus sick leave and national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region)
Retirement savings plans
A higher education certification policy
Commuter benefits (varies by region)
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Synclusive’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law
Show more
Show less","Python, Data Engineering, Data Integration, Integration Engineer, ETL, Data Warehouse, Azure Data Factory, Databricks, Python Frameworks, Django, Flask, Pyramid, Tornado, Python Testing, Pylint, Pytest, SQL, SSIS, Troubleshooting, Communication","python, data engineering, data integration, integration engineer, etl, data warehouse, azure data factory, databricks, python frameworks, django, flask, pyramid, tornado, python testing, pylint, pytest, sql, ssis, troubleshooting, communication","azure data factory, communication, data engineering, data integration, databricks, datawarehouse, django, etl, flask, integration engineer, pylint, pyramid, pytest, python, python frameworks, python testing, sql, ssis, tornado, troubleshooting"
Staff Air Data Systems Engineer,Archer,"San Jose, CA",https://www.linkedin.com/jobs/view/staff-air-data-systems-engineer-at-archer-3667433898,2023-12-17,Livermore,United States,Associate,Onsite,"Archer is an aerospace company based in San Jose, California building an all-electric vertical takeoff and landing aircraft with a mission to advance the benefits of sustainable air mobility. We are designing, manufacturing, and operating an all-electric aircraft that can carry four passengers for 100 miles at speeds of up to 150 mph while producing minimal noise.
Our sights are set high and our problems are hard, and we believe that diversity in the workplace is what makes us smarter, drives better insights, and will ultimately lift us all to success. We are dedicated to cultivating an equitable and inclusive environment that embraces our differences, and supports and celebrates all of our team members.
What you’ll do:
Provide architecture development, integration, and validation / verification definition for the Archer State Estimation Air Data system.
Ensure proper calibration and sensor characterization of the air data probes and validate the installation locations for proper aerodynamic and aircraft propulsion effects.
Contribute to the architecture, design, integration, and test of the entire Archer aircraft state estimation system including all the sensors used for piloting and navigation.
Author and take responsibility for the system-level and item-level product requirements, defining and validating requirements for the air data system sensors and state estimation software including sensor selection, voting, monitoring, fault detection, and sensor fusion.
Ensure the requirement set is complete and correct, with traceability to and fulfilling business, technical, and operational objectives.
Coordinate with other Systems Engineers to ensure the fully integrated air data system meets its requirements and is optimized.
Contribute to the integration and test of the state estimation system, support aircraft integration and flight test campaigns.
Support development of compliance documentation such as the system certification plan, system compliance checklists, system review reports, system descriptions, issue paper development and responses
Support development and review of aircraft level technical documentation such as aircraft manuals and procedures.
What we are looking for:
General
Collaborative mindset
Excellent verbal and written interpersonal communication
Excellent organizational and communication skills
Passion for learning and problem solving
6+ years of relevant experience
Degree in Electrical Engineering, Mechanical Engineering, Aerospace Engineering, or related field
Ability to code, design or review Matlab and Simulink models
Technical
Deep Technical knowledge and experience working with Air Data Systems and its environmental operating conditions
Experience working on 14 CFR parts 23/25/91/135 and applicable guidance materials
Technical knowledge of aircraft sensor types including ADAHRS, INS, GNSS, Weight on Wheels
(WOW), Radar Altimeters
Familiarity with filters, signal processing and sensor fusion algorithms such as Kalman Filters
Experience working on computer architecture, embedded software, communication standards and protocols
Experience working either ARP-4754A, DO-254 or DO-178C standards
Familiarity with DO-160G qualification standard
Bonus Qualifications
Experience developing or working with highly integrated aerospace systems
Experience executing formal Validation and Verification activities in a civil aerospace program
Experience with Agile/SCRUM methodologies
At Archer we aim to attract, retain, and motivate talent that possess the skills and leadership necessary to grow our business. We drive a pay-for-performance culture and reward performance that supports the Company’s business strategy. For this position we are targeting a base pay between $164,000 $205,000. Actual compensation offered will be determined by factors such as job-related knowledge, skills, and experience.
Archer is committed to working with and providing reasonable accommodations to job applicants with physical or mental disabilities, and those with sincerely held religious beliefs. Applicants who may require reasonable accommodation for any part of the application or hiring process should provide their name and contact information to Archer’s People Team at people@archer.com . Reasonable accommodations will be determined on a case-by-case basis.
Archer is proud to be an Equal Opportunity employer committed to diversity and inclusivity in the workplace. All aspects of employment are decided on the basis of merit, qualifications, and business needs. We do not discriminate based upon race, color, religion, sex, sexual orientation, age, national origin, disability status, protected veteran status, gender identity or any other characteristic protected by federal, state or local laws.
Show more
Show less","Matlab, Simulink, Air Data Systems, 14 CFR parts 23/25/91/135, ADAHRS, INS, GNSS, Weight on Wheels, Radar Altimeters, Kalman Filters, Computer architecture, Embedded software, Communication standards, Protocols, ARP4754A, DO254, DO178C, DO160G, Aerospace systems, SCRUM methodologies","matlab, simulink, air data systems, 14 cfr parts 232591135, adahrs, ins, gnss, weight on wheels, radar altimeters, kalman filters, computer architecture, embedded software, communication standards, protocols, arp4754a, do254, do178c, do160g, aerospace systems, scrum methodologies","14 cfr parts 232591135, adahrs, aerospace systems, air data systems, arp4754a, communication standards, computer architecture, do160g, do178c, do254, embedded software, gnss, ins, kalman filters, matlab, protocols, radar altimeters, scrum methodologies, simulink, weight on wheels"
Data Engineer- JAVA [Onsite] - C2C/W2,SmartIPlace,"Fremont, CA",https://www.linkedin.com/jobs/view/data-engineer-java-onsite-c2c-w2-at-smartiplace-3732622664,2023-12-17,Livermore,United States,Mid senior,Onsite,"Title: Data Engineer- JAVA
Location: Fremont, CA(Onsite role)
Visa: Any - No H1
Duration: 6+ months
Job Description
Build data pipelines using Java.
Minimum 3 - 5 years of experience on primary skills.
Develop analytics-based solutions that produce quantitative and qualitative business insights.
Work with partners as necessary to integration systems and data quickly and effectively, regardless of technical challenges or business environments.
Design, Build & Test the data streaming / batch processing data pipeline as per the business requirements
Collaborate with the team to use best practices to deliver and address any dependencies
Provide regular status update including any risks/issues to the project lead
Proactive in learning new skills and communicating ideas effectively
Must Require Apache Flink
Requirement
Experience and Knowledge
Apache Kafka
Data Integrations, Data pipeline development and deployment
Distributed systems (any of spark, flink or Confluent Kafka)
Database (GraphDB preferable, NOSQL)
Rest API development
Java
Functional Domain – Manufacturing, CRM (Plus)
Show more
Show less","Java, Apache Flink, Apache Kafka, Data Integration, Data Pipeline Development and Deployment, Distributed Systems, Spark, Confluent Kafka, NoSQL, GraphDB, REST API Development, Functional Domain, Manufacturing, CRM","java, apache flink, apache kafka, data integration, data pipeline development and deployment, distributed systems, spark, confluent kafka, nosql, graphdb, rest api development, functional domain, manufacturing, crm","apache flink, apache kafka, confluent kafka, crm, data integration, data pipeline development and deployment, distributed systems, functional domain, graphdb, java, manufacturing, nosql, rest api development, spark"
"Senior Software Engineer, Data Science - LLM MLOps Platform",NVIDIA,"Santa Clara, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-science-llm-mlops-platform-at-nvidia-3657601214,2023-12-17,Livermore,United States,Mid senior,Onsite,"NVIDIA has been transforming computer graphics, PC gaming, and accelerated computing for more than 25 years. It’s an outstanding legacy of innovation that’s fueled by phenomenal technology—and amazing people! Today, we’re tapping into the unlimited potential of AI to define the next era of computing. An era in which our GPU acts as the brains of computers, robots, and self-driving cars that can understand the world. Doing what’s never been done before takes vision, innovation, and the world’s best talent. As an NVIDIAN, you will be immersed in a diverse, encouraging environment where everyone is inspired to do their best work. Come join the team and see how you can make a lasting impact on the world.
What You'll Be Doing
Build data cleaning and personally identifiable information (PII) removal Solutions by applying data science and related technologies.
Build and Operate sophisticated ML data analytics services to actively perform PII detections and removal, and policy recommendations.
Collaborate across high-performance software engineering teams to develop innovative ML solutions using NVIDIA Deep Learning Software and GPU stack.
Showcase leadership and lead the product development roadmap to align with business priorities and vision.
Build high performance data pipelines of Big Data solutions in real time for inferencing, training and ETL.
Utilizing Data Engineering, ETL, machine learning technologies for training, inferencing and deployment of ML models into production applications.
What We Need To See
Masters in Computer Science, Electrical Engineering or equivalent experience
5+ years of experience architecting, crafting and implementing software solutions, preferably in product development space and 3+ years hands-on experience in distributed computing, data engineering and data analytics
Experience using end-to-end MLOps platforms such as Kubeflow, MLFlow, AirFlow
Experience with data pipelines/analysis/visualization tooling such as PowerBI, Elastic stack, Logstash, Kibana, Kafka, Grafana, Splunk, Pandas, Message brokers, Data modeling etc.
Software development experience in any of the following core languages/frameworks: Python, Java, GO, Spring/Hibernate
Knowledge of parallel programming architectures or experience programming distributed systems.
Proven experience with log management, data analysis, data query approaches and dashboarding.
Experience with big data and related technologies (e.g. Hadoop, Spark, Cassandra).
Background with streaming architectures (e.g. NiFi, Streamz, ReactiveX) and pub/sub systems (e.g. Kafka, Pulsar)
Solid understanding of Amazon Web Services, Kubernetes, Docker is a plus
Ways To Stand Out From The Crowd
ML/DL/Cloud certifications
Knowledge of Cloud Based solutions like Kendra, SageMaker, Auto-ML, Big Query, RedShift, Glue, Athena, FireHose etc.
Excellent communication and teamwork skills
With highly competitive salaries and a comprehensive benefits package, NVIDIA is widely considered to be one of the technology industry's most desirable employers. We have some of the most forward-thinking and hardworking people in the world working with us and our engineering teams are growing fast in some of the hottest innovative fields: Deep Learning, Artificial Intelligence, and Large Language Models. If you're a creative engineer with a real passion for robust and enjoyable user experiences, we want to hear from you!
The base salary range is 144,000 USD - 270,250 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","Data Science, Machine Learning, Python, Java, GO, Spring, Hibernate, Hadoop, Spark, Cassandra, NiFi, Streamz, ReactiveX, Kafka, Pulsar, Kubernetes, Docker, Amazon Web Services, Data pipelines, Data analysis, Data visualization, Data Engineering, ETL, MLOps, MLFlow, AirFlow, PowerBI, Elastic Stack, Logstash, Kibana, Grafana, Splunk, Pandas, Message Brokers, Data Modeling, Software Development, Cloud Computing, Kendra, SageMaker, AutoML, Big Query, RedShift, Glue, Athena, FireHose","data science, machine learning, python, java, go, spring, hibernate, hadoop, spark, cassandra, nifi, streamz, reactivex, kafka, pulsar, kubernetes, docker, amazon web services, data pipelines, data analysis, data visualization, data engineering, etl, mlops, mlflow, airflow, powerbi, elastic stack, logstash, kibana, grafana, splunk, pandas, message brokers, data modeling, software development, cloud computing, kendra, sagemaker, automl, big query, redshift, glue, athena, firehose","airflow, amazon web services, athena, automl, big query, cassandra, cloud computing, data engineering, data science, dataanalytics, datamodeling, datapipeline, docker, elastic stack, etl, firehose, glue, go, grafana, hadoop, hibernate, java, kafka, kendra, kibana, kubernetes, logstash, machine learning, message brokers, mlflow, mlops, nifi, pandas, powerbi, pulsar, python, reactivex, redshift, sagemaker, software development, spark, splunk, spring, streamz, visualization"
"Sr. Software Engineer, Data Pipelines, Cell Manufacturing",Tesla,"Fremont, CA",https://www.linkedin.com/jobs/view/sr-software-engineer-data-pipelines-cell-manufacturing-at-tesla-3737826452,2023-12-17,Livermore,United States,Mid senior,Onsite,"What To Expect
The Cell Manufacturing team is responsible for performing advanced analytics, creating infrastructure and data pipelines, developing predictive models and making data applications that enable cross functional teams to leverage a wealth of manufacturing, equipment and vehicle data both efficiently and effectively. In this role, you will focus on building data pipelines and infrastructure that power data science systems, tools, software and applications for the Cell Engineering team.
What You'll Do
Employ and improve industry-leading, scalable, distributed open-source technologies
Build back-end systems from scratch that are capable of handling trillion+ events per day
Facilitate operation of highly-available distributed systems at scale and across multiple locations
Facilitate others in deploying, operating, and extending upon your clean, tested code
Help define a platform that is highly leveraged, multi-tenant, and self-serviced
Work with data engineers and data scientists to drive efficient solutions from the platform
Help define the data story and enable data-driven solutions at Tesla, both technically and culturally
What You'll Bring
Strong programming fundamentals, particularly in data structures, concurrency, Go, Python, Java and Scala (preferred)
Deep understanding of a complex distributed system, such as Kafka, Spark, HBase, ElasticSearch
Have built and optimized highly available, scalable, distributed back-end services
Ability to break down and deeply understand complex problems and communicate complex matters efficiently
Strong problem-solving skills, optimizing for the simplest, most robust yet practical solutions
Benefits
Compensation and Benefits
Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire:
Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction
Family-building, fertility, adoption and surrogacy benefits
Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution
Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA
Healthcare and Dependent Care Flexible Spending Accounts (FSA)
LGBTQ+ care concierge services
401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits
Company paid Basic Life, AD&D, short-term and long-term disability insurance
Employee Assistance Program
Sick and Vacation time (Flex time for salary positions), and Paid Holidays
Back-up childcare and parenting support resources
Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance
Weight Loss and Tobacco Cessation Programs
Tesla Babies program
Commuter benefits
Employee discounts and perks program
Expected Compensation
$104,000 - $348,000/annual salary + cash and stock awards + benefits
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
Tesla
Show more
Show less","Data Structures, Concurrency, Go, Python, Java, Scala, Kafka, Spark, HBase, ElasticSearch, Distributed Systems, BackEnd Services, ProblemSolving, Big Data, Data Analytics, Data Pipelines, Data Science, Data Applications, Machine Learning, Predictive Modeling, Software Development, Infrastructure Development","data structures, concurrency, go, python, java, scala, kafka, spark, hbase, elasticsearch, distributed systems, backend services, problemsolving, big data, data analytics, data pipelines, data science, data applications, machine learning, predictive modeling, software development, infrastructure development","backend services, big data, concurrency, data applications, data science, data structures, dataanalytics, datapipeline, distributed systems, elasticsearch, go, hbase, infrastructure development, java, kafka, machine learning, predictive modeling, problemsolving, python, scala, software development, spark"
Data Engineer/Analyst,"RIT Solutions, Inc.","Fremont, CA",https://www.linkedin.com/jobs/view/data-engineer-analyst-at-rit-solutions-inc-3770126983,2023-12-17,Livermore,United States,Mid senior,Onsite,"Title : Data Engineer/Analyst
Location : Fremont, CA (Hybrid 3 days on site)
Duration: 6+ months
Required exp : 10+ years
Job Description
Responsibilities:
Data mining in PLM/SAP
Ensure data quality and integrity by performing data cleansing, validation, and error handling.
Optimize data infrastructure to support data processing, storage, and retrieval efficiently.
Perform exploratory data analysis to uncover trends, patterns, and insights from the data.
Conduct statistical analysis and data modeling to support business decision-making.
Develop and maintain data documentation, including data dictionaries and data lineage.
Create visually appealing and interactive dashboards that effectively communicate complex data insights to stakeholders.
Leverage best practices to present data in a clear, concise, and compelling manner.
Collaborate with business users & cross functional teams to understand their data requirements, analytical needs and translate them into actionable visualizations.
Monitor and maintain existing dashboards, making improvements and updates as needed
Minimum Qualifications
5+ years of related experience with a bachelor's degree in Computer Science, Data Science, Statistics, Supply Chain Management, Operations Research, or a related field.
Expertise with SharePoint, Power BI
Prefer experience in ENOVIA PLM and SAP
Proven experience in data analysis, data modeling, and statistical analysis.
Proficient in data visualization tools such as Power BI
Experience managing IT programs
Strong analytical and problem-solving skills
Ability to develop front end applications with custom workflows and automations. (PowerApps, SharePoint Classic and Online, QuickBase Applications)
Ability to integrate front end applications with databases (SAP/SILK) to show real time data (Custom APIs, Webservices)
Show more
Show less","Data mining, Data cleansing, Data validation, Error handling, Data infrastructure, Data processing, Data storage, Data retrieval, Exploratory data analysis, Trend analysis, Pattern recognition, Data insights, Statistical analysis, Data modeling, Business decisionmaking, Data documentation, Data lineage, Data visualization, Data dashboards, Data communication, SharePoint, Power BI, ENOVIA PLM, SAP, PowerApps, SharePoint Classic, SharePoint Online, QuickBase Applications, Custom APIs, Webservices","data mining, data cleansing, data validation, error handling, data infrastructure, data processing, data storage, data retrieval, exploratory data analysis, trend analysis, pattern recognition, data insights, statistical analysis, data modeling, business decisionmaking, data documentation, data lineage, data visualization, data dashboards, data communication, sharepoint, power bi, enovia plm, sap, powerapps, sharepoint classic, sharepoint online, quickbase applications, custom apis, webservices","business decisionmaking, custom apis, data communication, data dashboards, data documentation, data infrastructure, data insights, data lineage, data mining, data processing, data retrieval, data storage, data validation, datacleaning, datamodeling, enovia plm, error handling, exploratory data analysis, pattern recognition, powerapps, powerbi, quickbase applications, sap, sharepoint, sharepoint classic, sharepoint online, statistical analysis, trend analysis, visualization, webservices"
Senior Azure Data Engineer,MathCo,San Francisco Bay Area,https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-mathco-3773574262,2023-12-17,Livermore,United States,Mid senior,Onsite,"COMPANY OVERVIEW
MathCo's
vision is to be the world’s largest problem solver. We enable viable and valuable data and analytics transformations for our clients. Our mission is to help Fortune 500 or similar organizations build core capabilities that set them on a path to achieve analytics self-sufficiency. We are transforming the way companies execute enterprise-wide data engineering and data science initiatives. With a holistic range of services, products & platforms across analytics consulting, data engineering, data science and management consulting, we are disrupting the analytics services and products space. To help us achieve our objectives, we are looking for passionate and experienced practitioners to be part of our US organization and become a part of the growth story for one of the fastest growing AI/ML companies in the world.
ROLE DESCRIPTION
Design, develop, and implement scalable and efficient data pipelines using Apache Spark on the Azure Databricks platform.
Transform raw data into a format suitable for analysis and reporting.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
Set up and configure Azure Databricks clusters to optimize performance and cost.
Monitor and troubleshoot issues related to the Azure Databricks environment.
Implement security measures to protect sensitive data within the platform.
Integrate data from various sources, both batch and real-time, into Azure Databricks.
Work with different data formats and storage solutions, such as Azure Data Lake Storage, Azure Blob Storage, Delta lake, or other Azure-based data services.
Design and implement data models to support business requirements.
Develop and maintain ETL processes for efficient data movement and transformation.
Optimize Spark jobs and queries for performance and resource utilization.
Identify and resolve bottlenecks in data processing pipelines.
Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders.
Communicate effectively to convey technical concepts to non-technical stakeholders.
Use version control systems (e.g., Git) to manage codebase changes.
Create and maintain documentation for data pipelines, data models, and other relevant artifacts.
Implement data quality checks to ensure the accuracy and reliability of data.
Adhere to data governance policies and best practices.
Proficiency in programming using Python and automation tasks.
Identify and resolve issues related to data processing, data movement, and platform configuration.
Perform debugging and troubleshooting of data pipeline failures.
Integrate Azure Databricks with other Azure services, such as Azure Synapse Analytics, Azure SQL Database, or Azure Machine Learning, as needed.
SKILLS & QUALIFICATIONS
6+ Years of experience working as a Data Engineer
Strong proficiency in Apache Spark and the Azure Databricks platform.
Experience with programming using Python, Spark and SQL.
Knowledge of data modeling, ETL processes, and data warehousing concepts.
Familiarity with cloud computing concepts and Azure services.
Understanding of data security and privacy considerations.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Experience with version control systems (e.g., Git) and CI/CD pipelines is a plus.
Full-time/Contract Positions
Show more
Show less","Apache Spark, Azure Databricks, Python, SQL, Data modeling, ETL, Data warehousing, Cloud computing, Azure services, Data security, Data privacy, Git, CI/CD, Data Lake Storage, Blob Storage, Delta lake","apache spark, azure databricks, python, sql, data modeling, etl, data warehousing, cloud computing, azure services, data security, data privacy, git, cicd, data lake storage, blob storage, delta lake","apache spark, azure databricks, azure services, blob storage, cicd, cloud computing, data lake storage, data privacy, data security, datamodeling, datawarehouse, delta lake, etl, git, python, sql"
Senior Data Platform Engineer,Mendel.ai,"San Jose, CA",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-mendel-ai-3767325025,2023-12-17,Livermore,United States,Mid senior,Onsite,"At Mendel, we harness AI to revolutionize healthcare. Our ambition is to ensure every patient's journey informs healthcare decisions, optimizing treatment plans and promoting drug discoveries. Through comprehensive analysis of patient health records, we aspire to deliver timely and precise care. If you share our zeal for healthcare advancement through technology, let's reshape the future together.
About The Role
We seek a highly skilled and experienced Data Platform Engineer to join our dynamic team. The ideal candidate will have a strong background in designing, building and maintaining AI/ML platforms for scale and resiliency.
You Will
Design, develop, and maintain an AI/ML platform to enable the AI to build LLM, Bert, and other models.
Build robust ML training and inference pipelines using MLFlow, Kubeflow, Ray, and Spark.
Ensure the performance, quality, and responsiveness of the models.
Collaborate with cross-functional teams to define, design, and ship new features.
Continuously discover, evaluate, and implement new technologies to maximize development efficiency.
You Have
Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field. Advanced degrees are preferred.
Minimum of 5 years of experience in building and maintaining AI/ML platforms using Kubernetes, MLFlow, Kubeflow, Ray, and Spark.
Strong knowledge of machine learning frameworks, libraries, data structures, data modeling, and software architecture.
Good understanding of machine learning algorithms, processes, tools, and platforms.
Experience with cloud services (GCP, AWS, Azure). Cloud-agnostic deployment experience is highly encouraged.
Excellent problem-solving skills and ability to work in a team environment.
Nice to Have
Experience with startup environments and agile development methodologies.
Strong communication skills with the ability to explain technical concepts to non-technical audiences.
Knowledge of data streaming technologies (e.g., Apache Kafka).
Experience with data visualization tools (e.g., Tableau, Power BI).
Certification in relevant data engineering technologies.
In this role, you will play a critical part in shaping our data infrastructure and ensuring that data is available, reliable, and ready for analysis. Your expertise in data engineering will contribute to our organization's data-driven decision-making and overall success.
Compensation
$170,000 - $230,000
Message to applicants applying to work in the U.S.:
When available, the salary range posted for this position reflects the projected hiring range for new hire salaries in U.S. locations. For non-sales roles, the hiring ranges reflect base salary and do not include bonuses, equity, or benefits. Hiring ranges for sales positions include base and incentive target, and do not include equity or benefits. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.
Why should you join our team:
At Mendel, we believe in the transformative potential of technology. We offer a dynamic and rewarding work environment, competitive compensation, and comprehensive benefits. If you share our passion for innovation and making a difference in healthcare, Mendel could be the place for you. Join us as we work to improve the future of healthcare.
Mendel is a very collaborative environment. You will be taking ownership of your work and collaborate directly with different teams to see it going into production and used by customers. At the same time, you will be mentored by world class AI scientists, software engineers, and clinical and business leaders. Hear from our team here.
Show more
Show less","LLM, Bert, MLFlow, Kubeflow, Ray, Spark, Kubernetes, Data engineering, Apache Kafka, Tableau, Power BI","llm, bert, mlflow, kubeflow, ray, spark, kubernetes, data engineering, apache kafka, tableau, power bi","apache kafka, bert, data engineering, kubeflow, kubernetes, llm, mlflow, powerbi, ray, spark, tableau"
Senior Azure Data Engineer - Contract,MathCo,San Francisco Bay Area,https://www.linkedin.com/jobs/view/senior-azure-data-engineer-contract-at-mathco-3777087033,2023-12-17,Livermore,United States,Mid senior,Onsite,"COMPANY OVERVIEW
MathCo's vision is to be the world’s largest problem solver. We enable viable and valuable data and analytics transformations for our clients. Our mission is to help Fortune 500 or similar organizations build core capabilities that set them on a path to achieve analytics self-sufficiency. We are transforming the way companies execute enterprise-wide data engineering and data science initiatives. With a holistic range of services, products & platforms across analytics consulting, data engineering, data science and management consulting, we are disrupting the analytics services and products space. To help us achieve our objectives, we are looking for passionate and experienced practitioners to be part of our US organization and become a part of the growth story for one of the fastest growing AI/ML companies in the world.
ROLE DESCRIPTION
Design, develop, and implement scalable and efficient data pipelines using Apache Spark on the Azure Databricks platform.
Transform raw data into a format suitable for analysis and reporting.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
Set up and configure Azure Databricks clusters to optimize performance and cost.
Monitor and troubleshoot issues related to the Azure Databricks environment.
Implement security measures to protect sensitive data within the platform.
Integrate data from various sources, both batch and real-time, into Azure Databricks.
Work with different data formats and storage solutions, such as Azure Data Lake Storage, Azure Blob Storage, Delta lake, or other Azure-based data services.
Design and implement data models to support business requirements.
Develop and maintain ETL processes for efficient data movement and transformation.
Optimize Spark jobs and queries for performance and resource utilization.
Identify and resolve bottlenecks in data processing pipelines.
Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders.
Communicate effectively to convey technical concepts to non-technical stakeholders.
Use version control systems (e.g., Git) to manage codebase changes.
Create and maintain documentation for data pipelines, data models, and other relevant artifacts.
Implement data quality checks to ensure the accuracy and reliability of data.
Adhere to data governance policies and best practices.
Proficiency in programming using Python and automation tasks.
Identify and resolve issues related to data processing, data movement, and platform configuration.
Perform debugging and troubleshooting of data pipeline failures.
Integrate Azure Databricks with other Azure services, such as Azure Synapse Analytics, Azure SQL Database, or Azure Machine Learning, as needed.
SKILLS & QUALIFICATIONS
6+ Years of experience working as a Data Engineer
Strong proficiency in Apache Spark and the Azure Databricks platform.
Experience with programming using Python, Spark and SQL.
Knowledge of data modeling, ETL processes, and data warehousing concepts.
Familiarity with cloud computing concepts and Azure services.
Understanding of data security and privacy considerations.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Experience with version control systems (e.g., Git) and CI/CD pipelines is a plus.
Show more
Show less","Apache Spark, Azure Databricks, Python, SQL, Data modeling, ETL, Data warehousing, Cloud computing, Azure services, Data security, Data privacy, Analytical skills, Problemsolving skills, Communication skills, Collaboration skills, Git, CI/CD pipelines, Delta Lake, Azure Data Lake Storage, Azure Blob Storage, Azure Synapse Analytics, Azure SQL Database, Azure Machine Learning","apache spark, azure databricks, python, sql, data modeling, etl, data warehousing, cloud computing, azure services, data security, data privacy, analytical skills, problemsolving skills, communication skills, collaboration skills, git, cicd pipelines, delta lake, azure data lake storage, azure blob storage, azure synapse analytics, azure sql database, azure machine learning","analytical skills, apache spark, azure blob storage, azure data lake storage, azure databricks, azure machine learning, azure services, azure sql database, azure synapse analytics, cicd pipelines, cloud computing, collaboration skills, communication skills, data privacy, data security, datamodeling, datawarehouse, delta lake, etl, git, problemsolving skills, python, sql"
Data Center Engineer,World Wide Technology,"Santa Clara County, CA",https://www.linkedin.com/jobs/view/data-center-engineer-at-world-wide-technology-3768724929,2023-12-17,Livermore,United States,Mid senior,Onsite,"World Wide Technology is looking for a Data Center Engineer . This job is part of WWT’s Strategic Resourcing services. The candidate will be supporting a WWT customer and will be employed by one of WWT’s preferred partners.
World Wide Technology Holding Co, LLC. (WWT) has an opportunity available for a
Data Center Engineer
to help the customer drive their enterprise operations.
Number of roles open:
1
Location:
Santa Clara, CA *Candidate must sit Onsite!*
Contract Duration:
6 month to start – ongoing support past 6 months
Job Description
Our client is working on their existing data center and they need technical help to make sure the servers are connected correctly and that they are pulling addresses as they should.
Qualifications include:
Experience in large scale data center environments and/or infrastructure (Power, Space, Cooling, Equipment) including rack and stack.
HPC server install
Managing daily operational duties such as error checking, inventory management, goods receiving, remote hands response and escorting of visitors.
Investigating and resolving hardware issues by replacement of end-user serviceable parts such as hard-drives, motherboard trays, RAM etc.
Managing the resolution of complex hardware issues through vendor support agreements.
Investigating, managing and resolving copper and fiber cabling issues.
Performing datacenter change activity such as installing or moving servers, network switches and associated cabling.
Copper and fiber cabling installation (loose, not structured), labeling and dressing for all datacenter hardware.
BIOS, console and basic Linux OS investigation or configuration.
Script deployment and asset audits.
Updating and maintaining capacity and configuration management data.
Responding to and performing workow tool assigned tasks.
Experience with: Supermicro, Dell, HPE Servers, Infiniband Fabric and Mellanox, Arista Hardware
Solid skills in UNIX (Ubuntu or RedHat) administration and knowledge of basic commands
A technical background and a logical approach to problem solving.
Extensive experience installing and trouble-shooting enterprise-level servers, storage and networking equipment.
A good understanding of datacenter infrastructure such as structured cabling, power distribution, cooling techniques and system resiliency.
Experience installing large volumes of data cabling to a high standard of presentation.
An ability to work independently and with others.
Basic Linux and networking knowledge.
Proficiency in MS Office suite, particularly Excel.
A strong desire to achieve and maintain high standards.
Show more
Show less","Data Center Engineer, Server Installation, Error Checking, Inventory Management, Goods Receiving, Remote Hands Response, Hardware Troubleshooting, Vendor Support, Cabling (Copper and Fiber), Datacenter Change Activity, BIOS Configuration, Script Deployment, Asset Audits, Capacity and Configuration Management, Workow Tool, Supermicro, Dell, HPE Servers, Infiniband Fabric, Mellanox, Arista Hardware, UNIX (Ubuntu or RedHat), Linux, Networking, Microsoft Office Suite (Excel)","data center engineer, server installation, error checking, inventory management, goods receiving, remote hands response, hardware troubleshooting, vendor support, cabling copper and fiber, datacenter change activity, bios configuration, script deployment, asset audits, capacity and configuration management, workow tool, supermicro, dell, hpe servers, infiniband fabric, mellanox, arista hardware, unix ubuntu or redhat, linux, networking, microsoft office suite excel","arista hardware, asset audits, bios configuration, cabling copper and fiber, capacity and configuration management, data center engineer, datacenter change activity, dell, error checking, goods receiving, hardware troubleshooting, hpe servers, infiniband fabric, inventory management, linux, mellanox, microsoft office suite excel, networking, remote hands response, script deployment, server installation, supermicro, unix ubuntu or redhat, vendor support, workow tool"
Staff Data Engineer - Data Platform,Tenstorrent Inc.,"Santa Clara, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-data-platform-at-tenstorrent-inc-3752002360,2023-12-17,Livermore,United States,Mid senior,Onsite,"Tenstorrent is leading the industry on cutting-edge AI technology, revolutionizing performance expectations, ease of use, and cost efficiency. With AI redefining the computing paradigm, solutions must evolve to unify innovations in software models, compilers, platforms, networking, and semiconductors. Our diverse team of technologists have developed a high performance RISC-V CPU from scratch, and share a passion for AI and a deep desire to build the best AI platform possible. We value collaboration, curiosity, and a commitment to solving hard problems. We are growing our team and looking for contributors of all seniorities.
As a Staff Data Engineer in our Data Science and Engineering team, you will help design, implement and manage the data platform which powers Tenstorrent’s data engineering and data science workflows, including batch processing, streaming, metadata management, and machine learning. The ideal candidate for this role excels at building software automations running a modern and scalable data infrastructure and its applications. You see the data platform as a product and the data integrations, analytics applications, data science and ML as customers. You highly appreciate the importance of IaC, CI/CD, monitoring and logging, security and documentation to achieve the highest levels of automation, quality and functionality.
Responsibilities
Help envision and build the future of our data platform. You will have a remarkable impact on the overall tech stack we use to process the rich data available at Tenstorrent.
Design and implement end-to-end data pipelines to process batch and streaming data at scale, employing cutting-edge data processing and orchestration methodologies, including distributed computing solutions when applicable.
Lead the integration of a wide range of technologies constituting our data infrastructure. Architect and build new ones when available solutions fall short.
Identify, evolve, and champion data/software engineering best practices.
Help define and implement scalable data governance, security and privacy solutions, including an AI-powered data catalog.
Build automation for CI/CD, IaC and containerization.
Contribute to our data engineering roadmap, working cross-functionally to define priorities.
Experience & Qualifications
Excellent Python programming skills, with a focus on data processing.
Experience designing, managing and optimizing SQL databases and schemas.
Advanced SQL queries and functions programming.
Experience working with no-SQL and time-series databases.
Experience building RESTful APIs.
Proficiency in containerization technologies.
Experience building automated CI/CD pipelines.
Experience working with AWS.
Familiarity with IaC technologies.
Familiarity with gRPC.
Experience deploying front-end data analytics tools.
Experience with distributed processing and querying solutions is a plus.
Comfortable using Agile development methodologies and industry standard software development lifecycle processes (Jira, Git, documentation).
Self-motivated individual with a strong desire to learn, explore, and excel in a fast-paced, innovative environment.
Compensation for all engineers at Tenstorrent ranges from $100k - $500k including base and variable compensation targets. Experience, skills, education, background and location all impact the actual offer made.
Show more
Show less","Python, SQL, NoSQL, Timeseries databases, RESTful APIs, Containerization technologies, CI/CD pipelines, AWS, IaC, gRPC, Frontend data analytics tools, Distributed processing, Distributed querying solutions, Agile development methodologies, Jira, Git","python, sql, nosql, timeseries databases, restful apis, containerization technologies, cicd pipelines, aws, iac, grpc, frontend data analytics tools, distributed processing, distributed querying solutions, agile development methodologies, jira, git","agile development methodologies, aws, cicd pipelines, containerization technologies, distributed processing, distributed querying solutions, frontend data analytics tools, git, grpc, iac, jira, nosql, python, restful apis, sql, timeseries databases"
Senior Azure Data Engineer / Data Modeler,First Soft Solutions LLC,"Fremont, CA",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-data-modeler-at-first-soft-solutions-llc-3634104941,2023-12-17,Livermore,United States,Mid senior,Onsite,"We are actively hiring for
Senior Azure Data Engineer / Data Modeler
Role: Senior Azure Data Engineer / Data Modeler
Location: Fremont CA ( Remote)
Type: 12 months+
C2C
Experience required: 10+ Years
Client will consider only local candidates
Must Have
Enterprise Data modeling / Design
Azure SQL Data Warehouse (Synapse)
Azure synapse serverless pool
T-SQL ( Hands on experience , writing quires , building stored procs, performance optimization , etc..)
ADF / Any Enterprise ETL Tool
ADLS / any cloud storage
Should be able to understand the requirements and convert them into technical specifications , also able to work independently with minimal or no supervision.
Nice To Have
NoSQL databases ( preferred Cosmos)
Any scripting language ( Python preferred)
Denodo / PowerBI
Databricks
Please share profiles to *************
732 609 2952
Show more
Show less","Azure SQL Data Warehouse, Azure synapse serverless pool, TSQL, ADF, ADLS, NoSQL databases, Python, Denodo, PowerBI, Databricks","azure sql data warehouse, azure synapse serverless pool, tsql, adf, adls, nosql databases, python, denodo, powerbi, databricks","adf, adls, azure sql data warehouse, azure synapse serverless pool, databricks, denodo, nosql databases, powerbi, python, tsql"
"Senior Data Science Engineer, LLM MLOps Platform",NVIDIA,"Santa Clara, CA",https://www.linkedin.com/jobs/view/senior-data-science-engineer-llm-mlops-platform-at-nvidia-3657602174,2023-12-17,Livermore,United States,Mid senior,Onsite,"NVIDIA is hiring a Senior Data Science Engineer to onboard ML teams on our unified LLM MLOps platform by developing ML pipelines and building the platform in a way that supports a wide range of diverse use cases. NVIDIA is in an outstanding position: we are developing AI-based products across multiple domains. Our vision is to bring to bear the MLOps platform we developed for Autonomous Vehicles and apply it to other use cases: NLP, speech, recommendation systems, computer vision, healthcare. To achieve this, we will embed ourselves into ML teams to help them develop their MLOps pipeline on to a single methodology that uses that unified platform, recognize what is common to all use cases and what is domain-specific and develop MLOps tools and features. The members of that team will realize the MLOps strategy for NVIDIA and understand both MLOps production pipelines and MLOps infrastructure in order to help us build what could be the best and most comprehensive MLOps platform in the world.
What You'll Be Doing
We expect you to analyze the datasets, raise and validate hypotheses, extract meaningful features and build models on top of them.
Data science project involve various types of tasks, ranging from LLM prompt, response, and chat analysis to document retrieval data and analysis, metadata collection and augmentation using machine learning techniques, LLM data sample categorization using prompt engineering techniques, product intelligence from production data, outlier/noise detection and much more.
Optimize your algorithms and data structures to make them applicable to large datasets and cluster-based processing. Perfect the models and algorithms until they reach the desired accuracy.
Prepare documentation for the proposed approaches, policies, data formats, test cases and the expected results within the scope of your projects.
What We Need To See
Bachelor's or equivalent experience
5+ years of industrial experience
Ability to write readable and maintainable code (primarily in Python/PySpark), knowledge of scientific libraries (Numpy, SciPy, Pandas, scikit-learn)
Experience with extracting data from storage systems (e.g. Hive, Cassandra, S3, Swiftstack) and understanding of how big data processing systems (e.g. Spark or Map/Reduce) work and help scale the processes.
Experience in solving problems using machine learning techniques (statistics, clustering, classification, outlier analysis, etc.). Experience with Deep Learning and LLM prompt engineering or prompt tuning is a plus. Strong background in taking care of time series data and hypothesis validation is desired.
Stress resistance, strong collaboration, interpersonal, presentation and reporting skills should be your strong sides. You are skilled and eager--a self-educated person with strong self-management skills. Be proactive in proposing innovative approaches, algorithms. Don't be scared of diving into scientific articles to come back with a fresh solution!
Strong communication skills: upper-intermediate oral and written technical English is required
Ways To Stand Out From The Crowd
Being a champion of user privacy first when handling user data for product analysis and machine learning model development
Experience utilizing differential privacy techniques for processing user data and developing machine learning models
With highly competitive salaries and a comprehensive benefits package, NVIDIA is widely considered to be one of the technology industry's most desirable employers. We have some of the most forward-thinking and hardworking people in the world working with us and our engineering teams are growing fast in some of the hottest innovative fields: Deep Learning, Artificial Intelligence, and Large Language Models. If you're a creative engineer with a real passion for robust and enjoyable user experiences, we want to hear from you!
The base salary range is 144,000 USD - 270,250 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","Python, PySpark, Numpy, SciPy, Pandas, scikitlearn, Hive, Cassandra, S3, Swiftstack, Spark, Map/Reduce, Deep Learning, LLM","python, pyspark, numpy, scipy, pandas, scikitlearn, hive, cassandra, s3, swiftstack, spark, mapreduce, deep learning, llm","cassandra, deep learning, hive, llm, mapreduce, numpy, pandas, python, s3, scikitlearn, scipy, spark, swiftstack"
Data Engineer 4,Russell Tobin,"San Jose, CA",https://www.linkedin.com/jobs/view/data-engineer-4-at-russell-tobin-3744952264,2023-12-17,Livermore,United States,Mid senior,Onsite,"What are we looking for in our Data Engineer 4?
Title: -Senior Data Science Engineer (P4)
Duration 11+months
Location CST/PST
Skills:
Strong proficiency in writing complex queries and manipulating large datasets using SQL-like languages on the cloud - SQL, Python, Spark, Databricks, Airflow, Azure, AWS, etc.
Proficiency in ML/Data Science/Statistical Modeling languages such as Python, R to build Propensity Models, Customer Segmentation, Churn Scores, Forecast Models etc.
Proficiency in ETL/ELT
Experience with design, development, and implementation of highly scalable, high-volume data architectures, source of truth systems for different business areas, developing and maintaining in an agile environment.
Strong fundamentals in Quantitative and Statistical methods as well as demonstrated proficiencies in data visualizations like Power BI / Tableau
Strong Project Management, Communication, Leadership, Networking skills to support multiple ongoing projects in a fast-paced environment.
Ability to support multiple ongoing projects in a fast-paced environment.
Strong background in data warehousing, data modeling, data access, and data storage techniques.
Education & Experience:
B.S. or M.Sc. or Ph.D. in an analytical field: statistics, applied mathematics, computer science, engineering, economics, physics, etc., or equivalent practical experience.
5+ years of proficiency in Python and SQL.
3+ years of expertise with modern data science workflows on the cloud (Git, Jupyter, Python, AWS, Azure).
Nice to have:
Experience crafting dashboards in Power BI and/or Tableau
Expert in one or more data science tools such as Pandas, NumPy, etc.
Experience with Databricks
Experience with building effective Presentations for all levels
Experience building data processing machine learning models in a production environment.
Familiarity with ML for propensity estimation and causal inference with observational data or experimental designs.
Rate/Salary: $80-$84
Show more
Show less","SQL, Python, Spark, Databricks, Airflow, Azure, AWS, ML, Data Science, Statistical Modeling, R, Propensity Models, Customer Segmentation, Churn Scores, Forecast Models, ETL/ELT, Data Architectures, Source of Truth Systems, Agile, Quantitative Methods, Statistical Methods, Data Visualization, Power BI, Tableau, Project Management, Communication, Leadership, Networking, Data Warehousing, Data Modeling, Data Access, Data Storage, Git, Jupyter, Pandas, NumPy, Presentations, Machine Learning, Propensity Estimation, Causal Inference, Observational Data, Experimental Designs","sql, python, spark, databricks, airflow, azure, aws, ml, data science, statistical modeling, r, propensity models, customer segmentation, churn scores, forecast models, etlelt, data architectures, source of truth systems, agile, quantitative methods, statistical methods, data visualization, power bi, tableau, project management, communication, leadership, networking, data warehousing, data modeling, data access, data storage, git, jupyter, pandas, numpy, presentations, machine learning, propensity estimation, causal inference, observational data, experimental designs","agile, airflow, aws, azure, causal inference, churn scores, communication, customer segmentation, data access, data architectures, data science, data storage, databricks, datamodeling, datawarehouse, etlelt, experimental designs, forecast models, git, jupyter, leadership, machine learning, ml, networking, numpy, observational data, pandas, powerbi, presentations, project management, propensity estimation, propensity models, python, quantitative methods, r, source of truth systems, spark, sql, statistical methods, statistical modeling, tableau, visualization"
"Staff Data Engineer, Quality Data Analytics & Systems",Tesla,"Fremont, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-quality-data-analytics-systems-at-tesla-3733235200,2023-12-17,Livermore,United States,Mid senior,Onsite,"What To Expect
The Quality Engineering team plays a key role in ensuring safety of our customers by providing world class Quality of our products. Quality Systems and Analytics team is looking for a key player on the team who can help drive Quality data analytics and help cross-functional engineering organizations to provide opportunities in product quality improvements. Candidate should have experience working with large data sets, finding best ways to engineer the data to help create critical KPI metrics, building innovative visualizations and dashboards all the while keeping in mind what improvements can be driven in underlying data systems.
Tesla's mission is to accelerate the world's transition to sustainable energy. We are committed to hiring the world's best and brightest people to help make this future a reality. Every Tesla is designed to be the safest, quickest car in its class—with industry-leading safety, range, and performance.
What You'll Do
Lead a team of Data Analysts/Engineers to prioritize and efficiently tackle critical/new/existing data engineering/data analytics projects utilizing JIRA, agile project management methodology
Motivate team while challenging them to grow in their skillsets and career
Provide actionable and concise updates on Quality data analytics/engineering projects/products to Senior Leadership
Lead cross-functional problem-solving teams to drive process systems, analytics, data engineering improvements; use of effective QA methodologies to manage and facilitate issue resolution including root cause investigations and the development, implementation and monitoring of effective corrective and preventive actions
Establish and maintain guidelines for analytics/data engineering, QA, change management and metric/project documentation best practices and ensure team adheres to those standards
Promote and maintain a culture and attitude of continuous improvement
Design and implement metrics, applications and tools that will enable engineers by allowing them to self-serve their data insights
Analyze manufacturing, equipment, and vehicle data to extract useful statistics and insights about failures in order to drive meaningful improvements to production quality and customer experience
Interpret trends in manufacturing, equipment, and vehicle data, analyzing results using statistical techniques and depicting the story via dashboards and reports
Automate analyses and author pipelines using SQL, Python, Airflow, and Kubernetes based ETL frameworks
Monitor key product metrics, understanding root causes of changes in metrics
Drive underlying data systems improvement by working with key cross-functional stakeholders
Work effectively with engineers and conduct end-to-end analyses, from data requirement gathering to data processing and modeling
Contribute to all stages of the factory and service quality data modeling including but not limited to problem formulation, data pre-processing, feature engineering, sample design, algorithm selection and evaluation, hyper-parameter tuning, deployment, implementation, and monitoring
Work with management to prioritize business and information needs
What You'll Bring
BS/MS in Management Information Systems, Computer Science, Math, Physics, Engineering, Statistics or another technical field. Equivalent experience also acceptable
5-6+ years of work experience in data analytics or engineering related field
Strong knowledge of SQL and experience with multiple data architecture paradigms (MySQL, MicrosoftSQL, Vertica, Oracle, kafka, Spark) with proficiency in Python, text processing, and python data analysis packages (eg. pandas, numpy)
Strong knowledge of data visualization techniques and tools using Tableau, Power BI, Superset, Matplotlib, Plotly etc
Strong understanding of various statistical techniques to effectively summarize data findings with strong knowledge of data warehousing concepts as well as data mining tools and techniques
Proficiency in using open-source ML toolkits (eg. Pytorch, Tensorflow) and building NLP applications, with Strong ML and NLP fundamentals
Strong knowledge of manipulating big data with Apache Spark (Python or Scale APIs)
Able to work under pressure while collaborating with cross-functional teams and managing competing demands with tight deadlines
Excellent communication skills and ability to work with different business stakeholders to understand, identify, and translate business challenges into data projects
A passion and curiosity for data and data-driven decision making
Benefits
Compensation and Benefits
Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire:
Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction
Family-building, fertility, adoption and surrogacy benefits
Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution
Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA
Healthcare and Dependent Care Flexible Spending Accounts (FSA)
LGBTQ+ care concierge services
401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits
Company paid Basic Life, AD&D, short-term and long-term disability insurance
Employee Assistance Program
Sick and Vacation time (Flex time for salary positions), and Paid Holidays
Back-up childcare and parenting support resources
Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance
Weight Loss and Tobacco Cessation Programs
Tesla Babies program
Commuter benefits
Employee discounts and perks program
Expected Compensation
$80,000 - $258,000/annual salary + cash and stock awards + benefits
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
Tesla
Show more
Show less","Data Engineering/Analytics, JIRA, Agile Project Management, Quality Data Analytics, Tableau, Power BI, Superset, Matplotlib, Plotly, SQL, Python, Pandas, Numpy, Text Processing, Statistical Techniques, Data Visualization, Data Warehousing, Data Mining, Apache Spark, Pytorch, Tensorflow, NLP, Machine Learning, MySQL, MicrosoftSQL, Vertica, Oracle, Kafka","data engineeringanalytics, jira, agile project management, quality data analytics, tableau, power bi, superset, matplotlib, plotly, sql, python, pandas, numpy, text processing, statistical techniques, data visualization, data warehousing, data mining, apache spark, pytorch, tensorflow, nlp, machine learning, mysql, microsoftsql, vertica, oracle, kafka","agile project management, apache spark, data engineeringanalytics, data mining, datawarehouse, jira, kafka, machine learning, matplotlib, microsoftsql, mysql, nlp, numpy, oracle, pandas, plotly, powerbi, python, pytorch, quality data analytics, sql, statistical techniques, superset, tableau, tensorflow, text processing, vertica, visualization"
"Sr. Scala Engineer, Database Engineering",Experfy,"San Jose, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590299712,2023-12-17,Livermore,United States,Mid senior,Onsite,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Apache Spark, Apache Arrow, Scala, Distributed SQL, Data science, Database optimization, Query planner, Execution engine, RPC, Data warehouse, Web3, SQL, Query optimization, Parallel computing, Fault tolerance, Meta data, HTAP database, Analytics, Data acquisition, Data processing, Data engineering, Data management, Blockchain, Decentralization","apache spark, apache arrow, scala, distributed sql, data science, database optimization, query planner, execution engine, rpc, data warehouse, web3, sql, query optimization, parallel computing, fault tolerance, meta data, htap database, analytics, data acquisition, data processing, data engineering, data management, blockchain, decentralization","analytics, apache arrow, apache spark, blockchain, data acquisition, data engineering, data management, data processing, data science, database optimization, datawarehouse, decentralization, distributed sql, execution engine, fault tolerance, htap database, meta data, parallel computing, query optimization, query planner, rpc, scala, sql, web3"
Senior Data Analyst,"FocusKPI, Inc.","San Jose, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-focuskpi-inc-3787726472,2023-12-17,Livermore,United States,Mid senior,Onsite,"FocusKPI is looking for a Senior Data Analyst to work for our client.
Work Location:
Hybrid (2 days onsite), candidate needs to be based in Bay Area
Duration:
12 months with potential for extension depending on budget and performance
Pay range:
$50/hour to $70/hour
Responsibilities:
Work closely with the SEO Analytics and Performance team
Leverage new techniques to investigate our data and incorporate external information to boost our knowledge of this market
Research and implement content recommendations for organic SEO success
Track, report, and analyze website analytics
Communicate complicated data findings clearly and concisely to our multi-disciplined teams
Collaborate with stakeholders, including product, marketing, finance, etc. on initiatives that improve our company-level KPIs and, match our business priorities, and provide valuable data to the overall strategy
Provide analytics insights about customer behavior, web user behavior
Ad-hoc analysis to address urgent high-impact analysis for business stakeholders
Qualifications:
Bachelor's degree, or foreign equivalent, in Computer Science, Data Analytics, Engineering, or a closely related field and MS degree preferred
6+ years of industry experience with analysis experience to build data-driven products for solving business problems
Proficient in SQL and/or other data query languages. Experience in handling large-scale data with efficiency
Proven SEO experience as evidenced by results achieved in previous roles
Proficient in scripting languages such as Python and data visualization tools like Tableau
Proficient in Microsoft Excel
Proven strong analytical and problem-solving skills
Experience with campaign analytics and promotions
Experience with eCommerce web-based products and companies
Thank you!
FocusKPI Hiring Team
Founded in 2010, FocusKPI, Inc. (FocusKPI) is a data science and technology firm specializing in predictive analytics practice and methodologies. FocusKPI is a US company headquartered in Silicon Valley, California, with an East Coast office in Boston, Massachusetts.
NOTICE:
Please be aware of fraudulent emails regarding job postings, job offers and fake checks. FocusKPI's recruiting team will strictly reach out via @focuskpi.com email domain. If you have received fraudulent emails now or in the past, please report it to
The domain @focuskpijobs.com is fraudulent and not related to FocusKPI. Please do not not reply or communicate to anyone with @focuskpijobs.com.
Powered by JazzHR
Kxy5t3437S
Show more
Show less","Data Analysis, SQL, Python, Tableau, Microsoft Excel, Data Visualization, SEO, Campaign Analytics, Web Analytics, eCommerce, Predictive Analytics, Business Intelligence, ProblemSolving, DataDriven Decision Making, Statistics, Machine Learning, Data Mining, Data Warehousing, Data Integration, Data Modeling, Data Governance, Data Security","data analysis, sql, python, tableau, microsoft excel, data visualization, seo, campaign analytics, web analytics, ecommerce, predictive analytics, business intelligence, problemsolving, datadriven decision making, statistics, machine learning, data mining, data warehousing, data integration, data modeling, data governance, data security","business intelligence, campaign analytics, data governance, data integration, data mining, data security, dataanalytics, datadriven decision making, datamodeling, datawarehouse, ecommerce, machine learning, microsoft excel, predictive analytics, problemsolving, python, seo, sql, statistics, tableau, visualization, web analytics"
Principal Data Engineer,Archer,"San Jose, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-archer-3783986532,2023-12-17,Livermore,United States,Mid senior,Onsite,"Archer is an aerospace company based in San Jose, California building an all-electric vertical takeoff and landing aircraft with a mission to advance the benefits of sustainable air mobility. We are designing, manufacturing, and operating an all-electric aircraft that can carry four passengers for 100 miles at speeds of up to 150 mph while producing minimal noise.
Our sights are set high and our problems are hard, and we believe that diversity in the workplace is what makes us smarter, drives better insights, and will ultimately lift us all to success. We are dedicated to cultivating an equitable and inclusive environment that embraces our differences, and supports and celebrates all of our team members.
What You’ll Do
As a Principal Data Engineer in our Data Architecture team, you will play a pivotal role in shaping the data landscape for Archer's product and enterprise initiatives. You will collaborate closely with cross-functional teams to ensure data is accessible, secure, and optimized for advanced analytics. Your responsibilities will include:
Contribute to the design and implementation of a data architecture strategy that outlines principles, standards, and guidelines for data management.
Establish data governance practices, including data stewardship, data quality management, and compliance measures.
Develop data integration frameworks and pipelines to support seamless data movement across the organization.
Design, create, and maintain data lakes and data warehouses to store and manage structured and unstructured data.
Drive company-wide standardization for data cataloging and metadata management to enable data discovery and understanding.
Orchestrate the creation, maintenance, and optimization of data pipelines, ensuring alignment with business systems and security protocols.
Collaborate with internal teams and external partners to ensure data pipelines are robust, scalable, and efficient.
Collaborate with cross-functional teams to identify key data sources, data needs, and priorities for analytics, including technology selection.
Implement data governance policies and security measures to protect sensitive information and ensure data quality.
Champion a data-driven culture across the organization, fostering data literacy and analytical capabilities.
Identify opportunities to leverage artificial intelligence and machine learning in data analytics and reporting.
Develop protocols for managing company intellectual property (IP) within AI/ML models to safeguard proprietary information.
Work with the Data Analytics team to integrate ML modeling toolkits into the low/no code platform to enable seamless development and deployment of machine learning models.
Document existing data architectures and data flows to ensure transparency and knowledge sharing.
Provide guidance and expertise on data engineering best practices, standards, and techniques.
What You Need
Minimum of 15 years of experience in data engineering, architecture, and integration.
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Proficiency in data technologies and platforms, including data lakes, data warehouses, ETL processes, and data governance.
Strong programming skills in languages such as Python, SQL, and any other relevant languages that facilitate data engineering.
Experience with REST APIs and data integration techniques.
Knowledge of data security best practices and compliance requirements.
Familiarity with artificial intelligence and machine learning concepts is a plus.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Strong communication and collaboration skills to work effectively with cross-functional teams.
At Archer we aim to attract, retain, and motivate talent that possess the skills and leadership necessary to grow our business. We drive a pay-for-performance culture and reward performance that supports the Company’s business strategy. For this position we are targeting a base pay between $163,200-$204,000. Actual compensation offered will be determined by factors such as job-related knowledge, skills, and experience.
Archer is committed to working with and providing reasonable accommodations to job applicants with physical or mental disabilities, and those with sincerely held religious beliefs. Applicants who may require reasonable accommodation for any part of the application or hiring process should provide their name and contact information to Archer’s People Team at people@archer.com . Reasonable accommodations will be determined on a case-by-case basis.
Archer is proud to be an Equal Opportunity employer committed to diversity and inclusivity in the workplace. All aspects of employment are decided on the basis of merit, qualifications, and business needs. We do not discriminate based upon race, color, religion, sex, sexual orientation, age, national origin, disability status, protected veteran status, gender identity or any other characteristic protected by federal, state or local laws.
Show more
Show less","Data architecture, Data engineering, Data governance, Data integration, Data management, Data pipelines, Data security, Data stewardship, Data warehousing, Data lakes, Database management, ETL, Data quality management, SQL, Python, REST APIs, Data visualization, Artificial intelligence, Machine learning, Cloud computing, Data analytics, Business intelligence, Data mining","data architecture, data engineering, data governance, data integration, data management, data pipelines, data security, data stewardship, data warehousing, data lakes, database management, etl, data quality management, sql, python, rest apis, data visualization, artificial intelligence, machine learning, cloud computing, data analytics, business intelligence, data mining","artificial intelligence, business intelligence, cloud computing, data architecture, data engineering, data governance, data integration, data lakes, data management, data mining, data quality management, data security, data stewardship, dataanalytics, database management, datapipeline, datawarehouse, etl, machine learning, python, rest apis, sql, visualization"
Software Engineer - Data Transmission,ByteDance,"San Jose, CA",https://www.linkedin.com/jobs/view/software-engineer-data-transmission-at-bytedance-3728590950,2023-12-17,Livermore,United States,Mid senior,Onsite,"Responsibilities
Founded in 2012, ByteDance's mission is to inspire creativity and enrich life. With a suite of more than a dozen products, including TikTok, Helo, and Resso, as well as platforms specific to the China market, including Toutiao, Douyin, and Xigua, ByteDance has made it easier and more fun for people to connect with, consume, and create content.
Why Join Us
Creation is the core of ByteDance's purpose. Our products are built to help imaginations thrive. This is doubly true of the teams that make our innovations possible.
Together, we inspire creativity and enrich life - a mission we aim towards achieving every day.
To us, every challenge, no matter how ambiguous, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always.
At ByteDance, we create together and grow together. That's how we drive impact - for ourselves, our company, and the users we serve.
Join us.
ByteDance Server platform team is responsible for architecting, designing, and building the best server and storage system to meet the requirements of high-performance, low cost and easy to operate. By joining this team, you will work with the best engineers and talents in this industry and have a broad opportunity to get in touch with the latest AI application system and newly emerged technology in computing , storage and silicon validation. You will gain remarkable hardware architect, development and validation experience in the most advanced hardware infrastructure at a massive scale.
We are looking for a Data Transmission Service Engineer with the following responsibilities:
Design of data transmission products.
Develop cloud solutions according on demand scenario.
Design Large-scale and high-concurrency system and various optimization studies (e.g. high availability/stability/performance/cost).
Qualifications
Bachelor/Master degree in computer-related fields.
Minimum of 4 years of working experience in Software development field.
Proficiency in one or more general-purpose programming languages, such as Golang, Python, C++, etc.
Demonstrated good coding practices, including code cleanliness, refactoring skills, and experience with unit testing.
Familiar with one or more databases, including relational databases, NoSQL databases, analytical databases, document databases, and vector databases, such as MySQL, PostgreSQL, Mongodb, Redis, Clickhouse, Cockroachdb, Tidb, Oceanbase, etc Familiar with database data replication mechanisms.
Preferred Qualifications
Experience with database kernel/database middleware/database management platform experience are preferred. (Commonly used databases such as MySQL, PostgreSQL, Redis, Mongodb, HBase, Clickhouse, etc.).
Familiarity with database log capture and analysis, message queue, ETL and other data processing technologies are preferred.
Strong motivation for technology, good communication skills and teamwork spirit, excellent problem analysis and problem-solving skills are preferred.
ByteDance is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At ByteDance, our mission is to inspire creativity and enrich life. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.
ByteDance Inc. is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at
dataecommerce.accommodations@bytedance.com
Job Information:
【For Pay Transparency】Compensation Description (annually)
The base salary range for this position in the selected city is $194000 - $380000 annually.
Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.
Benefits
Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees:
We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care.
Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability.
We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.
Show more
Show less","Data Transmission Service Engineer, Software development, Golang, Python, C++, Coding practices, Code cleanliness, Refactoring, Unit testing, Databases, Relational databases, NoSQL databases, Analytical databases, Document databases, Vector databases, MySQL, PostgreSQL, Mongodb, Redis, Clickhouse, Cockroachdb, Tidb, Oceanbase, Database kernel, Database middleware, Database management platform, Database log capture, Database analysis, Message queue, ETL, Data processing technologies","data transmission service engineer, software development, golang, python, c, coding practices, code cleanliness, refactoring, unit testing, databases, relational databases, nosql databases, analytical databases, document databases, vector databases, mysql, postgresql, mongodb, redis, clickhouse, cockroachdb, tidb, oceanbase, database kernel, database middleware, database management platform, database log capture, database analysis, message queue, etl, data processing technologies","analytical databases, c, clickhouse, cockroachdb, code cleanliness, coding practices, data processing technologies, data transmission service engineer, database analysis, database kernel, database log capture, database management platform, database middleware, databases, document databases, etl, golang, message queue, mongodb, mysql, nosql databases, oceanbase, postgresql, python, redis, refactoring, relational databases, software development, tidb, unit testing, vector databases"
Senior Data Center Network Engineer,Amtec Inc.,"San Jose, CA",https://www.linkedin.com/jobs/view/senior-data-center-network-engineer-at-amtec-inc-3741370110,2023-12-17,Livermore,United States,Mid senior,Remote,"Location: USA (remote)
Senior Data Center Networking Engineer with more than 10 years of experience.
Required Skills
Network Security Compliance
SDWAN
BGP
ACI
Load Balancing
Ansible coding
Python Coding
Expert in Routing and Switching
Network Troubleshooting
As a Senior Data Center Networking Engineer, you will have the opportunity to:
Collaborate with Engineering and business owners to define program requirements, set priorities, and establish scope which includes defining the roadmap and long-term strategy of the teams that you are partnering with.
Manage cross-functional dependencies, risks, and changes effectively by optimizing scope, schedule, and resources accordingly.
Develop and own communication plans to effectively and proactively communicate program status, issues, and risks to stakeholders.
Partner with cross-functional teams to drive compliance, technical review, design, development, testing, implementation, and post-implementation phases.
Define and track key metrics and key quality and performance indicators and drive cross-functional execution of program deliverables.
Proactively identify and analyze complex, long-term, critical infrastructure problems with engineering leaders and stakeholders.
Influence product decisions to align with higher company initiatives around privacy and security compliance.
Drive internal and external process improvements across multiple teams and functions, including reducing manual efforts through automation.
Experience
Experience with a large company in IT Security Compliance. 902-100, CMMC, DFARS
Education
Cisco CCNP Cisco CCIE
Show more
Show less","Data Center Networking, Network Security Compliance, SDWAN, BGP, ACI, Load Balancing, Ansible, Python, Routing, Switching, Network Troubleshooting, Crossfunctional dependencies, Program management, Communication plans, Compliance, Technical review, Design, Development, Testing, Implementation, Postimplementation phases, Key metrics, Key quality and performance indicators, Crossfunctional execution, Proactive problem identification, Product decisions, Process improvements, Automation, IT Security Compliance, 902100, CMMC, DFARS, CCNP, CCIE","data center networking, network security compliance, sdwan, bgp, aci, load balancing, ansible, python, routing, switching, network troubleshooting, crossfunctional dependencies, program management, communication plans, compliance, technical review, design, development, testing, implementation, postimplementation phases, key metrics, key quality and performance indicators, crossfunctional execution, proactive problem identification, product decisions, process improvements, automation, it security compliance, 902100, cmmc, dfars, ccnp, ccie","902100, aci, ansible, automation, bgp, ccie, ccnp, cmmc, communication plans, compliance, crossfunctional dependencies, crossfunctional execution, data center networking, design, development, dfars, implementation, it security compliance, key metrics, key quality and performance indicators, load balancing, network security compliance, network troubleshooting, postimplementation phases, proactive problem identification, process improvements, product decisions, program management, python, routing, sdwan, switching, technical review, testing"
Data Platform Engineer,Harnham,San Francisco Bay Area,https://www.linkedin.com/jobs/view/data-platform-engineer-at-harnham-3731475401,2023-12-17,Livermore,United States,Mid senior,Hybrid,"Data Platform Engineer
Hybrid- San Francisco, CA and Seattle, WA, Boston, MA
125,000-145,000K with bonus
This role cannot sponsor at this time.
The Company
I am working with a huge player in the biotech and pharma space who is looking for an experienced Data Platform Engineer to join their team.
THE ROLE
Responsibilities Will Include
Automating various data flows.
Support end-to-end code traceability and data provenance.
Partner with tech to modify tools as needed.
Build and manage reusable components and architecture s designed to make it both fast and easy to build robust, scalable, production-grade data products and services.
Your Skills Include
3+ years of relevant experience.
Experience with distributed data tools ( Spark, Kafka, Hive)
Experience with cloud platforms.
Experience with specialized data architecture.
Demonstrated excellence in writing production Python, Java, Scala, Go, and/or C#/C++
Experience building and designing a DevOps first way of working.
The Benefits
competitive salary plus benefits.
How To Apply
Please register your interest by sending your CV to Kyle Margolies via the Apply link on this page.
Show more
Show less","Spark, Kafka, Hive, Python, Java, Scala, Go, C#/C++, DevOps","spark, kafka, hive, python, java, scala, go, cc, devops","cc, devops, go, hive, java, kafka, python, scala, spark"
Senior Data Engineer,Rippling,San Francisco Bay Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-rippling-3764316291,2023-12-17,Livermore,United States,Mid senior,Hybrid,"About The Role
We are looking for a hands-on senior engineer to play a key role in Rippling’s data team. As a senior engineering resource on the data engineering team, you will be leading the design and development of systems that will enable analytics, experimentation, and user-facing features. You will be closely involved with multiple data adjacent teams and stakeholders, alongside helping junior talent in the team learn and grow.
The Data Engineering Team at Rippling is a combination of warehousing and data platform engineering, supporting a variety of orgs across the company (Data Science, Marketing, Bizops, Revops, Finance to name a few). Here’s an idea of some of the initiatives we’re working on:
A realtime, central data lake to operationalize warehouse data.
Creating a metrics layer to make reporting more efficient and accurate.
Building a catalog to make our data assets searchable and easy to discover.
Making our globally distributed data stack compliant and scalable
What You'll Do
Help architect, build, and scale our data pipelines from our OLTP database, other internal systems and third party tools to our warehouse (Snowflake)
Leverage data technologies like Kafka, Presto, Flink, Airflow, Mongo, Snowflake and Spark
Support reporting, data science, operations and machine learning functions
Create data platforms, data lakes, and data ingestion systems that work at scale
Define and enforce data quality checks and audits for code warehousing datasets
Define and support internal SLAs for core data sets
Qualifications
5+ Years experience in Data and Software Engineering
Expertise in writing complex data transforms in SQL and Python
Knowledge of data warehousing concepts around building custom ETL integrations, building data infrastructure (SCD,CDC,Snapshots,indexing,partitioning)
Knowledge on Data Security and Governance (nice to have)
Experience in analytics, dimensional modeling, and ETL optimization preferred
BS/BA in a technical field such as Computer Science or Mathematics preferred
Additional Information
Rippling is an equal opportunity employer. We are committed to building a diverse and inclusive workforce and do not discriminate based on race, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, veteran or military status, or any other legally protected characteristics, Rippling is committed to providing reasonable accommodations for candidates with disabilities who need assistance during the hiring process. To request a reasonable accommodation, please email accomodations@rippling.com
Rippling highly values having employees working in-office to foster a collaborative work environment and company culture. For office-based employees (employees who live within a 40 mile radius of a Rippling office), Rippling considers working in the office, at least three days a week under current policy, to be an essential function of the employee's role.
This role will receive a competitive salary + benefits + equity. The salary for US-based employees will be aligned with one of the ranges below based on location; see which tier applies to your location here.
A variety of factors are considered when determining someone’s compensation–including a candidate’s professional background, experience, and location. Final offer amounts may vary from the amounts listed below.
Show more
Show less","Data Engineering, SQL, Python, Kafka, Presto, Flink, Airflow, MongoDB, Snowflake, Spark, Data Architecture, Data Pipelines, Data Warehousing, ETL, Data Lakes, Data Ingestion, Data Quality, Data Audits, Data Security, Data Governance, Analytics, Dimensional Modeling, ETL Optimization","data engineering, sql, python, kafka, presto, flink, airflow, mongodb, snowflake, spark, data architecture, data pipelines, data warehousing, etl, data lakes, data ingestion, data quality, data audits, data security, data governance, analytics, dimensional modeling, etl optimization","airflow, analytics, data architecture, data audits, data engineering, data governance, data ingestion, data lakes, data quality, data security, datapipeline, datawarehouse, dimensional modeling, etl, etl optimization, flink, kafka, mongodb, presto, python, snowflake, spark, sql"
"Staff Data Engineer, AI",Recruiting from Scratch,San Francisco Bay Area,https://www.linkedin.com/jobs/view/staff-data-engineer-ai-at-recruiting-from-scratch-3778845572,2023-12-17,Livermore,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a
hybrid
role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What we’ll love about you
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What you'll love about us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data OPs, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Data Platforms, Frameworks, Large Data Processing, Realtime Data Processing, Batch Data Processing, NLP, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Classification, Data Retention, Data Management Tools, 401K Plan, Health Insurance, Dental Insurance, Vision Insurance, Paid Time Off, Flexible Vacation Policy, Cell Phone Stipend, Internet Stipend, Wellness Stipend, Food Stipend, HomeOffice Setup Stipend","data engineering, ml data ops, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, data platforms, frameworks, large data processing, realtime data processing, batch data processing, nlp, large language models, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data classification, data retention, data management tools, 401k plan, health insurance, dental insurance, vision insurance, paid time off, flexible vacation policy, cell phone stipend, internet stipend, wellness stipend, food stipend, homeoffice setup stipend","401k plan, airflow, aws, azure, bash, batch data processing, cell phone stipend, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data platforms, data retention, datamodeling, dental insurance, docker, dynamodb, etl, flexible vacation policy, food stipend, frameworks, gcp, git, health insurance, helm, homeoffice setup stipend, internet stipend, java, kafka, kubernetes, large data processing, large language models, machine learning, ml data ops, nlp, nosql databases, paid time off, pandas, python, r, realtime data processing, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, vision insurance, visualization, wellness stipend"
Associate Data Analyst,FedPoint,"Portsmouth, NH",https://www.linkedin.com/jobs/view/associate-data-analyst-at-fedpoint-3784857284,2023-12-17,Kittery,United States,Mid senior,Onsite,"📊 **Join Our Team as an Associate Data Analyst!** 📊
Are you a data enthusiast with an eye for detail and a passion for clear communication? FedPoint is seeking an Associate Data Analyst to join our Reporting & Data Analysis Department. This is your chance to contribute to our mission of maintaining accurate, meaningful, and strategically sound data.
Why Choose Us
Impactful Role
: This Junior will play a crucial role in maintaining data integrity, conducting trend analysis, and creating visual representations to drive data-driven decision-making.
Collaborative Environment:
Join a cohesive team that collaborates with various departments, ensuring that data is not just accurate but meaningful.
Growth Opportunities:
We value your professional development and encourage innovation in our dynamic work environment.
Key Responsibilities
Aggregate and assess data sets and performance metrics.
Conduct data quality assurance checks and resolve discrepancies.
Collaborate with senior team members to document metric methodology.
Assist with ad hoc data requests and communicate findings effectively.
Manage access to shared data and reports.
Qualifications
College coursework in mathematics, accounting, business, or related field preferred or 1+ years of data analysis experience, including data visualization.
Exposure to business requirements and documentation.
Experience with project management is a plus.
Skills And Competencies
Strong communication skills for client interaction.
Analytical mindset to identify trends and articulate them.
Familiarity with Microsoft tools, especially SharePoint and Excel.
Self-motivated and capable of working both independently and as part of a team.
Exceptional attention to detail and time management skills.
If you're ready to make an impact by maintaining data accuracy and driving effective decision-making, apply now and join us on our journey to success! 💼🚀
📍 Location: Portsmouth, NH – Hybrid role (2 days in office)
🕒 Full-Time Position M-F with core business hours
Compensation $20-24/hr based on experience
The
CORE
of who we are…
At
FedPoint
, innovation is at the core of our business model. We believe there's no such thing as big bang innovation. Rather, sustainable innovation that occurs organically during the daily pursuit of excellence is what enables us to provide world-class customer service.
We believe
in-person collaboration fosters creativity, innovation, effective teamwork, and critical personal connections. Located in Portsmouth, NH, we offer a hybrid model that strikes a balance by allowing face-to-face interactions and spontaneous brainstorming sessions that lead to better collaboration and problem-solving.
Benefits
In addition to working for a company with great people and excellent reputation,
what’s in it for you
?
Here’s just a sampling of some of the benefits that full-time members of the FedPoint community receive:
Generous 401k employer match: 100% of employee's contribution, up to a maximum of 6% salary, vests immediately.
Lots of paid time off – 3 weeks’ vacation, 7 sick days, 3 personal days, and 12 paid holidays!
Competitive benefits include health, dental, vision, disability insurance, life, legal and flexible spending accounts (FSA).
Generous tuition reimbursement program to support career goals.
Corporate giving and matching gifts program.
Volunteering program: Paid time to volunteer and company organized opportunities.
A wide variety of personal, professional, and career development programs are available.
Comprehensive wellness program offering a variety of resources and activities to help your wellbeing in the following areas: career, financial, mental, emotional, physical, social and community.
FedPoint is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, age, creed, marital status, familial status, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
Show more
Show less","Data analysis, Data visualization, Trend analysis, Data quality assurance, Data integrity, Project management, Microsoft tools, SharePoint, Excel, Communication, Analytical mindset, Attention to detail, Time management","data analysis, data visualization, trend analysis, data quality assurance, data integrity, project management, microsoft tools, sharepoint, excel, communication, analytical mindset, attention to detail, time management","analytical mindset, attention to detail, communication, data integrity, data quality assurance, dataanalytics, excel, microsoft tools, project management, sharepoint, time management, trend analysis, visualization"
Sr. Data Analyst,Bottomline,"Portsmouth, NH",https://www.linkedin.com/jobs/view/sr-data-analyst-at-bottomline-3750622455,2023-12-17,Kittery,United States,Mid senior,Hybrid,"Bottomline is at the forefront of digital transformation. We are a growing global market leader uniquely equipped to address the changing needs of how businesses pay and get paid. Our culture of
Working with and for each other
enables us to
delight our customers
. We empower our teams to
think like owners
driving customer delight, helping them grow their business and win in their markets.
We are looking for
Sr. Customer Data Analyst
to innovate, win, and grow with us in
Portsmouth, NH
or in a
Remote
capacity.
We are seeking a candidate that is data driven and passionate about expanding our business intelligence capabilities with our Customer Success, Support & Professional Services organizations. This role will work directly with our leadership team to design KPIs and compelling analytics to facilitate strategic decision making. The successful candidate will be capable of turning data into information, information into insight and insight into recommendations.
Bottomline has invested in best-in-class technologies to establish a robust analytical capability and this candidate will have an opportunity to help influence and drive the analytic roadmap for the Customer Operations organization.
This position can be based anywhere within the United States (eastern or central time zone)
How You’ll Contribute
Define, build, and track key performance indicators and metrics for Customer Operations team including finance/revenue, product adoption, customer support, and customer success.
Support leadership and end users with KPIs, activity tracking, and dashboards in Salesforce.com, Snowflake and PowerBI.
Work closely with data engineers and internal systems to define data requirements, build analyses, and develop a single source of truth for reporting.
Own delivery of analytic projects, ensuring that projects are delivered on-time with high quality.
Utilize statistical techniques and analytical skills to collect, organize, analyze, and visualize data.
Convert quantitative analysis into recommendations to business stakeholders.
Develop dashboards and frameworks to monitor business performance and KPIs.
Mentor and train internal team members on reporting, analysis, and business intelligence tools
Locate, define, and recommend process improvement opportunities to improve data quality.
Understand the concepts of machine learning and advance analytics to collaborate with Data Scientists to complete new projects.
What Will Make You Successful
3-5 years’ experience in data analytics
Experience with Power BI or a similar tool business intelligence tool such as Tableau or other
Experience working with SQL.
Proven ability to drive data-driven insights and actions in an ambiguous, evolving environment.
Ability to clearly and concisely articulate complex data analyses to non-technical stakeholders
Strong written and verbal communication skills
Experience with financial reporting
Advanced knowledge of Business Intelligence tools, with experience effectively defining data requirements and designing effective interactive dashboards
Ability to quickly learn and apply new analytical technologies.
Experience with Salesforce.com or Snowflake a plus
You’ll love Bottomline because in everything we do we seek to
delight our customers
and we are passionate about building a company of which we can all be proud, and this starts with building amazing teams filled with team members that challenge you every day.
Bottomline Technologies is proud to be an Equal Employment Opportunity and Affirmative Action Employer. All aspects of employment are based on
merit,
and we strive to
delight our customers
with a team of
highly motivated and talented people
that represent a diverse set of identities and backgrounds.
Start your
LI-DNI
Show more
Show less","Data Analytics, Power BI, Tableau, SQL, Machine Learning, Advanced Analytics, Salesforce.com, Snowflake, Business Intelligence tools, Financial Reporting, Data Requirements, Interactive dashboards, Data visualization","data analytics, power bi, tableau, sql, machine learning, advanced analytics, salesforcecom, snowflake, business intelligence tools, financial reporting, data requirements, interactive dashboards, data visualization","advanced analytics, business intelligence tools, data requirements, dataanalytics, financial reporting, interactive dashboards, machine learning, powerbi, salesforcecom, snowflake, sql, tableau, visualization"
BI Data Analyst,GTT,"Idaho, United States",https://www.linkedin.com/jobs/view/bi-data-analyst-at-gtt-3782542988,2023-12-17,Idaho,United States,Mid senior,Onsite,"As a GTT Data Analyst, you will be part of a dynamic environment helping internal and external clients solve complex business problems with insight data analysis by partnering with internal and external clients to evaluate the business needs with respect to GTT’s strategy and goals. We seek someone who genuinely enjoys connecting with people, developing a strong relationship with data owners to positively influence their journey in adopting and adhering to data governance policies and procedures.
We have a fun, remote-first culture that celebrates individual differences. We believe in and are committed to creating a diverse, equitable, and inclusive workplace. We value those who embrace adventure, subscribe to life-long learning, and take ownership and pride in their work. In our professional-yet-relaxed environment, we want everyone to enjoy what they do, seek balance outside of work, and lean into an entrepreneurial mindset to help us and our clients grow in their analytics capabilities.
Responsibilities
As a GTT Data Analyst, your responsibilities may include:
Interpret data, analyze results using statistical techniques and providing ongoing reports.
Follows an agile methodology for all standard SDLC phases that includes but not limited to:
Validation of performance metrics requirements
Creation of Epics/User Stories/Tasks
Testing – Integration, load, and stress test
Deployment publication internally and externally
Work with stakeholders to prioritize data and information needs across all projects.
Perform data analysis to assess the quality and meaning of data.
Prepares reports for management stating trends, patterns and predictions using relevant data, based on business requirements.
Establish GTT’s best practices for data owners to understand and utilize the data self-serve source functionality.
Work closely with the Data Architects and Data Engineers on implementation designs and data acquisition strategies for an optimal data design.
Support requirements gathering and note-taking during client interactions - including new data insights requests and ongoing client project check-ins.
Support interactive Tableau training programs focused on reporting tool proficiency, data visualization best practices, and end-user data training and enablement.
Work with management to ensure data governance policies continue to expand in adoption.
Works closely with data owners on data lineage and data dictionary
In addition to the above responsibilities, you may also need to collaborate with other developers, data architects, and business and data analysts to ensure data quality and consistency across reporting solutions.
Skill Set
3+ years of professional experience in building dashboards, scorecards using Tableau.
Hands-on professional with thorough knowledge of scripting, data source integration and advanced GUI development in Tableau.
Full understanding of the processes of data quality, data cleansing, and data transformation.
Experience designing and developing scripts using Python.
Experience on agile sprint team
Strong knowledge of Tableau server architecture, applying business rules and data validations.
Experience in end-to-end implementation of Business Intelligence (BI) reports & dashboards.
Comfortable in manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources.
Knowledge of formal database architecture and design
Ability to write complicated yet efficient PSQL and SQL queries and stored procedures.
Ability to effectively interact with, present information to, and respond to questions from all levels of the organization and partners.
Ability to communicate complex analysis in a clear, precise, and actionable manner.
Comfortable with ambiguity, creative thinking, and leading change.
Knowledge of ETL concepts and experience in working with ETL tools
Familiarity with data warehousing concepts
A mentoring mindset and comfort engaging and training less experienced colleagues.
EEO Statement (Americas Only)
GTT provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, GTT complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
GTT expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of GTT’s employees to perform their job duties may result in discipline up to and including discharge.
Show more
Show less","Data Analysis, Data Governance, Tableau, Python, Agile Methodology, SQL, PSQL, Data Warehousing, ETL, Statistical Techniques, Dashboarding, Data Interpretation, Reporting, Data Quality, Data Cleansing, Data Transformation, Communication, Data Lineage, Data Dictionary","data analysis, data governance, tableau, python, agile methodology, sql, psql, data warehousing, etl, statistical techniques, dashboarding, data interpretation, reporting, data quality, data cleansing, data transformation, communication, data lineage, data dictionary","agile methodology, communication, dashboard, data dictionary, data governance, data interpretation, data lineage, data quality, data transformation, dataanalytics, datacleaning, datawarehouse, etl, psql, python, reporting, sql, statistical techniques, tableau"
Strategic Data Analyst Senior,St. Luke's Health System,"Boise, ID",https://www.linkedin.com/jobs/view/strategic-data-analyst-senior-at-st-luke-s-health-system-3754709854,2023-12-17,Idaho,United States,Mid senior,Onsite,"St. Luke’s Health System in Boise, ID is seeking a Strategic Data Analyst Senior to join our Digital & Analytics team.
The Strategic Data Analyst Senior serves as a data and analytics and strategic partner to the business segments within the organization. In alignment with Digital & Analytics strategies and tactics, leads the development and delivery of insights and data analysis.
Responsibilities
Develops and presents insights to business unit leadership and key stakeholders. Performs high value analysis and interpretation of key business metrics to aid decision making and operational planning. Provides operational context, highlights opportunities, and proactively supports decision making.
Understands and interprets operational processes and business context to translate clinical, financial, and operational data into insights and information to support decision making.
Communicates with senior leaders within the organization to ensure alignment to strategic business initiatives.
Serves as a thought partner; evaluates options and makes recommendations on courses of action to leadership and key stakeholders as appropriate.
Challenges and supports business decisions with analytical rigor, insights, and judgments to drive better decisions.
Responsible for and performs all but the most complex assignments and work requiring independent judgment and decision making.
Supports and maximizes the business units’ operational and strategic performance delivering insights that ensure high level customer service. Builds successful partnerships with key internal customers and cross functional teams.
Partners with Business Intelligence and Data Management teams to industrialize proven analytical solutions to meet recurring insight needs.
Works with and contributes to Data Management and Data Governance to understand and help define data policies and standards to ensure high quality data and analytics.
Develops and maintains a high degree of functional, analytical, and technical acumen.
Typically handles the most complex situations and serves as a subject matter expert for both function and lower lever colleagues.
Leads special projects and participates on workgroups and teams, as assigned.
Completes other duties and responsibilities as assigned.
Qualifications
Education: Bachelor’s degree or 4 years of relevant experience in lieu of degree
Experience: 6 years related experience
Licenses/Certifications: None
Why St. Luke's
A strong, talented staff is at the heart of St. Luke’s Health System. We are the state’s largest employer with more than 15,000 employees and a medical staff of more than 1,800 physicians and advanced practice providers. We’re proud of our people who deliver skilled, compassionate care every day, and are looking to add dedicated individuals who will continue this same tradition of excellence.
St. Luke’s is an equal opportunity employer and does not discriminate against any person on the basis of race, religion, color, gender, gender identity, sexual orientation, age, national origin, disability, veteran status, or any other status or condition protected by law.
Show more
Show less","Data Analysis, Data Interpretation, Data Visualization, Decision Making, Analytical Skills, Business Intelligence, Data Management, Data Governance, SQL, Python, R, Tableau, Power BI, Data Modeling, Data Warehousing, Machine Learning, Artificial Intelligence, Cloud Computing, AWS, Azure, GCP","data analysis, data interpretation, data visualization, decision making, analytical skills, business intelligence, data management, data governance, sql, python, r, tableau, power bi, data modeling, data warehousing, machine learning, artificial intelligence, cloud computing, aws, azure, gcp","analytical skills, artificial intelligence, aws, azure, business intelligence, cloud computing, data governance, data interpretation, data management, dataanalytics, datamodeling, datawarehouse, decision making, gcp, machine learning, powerbi, python, r, sql, tableau, visualization"
Senior Business Data Analyst - Seasonal 6 months,Intuit,"Eagle, ID",https://www.linkedin.com/jobs/view/senior-business-data-analyst-seasonal-6-months-at-intuit-3776795965,2023-12-17,Idaho,United States,Mid senior,Onsite,"Overview
The CX Analytics team within Consumer Group is customer obsessed and focused on driving change with critical insights and scalable solutions. Enabling measurable improvement for customers, employees and shareholders.
We actively monitor the end-to-end experience as customers use our product, service, digital self help, and discuss questions live with our experts.
To accomplish this monumental task we leverage a broad toolkit of modern analytics technologies to identify opportunities in each of these experiences and partner with leadership to drive measurable improvement.
What You'll Bring
Bachelor’s Degree or equivalent work experience in Quantitative field (i.e., statistics, finance, economics, mathematics)
2+ years’ professional experience with data and statistical analysis, with a deep understanding of explanatory power and hypothesis-driven analyses
Advanced working knowledge with tools used to analyze, manipulate and visualize datasets (e.g. Excel, Sheets, Splunk, Tableau, Qlik)
Advanced working knowledge of Redshift, Hive, Spark
Experience with languages such as SQL, Python, R
Experience with data mining algorithms and statistical modeling techniques such as clustering, classification, regression, decision trees, neural nets, support vector machines, anomaly detection, recommendation systems, sequential pattern discovery and text mining
Proven track record using data visualization and story-telling, to generate action that leads to business impact.
Experience working with operational KPI metrics & reporting systems.
How You Will Lead
Investigator: Translate business questions into verifiable hypotheses, create experiments and/or analysis to test them, and generate recommendations based on the findings. You collect requirements and execute queries to answer the team’s most important data questions.
Dot-Connector: Your business and operational acumen help you recognize patterns, connections and relationships across variables. You make the connections across data sets to help others understand the broader system.
Artist: your visualization, simplicity and clarity of analysis brings data to life and inspires action and understanding.
Craftsman: You build models and reports that allow others to interact, utilize and better understand the underlying data, relationships and insights.
Show more
Show less","Statistics, Data Analysis, Machine Learning, Data Mining, Data Visualization, Storytelling, KPI Metrics & Reporting Systems, Excel, Sheets, Splunk, Tableau, Qlik, Redshift, Hive, Spark, SQL, Python, R, Clustering, Classification, Regression, Decision Trees, Neural Networks, Support Vector Machines, Anomaly Detection, Recommendation Systems, Sequential Pattern Discovery, Text Mining","statistics, data analysis, machine learning, data mining, data visualization, storytelling, kpi metrics reporting systems, excel, sheets, splunk, tableau, qlik, redshift, hive, spark, sql, python, r, clustering, classification, regression, decision trees, neural networks, support vector machines, anomaly detection, recommendation systems, sequential pattern discovery, text mining","anomaly detection, classification, clustering, data mining, dataanalytics, decision trees, excel, hive, kpi metrics reporting systems, machine learning, neural networks, python, qlik, r, recommendation systems, redshift, regression, sequential pattern discovery, sheets, spark, splunk, sql, statistics, storytelling, support vector machines, tableau, text mining, visualization"
Senior Data Protection Operations Engineer,BECU,"Idaho, United States",https://www.linkedin.com/jobs/view/senior-data-protection-operations-engineer-at-becu-3749161617,2023-12-17,Idaho,United States,Mid senior,Remote,"As the nation's largest community credit union, we begin every day focused on delivering superior financial products and services for our 1.3 million members and more than $30 billion in managed assets. Our work has an economic impact as we support our members' financial goals. We are unapologetic about being devoted to our members and the communities we serve. Our business is guided by our people helping people philosophy – which includes our team members.
BECU has been in business for more than 85 years, driven by unwavering core values and a dedication to improving the communities we serve. While we have a rich history, the future of our company, accelerated by business and technology transformation, is even brighter. There's never been a better time to work for BECU.
To learn more visit becu.org/careers.
PAY RANGE
The Target Pay Range for this position is $130,200-$159,200 annually. The full Pay Range is $101,100-$188,300 annually. At BECU, compensation decisions are determined using factors such as relevant job-related skills, experience, and education or training. Should an offer for employment be made, we will consider individual qualifications. In addition to your salary, compensation incentives are available for the hired applicant. Incentives are performance based and targets vary by role.
Benefits
Employees and their eligible family members have access to a wide array of employee benefits, such as medical, dental, vision and life insurance coverage. Employees have access to disability and AD&D insurance. We also offer health care and dependent care flexible spending accounts, as well as health savings accounts, to eligible employees. Employees are able to enroll in our company’s 401k plan and employer-funded retirement plan. Newly hired employees accrue 6.16 hours of paid time off (PTO) on a per pay period basis based on hours worked (up to a maximum of 160 PTO hours per year) and receive ten paid holidays throughout the calendar year. Additional details regarding BECU Benefits can be found here.
Impact You’ll Make
Are you looking to take your data protection career to a new level? Monitor. Detect. Analyze. Could you be our next guardian on our forefront team of defense as a Senior Data Protection Operations Engineer? In this role, you'll safekeep our digital realms, proactively monitoring and swiftly analyzing events in real time to keep our data fortress unbreachable.
You’ll use our tools and craft scripts crucial in rapid threat detection and response. You'll also sculpt and maintain systems ensuring top-tier data protection. Collaborate with fellow tech enthusiasts, sharing knowledge and elevating our protective expertise. You’ll maintain the defenses that will shape the future of our data safety.
What You’ll Do
Compliance Captain & Relationship Builder:
Uphold BECU standards and stay compliant while handling data protection. Collaborate and guide others, such as data owners and data analysts. Use your exceptional problem solving and interpersonal skills to effectively communicate the complex in simple terms as you provide customer service.
Design Lead:
Take the lead in shaping and rolling out data protection patterns for vendor solutions, ensuring sensitive info is always safeguarded. Stay updated on tech threats and emerging security trends. Review existing cloud security architecture and recommend improvement road map.
Systems Steward & Data Privacy:
Seamlessly manage and recover systems that track sensitive data, focusing on a user-friendly experience. Champion for enhancing our data privacy practices, boosting team efficiency in the process.
Risk Analyst & Incident Partner:
Dive deep into risk assessments, considering asset value, potential threats, and existing controls. Engage in incident responses, directing vital alerts to the Security Operations Center.
Tech Advisor & Vendor Liaison:
Guide Tech teams on best practices and tools within our tech stack to shield crucial info. Quick learner, Adjunct IT professor, team leadership experience, Gov tenant. Bridge the gap with our data protection vendors, ensuring we're always in good hands.
And yes, there might be a few extra tasks thrown in, and maybe some afterhours work because cybersecurity is always an ongoing threat, but you've got this! If this sounds like the next step in your career, let’s connect.
Basic Qualifications
Bachelor’s degree in a relevant technology or business management area or equivalent work experience.
Minimum of seven years working with security tools and/or information technology operations
Minimum of four years working with data tools, including performing deployment and configuration, and maintaining operations and content development
Implementation knowledge of Microsoft defender for endpoint, Microsoft defender for Identity, Microsoft, cloud App Security, Microsoft defender for O365
Preferred Qualifications
Advanced scripting skills (e.g., PowerShell, Python, JAVA) preferred.
Configured Microsoft Cloud App Security (MCAS) for customer cloud workload discovery, MCAS integrations with AWS, ServiceNow, Workday, Data loss prevention policies and integrated with AIP labels, Office 365 Advanced threat protection, Azure logs for AIP label analytics, and endpoint solutions for antivirus, antimalware, host-based intrusion prevention, and EDR.
Cloud security, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Identity mgmt., Azure MFA,
eDiscovery, Compliance experience
Perform hands-on activities involved in creating cloud infrastructure components.
Deployed AIP/MIP labels using Crawl, Walk, Run
Deployed and configured on-premises AIP scanner to discover data
High level knowledge and configuration of Microsoft Defender ATP, Microsoft Threat Experts, EDR
Advanced understanding of Cyber Security Operations (monitoring, detection, incident response, forensics) required.
Advanced scripting skills (e.g., PowerShell, Python, JAVA)
Knowledge of TCP/IP and OSI network protocol stack required.
EEO Statement
BECU is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Show more
Show less","Microsoft Defender for Endpoint, Microsoft Defender for Identity, Microsoft Cloud App Security, Microsoft Defender for O365, PowerShell, Python, JAVA, MCAS, AWS, ServiceNow, Workday, Office 365, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Azure MFA, Cloud security, eDiscovery, Compliance, Microsoft Defender ATP, Microsoft Threat Experts, EDR, TCP/IP, OSI network protocol stack","microsoft defender for endpoint, microsoft defender for identity, microsoft cloud app security, microsoft defender for o365, powershell, python, java, mcas, aws, servicenow, workday, office 365, azure sentinel, splunk, cyberark, iaas, paas, azure mfa, cloud security, ediscovery, compliance, microsoft defender atp, microsoft threat experts, edr, tcpip, osi network protocol stack","aws, azure mfa, azure sentinel, cloud security, compliance, cyberark, ediscovery, edr, iaas, java, mcas, microsoft cloud app security, microsoft defender atp, microsoft defender for endpoint, microsoft defender for identity, microsoft defender for o365, microsoft threat experts, office 365, osi network protocol stack, paas, powershell, python, servicenow, splunk, tcpip, workday"
Senior Data Engineer,BambooHR,"Boise, ID",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760292963,2023-12-17,Idaho,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Spark, PySpark, Data lake, Lakehouse, Data warehouse, Hadoop, S3, EMR, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, AWS, RDS, IAM, Security groups, AMIs, Cloudwatch, Cloudtrail, Secrets manager, Git, Terraform, CI/CD, Tableau, Hudi, Iceberg, Delta, Flink, Presto, Dremio, Kubernetes","spark, pyspark, data lake, lakehouse, data warehouse, hadoop, s3, emr, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, aws, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, git, terraform, cicd, tableau, hudi, iceberg, delta, flink, presto, dremio, kubernetes","amis, aws, cicd, cloudtrail, cloudwatch, data lake, databricks, datawarehouse, delta, dremio, emr, flink, git, greenplum, hadoop, hudi, iam, iceberg, kafka, kinesis, kubernetes, lakehouse, presto, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, vertica"
Marketing Data Analyst,Idaho Central Credit Union,"Chubbuck, ID",https://www.linkedin.com/jobs/view/marketing-data-analyst-at-idaho-central-credit-union-3780831547,2023-12-17,Idaho,United States,Mid senior,Hybrid,"A Marketing Data Analyst will interpret credit union data and analyze results. Responsible for turning data into information and insight that aids in campaign and business decisions. Also responsible for performing a range of functions in support of market research, market assessment and presenting results and recommendations.
Duties and Responsibilities
Distribute data and present recommendations for new opportunities within membership.
Assist with data mining, data analysis, and developing and distributing monthly campaigns and reports.
Work with internal teams to identify opportunities for optimization of marketing campaigns and provide recommendations.
Track mailings and provide other meaningful data as needed.
Competitor research and evaluation.
Represent the Credit Union at community events as needed.
Assist with maintaining marketing databases and email marketing systems.
Assist with building email and automated campaigns.
Qualifications
Bachelor’s Degree in Marketing, Mass Communication, Advertising, or a related field. 0-4 years of experience with database marketing required. Preferable experience with Raddon Integrator MCIF, Salesforce Marketing Cloud, Salesforce Advertising Studio, Salesforce CRM, Adobe Analytics, Microsoft Power BI, SQL, and data warehouse experience. Strong analytical skills. Knowledge of marketing automation systems and integrating those systems with other technologies. Technically capable, excellent communicator and a desire to improve processes. Math skills, Secondary-level computer knowledge including Microsoft Office, especially Excel and pivot tables, etc. Ability to always maintain the confidentiality of Credit Union and member records.
Performance Standard
A demonstrated cooperative and positive attitude toward members and other Credit Union staff. Professional in appearance, attendance, quality, and quantity of work performed. Ability to analyze member needs, develop, and coordinate marketing activities to fit member needs with Credit Union products and services. Must be willing to comply with the Bank Secrecy Act and USA Patriot Act as implemented by Idaho Central Credit Union. Team member will be asked to work extended/unique hours.
Physical Requirements
Perform tasks requiring manual dexterity (processing paperwork, filing, stapling, sorting, collating, typing, counting cash, etc.).
Sit for extended periods of time.
Lift 20-40 pounds of applicable supplies including but not limited to copy paper, cash drawers, marketing material, etc.
Repetitive motion using wrists, hands, and fingers.
Reach keyboards.
Ability to operate basic office machines (calculator, computer, telephone, copy machine, fax machine, etc.).
The above statements reflect the general details considered necessary to describe the essential functions of the job and should not be construed as a detailed description of all the work requirements that may be inherent of the job.
Must be eligible for membership at Idaho Central Credit Union to obtain employment.
Idaho Central Credit Union is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, age, disability, protected veteran status or other characteristics protected by law.
Show more
Show less","Data analysis, Data mining, SQL, Salesforce Marketing Cloud, Salesforce Advertising Studio, Salesforce CRM, Adobe Analytics, Microsoft Power BI, Raddon Integrator MCIF, Data warehouse, Excel, Pivot tables, Marketing automation systems, Microsoft Office","data analysis, data mining, sql, salesforce marketing cloud, salesforce advertising studio, salesforce crm, adobe analytics, microsoft power bi, raddon integrator mcif, data warehouse, excel, pivot tables, marketing automation systems, microsoft office","adobe analytics, data mining, dataanalytics, datawarehouse, excel, marketing automation systems, microsoft office, microsoft power bi, pivot tables, raddon integrator mcif, salesforce advertising studio, salesforce crm, salesforce marketing cloud, sql"
Senior Data Insights Analyst,Cambia Health Solutions,"Idaho, United States",https://www.linkedin.com/jobs/view/senior-data-insights-analyst-at-cambia-health-solutions-3770785020,2023-12-17,Idaho,United States,Mid senior,Hybrid,"Remote Within WA, OR, UT or ID
Primary Job Purpose
The Senior Data Insights Analyst is responsible for developing and providing analytics. These roles provide regular reporting and analytics on the performance and value of apps, web and mobile platforms and perform ad hoc analyses that inform site optimization and user experience design. These roles are also responsible for working with our product management and client service teams to perform analyses of impact of platforms which entails analysis method design, consulting, execution of the statistical analysis, mathematical modelling and reporting.
General Functions And Outcomes
Works cross functionally to understand business objectives, and provide analyses on routine and ad hoc projects that are directly relatable to business stakeholders.
Designs and executes analytics projects (conceptualization, method, data sourcing, analysis structure, and report/data visualization).
Ensures the timely delivery of accurate project reporting and analyses.
Provides insights to clients on tracking deliverables and ad hoc analysis projects which leverage insights from consumer digital site behavior with campaign, survey and consumer activity data, and claims data.
Performs analyses that move beyond an explanation of ‘what happened’ to ‘why it happened’ and ‘what should be done’. The successful analyst turns data observations into insights, insights into recommended actions, and ultimately developing strategy and site optimization.
Provide reporting to meet stakeholder agreements.
Creates, manages and drives analyses, KPI definition/measurement and analytics in support of corporate initiatives.
Collaborates with team members to define goals and KPI’s and identifies the best way to track metrics within digital platforms to assess success of new and existing site features and functionality in order to shape the strategy of future plans.
Collaborates with various internal and external teams to identify and prioritize recommendations based on insights uncovered from data analysis.
Collaborates with team members to develop analytical and statistical methods, best practices and systemic processes that enable standardization and automation of tracking and reporting and client self-service for routine reports.
Drive the continued development of customer understanding via quantitative and qualitative methodologies as it pertains to both online behavior as well as the intersection of online behavior and all other customer touch points.
Builds models that represent the consumer digital data in a manner that promotes understanding of influences on digital behavior.
Able to provide recommendations to client on appropriate methodology.
Provide guidance to level I analysts.
Provides work team leadership for cross functional projects.
Provides review of data sourcing and manipulation strategy, analytical methods and computational integrity.
Provide mentoring for developing Digital Insights Analysts, assisting in project design, method selection and review of analyses and quality.
Able to work with clients and/or senior leadership to make recommendations which allow clients and/or leadership to gain greater understanding regarding impacts of features/ functionality.
Able to guide company wide data strategy including recommendations for enhanced data capture and how to leverage data and analytics for greater impact to company
Minimum Requirements
Whole-brain thinking: Possessing the ability to think creatively and demonstrate analytical skills, analyzing complex situations both alone and as part of a team, learning quickly and synthesizing solutions, options and action plans.
Experience with tools for data mining, statistics, analysis, and scripting (e.g., Python, R, SAS, Scala, MATLAB, Ruby).
Demonstrated experience creating complex SQL queries for standard as well as ad hoc data mining purposes, analyze large amounts of data, interpret qualitative data (research, feedback) and incorporate into analyses.
Experience with data reporting tools- Tableau, Micro-strategy, SPSS, SQL Server Reporting Service)
Demonstrated understanding of research and ability to develop methodologies in data mining and other innovative statistical/mathematical approaches.
Ability to collaborate with team members and business partners to define, develop, and deliver web and digital analytics that meet the needs of the business to manage routine operations, identify tactical decisions, and inform strategic direction.
Ability to present complex data, analysis or findings to teams in a way that is clear and understandable and supports the overall business decisions and program efforts.
Demonstrated experience analyzing consumer web, app and mobile behavior.
Experience with web analytics software- Web-trends, Google Analytics, Adobe Analytics
Ability to work independently, managing deliverable timelines and clients.
Ability to develop and produce accurate, error free final deliverables and to participate in and support a peer review culture.
Experience with statistical modelling of results to drive enhanced decision-making.
Advanced skills in data extraction and manipulation.
Experience leading work teams both internal to the department and comprised of multi-disciplinary staff.
Expert in data extraction design and manipulation, coaching junior level analysts in best practices.
Normally to be proficient in the competencies listed above
Digital Insights Analyst Senior would have a Master’s degree in Statistics, Mathematics, Computer Science, Information Technology, or Economics or related field and 7+ years of related work experience or equivalent combination of education and experience.
The expected hiring range for a Senior Data Insights Analyst is $97,000 - $132,000 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for this position is 15% . The current full salary range for this role is $91,500 - $149,000
Benefits
Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
medical, dental, and vision coverage for employees and their eligible family members
annual employer contribution to a health savings account ($1,200 or $2,500 depending on medical coverage, prorated based on hire date)
paid time off varying by role and tenure in addition to 10 company holidays
up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
one-time furniture and equipment allowance for employees working from home
up to $225 in Amazon gift cards for participating in various well-being activities. for a complete list see our External Total Rewards page.
We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
If you need accommodation for any part of the application process because of a medical condition or disability, please email CambiaCareers@cambiahealth.com. Information about how Cambia Health Solutions collects, uses, and discloses information is available in our Privacy Policy. As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our Careers site.
Show more
Show less","Data Analytics, Data Mining, Statistics, Machine Learning, Python, R, SAS, Scala, MATLAB, Ruby, SQL, Tableau, Microstrategy, SPSS, SQL Server Reporting Service, Webtrends, Google Analytics, Adobe Analytics, Data Extraction, Data Manipulation, Data Modelling, Decision Making, Team Leadership, Statistics, Mathematics, Computer Science, Information Technology, Economics","data analytics, data mining, statistics, machine learning, python, r, sas, scala, matlab, ruby, sql, tableau, microstrategy, spss, sql server reporting service, webtrends, google analytics, adobe analytics, data extraction, data manipulation, data modelling, decision making, team leadership, statistics, mathematics, computer science, information technology, economics","adobe analytics, computer science, data extraction, data manipulation, data mining, data modelling, dataanalytics, decision making, economics, google analytics, information technology, machine learning, mathematics, matlab, microstrategy, python, r, ruby, sas, scala, spss, sql, sql server reporting service, statistics, tableau, team leadership, webtrends"
Staff Cybersecurity Data Platform Engineer,Adobe,"Lehi, UT",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767919835,2023-12-17,American Fork,United States,Mid senior,Onsite,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, Cloud computing, Data orchestration, Data ingestion, Medallion architectures, Unity catalog, Autoloader jobs, Delta lakehouse, Spark, PySpark, AWS, Kafka, Flink, Threat management, Incident response, Enterprise security, Security Operations Center (SOC) Operations, Security management workflows","databricks, cloud computing, data orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, delta lakehouse, spark, pyspark, aws, kafka, flink, threat management, incident response, enterprise security, security operations center soc operations, security management workflows","autoloader jobs, aws, cloud computing, data ingestion, data orchestration, databricks, delta lakehouse, enterprise security, flink, incident response, kafka, medallion architectures, security management workflows, security operations center soc operations, spark, threat management, unity catalog"
Senior Database Reliability Engineer,Adobe,"Lehi, UT",https://www.linkedin.com/jobs/view/senior-database-reliability-engineer-at-adobe-3763107724,2023-12-17,American Fork,United States,Mid senior,Onsite,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
The Opportunity
Adobe’s Database Reliability Engineering (DBRE) team has an exciting and challenging mission: Build, deploy, operate, scale, and maintain database systems for customers facing Adobe SaaS solutions used by millions of users worldwide. We are seeking an experienced DBRE to join us in building database services that will help define the future of Creative Cloud and Adobe’s Digital Media business.
What you'll Do
Work on reliability and performance aspects for core database infrastructures that allow Adobe products to scale.
Ensure the highest level of uptime and Quality of Service (QoS) for Adobe’s critical database environments through operational excellence.
Partner with Engineering teams and database vendors on roadmap planning and architectural discussions to ensure we have architectures in place to scale for the future
Implement solutions for automating deployment, provisioning and managing large-scale database environments.
Improve observability by implementing smart monitoring, tracing, and logging.
Act as main point of contact for production incidents, perform root cause analysis, identify, and resolve underlying problem patterns, while working towards developing automated and self-healing solutions
Participate in a cross-regional on-call rotation using follow-the-sun model.
What you need to succeed
Bachelor's degree in computer science or related technical field
At least 8 years relevant production experience in supporting at scale, highly available, mission-critical database environments. Deep understanding in all areas such as database backups, replication, security, DevOps for databases (IaC, CI/CD), observability, and disaster recovery.
Proficiency in at least two of the following open-source database management systems: MongoDB, Cassandra, MySQL, Percona XtraDB Cluster, MariaDB Galera Cluster, PostgreSQL
Experience working with hyper-scale cloud providers (AWS and/or Azure, GCP) and running at scale database environments on virtual computing environments (Amazon EC2, Azure VM)
Experience with Cloud database technologies such as AWS RDS, AWS Keyspaces, Azure SQL/CosmosDB, MongoDB Atlas, and Datastax Astra.
Database migration experience
Experience with infrastructure automation and configuration management tools such as Chef, Ansible, Puppet, Terraform
Deep understanding of cluster management areas, such as scaling, consistency tuning, replication, and multi-datacenter configuration
Experience in performance monitoring, storage performance optimization, tuning database server configurations, queries, and indexes
Strong data modeling and data structure design skills
Exposure to containerization platforms (Kubernetes/Docker)
Good understanding of Linux OS concepts
Proficiency in any of the scripting language (Python /Perl/ Ruby)
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $123,000 -- $250,900 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Database Reliability Engineering, Database Systems, DevOps, Observability, Monitoring, Tracing, Logging, Root Cause Analysis, SelfHealing Solutions, Cloud Providers, HyperScale Cloud Providers, AWS, Azure, GCP, Amazon EC2, Azure VM, Cloud Database Technologies, AWS RDS, AWS Keyspaces, Azure SQL/CosmosDB, MongoDB Atlas, Datastax Astra, Database Migration, Infrastructure Automation, Configuration Management, Chef, Ansible, Puppet, Terraform, Clustering, Scaling, Consistency Tuning, Replication, MultiDatacenter Configuration, Performance Monitoring, Storage Performance Optimization, Database Server Configuration Tuning, Queries, Indexes, Data Modeling, Data Structure Design, Containerization Platforms, Kubernetes, Docker, Linux OS Concepts, Scripting Languages, Python, Perl, Ruby","database reliability engineering, database systems, devops, observability, monitoring, tracing, logging, root cause analysis, selfhealing solutions, cloud providers, hyperscale cloud providers, aws, azure, gcp, amazon ec2, azure vm, cloud database technologies, aws rds, aws keyspaces, azure sqlcosmosdb, mongodb atlas, datastax astra, database migration, infrastructure automation, configuration management, chef, ansible, puppet, terraform, clustering, scaling, consistency tuning, replication, multidatacenter configuration, performance monitoring, storage performance optimization, database server configuration tuning, queries, indexes, data modeling, data structure design, containerization platforms, kubernetes, docker, linux os concepts, scripting languages, python, perl, ruby","amazon ec2, ansible, aws, aws keyspaces, aws rds, azure, azure sqlcosmosdb, azure vm, chef, cloud database technologies, cloud providers, clustering, configuration management, consistency tuning, containerization platforms, data structure design, database migration, database reliability engineering, database server configuration tuning, database systems, datamodeling, datastax astra, devops, docker, gcp, hyperscale cloud providers, indexes, infrastructure automation, kubernetes, linux os concepts, logging, mongodb atlas, monitoring, multidatacenter configuration, observability, performance monitoring, perl, puppet, python, queries, replication, root cause analysis, ruby, scaling, scripting languages, selfhealing solutions, storage performance optimization, terraform, tracing"
Staff Data Engineer,Recruiting from Scratch,"Provo, UT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744396159,2023-12-17,American Fork,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Scala, Hadoop, SQL, NoSQL, Hive, Pig, Kafka, Storm, Spark Streaming, Data Warehousing, ETL, Data Governance, Data Security, Data Scalability, Big Data, Machine Learning, Data Science, Cloud Computing, Software Engineering, Agile Development, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Databases, Hadoop, Hive, Pig, Oozie, Flume, Sqoop, HBase, Cassandra, MongoDB, Couchbase, Redis, Memcached","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, scala, hadoop, sql, nosql, hive, pig, kafka, storm, spark streaming, data warehousing, etl, data governance, data security, data scalability, big data, machine learning, data science, cloud computing, software engineering, agile development, tdd, pair programming, continuous integration, automated testing, deployment, databases, hadoop, hive, pig, oozie, flume, sqoop, hbase, cassandra, mongodb, couchbase, redis, memcached","agile development, airflow, automated testing, big data, cassandra, cloud computing, continuous integration, couchbase, data governance, data scalability, data science, data security, databases, datawarehouse, deployment, docker, etl, flume, hadoop, hbase, helm, hive, kafka, kubernetes, machine learning, memcached, mongodb, nosql, oozie, pair programming, pig, python, redis, scala, snowflake, software engineering, spark, spark streaming, sql, sqoop, storm, tdd"
Staff Data Engineer,Ancestry,"Lehi, UT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-ancestry-3766411449,2023-12-17,American Fork,United States,Mid senior,Remote,"About Ancestry
When you join Ancestry, you join a human-centered company where every person’s story is important. Ancestry®, the global leader in family history, empowers journeys of personal discovery to enrich lives. With our unparalleled collection of more than 40 billion records, over 3 million subscribers and over 23 million people in our growing DNA network, customers can discover their family story and gain a new level of understanding about their lives. Over the past 40 years, we’ve built trusted relationships with millions of people who have chosen us as the platform for discovering, preserving and sharing the most important information about themselves and their families.
We are committed to our location flexible work approach, allowing you to choose to work in the nearest office, from your home, or a hybrid of both (subject to location restrictions and roles that are required to be in the office- see the full list of eligible US locations HERE). We will continue to hire and promote beyond the boundaries of our office locations, to enable broadened possibilities for employee diversity.
Together, we work every day to foster a work environment that's inclusive as well as diverse, and where our people can be themselves. Every idea and perspective is valued so that our products and services reflect the global and diverse clients we serve.
Ancestry encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants. Passionate about dedicating your work to enriching people’s lives? Join the curious.
We are seeking a Staff Data Engineer to join our Enterprise Data Management (EDM) organization. The EDM team is the hub of data within Ancestry, ingesting data across the organization to process, transform, and deliver to our internal and external stakeholders.
What you will do…
Develop extract-transform-load (ETL) pipelines
Practice good coding techniques including writing unit and integration tests, doing commits and pull requests (Git), etc.
Mentor, train, and collaborate with other engineers to develop scalable, resilient, and maintainable ETL pipelines
Participate in an on-call rotation to ensure pipelines are successful and data meets quality requirements
Provide technical leadership to team and across the EDM organization
Ensure ETL pipelines meet quality standards and data quality requirements
Work with internal and external stakeholders to identify and define data requirements, design solutions, and ensure timely delivery
Manage individual data projects, ensuring appropriate designs and stakeholder involvement
Who you are…
10+ years of experience as a data engineer, with specific experience in developing ETL pipelines
10+ years of industry experience programming in Python and SQL (or related languages) with significant experience in data warehouses, Spark, and other related technologies (experience with Airflow a plus)
Experience training and mentoring other engineers, leading teams, and coordinating with stakeholders
Excellent written and verbal communication skills
Familiarity with agile software development
Familiarity with AWS technologies (specifically, EMR)
Bachelors or 4-year degree in Computer Science or equivalent industry experience
Helping people discover their story is at the heart of ours. Ancestry is the largest provider of family history and personal DNA testing, harnessing a powerful combination of information, science and technology to help people discover their family history and stories that were never possible before. Ancestry’s suite of products includes: AncestryDNA, AncestryProGenealogists, Fold3, Newspapers.com, Find a Grave, Archives.com, and Rootsweb. We offer excellent benefits and a competitive compensation package. For additional information, regarding our benefits and career information, please visit our website at http://ancestry.com/careers
As a signatory of the ParityPledge in Support of Women and the ParityPledge in Support of People of Color, Ancestry values pay transparency and pay equity. We are pleased to share the base salary range for this position: $133,200 - $200,550 with eligibility for bonus, equity and comprehensive benefits including health, dental and vision. The actual salary will vary by geographic region and job experience. We will share detailed compensation data for a specific location during the recruiting process. Read more about our benefits HERE.
Note: Disclosure as required by sb19-085(8-5-20) and sb1162(1-1-23)
#GDSponsored
#IND2
Additional Information
Ancestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.
All job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.
Ancestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means.
Show more
Show less","ETL pipelines, Unit and integration tests, Git, Python, SQL, Data warehouses, Spark, Airflow, AWS technologies, EMR, Agile software development, Computer Science","etl pipelines, unit and integration tests, git, python, sql, data warehouses, spark, airflow, aws technologies, emr, agile software development, computer science","agile software development, airflow, aws technologies, computer science, data warehouses, emr, etl pipelines, git, python, spark, sql, unit and integration tests"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Provo, UT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759710406,2023-12-17,American Fork,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, R, Pandas, SQL, Git, Kubernetes, Apache Airflow, Kubeflow, Airflow, Helm, Spark, PySpark, Kafka, Storm, Spark Streaming, Snowflake, Docker, DynamoDB, Applied Machine Learning, NLP, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Distributed Systems, Microservices","python, java, r, pandas, sql, git, kubernetes, apache airflow, kubeflow, airflow, helm, spark, pyspark, kafka, storm, spark streaming, snowflake, docker, dynamodb, applied machine learning, nlp, data mining, data cleaning, data normalization, data modeling, distributed systems, microservices","airflow, apache airflow, applied machine learning, data cleaning, data mining, data normalization, datamodeling, distributed systems, docker, dynamodb, git, helm, java, kafka, kubeflow, kubernetes, microservices, nlp, pandas, python, r, snowflake, spark, spark streaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Provo, UT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087719,2023-12-17,American Fork,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, ETL, Hadoop, Pig, Hive, HBase, Flume, Sqoop, Oozie, ZooKeeper, Ambari, Hue","data engineering, machine learning, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, etl, hadoop, pig, hive, hbase, flume, sqoop, oozie, zookeeper, ambari, hue","airflow, ambari, aws, azure, bash, data engineering, docker, dynamodb, etl, flume, gcp, git, hadoop, hbase, helm, hive, hue, java, kafka, kubeflow, kubernetes, machine learning, oozie, pig, python, snowflake, spark, sparkstreaming, sql, sqoop, storm, zookeeper"
Engineering Manager – Network and Datacenter Operations,Ascendion,"Colorado, United States",https://www.linkedin.com/jobs/view/engineering-manager-%E2%80%93-network-and-datacenter-operations-at-ascendion-3752851241,2023-12-17,Florissant,United States,Mid senior,Remote,"About Ascendion
Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life
We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:
Build the coolest tech for world’s leading brands
Solve complex problems – and learn new skills
Experience the power of transforming digital engineering for Fortune 500 clients
Master your craft with leading training programs and hands-on experience
Experience a community of change makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:
Job Title:
Engineering Manager – Network and Datacenter Operations- It's remote but Need someone in Colorado
Description:
10+ years of project management experience
5+ years working within telecommunications, cable, and/or data centers
Experience implementing end-to-end infrastructure projects
Background managing network upgrades, data center operations, and/or infrastructure upgrades
Excellent written, verbal, and presentation skills
Bonus Qualifications:
PMP
CSM/Certified Scrum Master
SAFe
Responsibilities:
We are seeking a highly skilled Project/Program Manager to oversee several enterprise client engagements within the networking and data center spaces.
Create detailed work plans which identify and sequence the activities and deliverables needed to successfully complete cutover/migration/upgrade/implementations.
Develop a schedule for migration completion; review the implementation schedule with leadership and all other team members who will be affected by the implementation activities; revise the schedule as required.
Work with leadership, determine the objectives and measures upon which the implementation will be evaluated at its completion.
Serve as the project manager, technical, and planning liaison with all Datacenter, ISP, Network, Firewall and Infrastructure, and business throughout the life cycles of network device migration project.
If needed, Data analysis of all the collected information from various internal tools by working with system/business analyst and guide them to convert into meaningful data.
Utilize Project management tools and system to monitor/track status of project, report the progress to leadership and escalate when needed to unblock critical deliverables.
Solid understanding of Data Center Operations, Network devices, Migration, Infrastructure Operations life cycle, ITSM process in telecom domain.
Track potential risks, provide routine schedule, and risk updates to all the stakeholders.
Manage the progress of the implementation and adjust as necessary to ensure the successful completion of the implementation.
Effectively communicate with stakeholders and leadership at each stage of implementation and provide on-going support.
Identify the automation and optimization opportunity to accelerate the migration and cutover.
Closely collaborate with engineering to support the migration process and coordinate the following:
Technical Specification Document approval
Solution Design approval
Test Strategy approval
Project Baseline Established (Scope, Schedule, Estimate, Requirements)
Traceability Matrix: BRD and Tech Spec Mapping
Validation of migration plan and post migration & Results Signed Off
Implementation Review / Training Materials Finalized /BAU Plan
Go/No Go Decision – Ready for Production
Implementation and updating documents, operating procedure.
Location:
- It's remote but Need someone in Colorado
Salary Range:
The salary for this position is between $120,000 – $125,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits:
The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more
Show less","Project Management, Telecommunications, Data Centers, Infrastructure Projects, Network Upgrades, Data Center Operations, Infrastructure Upgrades, PMP, CSM/Certified Scrum Master, SAFe, Migration, Implementation, Data Analysis, Project Management Tools, ITSM, Risk Management, Communication, Automation, Optimization, Technical Specifications, Solution Design, Test Strategy, Project Baseline, Traceability Matrix, Validation, Implementation Review, Training Materials, Go/No Go Decision, Production, Documentation, Operating Procedure","project management, telecommunications, data centers, infrastructure projects, network upgrades, data center operations, infrastructure upgrades, pmp, csmcertified scrum master, safe, migration, implementation, data analysis, project management tools, itsm, risk management, communication, automation, optimization, technical specifications, solution design, test strategy, project baseline, traceability matrix, validation, implementation review, training materials, gono go decision, production, documentation, operating procedure","automation, communication, csmcertified scrum master, data center operations, data centers, dataanalytics, documentation, gono go decision, implementation, implementation review, infrastructure projects, infrastructure upgrades, itsm, migration, network upgrades, operating procedure, optimization, pmp, production, project baseline, project management, project management tools, risk management, safe, solution design, technical specifications, telecommunications, test strategy, traceability matrix, training materials, validation"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"La Grange, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742676555,2023-12-17,Naperville,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Apache Beam, Spark, PySpark, Kafka, Scala, AWS, Azure, GCP, ETL, MLOps, Data Engineer","python, apache beam, spark, pyspark, kafka, scala, aws, azure, gcp, etl, mlops, data engineer","apache beam, aws, azure, dataengineering, etl, gcp, kafka, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Bellwood, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678280,2023-12-17,Naperville,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, ELT, MLOps, Python, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC, Apache Beam, Kafka, DevOps","etl, elt, mlops, python, spark, scala, pyspark, aws, azure, gcp, cicd, iac, apache beam, kafka, devops","apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, mlops, python, scala, spark"
Pensions Implementation Analyst - Data,IRIS | Networx | Recruitment Software & Services,"Abbey Wood, England, United Kingdom",https://uk.linkedin.com/jobs/view/pensions-implementation-analyst-data-at-iris-networx-recruitment-software-services-3784213017,2023-12-17,Maidstone, United Kingdom,Mid senior,Hybrid,"Are you seeking a Pensions Implementation Analyst role with exciting challenges, a varied portfolio, and the opportunity to be a key individual delivering a market-leading service to our clients?
At Hymans Robertson you will not only have all the above but will be a valued member of our Firm. You will be given excellent development opportunities and work in a supportive and collaborative culture, with great colleagues. The role is demanding but comes with great rewards. It’s an exciting time to join our business as we evolve our model and change the future of administration delivery for our clients.
The Role
This role is in our Third Party Administration Systems Team based in Glasgow.
The focus will be on mapping, validating, transforming and loading data from various sources to our pensions administration system (UPM2). This data is required for onboarding new clients or services that support our pension market propositions.
Key Responsibilities
Extraction, analysis, transformation and loading of data from a range of data sources.
Migration of data and services during client onboarding into UPM.
Working with the client project team to ensure delivery of functional requirements.
Identifying and resolving data issues and risks throughout the installation project lifecycle
Working with pensions administration teams, stakeholders and clients to resolve queries.
And You Will Be Happy To
Represent the team in Trustee meetings and with internal stakeholders.
Proactively work with colleagues to solve problems.
Engage with a culture of continuous improvement.
Manage own workload while collaborating with the team and wider audience.
Comply with the firms’ Data Protection Policy and adhere to the firm’s Information Security standards, policies and procedures.
Qualifications And Experience
Experience of working with a Pensions Administration system e.g. UPM.
Experience in pension data validation, analysis, transformation and migration between systems.
Experience of working with SQL databases or similar, particularly in the development of automated scripts and reports.
Experience of various data sources and coming up with creative solutions.
Analytical skills and attention to detail, balanced with pragmatism and strategic awareness.
Show more
Show less","Pensions Administration, Data Extraction, Data Transformation, Data Loading, Data Migration, Data Analysis, Data Validation, SQL Databases, Automated Scripts, Data Sources, Analytical Skills, Strategic Awareness, UPM, UPM2","pensions administration, data extraction, data transformation, data loading, data migration, data analysis, data validation, sql databases, automated scripts, data sources, analytical skills, strategic awareness, upm, upm2","analytical skills, automated scripts, data extraction, data loading, data migration, data sources, data transformation, data validation, dataanalytics, pensions administration, sql databases, strategic awareness, upm, upm2"
Open Application Data Insight Analysts,Watchfinder,"West Malling, England, United Kingdom",https://uk.linkedin.com/jobs/view/open-application-data-insight-analysts-at-watchfinder-3199947669,2023-12-17,Southend-on-Sea, United Kingdom,Associate,Hybrid,"What is an open application role?
Even if we have no vacancies in this area, we are interested in your profile. By registering for our open application, we have the opportunity to contact you directly about new or future vacancies that arise here at Watchfinder.
What do our Analyst do here at Watchfinder?
As an Analyst at Watchfinder you will play a key role in working with our stakeholders to find out how Watchfinder can best use the vast amount of data available, so our business can make the most informed key decisions.
We work with all business areas across the whole of Watchfinder with the vision to harness the power of data to fundamentally change the way that we approach opportunities.
You’ll extract and analyse data from multiple sources to help assess performance levels, trends, and opportunities across the Watchfinder business.
What do we look for?
Experience as a Data Analyst or Business Intelligence Analyst, with a track record of producing data-driven insights and recommendations to various stakeholders.
Experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, JavaScript, or ETL frameworks)
Knowledge of statistics and experience using statistical packages for analysing datasets (Excel, SPSS, SAS etc)
How do we keep you smiling?
Firstly, what makes Watchfinder a great place to work is the people! Whether that be within your immediate team or across other areas of the business, we all strive to come together to ‘Get the Job Done’ ensuring that our execution is flawless and people centric.
You may be excited to hear to that we have recently expanded internationally, boasting luxurious locations in Hong Kong, New York, Paris, Milan, Geneva, Zurich, and Munich. With no sign of slowing down!
How do we keep you smiling?
Private healthcare and dental
Competitive pension scheme
Season ticket loan
Holiday scheme
Cycle to work scheme
Employee Assistant programme
Income protection
Life Assurance
Extensive group discounts
Show more
Show less","Data Analyst, Business Intelligence Analyst, Datadriven insights, Business Objects, SQL, XML, JavaScript, ETL frameworks, Statistics, Excel, SPSS, SAS","data analyst, business intelligence analyst, datadriven insights, business objects, sql, xml, javascript, etl frameworks, statistics, excel, spss, sas","business intelligence analyst, business objects, dataanalytics, datadriven insights, etl frameworks, excel, javascript, sas, spss, sql, statistics, xml"
HRIS & Data Analyst,Shawbrook Bank,"Brentwood, England, United Kingdom",https://uk.linkedin.com/jobs/view/hris-data-analyst-at-shawbrook-bank-3771381332,2023-12-17,Southend-on-Sea, United Kingdom,Associate,Hybrid,"The Opportunity
Job location: Brentwood or Glasgow
Shawbrook is a specialist bank driven by a purpose to power up ingenuity to create opportunity, every single day.
We offer a diverse range of savings and loan products. From personal and business savings accounts and loans for weddings and new cars, to complex financial credit facilities for businesses requiring significant investment and mortgages for landlords with multiple properties – no two customers are ever the same.
We give our customers the best-of-both worlds; uniquely combining strong digital capabilities with human expertise and ingenuity to deliver the best outcomes. We rise to the challenge of a complex case or unconventional circumstances, and we love to make things happen. By being creative, practical, and personal, we know we can always find the right solution for our customers.
If you’re willing to roll up your sleeves, contribute new ideas and believe anything is possible, you’re our kind of person
Work with us because you:
Want to be part of a bank built for the dynamics of the modern world
Relish a challenge and enjoy a fast-paced, innovative and hardworking culture
Enjoy finding new and better ways to solve complexity and make things happen
Want to belong to a diverse culture that stands shoulder to shoulder with minority and underrepresented groups
Care about society and the environment and want to be part of a business that cares too
Want to continue to grow professionally and be the best version of yourself
The Role & Responsibilities
Acting as a super user of the HR Information Systems (HRIS), assisting in scoping, and implementing functionality to deliver efficient and effective system solutions to support the first line HR team.
Supporting in the delivery of enablement training for the HR team, Managers and Employees.
To maintain the HR Information System data, in order to organise, analyse and interpret data, reporting findings to the wider HR function and Business areas.
Perform regular HRIS data audits to ensure system accuracy, identifying and implementing appropriate validation rules to maintain data
Assist with HRIS configuration, implementing new functionality such as workflows, managing picklists and field dependencies
Identify and share proposals for enhancement opportunities to create efficiencies within the HR team and enhance user experience
Act as the first point of escalation to HRIS help desks to resolve service issues and acts as a point of contact with HRIS providers escalating service issues in Shawbrook to vendor Account Managers
Create and manage dashboards and other information delivery services for internal customers
Management of day-to-day delivery of HR Management information packs
Ensure workflows, approvals and notifications are operating as designed
Review and test upgrades issued by system providers and new functionality builds
Work with the HR People Partnering team to manage business re-organisations
Responsible for helping develop skills and abilities of employees
The Person
Experience of managing and developing HR Systems
Proven working knowledge of Sage People
Experience of managing HR reporting and analytics
Experience of change management in HRIS
Experience of working in a confidential environment, dealing with sensitive information
Strong IT skills
Microsoft Excel advanced level
Experience of working in a HR setting, familiar with employee life-cycle processes
Strong analytical skills
Attention to detail and methodical in work practices
Reward
Your Wellbeing - We take your health and well-being very seriously by providing a range of benefits to give you and your family peace of mind. These include:
Market leading family friendly policies such as access to our Maternity, Adoption and Paternity policies from Day 1 of your employment
Free access to Headspace, a mindfulness & meditation digital health app
Free access to Peppy digital health app that offers personalised support through fertility treatment becoming a parent or menopause
EAP (Employee Assistance Programme) - Offering you support on a wide range of subjects including financial concerns, mental wellbeing and more general queries around family, work, housing, and health
Cycle to work scheme
Discounts on gym membership
Contributory pension scheme & death in service
Your Lifestyle - It’s important you strike the right balance between your work and personal life. We provide benefits to support you when at work and when you’re enjoying your leisure time.
Minimum of 27 days holiday per year
Option to buy or sell holiday days through our flexi-holiday scheme
Discounts on gym membership nationwide
Access to discounts on a range of high street and online brands
Community support and charitable giving
Your Contribution - We’re focused on rewarding those that go the extra mile in helping us achieve our goals.
Participation in our annual discretionary bonus scheme designed to reward your contribution to our success
Proudly Shawbrook recognition scheme focused on recognising our role models and thanking our colleagues
Show more
Show less","HR Information Systems (HRIS), Sage People, HR reporting, HR analytics, Change management, Microsoft Excel, HR setting, Employee lifecycle processes, Analytical skills, Attention to detail, Methodical work practices","hr information systems hris, sage people, hr reporting, hr analytics, change management, microsoft excel, hr setting, employee lifecycle processes, analytical skills, attention to detail, methodical work practices","analytical skills, attention to detail, change management, employee lifecycle processes, hr analytics, hr information systems hris, hr reporting, hr setting, methodical work practices, microsoft excel, sage people"
Principal Engineer - Diagnostics & Data,Consilium Recruit,"Essex, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-engineer-diagnostics-data-at-consilium-recruit-3781183383,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Onsite,"A leading developer of hydrogen energy and propulsion solutions is seeking a Principal Engineer for Data and Diagnostics. The Principal Engineer will support and own the analysis of data and diagnostic information for dual-fuel energy systems used in heavy duty industrial and marine applications. This is a permanent role.
Commutable from: Basildon, Brentwood, East London, Ipswich, Cambridge
Salary: £ Negotiable + Bonus + Pension
The Role
The Principal Engineer for Data and Diagnostics will own the data analysis for dual fuel engine and system data, extracting valuable insights to enhance hydrogen system performance and reliability.
You will actively monitor all running applications and report on a regular basis on system performance and additionally own and organise extensive datasets related to dual fuel engine performance and fleet operations.
The Principal Engineer will lead the design and development of diagnostic methodologies and tools to identify and address engine and system-related issues efficiently and own the data monitoring of hydrogen fuel dual fuel systems within the vehicle fleet, ensuring maximum uptime and efficiency.
You will collaborate closely with cross-functional teams (incl. Chief Engineers, Principal Engineers, and Electrical Engineers/Designers) and across departments (incl. Product Owner, Production, Procurement, Sales, Advanced Development, Aftersales and Business Development) to optimise system performance and efficiency based on data-driven insights.
Additionally, you will investigate and resolve complex technical issues by identifying root causes and collaborating with the engineering team to implement effective solutions. The Principal Engineer will create and maintain documentation and produce detailed reports with analysis and recommendations based on data analysis.
It essential that you are comfortable presenting data findings to small groups and senior managers and participating in design reviews, risk assessments, and technical discussions to provide valuable input and contribute to the continuous improvement of the hydrogen system engineering process.
The Principal Engineer will ensure that all fleet operations and data analysis activities adhere to safety and regulatory standards, particularly in the context of hydrogen systems – you will stay current with industry trends, advancements, and best practices related to hydrogen fuel systems, dual fuel technology, and data/diagnostics development.
The Person
The Principal Engineer for Data and Diagnostics will hold a bachelor’s or master’s degree in engineering, Data Science, or a related field or equivalent. You will have proven experience in data analysis, diagnostics, and fleet management, preferably in a relevant industry and display a strong proficiency in data analysis tools such as Python, or other relevant languages used in data analysis.
The Principal Engineer will have a strong proficiency in relational databases (e.g. MySQL, PostgreSQL) and SQL for data retrieval and manipulation and show a strong proficiency with cloud platforms like AWS, Azure, or Google Cloud.
Familiarity with communication protocols (CAN, Ethernet, etc.) and real-time operating systems (RTOS) for embedded systems is an essential requirement for the role in addition to knowledge of control algorithms, sensors, actuators, and feedback systems.
Prior experience in the heavy-duty vehicle, industrial, or marine sectors is advantageous, alongside knowledge of cybersecurity tools and practices to ensure data security and compliance.
To apply, please contact James Colley with a current CV via email.
Show more
Show less","Data Analysis, Diagnostics, Fleet Management, Hydrogen Systems, Dual Fuel Technology, Python, MySQL, PostgreSQL, SQL, AWS, Azure, Google Cloud, CAN, Ethernet, RTOS, Control Algorithms, Sensors, Actuators, Feedback Systems, Cybersecurity","data analysis, diagnostics, fleet management, hydrogen systems, dual fuel technology, python, mysql, postgresql, sql, aws, azure, google cloud, can, ethernet, rtos, control algorithms, sensors, actuators, feedback systems, cybersecurity","actuators, aws, azure, can, control algorithms, cybersecurity, dataanalytics, diagnostics, dual fuel technology, ethernet, feedback systems, fleet management, google cloud, hydrogen systems, mysql, postgresql, python, rtos, sensors, sql"
Senior Data Protection Engineer,Energy Jobline,"Basildon, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-protection-engineer-at-energy-jobline-3773892743,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Onsite,"Location: Hybrid role, 1 day in the Office Basildon
Salary: £60,000 - £80,000 dependant on experience
Role
My client, a large global investor service provider is looking for a Senior Data Protection Engineer to join their expanding team. You will be crafting solutions for the next generation of Storage and Backup services and contribute to the future architecture. You will be required to work on implementation, maintenance, and support services for the enterprise data protection service.
Responsibilities
Maintain documentation of the backup technology’s configuration, contacts, and functionality. Contribute and maintain common troubleshooting tips in the designated knowledge base.
Lead and coordinate the Backup (Cohesity/ Data Domain/ Avamar/ Veeam/ Commvault/ Cloud Native) technical and business discussions related to future architecture direction across the product portfolio or product line.
Planning and scheduling data protection hardware and application upgrades, updates, and capacity additions
Design complete Backup and restore strategy, the configuration of Backups- Full/Incremental/Image/Sub-file, and troubleshooting.
Project management and task coordination with other teams and vendors as required to complete the installation of new data protection technology.
Key Skills Required
Strong understanding of technical concepts including, but not limited to, databases, networking, hardware, software, interfaces, and operating systems.
Able to perform the professional and technical competencies in at least one of the following data protection applications, required Cohesity; preferably Commvault, Avamar, or Networker
Experience in a similar role involved in solutions design and implementation services for backup and recovery, high availability architecture, and data replication for disaster recovery.
Prior experience developing plans for migration, consolidation, or archiving of data from legacy systems where applicable.
Able to perform professional and technical competencies in Data Domain or NAS and tape technologies.
Working knowledge of Scripting (PowerShell or Python), API Development, and Automation.
Ability to use backup application functionality to create, schedule, and distribute reports.
Please apply ASAP to be considered
Show more
Show less","Data Protection, Storage, Backup, Cohesity, Data Domain, Avamar, Veeam, Commvault, Cloud Native, Databases, Networking, Hardware, Software, Interfaces, Operating Systems, PowerShell, Python, API Development, Automation","data protection, storage, backup, cohesity, data domain, avamar, veeam, commvault, cloud native, databases, networking, hardware, software, interfaces, operating systems, powershell, python, api development, automation","api development, automation, avamar, backup, cloud native, cohesity, commvault, data domain, data protection, databases, hardware, interfaces, networking, operating systems, powershell, python, software, storage, veeam"
Senior AI Data Engineer (UK REMOTE),Turnitin,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-remote-at-turnitin-3759700866,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Remote,"Company Description
100% REMOTE
MUST BE UK BASED
At Turnitin, an AI-centric leader in the educational and research sectors, we've been innovating and promoting academic integrity for over two decades. We have an established reputation for our advanced solutions, utilized by numerous academic institutions, corporations, and publishers worldwide.
Offering remote work as a default arrangement, we honor individual choices, value diversity, and respect local cultures. However, for those who prefer the office environment, we have multiple locations across the globe including Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands). Our team is diverse, but unified by our commitment to significantly impacting the realm of education.
As a Senior Data Engineer at Turnitin, you will be part of a global team of proactive, supportive, and independent professionals, striving to deliver sophisticated, well-structured AI and data systems. Collaborating closely with different teams within Turnitin, you'll integrate AI and data science across our broad suite of products, further enriching learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer entails a range of responsibilities, necessitating a balanced skillset:
AI Data Engineering: Design, build, operate and deploy real-time data pipelines at scale using AI techniques and best practices. Support Turnitin's AI R&D efforts by applying advanced data warehousing, data science, and data engineering technologies. Aim for automation to enable a faster time-to-market and better reusability of new AI initiatives.
Collaboration: Work in tandem with the AI R&D teams and the Data Platform Team to collect, create, curate and maintain high-quality AI datasets. Ensure alignment of data architecture and data models across different products and platforms.
Innovation: Unearth insights from Turnitin's rich data resources through innovative research and development.
Hands-on Involvement: Engage in data engineering and data science tasks as required to support the team and the projects. Conduct and own external data collection efforts - including state of the art prompt engineering techniques - to support the construction of state of the art AI models.
Communication: Foster clear communication within the team and the organization, and ensure understanding of the company's vision and mission.
Continuous Learning: Keep abreast of new tools and development strategies, bringing innovative recommendations to leadership.
Qualifications
At least 4 years of experience in data engineering, ideally focused on enabling and accelerating AI R&D.
Strong proficiency in Python, Java, and SQL.
Proficiency with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP).
Familiarity interacting with AI frameworks including PyTorch and TensorFlow and AI libraries such as Huggingface and Scikit-Learn.
Experience with Large Language Models (LLMs) and LLM APIs.
Strong problem-solving, analytical, and communication skills, along with the ability to thrive in a fast-paced, collaborative environment.
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects.
Experience in a technical leadership role.
Familiarity with natural language processing (NLP) techniques and tools.
Experience in the education or education technology sectors.
Experience with data visualization and data communications.
Characteristics for Success
As a Senior Data Engineer, you should possess:
A passion for creatively solving complex data problems.
The ability to work collaboratively and cross-functionally.
A continuous learning mindset, always striving to improve your skills and knowledge.
A proven track record of delivering results and ensuring a high level of quality.
Strong written and verbal communication skills.
Curiosity about the problems at hand, the field at large, and the best solutions.
Strong system-level problem-solving skills.
Additional Information
Total Rewards @ Turnitin
Turnitin maintains a Total Rewards package that is competitive within the local job market. People tend to think about their Total Rewards monetarily – solely as regular pay plus bonus or commission. This what they earn in exchange for what they do. However, Turnitin delivers more than just these components. Beyond the intrinsic rewards of making a difference in the lives of educators, administrators, learners and researchers around the world, and thriving in an organization that is free of politics and full of humble, inclusive and collaborative teammates, the extrinsic rewards at Turnitin include generous time off and health and wellness programs that offer choice and flexibility and provide a safety net for the challenges that life presents from time to time. In our Remote-First approach to collaborating, you are also able to work the way that best fits your style and situation – whether that be remote, in one of our offices/rented spaces or hybrid.
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric
- We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning
- We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
Integrity
- We believe integrity is the heartbeat of ExamSoft. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership
- We have a bias toward action and empower teammates to make decisions.
One Team
- We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset
- We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
3 all-company global holidays (Juneteenth + 2 Founder’s Days)
Paid Volunteer Time*
Charitable cContribution Match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
* varies by country
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. We strongly encourage applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veterans status.
Show more
Show less","AI Data Engineering, AI R&D, Data Warehousing, Data Science, Data Engineering, Data Pipelines, Automation, Collaboration, Data Architecture, Data Models, Innovative Research and Development, Handson Involvement, Data Collection, Prompt Engineering, AI Models, Communication, Continuous Learning, Python, Java, SQL, Redshift, Hadoop, Elasticsearch, Cloud Platforms (AWS Azure GCP), PyTorch, TensorFlow, Huggingface, ScikitLearn, Large Language Models (LLMs), LLM APIs, ProblemSolving, Analytical Skills, Communication Skills, Fastpaced Environment, Collaborative Environment, Natural Language Processing (NLP), Machine Learning, Technical Leadership, Data Visualization, Data Communications, Complex Data Problems, CrossFunctional Collaboration, Continuous Learning Mindset, High Level of Quality, Written and Verbal Communication Skills, SystemLevel ProblemSolving","ai data engineering, ai rd, data warehousing, data science, data engineering, data pipelines, automation, collaboration, data architecture, data models, innovative research and development, handson involvement, data collection, prompt engineering, ai models, communication, continuous learning, python, java, sql, redshift, hadoop, elasticsearch, cloud platforms aws azure gcp, pytorch, tensorflow, huggingface, scikitlearn, large language models llms, llm apis, problemsolving, analytical skills, communication skills, fastpaced environment, collaborative environment, natural language processing nlp, machine learning, technical leadership, data visualization, data communications, complex data problems, crossfunctional collaboration, continuous learning mindset, high level of quality, written and verbal communication skills, systemlevel problemsolving","ai data engineering, ai models, ai rd, analytical skills, automation, cloud platforms aws azure gcp, collaboration, collaborative environment, communication, communication skills, complex data problems, continuous learning, continuous learning mindset, crossfunctional collaboration, data architecture, data collection, data communications, data engineering, data models, data science, datapipeline, datawarehouse, elasticsearch, fastpaced environment, hadoop, handson involvement, high level of quality, huggingface, innovative research and development, java, large language models llms, llm apis, machine learning, natural language processing nlp, problemsolving, prompt engineering, python, pytorch, redshift, scikitlearn, sql, systemlevel problemsolving, technical leadership, tensorflow, visualization, written and verbal communication skills"
Senior IT Data Engineer (4035),Laing O'Rourke,"Dartford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-it-data-engineer-4035-at-laing-o-rourke-3783689936,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"Do you have what it takes to transform an organisation into a Digital & Data led one? Then you are the Senior Data Engineer we want in our IT function.
As part of Laing O'Rourke's transformation, we need candidates like you to help with our journey. The Senior Data Engineer role is key to helping the right ‘data delivery' from the onset for all Digital Initiatives. Are you the key candidate to open the door? The Senior Data Engineer role will help us achieve industry leading data & analytics transformation. Can you create, execute and lead compelling strategies around Digital Data Architecture, Data Governance and Data Quality? Then keep reading.
Your role is to create and maintain optimal data pipeline architecture. You will be assembly large, complex data sets that meet the functional / non-functional business requirements. To do this you will be Identifying, designing, and implementing internal process improvements: automating manual processes, optimising data delivery and re-designing infrastructure for greater scalability.
Duties Include
Building the infrastructure for optimal extraction, transformation and loading of data
Working with stakeholders and the Executive, Product and Data teams
Keeping internal data separate and secure through architected sensitive data patterns
Creating data tools for analytics
Essential Skills And Experience
Around 10+ years of experience working in the data space and 5 years leading digital data transformation
Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of database
Experience with Microsoft cloud services: Azure Databricks, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure Data Lake, Azure Search, Purview
Experience with Microsoft Power Shell
Experience with version and source control (Git) and Azure DevOps
Desirable Skills And Experience
Experience in APIs and API Management
Stakeholder communication
Ability influence and promote data agendas
Able to coach and mentor for a consistent, high level standard
Ability to understand and articulate complex data in a simplified manor
The position is working within our IT Function and supporting our Digital Function. It is an excellent opportunity to learn from like-minded mentors and experts that make up our team. For more information apply today and don't miss out on an opportunity to accelerate your career.
Don't match all the criteria? We are open and always happy to hear from people with transferable skill sets and a commitment to learning.
About Us
We are an international engineering and construction company delivering state-of-the-art infrastructure and buildings projects for clients in the UK, Middle East and Australia.
Certainty, reliability, quality – this is what our clients want. And at Laing O'Rourke, we have more than 150 years of experience delivering it. Laing O'Rourke's story is one of energy, passion, ambition, people and teamwork. We harness the power of our experience, stretching back over a century and a half to deliver certainty for our clients.
As part of the Disability Confident scheme, we would like to enable access to candidates with long term health conditions and disabilities through the ‘Offer an interview scheme'. This supports applicants that meet the essential criteria by offering an interview for the advertised position. Please let us know prior to interview what adjustments are required as well as discussing how we can support you in the workplace.
We want to ensure our recruitment process is accessible to all. If you need the application form in an alternative format or you would like to know more about our recruitment process, please email
resourcingteam@laingorourke.com
Show more
Show less","Data Engineering, Data Architecture, Data Governance, Data Quality, Data Pipeline, Data Analytics, SQL, Relational Databases, Azure Databricks, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure Data Lake, Azure Search, Azure Purview, Data Tools, Data Visualization, API Management, Version Control, Git, Azure DevOps, Stakeholder Communication, Data Agendas, Coaching, Mentoring","data engineering, data architecture, data governance, data quality, data pipeline, data analytics, sql, relational databases, azure databricks, azure data factory, azure logic apps, azure functions, azure storage, azure data lake, azure search, azure purview, data tools, data visualization, api management, version control, git, azure devops, stakeholder communication, data agendas, coaching, mentoring","api management, azure data factory, azure data lake, azure databricks, azure devops, azure functions, azure logic apps, azure purview, azure search, azure storage, coaching, data agendas, data architecture, data engineering, data governance, data pipeline, data quality, data tools, dataanalytics, git, mentoring, relational databases, sql, stakeholder communication, version control, visualization"
Principal Engineer - Data and Diagnostics,Consilium Recruit,"Brentwood, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-engineer-data-and-diagnostics-at-consilium-recruit-3781592751,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"A leading developer of hydrogen energy and propulsion solutions is seeking a Principal Engineer for Data and Diagnostics. The Principal Engineer will support and own the analysis of data and diagnostic information for dual-fuel energy systems used in heavy duty industrial and marine applications. This is a permanent role.
Commutable from: Basildon, Brentwood, East London, Ipswich, Cambridge
Salary: £ Negotiable + Bonus + Pension
The Role
The Principal Engineer for Data and Diagnostics will own the data analysis for dual fuel engine and system data, extracting valuable insights to enhance hydrogen system performance and reliability.
You will actively monitor all running applications and report on a regular basis on system performance and additionally own and organise extensive datasets related to dual fuel engine performance and fleet operations.
The Principal Engineer will lead the design and development of diagnostic methodologies and tools to identify and address engine and system-related issues efficiently and own the data monitoring of hydrogen fuel dual fuel systems within the vehicle fleet, ensuring maximum uptime and efficiency.
You will collaborate closely with cross-functional teams (incl. Chief Engineers, Principal Engineers, and Electrical Engineers/Designers) and across departments (incl. Product Owner, Production, Procurement, Sales, Advanced Development, Aftersales and Business Development) to optimise system performance and efficiency based on data-driven insights.
Additionally, you will investigate and resolve complex technical issues by identifying root causes and collaborating with the engineering team to implement effective solutions. The Principal Engineer will create and maintain documentation and produce detailed reports with analysis and recommendations based on data analysis.
It essential that you are comfortable presenting data findings to small groups and senior managers and participating in design reviews, risk assessments, and technical discussions to provide valuable input and contribute to the continuous improvement of the hydrogen system engineering process.
The Principal Engineer will ensure that all fleet operations and data analysis activities adhere to safety and regulatory standards, particularly in the context of hydrogen systems – you will stay current with industry trends, advancements, and best practices related to hydrogen fuel systems, dual fuel technology, and data/diagnostics development.
The Person
The Principal Engineer for Data and Diagnostics will hold a bachelor’s or master’s degree in engineering, Data Science, or a related field or equivalent. You will have proven experience in data analysis, diagnostics, and fleet management, preferably in a relevant industry and display a strong proficiency in data analysis tools such as Python, or other relevant languages used in data analysis.
The Principal Engineer will have a strong proficiency in relational databases (e.g. MySQL, PostgreSQL) and SQL for data retrieval and manipulation and show a strong proficiency with cloud platforms like AWS, Azure, or Google Cloud.
Familiarity with communication protocols (CAN, Ethernet, etc.) and realtime operating systems (RTOS) for embedded systems is an essential requirement for the role in addition to knowledge of control algorithms, sensors, actuators, and feedback systems.
Prior experience in the heavy-duty vehicle, industrial, or marine sectors is advantageous, alongside knowledge of cybersecurity tools and practices to ensure data security and compliance.
To apply, please contact Tak Bakdi with a current CV via email.
Show more
Show less","Data Analysis, Diagnostics, Fleet Management, Python, SQL, MySQL, PostgreSQL, AWS, Azure, Google Cloud, CAN, Ethernet, RTOS, Control Algorithms, Sensors, Actuators, Feedback Systems, Cybersecurity","data analysis, diagnostics, fleet management, python, sql, mysql, postgresql, aws, azure, google cloud, can, ethernet, rtos, control algorithms, sensors, actuators, feedback systems, cybersecurity","actuators, aws, azure, can, control algorithms, cybersecurity, dataanalytics, diagnostics, ethernet, feedback systems, fleet management, google cloud, mysql, postgresql, python, rtos, sensors, sql"
Senior Data Centre Installation Engineer,Nanotek Ltd,"Chelmsford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-centre-installation-engineer-at-nanotek-ltd-3782089428,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"** Candidates MUST be UK based with FULL UK working rights **
** Salary £38 - £62K p.a **
Candidate Criteria:
** Data Centre Installation
** Senior
Data Centre Installation
** Hardware Installation
** Racking
** Telecoms
** Team Manager
** Budgets
Mobile Data Centre Engineer
Senior Data Centre Engineer
Telecoms Engineer
It is essential that you have worked in data centres and are comfortable in that environment. Ideally you have experience working night shifts and antisocial hours. Due to the nature of this industry weekend and unsociable hours will be required.
All technical and work information is electronic based and so your computer and especially Microsoft Excel skills, must be intermediate as a minimum with advanced preferred.
Experience of Hardware, Cable and Connection Auditing.
Proven record of IT support/breakfix hardware/cabling/cable troubleshooting.
Job Role:
·      Lifecycle Support - Equipment & Connectivity Auditing, Install, Moves & Changes, Relocation, Decommissioning & Disposal, Patching, Troubleshooting
·      Managed Data Centre Services - Data Centre Migration, Capacity Management, Rollout & Deployment
·      Infrastructure - Structured Cabling, Rack Deployment & Aisle Containment, Wireless Networking
Show more
Show less","Data Centre Installation, Hardware Installation, Racking, Telecoms, Team Management, Budgets, Microsoft Excel, Hardware Auditing, Cable Auditing, Connection Auditing, IT Support, Breakfix Hardware, Cabling, Cable Troubleshooting, Equipment Auditing, Connectivity Auditing, Moves & Changes, Relocation, Decommissioning, Disposal, Patching, Troubleshooting, Data Centre Migration, Capacity Management, Rollout, Deployment, Structured Cabling, Rack Deployment, Aisle Containment, Wireless Networking","data centre installation, hardware installation, racking, telecoms, team management, budgets, microsoft excel, hardware auditing, cable auditing, connection auditing, it support, breakfix hardware, cabling, cable troubleshooting, equipment auditing, connectivity auditing, moves changes, relocation, decommissioning, disposal, patching, troubleshooting, data centre migration, capacity management, rollout, deployment, structured cabling, rack deployment, aisle containment, wireless networking","aisle containment, breakfix hardware, budgets, cable auditing, cable troubleshooting, cabling, capacity management, connection auditing, connectivity auditing, data centre installation, data centre migration, decommissioning, deployment, disposal, equipment auditing, hardware auditing, hardware installation, it support, microsoft excel, moves changes, patching, rack deployment, racking, relocation, rollout, structured cabling, team management, telecoms, troubleshooting, wireless networking"
WMS Data Analyst,Swi-tch,"Chelmsford, England, United Kingdom",https://uk.linkedin.com/jobs/view/wms-data-analyst-at-swi-tch-3775990152,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"Swi-Tch Are partnered with leading client who are in the market for a WMS Data Analyst to join their growing team on an exciting long term programme
Location:
Hybrid / Predominantly Remote
Rate:
£Negotiable
Status:
Outside IR35
Duration:
6 months initial contract
The Role:
Work with the Business to define the scope of historical data migration required.
Work with the programme team, tech specialists and the business to define the data migration approach
Analysing source data, including detailed data analysis of historical data to be migrated from legacy systems.
Specify, design and implement the appropriate data tooling set appropriate for the data migration to the new Warehouse Management System (WMS) and historic data to our Enterprise Data Warehouse (Snowflake).
Specifying, scoping and developing Extract, Transformation and Load (ETL) rules for that data.
Specifying, scoping and completing data mapping activities for that data.
Specifying, scoping and developing programmatic routines for reconciliation of transformed data.
Working independently with business and technology resources to define the ‘as-is’ and ‘to-be’ data structures.
Providing such other technical support as may be required within the data migration workstream from time to time.
Maintaining the highest standard of professional conduct at all times with colleagues and service partners.
Performing any other duties which are reasonably deemed to be consistent with the demands and responsibilities of this role.
Show more
Show less","Data Analysis, Data Migration, Data Warehousing, Data Modeling, Data Mapping, Data Reconciliation, ETL (Extract Transform Load), Snowflake, WMS (Warehouse Management System), Technical Support","data analysis, data migration, data warehousing, data modeling, data mapping, data reconciliation, etl extract transform load, snowflake, wms warehouse management system, technical support","data mapping, data migration, data reconciliation, dataanalytics, datamodeling, datawarehouse, etl extract transform load, snowflake, technical support, wms warehouse management system"
Senior Data Protection Engineer,SS&C Technologies,"Basildon, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-protection-engineer-at-ss-c-technologies-3757165384,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"SS&C is the largest global investor service provider, servicing more than 55 million investors. Continually investing in global technology and services across the spectrum of distribution channels including Asset Managers, Financial Advisors, Wealth Managers, and large financial institutions such as Banks and Insurers. Investor servicing is offered in many different countries, including the U.S., Canada, the U.K., Ireland, Luxembourg, Australia, Hong Kong, and Singapore. SS&C also services mutual fund structures in many other fund domiciles.
Global Investor and Distribution Solutions (GIDS) is the business unit offering this role. GIDS delivers global omni-channel investor servicing, including contact centre, using digital services.
Hybrid Opportunity - Basildon/London Office
We now seek a
Senior Data Protection Engineer
to join our Open Storage team and work on crafting solutions for the next generation of Storage and Backup services and contribute to the future architecture direction across the product portfolio!
This is a critical role and requires deep knowledge of backup software integration/configuration and high availability architecture. The successful candidate will work on implementation, maintenance, and support services for our enterprise data protection service.
***You can expect to participate in an on-call rotation that will include after-hours, weekends, and during holidays.
Your Responsibilities
Lead and coordinate the Backup (Cohesity/ Data Domain/ Avamar/ Veeam/ Commvault/ Cloud Native) technical and business discussions related to future architecture direction across the product portfolio or product line.
Use of backup technologies monitoring tools to resolve issues that could cause failure to backup data. Ensure retention periods for application data follow SS&C, customer, and regulatory requirements. Verifies that systems are completing a normally scheduled and approved backup methodology.
Planning and scheduling data protection hardware and application upgrades, updates, and capacity additions. This may include coordination with other IT teams, vendors, and users. Follows department change request processes.
Assist in troubleshooting and root cause analysis for environmental issues as they arise.
Design complete Backup and restore strategy, the configuration of Backups- Full/Incremental/Image/Sub-file, and troubleshooting.
Understand the technology direction for the Backup & Storage service and apply this to technical allocations and deployments.
Maintain documentation of the backup technology’s configuration, contacts, and functionality. Contributes and maintains common troubleshooting tips in the designated knowledge base.
Write and maintain complete and concise technical documentation. Serve as the highest point of escalation for Backup incidents across all regions.
Project management and task coordination with other teams and vendors as required to complete the installation of new data protection technology.
Your Experience
Able to perform the professional and technical competencies in at least one of the following data protection applications: Cohesity preferably with knowledge of others Commvault/ Avamar/ Networker.
Deep knowledge of Backup software integration/configuration with the most used applications in the industry (e.g., Backing up SQL, Oracle, VMWare, NAS shares, etc.)
Experience in a similar role involved in solutions design and implementation services for backup and recovery, high availability architecture, and data replication for disaster recovery.
Prior experience developing plans for migration, consolidation, or archiving of data from legacy systems where applicable.
Able to perform professional and technical competencies in Data Domain or NAS and tape technologies.
Working knowledge of Scripting (PowerShell or Python), API Development, and Automation.
Ability to use backup application functionality to create, schedule, and distribute reports.
Excellent critical thinking skills when dealing with applications, storage, tape libraries, and client issues related to data protection.
Excellent communication people skills, necessary in building out relationships with various vendors.
Strong understanding of technical concepts including, but not limited to, databases, networking, hardware, software, interfaces, and operating systems.
Familiarity with various encryption technologies (for both Data At Rest and in transit) and ensuring compliance.
We encourage applications from people of all backgrounds and particularly welcome applications from under-represented groups, to enable us to bring a diversity of perspectives to our thinking and conversation. It is important to us that we strive to have a workforce that is diverse in the widest sense. Unless explicitly requested or approached by SS&C Technologies, Inc. or any of its affiliated companies, the company will not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services.
All offers of employment at SS&C are subject to background verification checks, including a 5-year employment history, proof of eligibility to work in the hiring location, proof of address, credit check and criminal record check (where permitted by local law).
Show more
Show less","Cohesity, Data Domain, Avamar, Veeam, Commvault, Cloud Native, Backup, Data Protection, High availability architecture, SQL, Oracle, VMWare, NAS, PowerShell, Python, API Development, Automation, Databases, Networking, Hardware, Software, Interfaces, Operating systems, Encryption","cohesity, data domain, avamar, veeam, commvault, cloud native, backup, data protection, high availability architecture, sql, oracle, vmware, nas, powershell, python, api development, automation, databases, networking, hardware, software, interfaces, operating systems, encryption","api development, automation, avamar, backup, cloud native, cohesity, commvault, data domain, data protection, databases, encryption, hardware, high availability architecture, interfaces, nas, networking, operating systems, oracle, powershell, python, software, sql, veeam, vmware"
Data Centre Technical Architect,Context Recruitment,"Essex, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-centre-technical-architect-at-context-recruitment-3786437801,2023-12-17,Southend-on-Sea, United Kingdom,Mid senior,Hybrid,"Data Centre Technical Architect - Essex (hybrid working)
Up to £50,000 PA
Service Provider seeking a Data Centre Architect to join a small, yet well-established technical team. They predominately provide data centre services across the UK to a range of different clients across banking, retail and legal. Services include DC migrations, support, auditing, DC rollout/refresh/deployment etc.
This role would be well suited to an ambitious and well-rounded Data Centre Engineer that is looking for a step-up into solutions architecture, this role reports into a senior architect so there is plenty of opportunity for learning and support within this role.
Responsibilities:
Take ownership eventually of the infrastructure design, structure cabling design, job technical preparations and works order creation
Scope and design services and solutions for their clients at the pre-sales stage as well as collaboration with the wider team for delivery of the projects
This role will require you to have a sound and hands-on understanding of data centres and their related technologies, you'll also need to possess the ability to break down technical teams to non-technical audiences and be confident in your delivery.
Technical requirements:
Experience of date centre audits, migrations, commissioning/decommissioning
Date centre IMACs and general maintenance
End of life / hardware refreshes across network, server, storage
Office set ups/moves, expansion and consolidation
WAN/LAN connectivity (Cisco), customer extranet and B2B networks
Structured cabling architecture, installation and testing
Works order and patch schedules
Desktop transformation and PC hardware refresh
Wireless surveying and networks
Offering up to £50,000 PA (potentially negotiable) plus some attractive benefits. Flexible working allowed after initial probation period (2-3 days per week WFH). Excellent work culture; grown up and mature environment with numerous social events
Show more
Show less","Data center architecture, Infrastructure design, Cabling design, Job technical preparations, Works order creation, Scope and design services, Presales stage collaboration, Data center audits, Migrations, Commissioning, Decommissioning, IMACs, General maintenance, End of life hardware refreshes, Network, Server, Storage, Office set ups, Moves, Expansion, Consolidation, WAN connectivity, LAN connectivity, Cisco, Customer extranet, B2B networks, Structured cabling architecture, Installation, Testing, Works order, Patch schedules, Desktop transformation, PC hardware refresh, Wireless surveying, Networks","data center architecture, infrastructure design, cabling design, job technical preparations, works order creation, scope and design services, presales stage collaboration, data center audits, migrations, commissioning, decommissioning, imacs, general maintenance, end of life hardware refreshes, network, server, storage, office set ups, moves, expansion, consolidation, wan connectivity, lan connectivity, cisco, customer extranet, b2b networks, structured cabling architecture, installation, testing, works order, patch schedules, desktop transformation, pc hardware refresh, wireless surveying, networks","b2b networks, cabling design, cisco, commissioning, consolidation, customer extranet, data center architecture, data center audits, decommissioning, desktop transformation, end of life hardware refreshes, expansion, general maintenance, imacs, infrastructure design, installation, job technical preparations, lan connectivity, migrations, moves, network, networks, office set ups, patch schedules, pc hardware refresh, presales stage collaboration, scope and design services, server, storage, structured cabling architecture, testing, wan connectivity, wireless surveying, works order, works order creation"
Sr Data Analytics Analyst,Marmon Foodservice Technologies,"Minnesota, United States",https://www.linkedin.com/jobs/view/sr-data-analytics-analyst-at-marmon-foodservice-technologies-3592148432,2023-12-17,Brainerd,United States,Mid senior,Onsite,"Marmon Foodservice Technologies, Inc.
Come join a team where People make the difference! As a part of Marmon Holdings, Inc., a highly decentralized organization, we rely heavily on people with the aptitude, attitude, and entrepreneurial spirit to drive our success, and we're committed to attracting and retaining top talent.
Under broad supervision, coordinates, diagnoses and troubleshoots incoming employee calls. Provides support services to employees with technical problems and information technology issues involving desktop, laptop or network services from local personnel or from employees using network remote access. Provides timely resolution of moderately complex problems or escalation on behalf of customer to appropriate technical personnel. Supports and maintains effective relationships with users. May develop or document standard operating procedures and customer service guidelines relating to IT support. Role typically requires between 3 and 5 years of experience.
Under broad supervision, coordinates, diagnoses and troubleshoots incoming employee calls. Provides support services to employees with technical problems and information technology issues involving desktop, laptop or network services from local personnel or from employees using network remote access. Provides timely resolution of moderately complex problems or escalation on behalf of customer to appropriate technical personnel. Supports and maintains effective relationships with users. May develop or document standard operating procedures and customer service guidelines relating to IT support. Role typically requires between 3 and 5 years of experience.
Following receipt of a conditional offer of employment, candidates will be required to complete additional job-related screening processes as permitted or required by applicable law.
We are an equal opportunity employer, and all applicants will be considered for employment without attention to their membership in any protected class. If you require any reasonable accommodation to complete your application or any part of the recruiting process, please email your request to careers@marmon.com, and please be sure to include the title and the location of the position for which you are applying.
Show more
Show less","IT support, Troubleshooting, Network services, Desktop support, Laptop support, Customer service, Standard operating procedures, Technical problems, Information technology, Employee calls, Network remote access","it support, troubleshooting, network services, desktop support, laptop support, customer service, standard operating procedures, technical problems, information technology, employee calls, network remote access","customer service, desktop support, employee calls, information technology, it support, laptop support, network remote access, network services, standard operating procedures, technical problems, troubleshooting"
Senior Data Engineer,"RBA, Inc.","Minnesota, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-rba-inc-3777343662,2023-12-17,Brainerd,United States,Mid senior,Remote,"Senior Data Engineer
RBA is an established leader and go-to partner for enterprise and mid-size organizations looking to transform their business through technology solutions. As a Digital and Technology consultancy, RBA brings a unique perspective to the conversation, considering technology capabilities and roadmaps and aligning with the business goals and objectives to drive innovative growth. In addition to working with some of the greatest companies in our market, RBA combines challenging career opportunities with a fun and exciting work environment and culture.
We have numerous opportunities and are looking to expand our capabilities to deliver our clients' thought leadership and transformational solutions. If your expertise aligns with the qualifications below, let’s connect!
We are open to fully remote employees and being hybrid and/or in office. We have offices in the beautiful downtown Wayzata, MN, area as well as downtown Duluth, MN where employees are welcome to work!
Senior Data Engineer
RBA is looking for a Senior Data Engineer to join our growing team. In this role, you will be pivotal in architecting, developing, and maintaining robust data pipelines and systems. You will collaborate closely with cross-functional teams to design and implement scalable data solutions that align with our clients' business objectives. The ideal candidate will have a strong background in data engineering, a deep understanding of cloud technologies, and expertise in building efficient data architectures.
Responsibilities:
Design, develop, and optimize end-to-end data pipelines for various data sources and applications.
Collaborate with stakeholders to understand data requirements and translate them into technical solutions.
Implement best data processing, storage, and retrieval practices within cloud environments.
Evaluate and select appropriate tools and technologies to build scalable data infrastructure.
Ensure data quality, integrity, and security across all data lifecycle stages.
Mentor and guide junior team members, providing technical leadership and expertise.
Stay updated with industry trends and emerging data engineering and analytics technologies.
Requirements:
Bachelor’s or Master’s degree in computer science, Engineering, or related field.
8+ years of proven experience in technology
5+ years of proven experience in data engineering or a similar role.
Proficiency in cloud platforms such as AWS, Azure, or GCP.
Strong programming skills in Python or equivalent
Extensive knowledge of database technologies (SQL, NoSQL) and data warehousing concepts.
Experience with data modeling, ETL processes, and workflow management tools.
Excellent problem-solving abilities and a proactive mindset towards tackling complex challenges.
Outstanding communication skills and the ability to work collaboratively in a team environment.
Preferred Qualifications:
Certifications in cloud computing
Familiarity with machine learning concepts and frameworks.
Experience with CI/CD deployment methodology
Experience with visualization tools like PowerBI or Tableau
Experience with Databricks, Snowflake, Alteryx, or other data and analytics platforms
Why work for RBA
We’re a team of creatives, strategists, and technologists consistently winning “Best Places to Work” while delivering innovative solutions for our clients. We believe our design, strategy, and technology teams working in parallel deliver the best solutions possible.
Benefits offered at RBA:
· Flexible work environment: Work from our offices in Wayzata or Duluth, MN, or your home office, or a combination.
· Health coverage: Staying healthy is important to us, and we offer competitive plans for medical, dental, and vision.
· LinkedIn Learning / Professional Development: We believe in lifelong learning and want every employee to strive to be their best. As such, every employee at RBA receives a free subscription to LinkedIn Learning to access over 17,000+ online courses for ongoing professional development.
· Profit-sharing & 401k: We offer revenue sharing and bonus opportunities throughout the year. Fidelity manages our 401k plan.
· Paid time off: We know balancing work and life is critical to your happiness.
· Social events: Getting together as a company is important to us, and even though the pandemic has significantly altered what that means, we prioritize and host social events, including lake cruises, baseball games, picnics, and more.
· Company cabins: Getting away occasionally and forgetting about work is nice. With RBA, you can make that happen at any of our free employee-use cabins in beautiful northern Minnesota.
· Company Pontoons: If taking a relaxing boat ride is your style, all employees get free access to employee-use pontoon boats in Wayzata and Duluth, MN
We look forward to hearing from you! If you have any questions, please reach out to recruiting@rbaconsulting.com! Check out our social channels to understand what daily life at RBA is like.
Instagram | Twitter | LinkedIn | Facebook
Show more
Show less","SQL, NoSQL, Python, AWS, Azure, GCP, Data engineering, Data modeling, ETL, Data warehousing, Cloud computing, Machine learning, CI/CD, PowerBI, Tableau, Databricks, Snowflake, Alteryx","sql, nosql, python, aws, azure, gcp, data engineering, data modeling, etl, data warehousing, cloud computing, machine learning, cicd, powerbi, tableau, databricks, snowflake, alteryx","alteryx, aws, azure, cicd, cloud computing, data engineering, databricks, datamodeling, datawarehouse, etl, gcp, machine learning, nosql, powerbi, python, snowflake, sql, tableau"
Senior Data Engineer,MGIC,"Minnesota, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mgic-3735279019,2023-12-17,Brainerd,United States,Mid senior,Remote,"Why work at MGIC?
Are you someone who wants to play a critical role in our company’s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
Preferred location: Milwaukee based – hybrid (3 days office, 2 days remote)
Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
How will you make an impact?
We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
Do you have what it takes?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework.
Enjoy these benefits from day one:
Competitive Salary & pay-for-performance bonus
Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)
On-site Fitness Center and classes (corporate office)
Paid-time off and paid company holidays
Business casual dress
For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.
Show more
Show less","Python, PySpark, Terraform, AWS, S3, Lambda, EMR, Dynamo DB, Glue, Snowflake, Continuous Integration, Continuous Delivery, Infrastructure as Code, Agile, Scrum","python, pyspark, terraform, aws, s3, lambda, emr, dynamo db, glue, snowflake, continuous integration, continuous delivery, infrastructure as code, agile, scrum","agile, aws, continuous delivery, continuous integration, dynamo db, emr, glue, infrastructure as code, lambda, python, s3, scrum, snowflake, spark, terraform"
Sr Pricing Data Analyst,Datasite,"Minnesota, United States",https://www.linkedin.com/jobs/view/sr-pricing-data-analyst-at-datasite-3787569563,2023-12-17,Brainerd,United States,Mid senior,Remote,"Datasite is where deals are made. We provide the data rooms and SaaS technology used in M&A and other high-value transactions, to deliver projects in more than 170 countries. Carrying that success into the future is all about you. Your useful skills, your unusual experience, your unique ideas. Everyone here brings something unexpected. What’s yours? Invest your talents in us, and we’ll return the compliment.
Job Description
The
Sr
Pricing Data Analyst
is responsible for data analysis and modeling to derive insights that support data driven decision making related to pricing, packaging, and commercial strategy. This role focuses on transforming raw data into actionable insights and leveraging data-driven storytelling to influence commercial outcomes. An essential responsibility includes the development, monitoring, maintenance, and updating of various pricing metrics, reports, and dashboards to support ongoing monetization and price improvement efforts.
The role requires an analytics background with equal parts technical, business and communication skills. This role will report directly to the Senior Director, Pricing and Packaging Strategy and will work closely with other cross-functional teams across Datasite.
Applicants must be authorized to work in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
P
referred location is MN or NY but we are open to US-remote.
Responsibilities
Aggregate, cleanse, and organize business data (e.g., product usage, pricing, billing, etc.) from disparate sources with varying degrees of data integrity.
Conduct thorough analyses to identify data driven insights from trends and anomalies to inform pricing, packaging, and commercial decisions.
Perform analyses and modeling to support various price improvement initiatives.
Develop, update, and maintain pricing metrics (e.g., Price-per-unit, ARPU, ASP) to measure pricing effectiveness.
Build, maintain and update dashboards, KPI tracking, and visualization/reporting on a regular basis.
Support weekly, monthly, quarterly reporting of pricing metrics, analysis, and guidance to stakeholders across the organization.
Support various analyses for pricing strategy related projects, including new product and feature rollouts.
Analyze customer spending trends and usage to assist in new product pricing and deal structuring for new contracts.
Collaborate with cross-functional teams, including Finance, Data Science, Operations, SF practice, Engineering.
Participate in presentation of pricing analytics and reporting to various internal stakeholders.
Education, Experience Requirements & Essential Skills
Bachelor’s degree in an analytical field (Statistics, Mathematics, Computer Science, or related).
4+ years of prior B2B (preferably SaaS) experience in an analytical business role, preferably in Pricing, Data Science, Finance or Sales Ops.
High proficiency in Excel skills (e.g., nested formulas, VLOOKUP/XLOOKUP, INDEX/MATCH, SUMIFS), pivots and charting.
Proficiency in data manipulation, analysis, and visualization/BI tools (e.g., Excel, SQL, SF, CRM analytics, Power BI, Python, R).
Proficient in the remainder of Microsoft Suite (PowerPoint, Word, etc.)
Strong analytical thinking and problem-solving skills, with the ability to translate data into actionable recommendations.
Detail-oriented mindset with a commitment to data accuracy and integrity.
Effective communication skills and ability to explain complex models in simple terms.
Ability to build relationships and collaborate regularly in a dynamic, cross-functional environment.
Self-motivated with the ability to work independently, maintain accountability, and prioritize multiple projects.
Adaptability to operate within an unstructured environment with minimal or unclear business requirements as the company continues to evolve.
As a global organization, Datasite knows that diverse perspectives are essential to our success. We’re committed to maintaining a diverse workforce to serve our customers around the world. Datasite is an equal opportunity employer (EEO) and furthers the principles of EEO through Affirmative Action.
Show more
Show less","Data Analysis, Data Modeling, Pricing, Packaging, Commercial Strategy, DataDriven Decision Making, Data Cleansing, Data Organization, Data Visualization, Reporting, Dashboarding, Excel, SQL, Python, R, Power BI, Salesforce, CRM Analytics, Microsoft Suite, PowerPoint, Word, Communication","data analysis, data modeling, pricing, packaging, commercial strategy, datadriven decision making, data cleansing, data organization, data visualization, reporting, dashboarding, excel, sql, python, r, power bi, salesforce, crm analytics, microsoft suite, powerpoint, word, communication","commercial strategy, communication, crm analytics, dashboard, data organization, dataanalytics, datacleaning, datadriven decision making, datamodeling, excel, microsoft suite, packaging, powerbi, powerpoint, pricing, python, r, reporting, salesforce, sql, visualization, word"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Elk Grove Village, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742675300,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC (Infrastructure as Code), Kafka, Apache Beam, ETL, ELT, ML Ops, Machine Learning, AI/ML","python, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac infrastructure as code, kafka, apache beam, etl, elt, ml ops, machine learning, aiml","aiml, apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac infrastructure as code, kafka, machine learning, ml ops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Arlington Heights, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679052,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ML Ops, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, DevOps, ETL, NFT, VR, AI/ML","python, ml ops, spark, scala, pyspark, aws, azure, gcp, cicd, iac infrastructure as code, apache beam, kafka, devops, etl, nft, vr, aiml","aiml, apache beam, aws, azure, cicd, devops, etl, gcp, iac infrastructure as code, kafka, ml ops, nft, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Des Plaines, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742676283,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ML Ops, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, Machine Learning, NFT, VR, AI/ML","python, etl, ml ops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, machine learning, nft, vr, aiml","aiml, apache beam, aws, azure, cicd, devops, etl, gcp, iac, kafka, machine learning, ml ops, nft, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Berkeley, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673812,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Apache Beam, AWS, Azure, CI/CD, DevOps, ETL, IaC, Kafka, Machine Learning, MLOps, Python, PySpark, Scala, Spark, VR","apache beam, aws, azure, cicd, devops, etl, iac, kafka, machine learning, mlops, python, pyspark, scala, spark, vr","apache beam, aws, azure, cicd, devops, etl, iac, kafka, machine learning, mlops, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Hillside, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674809,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ELT, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, Machine Learning, MLOps","python, etl, elt, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, machine learning, mlops","apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Wood Dale, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679057,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, ETL, Machine Learning, Data Engineering","python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, etl, machine learning, data engineering","apache beam, aws, azure, cicd, data engineering, devops, etl, gcp, iac, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Wood Dale, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742677359,2023-12-17,Elgin,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ELT, Apache Beam, Apache Kafka, AWS, Azure, GCP, Spark, Scala, PySpark, IaC (Infrastructure as Code), MLOps, Machine Learning, CI/CD, DevOps, Data Engineering","python, etl, elt, apache beam, apache kafka, aws, azure, gcp, spark, scala, pyspark, iac infrastructure as code, mlops, machine learning, cicd, devops, data engineering","apache beam, apache kafka, aws, azure, cicd, data engineering, devops, elt, etl, gcp, iac infrastructure as code, machine learning, mlops, python, scala, spark"
"Sr Data Analyst II - Power BI tools, trend analysis",Staples,"Lincolnshire, IL",https://www.linkedin.com/jobs/view/sr-data-analyst-ii-power-bi-tools-trend-analysis-at-staples-3762304537,2023-12-17,Elgin,United States,Mid senior,Hybrid,"Location: onsite in Lincolnshire IL required.
Staples is business to business.
You’re what binds us together.
Quill
makes the job of ordering supplies easier and more rewarding. While delivering everything from paper, ink, and toner to cleaning supplies and technology. Quill showcases exceptional customer service. Quill had proven to be a trusted partner of Staples since being acquired in 1998.
What you’ll be doing
:
Use analytical skills within SQL and excel to support business decisions that drive revenue and profitability.
Develop and maintain dashboards and reports to track key performance indicators (KPIs) and provide regular updates to stakeholders across the organization, specifically in sales and/ or loyalty & promotions, and trend analysis, customer analytics.
Lead data-oriented projects and collaborate with cross-functional teams to drive business impact.
Analyze a range of datasets to identify trends and patterns, creating dashboards, reports, and visualizations to track KPIs.
Perform ad-hoc analysis to identify changes in KPIs and key drivers within the business.
Ensure data integrity and accuracy within the outputs of the team
Continuously evaluate and improve analytical methods, tools, and processes to ensure the company remains competitive in the market.
Provide insights and recommendations to stakeholders, informing decisions across the organization.
Build models to forecast outcomes related to changes within the business.
Collaborate with cross-functional teams to drive improvement in customer experience, operational efficiency, and business growth.
What you bring to the table
:
Ability to create a culture of success and inclusion, be a values ambassador
Ability to build trusting relationships with business leaders and merchandising partners
Ability to proactively address and independently guide problem solving efforts
Strong interpersonal skills for interacting with Business Unit and Finance partners
Strong analytical and problem-solving skills, excellent attention to detail, including trend analysis and customer analytics.
Strong project management skills, including experience managing timelines and stakeholders.
Excellent communication skills, including the ability to present complex data & analysis to stakeholders in a clear and concise manner
Strong analytical skills, including technical proficiency in SQL, Snowflake, advanced Excel, and data visualization
Qualifications:
What’s needed- Basic Qualifications
:
5 + years total work experience in data analysis, trend analysis, customer analytics
5 + years experience working in data analysis using
SQL, Excel, and BI tools,
utilizing customer data
5 years experience working in roles requiring project management and cross functional collaboration
3 + years experience working in an industry requiring rigorous analytical training i.e. finance, consulting, technology, etc.
What’s needed- Preferred Qualifications
:
Bachelor’s Degree
Experience with PowerBI or a comparable tool
We Offer
:
Inclusive culture with associate-led Business Resource Groups
Flexible PTO (22 days) and Holiday Schedule (7 observed paid holidays)
Online and Retail Discounts, Company Match 401(k), Physical and Mental Health Wellness programs, and more!
Interested in joining the team? Check out our perks and benefits !
Show more
Show less","SQL, Snowflake, Excel, Data visualization, PowerBI, Dashboard, KPI, Trend analysis, Customer analytics, Project management, Crossfunctional collaboration, Data integrity, Data accuracy","sql, snowflake, excel, data visualization, powerbi, dashboard, kpi, trend analysis, customer analytics, project management, crossfunctional collaboration, data integrity, data accuracy","crossfunctional collaboration, customer analytics, dashboard, data accuracy, data integrity, excel, kpi, powerbi, project management, snowflake, sql, trend analysis, visualization"
PowerBI Developer / Data Analyst,"RSM Solutions, Inc","Niles, IL",https://www.linkedin.com/jobs/view/powerbi-developer-data-analyst-at-rsm-solutions-inc-3766149986,2023-12-17,Elgin,United States,Mid senior,Hybrid,"If you have read my job descriptions before, welcome back. If you are new, welcome to the party....you will notice that I like to add humor to my job descriptions...so you will see (hopefully) some chuckles as you read this.
Allow me to introduce myself. My name is Tom. I am Partner & VP at RSM Solutions, Inc. I have been recruiting technical talent for over 21 years (it was 21 in January 2023) and been in the tech space since the .com days of the 1990's.
For this role, social fit is almost as important as the technical fit. This is a relatively small organization (roughly 60 users or so), this creates a rather interesting environment. Have you ever been a part of an organization where your team members are really, actually, 'team members' (as opposed to just saying 'well...we work as a team' with the kind of sincerity that you hear from a Politician)...where people actually get together, help each other, and pitch in to help? Well, that's these guys.
So, what creates a good 'social fit' here? Well, I am glad you asked...here are some of those characteristics:
There is an expectation that you know what you are doing. So, there is limited - if any - micro management here. There are people here to help you, so you won't be an island. But, there is an expectation that - if you have questions with regard to how to do x, y or z, that you will already have tried a few potential solutions first as opposed to just asking 'how do I do this?'.
This is an environment where you are encouraged to try stuff. No one is going to chastise you or belittle you if you fail...that's how we all learn (I don't know about you, but I learn far more from my mistakes then my successes).
If you believe the world revolves around you or that you are God's (or whatever higher power you believe in) gift to IT, then you should probably look elsewhere. This is an environment with limited ego and limited drama.
It seems that a lot of job descriptions will say 'fast paced environment', or 'ability to multi task'...I roll my eyes when I see that and I bet you do as well. So, I won't state those here. However, if you have a keen understanding of that Pareto principle (that 80 / 20 rule) you will be off to a great start here. Their is a saying with these guys 'when everything is urgent, nothing is urgent'... So, that keen understanding of what is really important (that 20% that creates 80% of the value) will be needed here. Yep, you will only learn that over time here...so, no one expects you to know all this on day 1.
If you follow that famous Steven Covey habit of 'first seek to understand, then to be understood', we will be off to a great start. This is not an environment where the answer is 'build another dashboard'...instead, it is one where you will understand what is needed and then work off of that understanding.
I can only work with US Citizens or Green Card Holders for this role. I cannot work with H1, OPT or EAD Visa holders for this role. In addition, I can only work with candidates that reside in the Chicago area for this role. I am not really seeking to work with firms out there trying to empty their bench (you know who you are).
This is a contract role that we are intending to go contract-to-hire.
This role is being done in a hybrid manner with 3 days onsite and 2 days remote. They are located in Niles, Illinois.
Here is what we are seeking: We are looking for a Data Analyst to join the analytics team. You will be responsible for producing the data reports and analytics that the rest of the firm will rely upon for strategic decision making and day to day operations. So, you will be definitely making an impact here.
The ideal candidate will have experience managing Power BI and owning the data modeling. You should have ample experience producing sophisticated reports in Power BI and managing the Power BI Service. Knowledge of DAX, M, SQL, Star Schema modeling and RLS is a must. Python/ Pandas/ Matplotlib and the rest Python data science universe experience is a plus. A subset of the role will involve master data management as this client currently does not have an MDM plan. You will be managing projects that you undertake...so, while I don't need a 'classic' PM or BA, those PM & BA skills will surely be beneficial as you determine 'if I need this to be done by x time, that means I will need to get A, B and C from these stakeholders by this time'.
You will work, closely, with the Data/Dev teams to make sure that data is reachable, consistent, updated, and ready for the different Data models.
Here are some of the key responsibilities:
Maintain Power BI Data models.
Support the monthly metrics reporting efforts.
Produce statistical analysis on this client's data to allow senior leaders to identify advance trends.
Control access to data in the data models via Row Level Security.
Produce Ad hoc data reports on demand.
Stay in constant touch with the development teams, serving as a steward for this client's data.
Work with data and analytics experts to strive for greater functionality in our data systems.
Here is what we are looking for:
At least 3 years of experience with DAX, M and SQL.
At least 3 years of experience with statistical analysis.
Any Master Data Management experience.
A solid understanding of the 'bigger picture'. If you take a look at your work from the perspective of 'what problem are we trying to solve...and then work backward from there', that is precisely the skill set we are seeking for this role.
This is not a role for a contractor...so, we are seeking duration on the roles you have held. If you have spent 1 year on one role, 6 months on another, and a year on the next, we probably will not have a good fit for this role.At least 5 years of experience as a a Data Analyst. This should include authority over PowerBI systems as well as experience building and optimizing Power BI Data Models, Flows and Dashboards.
Clear troubleshooting skills when it comes to data integrity and data normalization.
A successful history of manipulating, processing and extracting value from disconnected datasets.
Strong project management and organizational skills.
Show more
Show less","Power BI, Data modeling, DAX, M, SQL, Star Schema modeling, RLS, Python, Pandas, Matplotlib, Master data management, Data integrity, Data normalization, Project management, Organizational skills, Statistical analysis, Troubleshooting","power bi, data modeling, dax, m, sql, star schema modeling, rls, python, pandas, matplotlib, master data management, data integrity, data normalization, project management, organizational skills, statistical analysis, troubleshooting","data integrity, data normalization, datamodeling, dax, m, master data management, matplotlib, organizational skills, pandas, powerbi, project management, python, rls, sql, star schema modeling, statistical analysis, troubleshooting"
Senior Data Engineer,Motion Recruitment,"Rosemont, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-motion-recruitment-3759643937,2023-12-17,Elgin,United States,Mid senior,Hybrid,"Senior Data Engineer We are looking for a Senior Data Engineering to join our team for a hybrid role in one of the following cities: Rosemont, IL; Baton Rouge, LA; Cedar Rapids; Des Moines, IA. In this role you will be responsible for the movement and capture of data across varies platforms and data stores.
We are a dynamic and innovative financial services company dedicated to empowering individuals and families to secure their financial future. With a strong commitment to customer-centric solutions, we offer a wide range of insurance and retirement products designed to provide peace of mind and financial stability. Our experienced team leverages cutting-edge technology and a deep understanding of the ever-evolving financial landscape to tailor solutions that meet the unique needs of our clients.
We're not just in the business of selling policies; we're in the business of building brighter futures. Join us on the path to financial security and prosperity for this exciting opportunity! Required Skills & Experience
Experience with the following tools:
Azure SQL Server
SSIS
Azure Data Factory
Data bricks
Spark
Azure Synapse
Ability to work independently and collaboratively in a team environment.
Excellent problem-solving skills and attention to detail
Desired Skills & Experience
Azure DevOps
Azure Event Hub or Kafka
Microsoft Visual Studio
What You Will Be Doing Daily Responsibilities
40% Hands On
30% Management Duties
30% Team Collaboration
The Offer
Bonus eligible
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k) {including match- if applicable}
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Mia Quinn
Show more
Show less","Data Engineering, Azure SQL Server, SSIS, Azure Data Factory, Databricks, Spark, Azure Synapse, Azure DevOps, Azure Event Hub, Kafka, Microsoft Visual Studio","data engineering, azure sql server, ssis, azure data factory, databricks, spark, azure synapse, azure devops, azure event hub, kafka, microsoft visual studio","azure data factory, azure devops, azure event hub, azure sql server, azure synapse, data engineering, databricks, kafka, microsoft visual studio, spark, ssis"
Data Analyst (MS EXCEL) - Contract - Hybrid - Schaumburg,"Resource 1, Inc.","Schaumburg, IL",https://www.linkedin.com/jobs/view/data-analyst-ms-excel-contract-hybrid-schaumburg-at-resource-1-inc-3785828406,2023-12-17,Elgin,United States,Mid senior,Hybrid,"Resource 1 is in need of a Data Analyst for a contract position. Will start off at 3 months with possible extensions. Client is based in
Schaumburg, IL
and selected candidate will be expected to
work onsite a few days per week
. Will play a crucial role in enabling our client’s sales and marketing teams to target prospective B2B customers in the construction industries, while laying the foundation for their future CRM database (Dynamics 365). The Data Analyst will gather data from various sources, such as internal databases, websites, membership lists and more (20K+ records), and then consolidate and prepare this data in MS Excel for short term use in lead generation efforts. This role requires a strong understanding of data extraction tools, data cleansing techniques and data integration processes to ensure that the system is populated with accurate and valuable information. Candidate can also make recommendations on how to automate data extraction/ population processes.
Responsibilities:
Data Sourcing
Will be provided with known lists of prospective business’s data sources
Identify and access additional relevant data sources, including other business entities, databases, associations, websites and files
Data Extraction
Develop and implement data extraction methods
Ensure the extraction process is efficient, accurate and compliant with data privacy regulations
Data Cleansing and Transformation
Clean and preprocess extracted data to remove duplicates, inconsistencies and errors
Standardize data formats for consistency
Transform data into a structured format that aligns with short- and long-term goals
Data Integration/ Database Creation
Integrate the cleaned and transformed data into a short-term solution to facilitate digital/ email marketing efforts
Set up automated data synchronization processes between source systems and database/ CRM
Quality Assurance and Documentation
Conduct data quality checks and audits to ensure data accuracy and integrity
Monitor data feeds for updates and discrepancies, taking corrective actions as needed
Maintain detailed documentation of data extraction, cleansing and integration processes
Create data mapping and transformation documentation for future reference
Ensure the data storage is compliant with data privacy regulations
Qualifications:
3+ years of experience as a Data Analyst
Experience with SQL to run queries and compare data
Experience using MS EXCEL to consolidate and prepare data
Strong understanding of data cleansing and transformation methodologies
Construction/ materials handling/ manufacturing/ supply chain industry experience is a plus
Experience with CRM/ marketing automation systems/ MS Dynamics is a plus
Knowledge of data privacy regulations, such as GDPR or CCPA
Show more
Show less","Data Analysis, Data Extraction, Data Cleansing, Data Transformation, Data Integration, Data Quality Assurance, Data Documentation, Data Mapping, SQL, MS Excel, Data Privacy Regulations (GDPR CCPA), CRM, Marketing Automation, MS Dynamics, Construction Industry Knowledge","data analysis, data extraction, data cleansing, data transformation, data integration, data quality assurance, data documentation, data mapping, sql, ms excel, data privacy regulations gdpr ccpa, crm, marketing automation, ms dynamics, construction industry knowledge","construction industry knowledge, crm, data documentation, data extraction, data integration, data mapping, data privacy regulations gdpr ccpa, data quality assurance, data transformation, dataanalytics, datacleaning, marketing automation, ms dynamics, ms excel, sql"
Senior Cloud Data Engineer,BDO USA,"Rosemont, IL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765466995,2023-12-17,Elgin,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, AI, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, SQL, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, ai, application development, data warehousing, data modeling, semantic model definition, star schema construction, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, sql, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai, ai algorithms, application development, automation tools, aws, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, computer vision, data lake medallion architecture, data ops, dataanalytics, datamodeling, datawarehouse, dbt, delta, devops, git, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, terraform"
Sr. Data Engineer,Diverse Lynx,"Schaumburg, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-diverse-lynx-3671627999,2023-12-17,Elgin,United States,Mid senior,Hybrid,"Job Title: Sr. Data Engineer
Location: Schaumburg, Illinois(day one onsite)
Position: Contract
Job Description
Integrate, enrich data from multiple sources and ETL the data to the requirements that support the reporting and analytics needs.
Should have Azure cloud (preferred), Public cloud data management experience and complex data processing.
Work with multiple data systems to source the data, Client, enrich and make the data available for users to consume.
Self driven data resource who can work independently. Healthcare data management experience is highly preferred.
Skill Set
Data Engineering (Pubic Cloud): Azure cloud, Snowflake, Snowflake ELT methodologies (Snowpark, streams& tasks etc.) ETL on Azure - ex. Azure Data Factory), Experience Data management work loads, scheduling, Automation, Scripting (Python, Scala etc), Cloud CICD/DevOps, Infrastructure as Code (IaC), API data ingestion, Kafka (nice to have)
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Data Engineering, Azure cloud, Snowflake, Snowflake ELT methodologies, ETL on Azure, Azure Data Factory, Data management work loads, Scheduling, Automation, Scripting, Python, Scala, Cloud CICD/DevOps, Infrastructure as Code (IaC), API data ingestion, Kafka","data engineering, azure cloud, snowflake, snowflake elt methodologies, etl on azure, azure data factory, data management work loads, scheduling, automation, scripting, python, scala, cloud cicddevops, infrastructure as code iac, api data ingestion, kafka","api data ingestion, automation, azure cloud, azure data factory, cloud cicddevops, data engineering, data management work loads, etl on azure, infrastructure as code iac, kafka, python, scala, scheduling, scripting, snowflake, snowflake elt methodologies"
User Insights Data Analyst,Demand.io,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/user-insights-data-analyst-at-demand-io-3775952247,2023-12-17,Alhambra,United States,Associate,Onsite,"At Demand.io (https://demand.io), we create AI-driven, community-centric digital experiences that help people shop smarter and connect around shared passions.
Newest out of our accelerator, SimplyCodes (https://simplycodes.com) is the world’s best coupon utility, powered by AI and an incredible community of shopping enthusiasts. We assist the coupon finding experience anywhere you shop, whether on your iPhone, on your browser, or via AI chat though our new ChatGPT plug-in. With over five million verified coupons and 3x more working codes than the leading coupon extensions, SimplyCodes is finally making finding coupons painless and easy, anywhere you shop.
We’re a small, nimble team of senior technologists with a passion for building innovative products that help millions of people shop smarter. Our mission is to solve everyday consumer problems at scale using advanced engineering, thoughtful design, and a community-first, web3-inspired mindset. As a media lab, we experiment, prototype and test new product concepts, investing where we see traction. As an e-commerce network, we drive over a billion dollars of sales each year to our retail partners through our network of popular shopping apps and websites.
What we're looking for
We are seeking a User Insights Data Analyst with a keen interest in evolving towards Data Science. This role is pivotal in driving data-driven decision-making and understanding user behavior to shape our product, marketing, and overall business strategy. It offers an opportunity to delve into predictive modeling and light data science tasks, with a path towards a more data science-focused role.
This role involves using data analytics, predictive modeling, and research to decode user behavior patterns and deliver actionable insights. It's a stepping stone toward a more specialized data science role.
Career-track-wise, this role provides a unique opportunity to advance your career in data science, offering a path to specialize further in predictive modeling and data-driven decision-making. If you're excited about harnessing data for strategic insights and growth, we invite you to join us.
What you’ll do:
Employ data analytics to deliver actionable insights that influence strategic decisions across various departments. Engage in predictive modeling to anticipate trends and user needs.
Analyze complex user data to decode behavior patterns, preferences, and challenges. Apply these insights to inform product development, marketing strategies, and customer experience enhancements.
Initiate and conduct data research to identify trends, untapped opportunities, and new growth areas. Transform research findings into viable, actionable strategies.
Partner with teams across the organization, such as product, marketing, design, and operations, to embed data-driven methodologies in all business aspects.
Design and maintain insightful dashboards and reports. Effectively communicate complex data insights and recommendations to diverse stakeholders.
About you:
Robust analytical skills with proficiency in data analysis tools (SQL, Python, R) and BI software (Tableau, Power BI).
Experience in user behavior analysis and customer data analytics.
Strong foundation in predictive modeling and an eagerness to develop further data science skills.
Excellent communication skills to articulate complex data in an understandable manner.
Bachelor’s or Master’s degree in Data Science, Statistics, Economics, Business, or a related field.
A proactive approach to learning and applying new data science methodologies and techniques.
About the job:
Starting cash compensation: $100K - $150K DOE.
Stock options: 0.05% to 0.10% initial grant
Eligibility for our Partners program. Unique profit-sharing mechanism tied to company performance.
Our new Santa Monica HQ is a large, state-of-the-art technology development facility offering prodigious open space, open work setup, large recreation & break room with free food, silence / focus facilities, podcasting and filming studio, and sweeping views from the ocean to downtown.
All meals provided free daily, fully stocked kitchen with free food, snacks, coffee and drinks.
Regular team-building events, dinners and activities.
Comprehensive health coverage (executive PPO, HMO options), dental, vision coverage, paid 100% for all your dependents.
Sponsorship for all ongoing education, courses, books and certifications.
401K
Hybrid, morning-focused in-office schedule concentrated on meetings and collaboration. Some employees work remotely in the latter half of the day. Additional hybrid options available for those with longer commutes.
Empowered, flexible, high trust work culture. Unlimited PTO.
Learn more about us at https://demand.io.
Commitment to diversity, equity & inclusion.
Demand.io believes diversity, equity and inclusion are paramount to building a meaningful company and culture. We’re committed to cultivating a safe and open environment for all voices to contribute.
Show more
Show less","SQL, Python, R, Tableau, Power BI, Data analysis, Predictive modeling, User behavior analysis, Customer data analytics, Data science, Communication skills, Datadriven decisionmaking, Data research, Dashboard design, Data reporting","sql, python, r, tableau, power bi, data analysis, predictive modeling, user behavior analysis, customer data analytics, data science, communication skills, datadriven decisionmaking, data research, dashboard design, data reporting","communication skills, customer data analytics, dashboard design, data reporting, data research, data science, dataanalytics, datadriven decisionmaking, powerbi, predictive modeling, python, r, sql, tableau, user behavior analysis"
Remote Business Data Analyst,Kforce Inc,"Los Angeles, CA",https://www.linkedin.com/jobs/view/remote-business-data-analyst-at-kforce-inc-3780633236,2023-12-17,Alhambra,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is seeking a remote Business Data Analyst. Responsibilities:
In this role, the Business Data Analyst will assist our client to realize the true potential of Big Data in positively influencing business decisions
Gather data from multiple sources and company data warehouses and analyze and interpret the data to extract such information that can be beneficial for businesses
The Business Data Analyst must visualize and report their findings by preparing comprehensive reports, graphs, charts, etc.
Visual representation of the data findings helps all the stakeholders (both technical and non-technical) to understand it better
Requirements
Bachelor of Science in Computer Science, Information Systems, Computer Engineering, or related field
Able to identify new sources of data and develop methods to improve data mining, analysis, and reporting
Able to present the findings in reports (in table, chart, or graph format) to help the management team in the decision-making process
Able to perform routine analysis tasks to support day-to-day business functioning and decision making
Able to identify, analyze, and interpret trends or patterns in complex data sets
Able to work with management to prioritize business and information needs
Able to locate and define new process improvement opportunities
Visa sponsorship is not available for this position
Experience With
Using the information that the business data analyst uncovers to identify problems and propose solutions
Ensuring that business data and reporting needs are met
Developing and monitoring data quality metrics
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $70,000 - $100,000 per year
Show more
Show less","Business Data Analysis, Data Mining, Data Analysis, Data Interpretation, Data Visualization, Reporting, Data Warehousing, Data Quality Metrics, Data Mining, Data Analysis, Data Interpretation, Data Visualization, Reporting, Data Quality Metrics, Tableau, Power BI, Excel, SQL, Python, R, Statistics, Machine Learning, Artificial Intelligence, Big Data","business data analysis, data mining, data analysis, data interpretation, data visualization, reporting, data warehousing, data quality metrics, data mining, data analysis, data interpretation, data visualization, reporting, data quality metrics, tableau, power bi, excel, sql, python, r, statistics, machine learning, artificial intelligence, big data","artificial intelligence, big data, business data analysis, data interpretation, data mining, data quality metrics, dataanalytics, datawarehouse, excel, machine learning, powerbi, python, r, reporting, sql, statistics, tableau, visualization"
Principal Command & Telemetry Database Engineer/Sr. Principal Command & Telemetry Database Engineer,Northrop Grumman,"Redondo Beach, CA",https://www.linkedin.com/jobs/view/principal-command-telemetry-database-engineer-sr-principal-command-telemetry-database-engineer-at-northrop-grumman-3757769621,2023-12-17,Alhambra,United States,Associate,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Strategic Space Systems Division (SSSD)
is an industry-leading provider for prime satellite and payload capabilities and directed energy and electronics solutions for national security, military, and civil customers. We are built on a heritage of providing innovative, affordable and reliable aerospace and defense products that our customers rely on to achieve mission success. Join the Space revolution and make the impossible, possible.
Responsibilities
Northrop Grumman Strategic Space Systems Division (SSSD) has an opening for a
Principal Command & Telemetry Database Engineer/Sr. Principal Command & Telemetry Database Engineer
to join our team of qualified, diverse individuals. This position will be located in
Redondo Beach, California
with responsibilities including (but not limited to) the following tasks for Space Systems programs:
Coordinate an integrated command and telemetry database solution between various technical teams
Work collaboratively with engineers of various disciplines and specialties across the program in a dynamic and fast-paced environment
Design, develop, and manage the system-level Command and Telemetry Database
Develop the database development plan, which includes the delivery plan, change management, and verification and validation tracking plans
Develop the IMS tasks and Giver-Receivers for the command and telemetry database process
Participate in, coordinate, and present material at working groups, design reviews, and technical product reviews
Support proposal process, as needed
Task/mentor junior engineers
This requisition may be filled at either a Principal Engineer level or Sr. Principal Engineer level based on the qualifications listed below.
Basic Qualifications for a Principal Command & Telemetry Database Engineer:
Bachelor's Degree in a STEM (Science, Technology, Engineering or Mathematics) discipline from an accredited university with 5 years of engineering experience OR Master's Degree in STEM with 3 years of engineering experience OR Ph.D. in a STEM discipline with 0 years of engineering experience.
Experience with systems engineering, software engineering, software systems engineering, database development, or hardware/software integration.
Candidate must be able and willing to obtain and maintain at least ONE of the following active U.S. Government security clearances: Dept of Defense Top Secret level - OR - SCI access with a Single Scope Background Investigation (SSBI)
Basic Qualifications for a Sr. Principal Command & Telemetry Database Engineer:
Bachelor's Degree in a STEM (Science, Technology, Engineering or Mathematics) discipline from an accredited university with 9 years of engineering experience OR Master's Degree in STEM with 7 years of engineering experience OR Ph.D. in a STEM discipline with 4 years of engineering experience.
Experience with systems engineering, software engineering, software systems engineering, database development, or hardware/software integration.
Candidate must be able and willing to obtain and maintain at least ONE of the following active U.S. Government security clearances: Dept of Defense Top Secret level - OR - SCI access with a Single Scope Background Investigation (SSBI)
Preferred Qualifications:
3+ years of experience with database design, development, or management.
Active DoD Top Secret or SCI clearance.
Experience with database tools/software scripting/programming language (e.g. MS Access, SQL, Java, Python, Visual Basic, XTEC, C/C++, etc.).
Familiarity with Command and Telemetry formats and parameters.
Spacecraft and/or payload electrical design integration, and/or command & telemetry database background with experience through the entire system lifecycle from concept, design, production, integration, test, launch, to operation support.
Experience working proposals.
Experience with Earned Value Management System.
Demonstrated success in managing programs/projects to cost, performance, and schedule requirements.
Demonstrated success in leading and developing cross-functional teams.
Detail-oriented and have excellent verbal and written communications skills, leadership skills, and organization skills.
Proficiency in Microsoft Office applications with strong presentation skills and capable of briefing audiences of varying technical knowledge.
Northrop Grumman offers a competitive and robust benefits program. As a full-time employee of Northrop Grumman, you are eligible for:
Medical, Dental & Vision coverage
401k
Educational Assistance
Life Insurance
Employee Assistance Programs & Work/Life Solutions
Paid Time Off
Health & Wellness Resources
Employee Discounts
Flexible Schedules: For example the ability to work a 9/80 work schedule, which allows an employee to work a nine-hour day Monday through Thursday and take every other Friday off of work.
Salary Range:
$104,600 - $157,000
Salary Range 2:
$129,700 - $194,500
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Systems engineering, Software engineering, Database development, Hardware integration, Microsoft Office, SQL, Java, Python, MS Access, Command and Telemetry, Spacecraft design, Earned Value Management, Communication skills, Leadership skills","systems engineering, software engineering, database development, hardware integration, microsoft office, sql, java, python, ms access, command and telemetry, spacecraft design, earned value management, communication skills, leadership skills","command and telemetry, communication skills, database development, earned value management, hardware integration, java, leadership skills, microsoft office, ms access, python, software engineering, spacecraft design, sql, systems engineering"
Data Governance Analyst,AltaMed Health Services,"Anaheim, CA",https://www.linkedin.com/jobs/view/data-governance-analyst-at-altamed-health-services-3771728266,2023-12-17,Alhambra,United States,Associate,Hybrid,"Job Overview
The Data Governance Management Office (DGMO) is responsible for the development, implementation, and continuous improvement of an enterprise data governance framework that will increase coordination and consistency across enterprise domains, establish a shared, cross-functional understanding of critical data assets and concepts, and improve AltaMed’s ability to make data-informed decisions using quality, trusted data & insights. The Data Governance (DG) Analyst is responsible for facilitating and managing data governance initiatives, including the development of data standards, policies and procedures, and working with stakeholders from across the organization to promote and implement data governance best practices (data classification, data lineage, lifecycle management). This role will convene cross-functional groups to understand and document data requirements, data flows, and data use. In addition, the DG Analyst will help define and execute toward the implementation of an enterprise data governance roadmap, collaborating with data & analytics technical teams to bridge business objectives to analytical products & solutions. To be successful in this role, individuals must have a blend of business acumen, a problem-solving attitude, data governance and data management knowledge, and a strong ability to communicate effectively with both technical and non-technical stakeholders. Other critical success factors include: comfort with ambiguity, curiosity and willingness to learn new subject matter, and an ability to build relationships with a focus on partnership. This role will support cross-functional teams with analysis and measurement of key data governance inputs, and requires some proficiency with data analysis and data quality measurement techniques.
Minimum Requirements
Bachelor’s degree in information technology, business, public health, mathematics, finance, or a related field, with appropriate professional and technical experience required.
Minimum of 4 years of experience in data management, data analytics, product management, or consulting, or other relevant work experience required.
Experience with project management techniques, such as cross-functional project coordination & delivery, work plan development, requirements gathering is required.
Prior professional experience working within a healthcare environment (payer, provider, vendor) required.
Proven ability to facilitate effective meetings and work sessions across various levels of staff and leadership required.
Prior experience with data governance tools and platforms highly preferred.
Previous experience in health care analytics, data strategy, business strategy, or clinical administration preferred.
Prior experience with product management & agile delivery principles (Scaled Agile - SAFe) preferred.
Skills and Abilities
Strong interpersonal skills with the ability to build and cultivate relationships with stakeholders, partners and team members, and cultivate a team environment.
Excellent communication and interpersonal skills, with the ability to clearly communicate (verbal or written) complex issues with both business and technical audiences.
Demonstrated critical thinking skills with the ability to identify, analyze, define and re-engineer processes.
Knowledge and experience with data governance practices and technology issues related to the management/governance of enterprise information assets.
Project, program, and/or product management capabilities.
Proven ability to work cross-functionally and manage multiple projects simultaneously.
Excellent problem-solving skills, decision-making capability, and a commitment to meeting deadlines.
Experience with SQL and relational databases.
Experience creating, synthesizing, and correlating business process workflows and data flows.
Strong customer service orientation, with an ability to understand and translate customer needs to data governance imperatives.
Excellent time management skills, with an ability to multi-task and work under time constraints.
Essential Job Functions
Contribute to and facilitate the creation of policies, standards, and documents on topics such as: data governance, data stewardship, master data management, data quality management.
Build and maintain data governance collateral to centralize, coordinate, and share DGMO content with a wide array of enterprise stakeholders.
Work with key business and technical stakeholders to create, manage, and curate an enterprise business glossary, data dictionary, and data catalog.
Capture and share detailed formal documents, such as governance meeting minutes for operational, tactical, strategic, and executive audiences.
Implement change control & change management processes and tools, and resolve enterprise data governance issues.
Develop and socialize data maps to ensure transparency in the lineage and flow of data within and outside of the organization.
Define and drive tactical roadmaps for enterprise data governance adoption, including socialization, stakeholder buy-in, operating model implementation, and data governance training.
Work with other data & analytics teams to develop, promote, and monitor adherence to data quality standards.
Develop, maintain, and socialize data inventory tools and methods to support data asset documentation & governance.
Work with key stakeholders to define, monitor, and share data governance key performance, quality, and adherence metrics.
Perform other duties as assigned.
Show more
Show less","Data Governance, Data Management, Data Analysis, Data Quality Measurement, SQL, Relational Databases, Business Process Workflows, Data Flows, Data Mapping, Data Lineage, Data Catalog, Data Dictionary, Business Glossary, Enterprise Data Governance Roadmap, Data Governance Best Practices, Data Classification, Data Lifecycle Management, CrossFunctional Project Coordination, Work Plan Development, Requirements Gathering, Data Governance Tools, Agile Delivery Principles, Scaled Agile (SAFe), SQL, Relational Databases, Healthcare Analytics, Data Strategy, Business Strategy, Clinical Administration, Product Management, Project Management, Program Management, ProblemSolving Skills, DecisionMaking, Time Management, MultiTasking, Customer Service Orientation, Data Stewardship, Master Data Management, Data Quality Management, Change Control, Change Management, Data Asset Documentation, Data Inventory Tools, Data Governance Key Performance Metrics","data governance, data management, data analysis, data quality measurement, sql, relational databases, business process workflows, data flows, data mapping, data lineage, data catalog, data dictionary, business glossary, enterprise data governance roadmap, data governance best practices, data classification, data lifecycle management, crossfunctional project coordination, work plan development, requirements gathering, data governance tools, agile delivery principles, scaled agile safe, sql, relational databases, healthcare analytics, data strategy, business strategy, clinical administration, product management, project management, program management, problemsolving skills, decisionmaking, time management, multitasking, customer service orientation, data stewardship, master data management, data quality management, change control, change management, data asset documentation, data inventory tools, data governance key performance metrics","agile delivery principles, business glossary, business process workflows, business strategy, change control, change management, clinical administration, crossfunctional project coordination, customer service orientation, data asset documentation, data catalog, data classification, data dictionary, data flows, data governance, data governance best practices, data governance key performance metrics, data governance tools, data inventory tools, data lifecycle management, data lineage, data management, data mapping, data quality management, data quality measurement, data stewardship, data strategy, dataanalytics, decisionmaking, enterprise data governance roadmap, healthcare analytics, master data management, multitasking, problemsolving skills, product management, program management, project management, relational databases, requirements gathering, scaled agile safe, sql, time management, work plan development"
Data Scientist,goop,"Santa Monica, CA",https://www.linkedin.com/jobs/view/data-scientist-at-goop-3775683541,2023-12-17,Alhambra,United States,Associate,Hybrid,"About The Company
Gwyneth Paltrow launched goop from her kitchen as a homespun weekly newsletter. It’s grown a lot since then; goop is a lifestyle platform encompassing curated products and content about beauty, wellness, fashion, food, and home. Pioneering the contextual commerce platform, goop allows readers to shop with meaning. goop is one of the rare places on the web where food, shopping, and mindfulness collide—where the ever-evolving intent is to make every choice count. We’re all resource-strapped, so goop hopes to surface the very best experiences, recipes, products, and advice.
About You
You thrive in an end-to-end data environment where you have the opportunity to build and deploy data pipelines, enhance and develop frameworks, and test + deploy production-level models that impact how we interact with our customers.
About The Role
goop seeks a talented and innovative Data Scientist to help us capitalize on opportunities in our broad and ever-growing datasets. As a part of the Data Science team, you will spend time working cross-functionally with engineers, product managers and marketers to build and test new consumer-facing algorithms and models.
Responsibilities Include:
Enhance, develop, deploy production level machine learning models and algorithms that will improve goop's customer experience.
Participate in the full development cycle: design, develop, QA, deploy, experiment, analyze, and iterate.
Collaborate and partner with key stakeholders throughout the organization. Identify opportunities to utilize Data Science to enhance business outcomes/customer experience.
Qualifications & Experience
Bachelor’s degree in engineering, Computer Science, Mathematics, Statistics, or other highly quantitative field, or equivalent practical experience.
3+ years of hands-on experience in data science, ideally with a focus in building production level machine learning solutions (product/content recommendation, personalization, prediction/forecasting, customer segmentation, NLP, classification etc.)
Advanced level in Python programming with extensive experience with the ML ecosystem - pandas, NumPy, scikit-learn, TensorFlow, PyTorch, etc.
Experience developing and deploying inference endpoints.
Advanced level experience with relational databases (SQL) and unstructured & semi-structured data
Understanding of A/B testing and practical application of inferential statistics
Working knowledge of AWS infrastructure and data tools (S3, SageMaker, Airflow, etc.)
FAQ
Compensation: $90,000-$102,500 + Equity. This is a full time, exempt role.
Please note that this range represents the low and high end of the anticipated base salary range for the Los Angeles, CA based position. Goop, in good faith, reasonably expects to pay the position within this salary range. Goop provides the salary range in compliance with all applicable federal, state and local laws. The actual base salary will depend on numerous factors such as: experience, training, knowledge and skills, and if the location of the job changes.
Benefits: Generous health benefits package, fertility benefits and paid parental leave.
Perks: “goopcation” paid company summer break, generous goop discounts, special offers with brand partners, access to custom lifestyle resources and events, and a beautiful work space in Santa Monica
Work Philosophy: At goop we believe that creativity, innovation and camaraderie are essential to our business, our culture and our employee’s growth and development. With our Hybrid Work Policy, we are committed to promoting collaboration, productivity and employee well-being by maximizing the benefits of both in-person and remote work. We are in office Tuesdays and Thursdays, as well as Mondays for those within a 15-mile radius of our Santa Monica office.
goop is an Equal Opportunity Employer. goop does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need. All information provided by the applicant is collected, stored and processed in accordance with the terms of our CCPA Notice for Job Candidates.
Job Disclosures: No applicant disclosures related to physical requirements or ADA-related considerations are relevant for this role
Application Requirements: All applicants will be reviewed through Greenhouse submission. Direct submissions to the People Operations Team members will not be reviewed separately.
Candidate Requirements: Applicant must have US work authorization.
Show more
Show less","Machine learning, Python, Pandas, NumPy, Scikitlearn, TensorFlow, PyTorch, SQL, A/B testing, Inferential statistics, AWS, Airflow, S3, SageMaker","machine learning, python, pandas, numpy, scikitlearn, tensorflow, pytorch, sql, ab testing, inferential statistics, aws, airflow, s3, sagemaker","ab testing, airflow, aws, inferential statistics, machine learning, numpy, pandas, python, pytorch, s3, sagemaker, scikitlearn, sql, tensorflow"
"Senior Risk Adjustment Data Analyst, Medicare Advantage",UCLA Health,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-risk-adjustment-data-analyst-medicare-advantage-at-ucla-health-3763387430,2023-12-17,Alhambra,United States,Associate,Hybrid,"Take on a key role within an award-winning health system. Help improve patient experiences and operational efficiency as part of a world-class healthcare team. Take your career in an exciting new direction. You can do all this and more at UCLA Health.
As an important member of our Medicare Advantage team, you will be responsible for leading the manipulation of large amounts of data and the development of dashboards in conjunction with our analytics team. You will also analyze large amounts of data to help identify opportunities for the development of new programs and interventions.
We’re seeking a self-motivated, flexible, goal-driven, service-oriented individual with:
Bachelor’s Degree in Business Administration, Economics, Health Care, Information Systems, Statistics, or other related field is required. Master’s Degree in related field preferred. Relevant combination of education and experience may be considered in lieu of degree.
Certification or progress toward certification is highly desired
In-depth understanding of Medicare Advantage (HCC model); statistically (V24 and V28) and chronic condition category levels required
Five or more years of analytics experience in healthcare, insurance or a related area
Proficiency with Visio; Visual Basic for automation in Excel, PowerPoint; SQL/server system tools; creation of file structures; Visual Analytics tools (Tableau) and Microsoft PowerBI
Expertise with SAS (Base, Macro, Graph, Email) a plus
Advanced knowledge of Excel (including pivot tables)
Strong communication, analytical, and problem-solving skills
Expertise with statistical software tools
Knowledge of R, Python, and SAS (including certification) is a plus
Actuarial or finance experience preferred
This position is hybrid and can be located anywhere in the US; must be able to work Pacific Standard Time hours
Salary Range: $78,800 - $175,000 Annually
UCLA Health is a world-renowned health system with four award-winning hospitals and more than 260 community clinics throughout Southern California. We’re also home to the world-class medical research and clinical education capabilities of the David Geffen School of Medicine. Through the efforts of our outstanding people, we have become Los Angeles’ trusted provider of exceptional, compassionate patient care. If you’re looking to experience greater challenge and fulfillment in your career, you can at UCLA Health.
Show more
Show less","Business Administration, Economics, Health Care, Information Systems, Statistics, Medicare Advantage (HCC model), Visual Basic, SQL, Visual Analytics, Tableau, Microsoft PowerBI, SAS, R, Python, Excel, Pivot tables, Statistical software, Actuarial, Finance","business administration, economics, health care, information systems, statistics, medicare advantage hcc model, visual basic, sql, visual analytics, tableau, microsoft powerbi, sas, r, python, excel, pivot tables, statistical software, actuarial, finance","actuarial, business administration, economics, excel, finance, health care, information systems, medicare advantage hcc model, microsoft powerbi, pivot tables, python, r, sas, sql, statistical software, statistics, tableau, visual analytics, visual basic"
Senior Data Engineer,Carvana,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-carvana-3758739478,2023-12-17,Alhambra,United States,Mid senior,Onsite,"About Carvana...
If you like disrupting the norm and are looking to join a company revolutionizing an industry then you will LOVE what Carvana has done for the car buying experience. Buying a car the old fashioned way sucks and we are working hard to make it NOT suck. At Carvana, our customers can hop online to...
Search and browse our inventory of over 40,000 vehicles that we own and certify
Narrow down search results using highly intelligent filtering tools/components
View vehicle details, Carfax reports and 360 rotating studio images for every vehicle
Secure financing in minutes using Carvana's in house service or their own bank
Interact with GUI components to easily customize loan length, down payment and monthly payment
Generate, upload and eSign all documents online (no ink necessary)
Schedule front door delivery or pick up at one of our vending machines
Trade in their existing vehicle or just sell it to Carvana (no purchase necessary)
Job Description...
The NGCP (Next Generation Communications Platform) team is a talented and humble group of people with backgrounds in Product, Design, Data Science, and Engineering, focused on Human-Computer Interaction (HCI). Our core product is a communication platform that car buyers, sellers, and Carvana Advocates rely on every day for a seamless car shopping experience. We are seeking a talented and driven Data Engineer to join our dynamic team, playing a critical role in developing and deploying reliable data systems and pipelines to support the analytics and processing of our communication platform.
As a core member of our NGCP team, you will work to support customer communication pipelines, architecting and maintaining enterprise data resources, and developing data design patterns to support business analytics. You will have the opportunity to work closely with our Product, Experience, and Software Engineering teams to deliver impact on every aspect of the purchase process, from search to vehicle delivery.
Responsibilities...
Data System Design and Scalability: Design robust and reusable data solutions for various platforms that include both batch and streaming systems, as well as understand and implement best practices in management of enterprise data.
ETL Pipeline Development: Building production streaming pipelines/ETL jobs to reliably move data between systems and and populate data in event streams and DB tables suitable for downstream consumption.
Deployment and Integration: Full software development lifecycle from design and development to testing and operating in production
Continuous Improvement and Innovation: Continuously improve data solutions to increase data quality and speed of data deliverability.
Improve Business Outcomes: Communicate effectively with engineers, product managers, data scientists, and data analysts to ensure correct and timely implementation of data requests.
Required Skills And Experience...
5+ years experience with Python
3+ years Spark or other big data processing (hadoop, flink) experience
Extensive background in SQL and relational databases like Postgres, MySQL, SQL Server
Experience with stream processing systems for analytics such as Amazon Kinesis, Kafka, RabbitMQ, Google Pub/Sub
Experience with data pipeline management tools such as Databricks Notebooks, Airflow, Prefect, Pachyderm or some other pipeline management system
Experience with CICD processes (GitHub, GitLab, dbx)
Strong experience with cloud APIs (e.g., GCP, AWS, Azure)
Expert knowledge of data integration concepts, business intelligence and data warehousing and implementing large systems
Understand how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure, etc.)
Production quality coding standards and patterns
BONUS SKILLS (helpful But Not Required)...
Snowflake or BigQuery
MLOps tools (Mlflow, Airflow, KServe, Kubeflow)
What You Can Expect In Return...
Full-Time Salary Position
Health, Dental and Vision benefits (Employee health 100% covered by Carvana)
Employee car discounts and perks
401K with company match
Access to opportunities to expand your skill set and share your knowledge with others across the organization
Legal Stuff...
Hiring is contingent on passing a complete background check. This role is eligible for visa sponsorship.
Carvana is an equal employment opportunity employer. All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law. Carvana also prohibits harassment of applicants or employees based on any of these protected categories.
Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Show more
Show less","Python, Spark, Hadoop, Flink, SQL, Postgres, MySQL, SQL Server, Amazon Kinesis, Kafka, RabbitMQ, Google Pub/Sub, Databricks Notebooks, Airflow, Prefect, Pachyderm, GitHub, GitLab, dbx, GCP, AWS, Azure, Snowflake, BigQuery, Mlflow, KServe, Kubeflow, CICD, Data warehousing, Data integration, Business intelligence, Data system design, Data pipeline development, Data processing, Cloud APIs, ETL","python, spark, hadoop, flink, sql, postgres, mysql, sql server, amazon kinesis, kafka, rabbitmq, google pubsub, databricks notebooks, airflow, prefect, pachyderm, github, gitlab, dbx, gcp, aws, azure, snowflake, bigquery, mlflow, kserve, kubeflow, cicd, data warehousing, data integration, business intelligence, data system design, data pipeline development, data processing, cloud apis, etl","airflow, amazon kinesis, aws, azure, bigquery, business intelligence, cicd, cloud apis, data integration, data pipeline development, data processing, data system design, databricks notebooks, datawarehouse, dbx, etl, flink, gcp, github, gitlab, google pubsub, hadoop, kafka, kserve, kubeflow, mlflow, mysql, pachyderm, postgres, prefect, python, rabbitmq, snowflake, spark, sql, sql server"
Database Administrator (DBA) / Senior Data Engineer,Trinus Corporation,"Commerce, CA",https://www.linkedin.com/jobs/view/database-administrator-dba-senior-data-engineer-at-trinus-corporation-3773879453,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Job Title: Database Administrator (DBA) / Senior Data Engineer
Location: Commerce CA (Hybrid Model)
Duration: 05+ Months Contract (With a possibility of Extension)
Pay Rate: 80$ - 85$ per hour on W2
Client Name: County of LA
Skills Preferred:
PostgreSQL database administration (DBA) experience o Database recovery and tuning o Maintain backups and disaster recovery o Monitor and optimize database performance o Manage security and authentication o Data extraction and transformation o Data modelling and schema design o Manage users - roles - and tablespaces o Resolve production data issues and preserve data integrity
Data engineering and data pipeline implementation experience
Microsoft Transact-SQL and Python development experience
Data Lake environment experience
Database design - security - and architecture experience
Docker experience
Git and GitHub experience
Apache Airflow configuration and management experience
Apache Spark experience
Microsoft SQL Server administration experience
Strong understanding of APIs
Grafana experience
Machine Learning pipeline experience
Experience with Cloud-based database platforms (Azure - AWS - GCP - etc.)
Linux Server or Linux subsystem experience such as YUM and DNF packages - implementing PostgreSQL - Centrify
Collaboration with data scientist teams - application teams - and project management teams
Experience Preferred:
Minimum of three (4) years of experience as a database administrator (DBA)
Minimum of two (2) years of experience as a data engineer
Experience in managing and maintaining Data Lakes - Data Warehouses - and public cloud or on-prem data environments
Education Preferred:
Bachelor’s degree in an IT-related or engineering field - or relevant certifications (e.g. Microsoft SQL Server - PostgreSQL DBA - AWS - Azure)
Additional Information:
Selected candidate will have to pass DPHs Live Scan check prior to engagement in addition to any requirements from the vendor
Work location is Commerce CA. with a hybrid of two days on-prem and three days work from home Address is 5555 Ferguson Drive Commerce CA 90022
Trinus Corporation, a leading provider of technology solutions and services with over 25 years of experience, is a certified WBE/MBE/SBE/SDB firm accredited by WBENC, NMSDC, and SBA.
Our mission is to shape the future of work by aligning the right mix of people, process, technology, and innovation to efficiently meet our clients' business objectives.
At Trinus, we understand that finding the right opportunity is pivotal in your career journey. Our staffing services go beyond mere placements; they are about matching your skills and aspirations with the perfect fit.
To learn more about us, please visit our website www.trinus.com
Show more
Show less","PostgreSQL, Data Recovery, Data Tuning, Backups, Disaster Recovery, Database Performance, Security, Authentication, Data Extraction, Data Transformation, Data Modeling, Schema Design, User Management, Role Management, Tablespaces, Data Integrity, Data Engineering, Data Pipeline Implementation, TransactSQL, Python, Data Lake, Database Design, Database Security, Database Architecture, Docker, Git, GitHub, Apache Airflow, Apache Spark, SQL Server, APIs, Grafana, Machine Learning Pipelines, Cloudbased Database Platforms, Azure, AWS, GCP, Linux Server, Linux Subsystem, YUM, DNF, PostgreSQL Implementation, Centrify, Collaboration with Data Scientist Teams, Collaboration with Application Teams, Collaboration with Project Management Teams, Live Scan Check","postgresql, data recovery, data tuning, backups, disaster recovery, database performance, security, authentication, data extraction, data transformation, data modeling, schema design, user management, role management, tablespaces, data integrity, data engineering, data pipeline implementation, transactsql, python, data lake, database design, database security, database architecture, docker, git, github, apache airflow, apache spark, sql server, apis, grafana, machine learning pipelines, cloudbased database platforms, azure, aws, gcp, linux server, linux subsystem, yum, dnf, postgresql implementation, centrify, collaboration with data scientist teams, collaboration with application teams, collaboration with project management teams, live scan check","apache airflow, apache spark, apis, authentication, aws, azure, backups, centrify, cloudbased database platforms, collaboration with application teams, collaboration with data scientist teams, collaboration with project management teams, data engineering, data extraction, data integrity, data lake, data pipeline implementation, data recovery, data transformation, data tuning, database architecture, database design, database performance, database security, datamodeling, disaster recovery, dnf, docker, gcp, git, github, grafana, linux server, linux subsystem, live scan check, machine learning pipelines, postgresql, postgresql implementation, python, role management, schema design, security, sql server, tablespaces, transactsql, user management, yum"
Junior Data Engineer,LinQuest,"El Segundo, CA",https://www.linkedin.com/jobs/view/junior-data-engineer-at-linquest-3690669825,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Position Summary
The Space Systems Command (SSC) Space Systems Integration Office (SSIO) Science & Engineering Technical Support team is looking for an energetic and innovative Data Engineer to assist in data mining information from multiple DoD repositories. Ideal candidates will have experience providing technical support and managing technical programs throughout the engineering life cycle of operational and/or new start technical demonstration space and ground systems. As a Data Engineer, the candidate will be adept at using large data sets to find opportunities for business optimization and using models to test the effectiveness of different courses of action. The Data Engineer candidate should have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using and creating algorithms and/or creating and running simulations. Candidates must have a proven ability to generate data-driven insights. Candidates must be comfortable working with a wide range of stakeholders and functional teams and have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. As a Data Engineer, you will have the opportunity to work with unstructured and structured data, develop databases, build data pipelines, and find innovative solutions to complex data problems. Candidates should be familiar with state-of-the-art data engineering techniques and have hands-on experience with data pipelines and should be comfortable manipulating, processing, and extracting value from large unstructured and structured datasets.
Key Responsibilities
Execute analytical experiments methodically to help solve various problems and make a true impact across various domains and industries.
Research state-of-the-art data mining methods and innovative statistical models
Identify relevant data sources and sets to mine for client business needs, and collect large structured and unstructured datasets and variables
Process, cleanse, and verify the integrity of data used for analysis
Develop code for predictive/prescriptive models with applied machine learning and other modeling techniques
Clearly document and present methodology, assumptions, and findings to stakeholders
Required Skills And Experience
Bachelor’s degree in Computer Science, Data Science, Data Analytics, Software Development, Information Technology, or related discipline
2-5 Years of experience in data engineering
Experience manipulating data with Python, SQL, R, VBA
Experience building data visualizations and communicating results to stakeholders
Experiences with databases, data repositories, and data management tools
Languages: Python, SQL
Expected to be able to adapt quickly to changing needs across several core areas, including data wrangling, data exploration, data analysis, and data visualization.
Must have the knowledge and communication skills to articulate trade-offs between analytic techniques, recommend a course of action to clients, and provide effective updates on project outcome uncertainty to leadership.
Developing data pipelines to support Analytics
Tools: Microsoft’s Power BI
US Citizenship and an active DoD SECRET, and the ability to obtain a DoD TOP SECRET with SCI eligibility Security Clearance
Preferred Skills And Experience
Languages: JavaScript, Java, C++, C#, HTML
Analysis tool(s): MATLAB/Simulink
MBSE tools: Sparx Enterprise Architect, Cameo
Agile tools: Jira, Confluence, Kanban
Experience with MS&A and MS&A tools
Experience with Configuration Management
Experience with System Administration
Background in web-design or dashboard design
Predictive and Prescriptive analysis
Experience working with and building REST APIs
Experience with Graph Analysis tools such as Neo4J and Cypher
Applying advanced analytics and machine learning techniques
Experience working as a member of an agile software development team (e.g., Scrum, Kanban)
Experience with MySQL, shell scripting, UNIX/LINUX, GIT, Docker
Experience with container deployment
Experience with continuous integration process and tools
Compensation for California:
Starting salary ranges from $100,000 to $130,000 depending on relevant experience and qualifications
Why LinQuest?
LinQuest Corporation has a stellar 40-year track record of providing end-to-end system-of-systems (SoS) architecture definition, engineering design, integration and test, and operations expertise to enable full lifecycle development and deployment of pre-eminent Space, Air, Land, Sea, Ground, and Cyberspace game-changing capabilities across US DOD and IC Customers’ portfolios. Unique combination of in-depth domain knowledge, lessons learned-honed best practices, and mission-specific applications of principles, tools, and techniques of Digital Engineering (DE), DE Ecosystem (DEE), and Model-Based Systems Engineering (MBSE) set LinQuest apart from the competition to consistently deliver stellar high-value results for our customers. LinQuest’s corporate vision and values place the employee at the center of utmost customer satisfaction, strategic business growth, and tactical execution excellence. Our employees’ creative and inspirational drive, sense of fulfillment of personal and professional growth, and tightknit camaraderie within and across lines of business are essential in gaining and maintaining exceptional LinQuest corporate-wide results of new business awards and renewed contracts.
Benefits
LinQuest offers comprehensive and competitive benefit offerings to our team members to include medical, dental, vision, retirement, paid time off, tuition reimbursement, company paid life insurance, and more! For additional information please visit: https://www.linquest.com/careers/our-benefits
Show more
Show less","Data Engineering, Data Mining, Data Analysis, Machine Learning, Python, SQL, R, VBA, Databases, Data Repositories, Data Management Tools, Data Visualization, Data Wrangling, Data Exploration, Data Analysis, Data Visualization, Agile Tools, MS&A Tools, Configuration Management, System Administration, Web Design, Dashboard Design, REST APIs, Graph Analysis Tools, Neo4J, Cypher, Advanced Analytics, MySQL, Shell Scripting, UNIX/LINUX, GIT, Docker, Continuous Integration, Scrum, Kanban","data engineering, data mining, data analysis, machine learning, python, sql, r, vba, databases, data repositories, data management tools, data visualization, data wrangling, data exploration, data analysis, data visualization, agile tools, msa tools, configuration management, system administration, web design, dashboard design, rest apis, graph analysis tools, neo4j, cypher, advanced analytics, mysql, shell scripting, unixlinux, git, docker, continuous integration, scrum, kanban","advanced analytics, agile tools, configuration management, continuous integration, cypher, dashboard design, data engineering, data exploration, data management tools, data mining, data repositories, data wrangling, dataanalytics, databases, docker, git, graph analysis tools, kanban, machine learning, msa tools, mysql, neo4j, python, r, rest apis, scrum, shell scripting, sql, system administration, unixlinux, vba, visualization, web design"
Data Warehouse Engineer/EDW Engineer/EDM Engineer,Intellectt Inc,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-warehouse-engineer-edw-engineer-edm-engineer-at-intellectt-inc-3655980917,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Hi,
Hope you are doing well,
This is
Bhanu
from
Intellectt INC
; we have an immediate opportunity with one of our clients. Please find the below job description and if you are interested, please forward your resume to
bhanuprakash@intellectt.com
Role:
Data Warehouse Engineer/EDW Systems Analyst
Location:
Los Angeles, CA
Duration:
12+ Month Contract (with extension)
Description
EDM System Analyst will work in Enterprise Data Management (EDM) team in Agile environment. EDM Analyst will be responsible for requirements gathering for the data systems of the EDM team; leading working sessions with business partners to accurately document all requirements, supporting developers to understand the requirements during implementation, and supporting quality assurance team to understand the requirements for testing. The EDM Analyst needs to perform activities with little supervisory support; and, working multiple projects in parallel.
This position directly requires good communications skills working with business, developers, quality assurance and other vendors.
Qualifications
Good understanding in database design concepts, logical/physical data models.
Experience on healthcare domain.
Good knowledge and experience of database SQL language.
Thorough knowledge of SDLC.
Experience on working on Agile methodology and familiar with JIRA and confluence tools.
Experience in documenting requirements and negotiating commitments.
Possess excellent communication skills, both written and verbal.
Proficient in data analysis and problem solving.
Must develop, foster, and maintain an open, professional relationship with developers, IT staff, management, and business customers.
Strong Understanding of DW/BI concepts.
Responsibilities
Conduct meetings to gather business requirements for new product development and enhancements.
Document business requirements with acceptance criteria in user stories and/or business requirements document.
Analyze data sources used in the project to facilitate mappings for project team.
Analyze any operational data issue.
Participate in design discussions and provide clarification of requirements.
Participate in issue discussions to provide clarification of requirements.
Work in iterative development methodology, collaborate with other project team members, update tasks in JIRA.
Work with EDM development team and QA team on implementations.
Perform other duties as assigned.
Preferred Experience
Experience on Confluence documentation.
Broad-based knowledge of application systems technologies.
Show more
Show less","Data Warehouse, Agile, SQL, SDLC, JIRA, Confluence, Business Requirements, Data Sources, Data Analysis, Problem Solving, Communication, DW/BI","data warehouse, agile, sql, sdlc, jira, confluence, business requirements, data sources, data analysis, problem solving, communication, dwbi","agile, business requirements, communication, confluence, data sources, dataanalytics, datawarehouse, dwbi, jira, problem solving, sdlc, sql"
Senior Data Engineer,Carvana,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-carvana-3707674065,2023-12-17,Alhambra,United States,Mid senior,Onsite,"About Carvana...
If you like disrupting the norm and are looking to join a company revolutionizing an industry then you will LOVE what Carvana has done for the car buying experience. Buying a car the old fashioned way sucks and we are working hard to make it NOT suck. At Carvana, our customers can hop online to...
Search and browse our inventory of over 40,000 vehicles that we own and certify
Narrow down search results using highly intelligent filtering tools/components
View vehicle details, Carfax reports and 360 rotating studio images for every vehicle
Secure financing in minutes using Carvana’s in house service or their own bank
Interact with GUI components to easily customize loan length, down payment and monthly payment
Generate, upload and eSign all documents online (no ink necessary)
Schedule front door delivery or pick up at one of our vending machines
Trade in their existing vehicle or just sell it to Carvana (no purchase necessary)
Job Description...
The NGCP (Next Generation Communications Platform) team is a talented and humble group of people with backgrounds in Product, Design, Data Science, and Engineering, focused on Human-Computer Interaction (HCI). Our core product is a communication platform that car buyers, sellers, and Carvana Advocates rely on every day for a seamless car shopping experience. We are seeking a talented and driven Data Engineer to join our dynamic team, playing a critical role in developing and deploying reliable data systems and pipelines to support the analytics and processing of our communication platform.
As a core member of our NGCP team, you will work to support customer communication pipelines, architecting and maintaining enterprise data resources, and developing data design patterns to support business analytics. You will have the opportunity to work closely with our Product, Experience, and Software Engineering teams to deliver impact on every aspect of the purchase process, from search to vehicle delivery.
Responsibilities...
Data System Design and Scalability: Design robust and reusable data solutions for various platforms that include both batch and streaming systems, as well as understand and implement best practices in management of enterprise data.
ETL Pipeline Development: Building production streaming pipelines/ETL jobs to reliably move data between systems and and populate data in event streams and DB tables suitable for downstream consumption.
Deployment and Integration: Full software development lifecycle from design and development to testing and operating in production
Continuous Improvement and Innovation: Continuously improve data solutions to increase data quality and speed of data deliverability.
Improve Business Outcomes: Communicate effectively with engineers, product managers, data scientists, and data analysts to ensure correct and timely implementation of data requests.
Required Skills And Experience...
5+ years experience with Python
3+ years Spark or other big data processing (hadoop, flink) experience
Extensive background in SQL and relational databases like Postgres, MySQL, SQL Server
Experience with stream processing systems for analytics such as Amazon Kinesis, Kafka, RabbitMQ, Google Pub/Sub
Experience with data pipeline management tools such as Databricks Notebooks, Airflow, Prefect, Pachyderm or some other pipeline management system
Experience with CICD processes (GitHub, GitLab, dbx)
Strong experience with cloud APIs (e.g., GCP, AWS, Azure)
Expert knowledge of data integration concepts, business intelligence and data warehousing and implementing large systems
Understand how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure, etc.)
Production quality coding standards and patterns
BONUS SKILLS (helpful But Not Required)...
Snowflake or BigQuery
MLOps tools (Mlflow, Airflow, KServe, Kubeflow)
What You Can Expect In Return...
Full-Time Salary Position
Health, Dental and Vision benefits (Employee health 100% covered by Carvana)
Employee car discounts and perks
401K with company match
Access to opportunities to expand your skill set and share your knowledge with others across the organization
Legal Stuff...
Hiring is contingent on passing a complete background check. This role is eligible for visa sponsorship.
Carvana is an equal employment opportunity employer. All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law. Carvana also prohibits harassment of applicants or employees based on any of these protected categories.
Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Show more
Show less","Python, Spark, Hadoop, Flink, SQL, Postgres, MySQL, SQL Server, Amazon Kinesis, Kafka, RabbitMQ, Google Pub/Sub, Databricks, Airflow, Prefect, Pachyderm, GitHub, GitLab, dbx, GCP, AWS, Azure, Data integration, Business intelligence, Data warehousing, Resource usage, System hardware, Data storage, Query optimization, AWS infrastructure, Production quality coding, Snowflake, BigQuery, MLOps, Mlflow, KServe, Kubeflow","python, spark, hadoop, flink, sql, postgres, mysql, sql server, amazon kinesis, kafka, rabbitmq, google pubsub, databricks, airflow, prefect, pachyderm, github, gitlab, dbx, gcp, aws, azure, data integration, business intelligence, data warehousing, resource usage, system hardware, data storage, query optimization, aws infrastructure, production quality coding, snowflake, bigquery, mlops, mlflow, kserve, kubeflow","airflow, amazon kinesis, aws, aws infrastructure, azure, bigquery, business intelligence, data integration, data storage, databricks, datawarehouse, dbx, flink, gcp, github, gitlab, google pubsub, hadoop, kafka, kserve, kubeflow, mlflow, mlops, mysql, pachyderm, postgres, prefect, production quality coding, python, query optimization, rabbitmq, resource usage, snowflake, spark, sql, sql server, system hardware"
Senior Data Engineer,Jobot,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobot-3784520991,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Senior Data Engineer with profitable Financial Services firm
This Jobot Job is hosted by Oliver Belkin
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $165,000 - $225,000 per year
A Bit About Us
We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team in the technology industry. The ideal candidate will have a strong background in AWS, SQL, Airflow, Python, PySpark, and Redshift, or equivalent technologies. As a Senior Data Engineer, you will be responsible for designing, building, and maintaining our data infrastructure to support our growing business needs. This is a full-time position that requires a minimum of 5 years of experience in the field.
Why join us?
With the ability to work remotely or in our Los Angeles office, we offer top compensation and benefits as we are a leader within our industry. You'll have the ability to work with a group of other like-minded individuals to build great software.
Job Details
Responsibilities
Design, build, and maintain our data infrastructure using AWS, SQL, Airflow, Python, PySpark, and Redshift.
Develop and implement data pipelines to extract, transform, and load data from various sources.
Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions.
Optimize data storage and retrieval for performance and scalability.
Monitor and troubleshoot data pipelines to ensure data accuracy and availability.
Develop and maintain documentation of data infrastructure and processes.
Stay up-to-date with the latest technologies and trends in data engineering.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or related field.
Minimum of 5 years of experience in data engineering or related field.
Strong proficiency in AWS, SQL, Airflow, Python, PySpark, and Redshift, or equivalent technologies.
Experience with data modeling, data warehousing, and ETL processes.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.
Ability to work independently and as part of a team.
Experience with Agile development methodologies is a plus.
If you are passionate about data engineering and have a proven track record of designing and implementing scalable data infrastructure, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits package, and a dynamic work environment where you can grow and thrive.
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","AWS, SQL, Airflow, Python, PySpark, Redshift, Data modeling, Data warehousing, ETL processes, Problemsolving skills, Attention to detail, Communication skills, Collaboration skills, Agile methodologies","aws, sql, airflow, python, pyspark, redshift, data modeling, data warehousing, etl processes, problemsolving skills, attention to detail, communication skills, collaboration skills, agile methodologies","agile methodologies, airflow, attention to detail, aws, collaboration skills, communication skills, datamodeling, datawarehouse, etl, problemsolving skills, python, redshift, spark, sql"
Staff Data Engineer,SIPE Education,"Los Angeles, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-sipe-education-3757944508,2023-12-17,Alhambra,United States,Mid senior,Onsite,"The Role
Linktree’s Data Platform team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, analytics engineering, product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modeling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Develop and implement the strategy for the Data Platform in close collaboration with other data and engineering leaders.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes providing other teams with the self-service capabilities to ingest, transform, consume and integrate data with other systems for both production and analytics use-cases.
You will be building and improving data-driven systems including, but not limited to, instrumentation at the source, event ingestion, event brokers, data warehouses, pipelines and integration systems used to activate data in third parties, such as an experimentation platform, product analytics and marketing tooling.
Ensure our data platform continues to scale with our rapidly growing user base and ever-evolving product. Linktree’s growth constantly creates new challenges like needing to master data governance and discoverability with many different teams producing data, or hitting the scalability limits of cloud providers.
What We Are Looking For
A platform mindset. Our Data Platform team does not do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
Data as well as a software engineering background. Our data platform spans from the SDKs that our teams use to emit and consume data in front- and back-end applications to the integration with various third-party systems, including everything in between (data ingestion, event brokers, data persistence, etc.). To be successful in your role you’ll need to be proficient in both. Having worked with distributed, event-driven systems is a plus.
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!
Where And How We Work
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets, and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid ""remote, but in-office sometimes"" approach.
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.
We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a ""normal"" 9-5 being a global company, but we aim to work asynchronously where possible.
Our Culture And Benefits
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!
Our Story
We're on a mission to empower anyone to curate and grow their digital universe. We created the ""link in bio"" category and are trusted by some of the world's biggest brands and celebrities including TikTok, The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in
going further together.
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at
recruiting@linktr.ee
– your needs are important to us.
Show more
Show less","Data engineering, Data platform, Datadriven systems, Instrumentation, Event ingestion, Event brokers, Data warehouses, Pipelines, Data integration, Data governance, Data discoverability, SDKs, Distributed systems, Eventdriven systems, Scalability, Compensation package, Cash compensation, Final offer, Remote work, Hybrid work, Collaboration, Diversity, Inclusion, Flexibility, Autonomy, Medical insurance, Dental insurance, Vision insurance, Disability insurance, Life insurance, Employee Stock Option Program, Parental leave program, Volunteering leave, DE&I initiatives","data engineering, data platform, datadriven systems, instrumentation, event ingestion, event brokers, data warehouses, pipelines, data integration, data governance, data discoverability, sdks, distributed systems, eventdriven systems, scalability, compensation package, cash compensation, final offer, remote work, hybrid work, collaboration, diversity, inclusion, flexibility, autonomy, medical insurance, dental insurance, vision insurance, disability insurance, life insurance, employee stock option program, parental leave program, volunteering leave, dei initiatives","autonomy, cash compensation, collaboration, compensation package, data discoverability, data engineering, data governance, data integration, data platform, data warehouses, datadriven systems, dei initiatives, dental insurance, disability insurance, distributed systems, diversity, employee stock option program, event brokers, event ingestion, eventdriven systems, final offer, flexibility, hybrid work, inclusion, instrumentation, life insurance, medical insurance, parental leave program, pipelines, remote work, scalability, sdks, vision insurance, volunteering leave"
Staff Data Engineer,Linktree,"Los Angeles, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-linktree-3727538284,2023-12-17,Alhambra,United States,Mid senior,Onsite,"The Role
Linktree’s Data Platform team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, analytics engineering, product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modeling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Develop and implement the strategy for the Data Platform in close collaboration with other data and engineering leaders.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes providing other teams with the self-service capabilities to ingest, transform, consume and integrate data with other systems for both production and analytics use-cases.
You will be building and improving data-driven systems including, but not limited to, instrumentation at the source, event ingestion, event brokers, data warehouses, pipelines and integration systems used to activate data in third parties, such as an experimentation platform, product analytics and marketing tooling.
Ensure our data platform continues to scale with our rapidly growing user base and ever-evolving product. Linktree’s growth constantly creates new challenges like needing to master data governance and discoverability with many different teams producing data, or hitting the scalability limits of cloud providers.
What We Are Looking For
A platform mindset. Our Data Platform team does not do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
Data as well as a software engineering background. Our data platform spans from the SDKs that our teams use to emit and consume data in front- and back-end applications to the integration with various third-party systems, including everything in between (data ingestion, event brokers, data persistence, etc.). To be successful in your role you’ll need to be proficient in both. Having worked with distributed, event-driven systems is a plus.
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!
Where And How We Work
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets, and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid ""remote, but in-office sometimes"" approach.
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.
We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a ""normal"" 9-5 being a global company, but we aim to work asynchronously where possible.
Our Culture And Benefits
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!
Our Story
We're on a mission to empower anyone to curate and grow their digital universe. We created the ""link in bio"" category and are trusted by some of the world's biggest brands and celebrities including TikTok, The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in
going further together.
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at
recruiting@linktr.ee
– your needs are important to us.
Show more
Show less","Data Engineering, Data Platform Development, Data Transformation, Data Integration, Data Warehousing, Data Pipelines, Event Ingestion, Event Brokers, Instrumentation, Data Governance, Data Discoverability, Data Analytics, Experimentation Platform, Product Analytics, Marketing Tooling, Cloud Providers, Distributed Systems, EventDriven Systems, Scalability, SQL, Python, Java, C++, AWS, Google Cloud Platform, Azure","data engineering, data platform development, data transformation, data integration, data warehousing, data pipelines, event ingestion, event brokers, instrumentation, data governance, data discoverability, data analytics, experimentation platform, product analytics, marketing tooling, cloud providers, distributed systems, eventdriven systems, scalability, sql, python, java, c, aws, google cloud platform, azure","aws, azure, c, cloud providers, data discoverability, data engineering, data governance, data integration, data platform development, data transformation, dataanalytics, datapipeline, datawarehouse, distributed systems, event brokers, event ingestion, eventdriven systems, experimentation platform, google cloud platform, instrumentation, java, marketing tooling, product analytics, python, scalability, sql"
Data Analyst,Amity Foundation,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-analyst-at-amity-foundation-3787786901,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Amity Foundation
, internationally acclaimed teaching, and therapeutic community is seeking compassionate and enthusiastic individuals with a desire to teach, learn and join our community as a Data Analyst. This groundbreaking opportunity not only will allow you to work with our prison programs to help the inmate population but will also enhance your training and experience in the field.
About Amity:
Amity Foundation
is a safe place where people can change in an environment that fosters trust; where new values can be formed; responsibility developed, and lasting relationships built. Amity is dedicated to the inclusion and habilitation of people marginalized by addiction, homelessness, trauma, criminality, incarceration, poverty, racism, sexism, and violence. Amity is committed to research, development implementation, and dissemination of information regarding community building.
Remembrance, Resolution, Reconciliation, Restoration, Renewal
About the Position:
As a Data Analyst, you will be responsible for the efficient extraction, transformation, and loading (ETL) of data from various sources, creating and maintaining documentation of all data sources, and developing insightful data visualizations using business intelligence tools such as Power Bi. Your role is essential in ensuring that data is accurate, accessible, and actionable for informed decision-making for all key stakeholders.
Salary Range
:
$72K/yr - $85K/yr
What You Will Do:
ETL (Extract, Transform, Load):
Design and implement ETL processes to extract data from diverse sources, transform it into a usable format, and load it into databases or data warehouses.
Collaborate with data engineers to optimize ETL pipelines for scalability, reliability, and performance.
Monitor and troubleshoot ETL processes to ensure data quality and consistency.
Automate ETL workflows to streamline data integration and reduce manual intervention.
Data Visualizations:
Create interactive and visually appealing dashboards, reports, and data visualizations using Power BI, or other relevant software.
Connect Power BI to various data sources, including databases, APIs, and online services.
Transform raw data into meaningful insights, trends, and actionable information.
Collaborate with stakeholders to understand their reporting needs and provide tailored Power BI
Data Governance:
Implement and enforce data governance practices, including data quality standards, data lineage, and metadata management.
Ensure compliance with data security and privacy regulations, including access controls and encryption.
Establish data retention policies and manage data archiving and purging processes.
Provide data-driven strategies that enhance operational efficiency and drive improvements.
Documentation and Training:
Document ETL processes, API endpoints, and Power BI reports for knowledge sharing and troubleshooting.
Collaborate with department Data Manager to provide training and support to end-users and stakeholders on data access, API usage, and Power BI reporting.
Continuous Learning and Innovation:
Stay updated with the latest industry trends, tools, and techniques in data analysis and visualization.
Identify opportunities for process improvement and automation to enhance data analysis efficiency.
Performs other related duties as assigned.
What You Will Bring:
Required:
1-3 years experience or related work and/or Bachelor’s degree in a technical or other applicable discipline.
Proven experience as a Data Analyst or similar role, with a track record of successfully delivering data-driven insights.
Proven experience in data visualization tools such as Tableau, Power BI, or similar (non-Excel) software.
Proven experience in ETL development, including proficiency with ETL tools (e.g., Apache NiFi, Python)
Understanding of data governance principles, data security, and compliance regulations.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills to work with cross-functional teams and stakeholders.
Ability to adapt to evolving data technologies and industry best practices.
Preferred:
Programming skills, including knowledge of scripting languages (e.g., Python, PowerShell) for automation.
Expertise in API development and management, with knowledge of RESTful API design principles.
Demonstrated experience with one or more of the following modern data management concepts - data lakes, nonrelational databases (NoSQL), cloud hosting (AWS), serverless storage or automated database recovery
Project management.
What We Offer:
100% Employer-Sponsored HMO plan.
PPO Medical, Dental, Vision.
Paid vacation, sick time, & 11 holidays.
401K, HSA, & Life insurance programs.
Community-oriented workplace.
An organization committed to community action & social justice.
Powered by JazzHR
p15epV0Qg7
Show more
Show less","Data Analyst, ETL, Data transformation, Data loading, Data visualization, Power BI, Data governance, Data security, Data privacy, Data management, Documentation, Training, Python, Apache NiFi, NoSQL, Cloud hosting, AWS, Serverless storage, Automated database recovery, Project management, Tableau","data analyst, etl, data transformation, data loading, data visualization, power bi, data governance, data security, data privacy, data management, documentation, training, python, apache nifi, nosql, cloud hosting, aws, serverless storage, automated database recovery, project management, tableau","apache nifi, automated database recovery, aws, cloud hosting, data governance, data loading, data management, data privacy, data security, data transformation, dataanalytics, documentation, etl, nosql, powerbi, project management, python, serverless storage, tableau, training, visualization"
Sr. Big Data Engineer,NR Consulting,"Santa Monica, CA",https://www.linkedin.com/jobs/view/sr-big-data-engineer-at-nr-consulting-3762821149,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Job Title: Sr. Big Data Engineer
Duration: FTE /Permanent Hiring
Location: Santa Monica CA ( Remote Till Covid )
Description
5&plus; years of data engineering experience developing large data pipelines
Strong SQL skills and ability to create queries to extract data and build performant datasets
Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data
Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
Good Scripting skills, including Bash scripting and Python
Familiar with Scrum and Agile methodologies
You are a problem solver with strong attention to detail and excellent analytical and communication skills
Bachelor's or Master's Degree in Computer Science, Information Systems or related field
Top skills:
Big Data using Spark - key technology
SQL - must
Programming- Python must have main language used for programming
nice to have- AWS/cloud experience- will be helpful if they have cloud environment.
5ys&plus; experience
Show more
Show less","Spark, Hadoop, Hive, Presto, PySpark, Snowflake, Redshift, Big Query, AWS, S3, EMR, EC2, Airflow, Data Modeling, Data Warehousing, Bash scripting, Python, Scrum, Agile","spark, hadoop, hive, presto, pyspark, snowflake, redshift, big query, aws, s3, emr, ec2, airflow, data modeling, data warehousing, bash scripting, python, scrum, agile","agile, airflow, aws, bash scripting, big query, datamodeling, datawarehouse, ec2, emr, hadoop, hive, presto, python, redshift, s3, scrum, snowflake, spark"
Senior Data Engineer,Jobot,"Universal City, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobot-3785312869,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
A Highly Sought-After Company is looking for a Senior Data Engineer to join their Enterprise team!
This Jobot Job is hosted by Christie Shell
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $140,000 - $180,000 per year
A Bit About Us
A leading retailer, and ever-expanding well-known household name, is growing their Data Engineering team and looking for a Senior Data Engineer to join them! You'll work alongside a team of some of the best and brightest engineers, and bring your expertise around data infrastructure to build scalable, secure solutions with low latency, and to ship large-scale systems to production. You'll drive the direction of enterprise data solutions while working with Deep Learning models, streaming technologies and cutting edge software. You'll implement Big Data and Analytics solutions and have the opportunity to influence the the technological decisions across the organization.
Why join us?
What We Offer
Competitive base salary and overall compensation package
Full benefits Medical, Dental, Vision
PTO, vacation, sick, and holidays
Life Insurance coverage
401 (K) with generous company match
Job Details
Job Details
We are seeking an experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a pivotal role in shaping the future of the organization by leveraging your skills and experiences in AWS Cloud, AWS Redshift, Data Engineering, ETL, Data Pipelines, and Data Warehousing. This is a permanent position, offering a unique opportunity to work with cutting-edge technologies and to influence our data strategy.
Responsibilities
Design, build, and maintain our data infrastructure using AWS Cloud, AWS Redshift, and other modern data technologies to ensure data is accessible and reliable for our team's use.
Develop and manage ETL processes to ingest and transform large volumes of data from various sources, ensuring the data's accuracy and availability.
Build and optimize data pipelines, architectures, and data sets to support business requirements.
Work closely with the data science team to ensure the data infrastructure supports their needs and helps them to deliver insights effectively.
Develop data set processes for data modeling, mining, and production.
Implement data governance and ensure data integrity by validating data for new and ongoing projects.
Proactively identify opportunities for data acquisition and new uses for existing data.
Collaborate with cross-functional teams to support their data infrastructure needs.
Qualifications
Bachelor's Degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Advanced degree is a plus.
Minimum of 3+ years of experience in a Data Engineer role.
Experience with AWS cloud services AWS Redshift, EC2, EMR, RDS, S3, etc.
Experience building and optimizing data pipelines, architectures, and data sets.
Strong experience in SQL, Python, Java, or other programming languages.
Experience with big data tools Hadoop, Spark, Kafka, etc. is a plus.
Strong analytic skills related to working with unstructured datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Ability to work independently and as part of a team, demonstrating initiative and flexibility.
Excellent communication and presentation skills with the ability to articulate complex concepts to non-technical stakeholders.
This is a fantastic opportunity to join a forward-thinking company where you will play a significant role in influencing our data strategy. If you are a problem solver, passionate about data engineering, and ready to take your career to the next level, we would love to hear from you. Apply now and let's shape the future of the sales industry together!
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","AWS Cloud, AWS Redshift, EC2, EMR, RDS, S3, SQL, Python, Java, Hadoop, Spark, Kafka, Data Engineering, ETL, Data Pipelines, Data Warehousing, Data governance, Data integrity, Data acquisition, Data Infrastructure, Big Data, Data Analytics, Data Mining, Data Science, Data Modeling, Data Architecture, Data Sets, Cloud Services, Data Processing, Data Analysis, Data Visualization, Data Management","aws cloud, aws redshift, ec2, emr, rds, s3, sql, python, java, hadoop, spark, kafka, data engineering, etl, data pipelines, data warehousing, data governance, data integrity, data acquisition, data infrastructure, big data, data analytics, data mining, data science, data modeling, data architecture, data sets, cloud services, data processing, data analysis, data visualization, data management","aws cloud, aws redshift, big data, cloud services, data acquisition, data architecture, data engineering, data governance, data infrastructure, data integrity, data management, data mining, data processing, data science, data sets, dataanalytics, datamodeling, datapipeline, datawarehouse, ec2, emr, etl, hadoop, java, kafka, python, rds, s3, spark, sql, visualization"
Data Center Infrastructure Engineer,"QuadraNet Enterprises, LLC.","Los Angeles, CA",https://www.linkedin.com/jobs/view/data-center-infrastructure-engineer-at-quadranet-enterprises-llc-3776096647,2023-12-17,Alhambra,United States,Mid senior,Onsite,"Challenges You Will Solve
We are looking for a leader who will be responsible for overseeing the day-to-day management and maintenance of our organization's data centers. Coordinates data center hardware installation, reconfiguration, and removal while supporting facility infrastructure for operational needs. Collaborates with business clients and operations personnel, and interfaces with clients, vendors, and technical experts. Ensures the availability of necessary resources, including hardware and infrastructure components, for site operations. Provides 24/7 support for data center infrastructure and troubleshoots issues. Ensures compliance with Quadranet policies and procedures, analyzes resource capacities, and recommends capacity optimization solutions. Participates in maintenance activities, develops, and responds to Methods of Procedure, incident reports, and budget planning. Leads training and technical development for operations personnel, manages data center security, and supports facility operations, including compliance audits.
What You’ll Do
Developing and implementing data center strategies: working with senior management to develop and implement a long-term data center strategy that aligns with the organization's overall business goals.
Managing and leading technical staff: responsible for managing a team of technical staff, including network administrators, systems administrators, and other IT professionals. This includes hiring, training, and mentoring staff members to ensure that they have the skills and knowledge necessary to perform their jobs effectively.
Ensuring data center uptime and availability: ensuring that the data center operates at peak efficiency and is available to users at all times. This includes managing and monitoring power and cooling systems, as well as the data center's physical security.
Budgeting and cost management: manage the data center's budget, ensuring that costs are controlled and that expenditures are aligned with the organization's goals and objectives.
Compliance and regulatory requirements: ensuring that the data center complies with all relevant regulations and standards, including those related to data security, privacy, and environmental sustainability.
Disaster recovery and business continuity planning: develop and implement a disaster recovery and business continuity plan to ensure that the data center can continue to operate in the event of an outage or other disaster.
Maintaining vendor relationships: maintain relationships with vendors and service providers to ensure that the data center has access to the latest technologies and services.
What You’ll Bring
Strong working knowledge of DWDM, Ethernet, IP and SONET
Hands-on experience with DC power and fiber optic cable install, termination, and test
Strong working knowledge of electrical power systems, including utility switchgear, AC and DC distribution, UPS/Battery power systems, diesel generators.
Strong working knowledge of colocation HVAC equipment
Experience in administering customer license agreements.
Hands on experience with design and construction of racks, cages, data cabinets, structured cablings systems and cabling support structures
Location:
Los Angeles, CA
Candidates must be located in commutable distance to LA data center locations.
530 W. 6th St. Los Angeles, CA
6171 W. Century Blvd/ Suite: Basement Los Angeles, CA 90045
Show more
Show less","DWDM, Ethernet, IP, SONET, DC power, Fiber optic cable, Electrical power systems, Colocation HVAC, Customer license agreements, Design and construction of racks, Cages, Data cabinets, Structured cablings systems, Cabling support structures","dwdm, ethernet, ip, sonet, dc power, fiber optic cable, electrical power systems, colocation hvac, customer license agreements, design and construction of racks, cages, data cabinets, structured cablings systems, cabling support structures","cabling support structures, cages, colocation hvac, customer license agreements, data cabinets, dc power, design and construction of racks, dwdm, electrical power systems, ethernet, fiber optic cable, ip, sonet, structured cablings systems"
Senior Data Engineer,Freestar,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-freestar-3100528488,2023-12-17,Alhambra,United States,Mid senior,Remote,"About Freestar
Freestar engineer teams develop cutting-edge monetization solutions for websites and mobile apps. By combining industry-leading technology, data, and massive scale, we enable busy site owners and mobile app publishers to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. The result is publishers having more time to do what they do best: create content and focus on their apps.
Senior Engineer (Remote)
We’re looking for a Senior Engineer for our Data Platform Team - the product that drives our core business. As a Senior Engineer at Freestar, you'll be one of the main contributors to a talented team of developers to innovate features for our product that separate us from our competitors.
What You'll Be Doing
Run points on whole projects by recommending, prototype, build and debug data infrastructures on Google Cloud Platform (GCP) with other principal and senior engineers within the team, and to mentor less experienced Data Engineers.
Demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.
Participate in early-stage conversation with our product development team about product / features.
Improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.
Skills And Experience You'll Need To Have
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub).
4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS.
4+ years of hands-on experience using SQL to perform complex data manipulation
3+ years of experience modeling data warehouses
3+ years of experience building data pipelines, CI/CD pipelines, and fit for purpose data stores
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Experience writing software in one or more languages such as Python, Java, Scala, etc.
Experience with systems monitoring/alerting, capacity planning and performance tuning
Enjoy analyzing and organizing rapidly-changing business data to support product and business solutions
Nice to have Qualifications: *
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
Why You Want To Work With Us
Full-Time, Salaried Position
Working remotely
Generous Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Show more
Show less","Google Cloud Platform (GCP), AWS, Python, Java, Scala, SQL, Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, CloudSQL, Spanner, Cloud Storage, Data Migration, Data Warehouse Modernization, Data Modeling, Data Pipelines, CI/CD Pipelines, Systems Monitoring/Alerting, Capacity Planning, Performance Tuning, IoT Architectures, Realtime Data Streaming Pipelines, Machine Learning Models, Statistical Tools, Data Analysis","google cloud platform gcp, aws, python, java, scala, sql, beam, airflow, hadoop, spark, hive, kafka, pubsub, dataflow, dataproc, bigquery, dataprep, composer, cloudsql, spanner, cloud storage, data migration, data warehouse modernization, data modeling, data pipelines, cicd pipelines, systems monitoringalerting, capacity planning, performance tuning, iot architectures, realtime data streaming pipelines, machine learning models, statistical tools, data analysis","airflow, aws, beam, bigquery, capacity planning, cicd pipelines, cloud storage, cloudsql, composer, data migration, data warehouse modernization, dataanalytics, dataflow, datamodeling, datapipeline, dataprep, dataproc, google cloud platform gcp, hadoop, hive, iot architectures, java, kafka, machine learning models, performance tuning, pubsub, python, realtime data streaming pipelines, scala, spanner, spark, sql, statistical tools, systems monitoringalerting"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Los Angeles, CA",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787781362,2023-12-17,Alhambra,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
ZuHeAtwB0l
Show more
Show less","F4S work style assessment, Predictive analytics, JazzHR","f4s work style assessment, predictive analytics, jazzhr","f4s work style assessment, jazzhr, predictive analytics"
Senior Data Engineer,Jenni Kayne,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jenni-kayne-3747934973,2023-12-17,Alhambra,United States,Mid senior,Remote,"Jenni Kayne is a California-based lifestyle brand that aims to empower an elevated approach to everyday living. Whether it's our edited style ethos or coveted interiors sensibility, we work hard to create a world that's inviting and intentional. From our stores across the country to our operations and corporate teams, we believe in the power of a workplace that's built on diversity and inclusion—where the varied voices and viewpoints of our community pave the way.
About This Role
:
Jenni Kayne is looking for a Sr. Data with expertise in design, development, test and deployment of large Lake House (Data Lake and enterprise data warehouse) data solutions using cloud technologies and modern data stack. This is an exciting role for individuals looking for an entrepreneurial environment with clear ownership and opportunity to make direct business impact.
Role and Responsibilities:
As the Senior Data Engineer, your primary responsibilities include the following:
Building data pipelines: Create, maintain, and optimize workloads from development to production for specific business use cases.
Responsible for using innovative and modern tools, techniques and architectures to drive automation of most-common, repeatable data preparation and integration tasks with goal of minimizing reducing defects and improving productivity.
Responsible of data engineering architecture and framework to seamlessly integrate different business application and data sources with different formats (API, XML, JSON , CSV etc.)
Create strategy for master data for customer, product by unifying disparate source of customer and product across digital and offline channels (retail)
Develop and follow data integration and data quality standards across all development initiatives according to the organization's policies as well as best practices.
Assist in data management infrastructure, governance data observability, integration with metadata management tools and techniques (TBD in future).
Continuously tracking data consumption in collaboration with Analytics and Data sciences teams to prioritize the highest impact projects.
Triage data issues, analyzing end to end data pipelines and working with data analyst, business users in troubleshooting and resolving data quality or pipeline issues.
Work in agile model alongside data architect, data analysts, data scientist, business partners and other developers in delivery of data
Build and continuously manage the data lake and enterprise data-warehouse pipelines and shared transformation libraries for code reusability, speed to market and lineage.
Qualifications:
Requires a bachelor's degree or equivalent experience.
Requires at least 6 years of prior relevant experience.
Hands on experience with programming languages including SQL, Python on cloud data platforms like Snowflake, Redshift etc.
Strong technical understanding of data modeling (dimensional model), master data management, data integration, data architecture, data warehousing and data quality techniques
Working knowledge of Git repositories (bitbucket, GitHub), CI/CD (Jenkins etc.) and software development tools, including incident tracking, version control, release management, subversion change management (Atlassian toolset – Jira/Confluence), testing tools and systems and scheduling software (Airflow)
Experience working with popular BI software tools like Looker, Tableau, Qlik, PowerBI etc.
Nice to have: Experience with enterprise ELT platforms like Talend, Fivetran and flexibility to build an in-house transformation code base using SQL, Python, Airflow etc.
Basic experience in working with data governance and data security and specifically information stewards and privacy and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification.
Adept in agile methodologies and capable of applying DevOps and increasingly Data Operations principles to data pipelines to improve the integration, reuse and automation of data flows to improve data trust and democratization.
Physical Requirements:
Prolonged periods sitting at a desk and working on a computer
Must be able to move and lift heavy objects (15 pounds or more) from time to time as required
Additional Notes:
This job description is not all inclusive. In addition, Kayne, LLC DBA Jenni Kayne reserves the right to amend this job description at any time. Kayne, LLC DBA Jenni Kayne is committed to a diverse and inclusive work environment.
The annual base salary range for this position is $120,000 - $165,000. The base salary is determined by experience, education, skills, and location.
Show more
Show less","Snowflake, Redshift, SQL, Python, Data modeling, Master data management, Data integration, Data architecture, Data warehousing, Data quality, Git, Jenkins, Airflow, Looker, Tableau, Qlik, PowerBI, Talend, Fivetran, DevOps, Data Operations","snowflake, redshift, sql, python, data modeling, master data management, data integration, data architecture, data warehousing, data quality, git, jenkins, airflow, looker, tableau, qlik, powerbi, talend, fivetran, devops, data operations","airflow, data architecture, data integration, data operations, data quality, datamodeling, datawarehouse, devops, fivetran, git, jenkins, looker, master data management, powerbi, python, qlik, redshift, snowflake, sql, tableau, talend"
"SR. Scala Engineer, Database Engineering",Experfy,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590301404,2023-12-17,Alhambra,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, SQL, Data Warehouse, Database Optimization, Query Planner, Query Optimization, Data Routing, Cluster Management, Open Source, Web3, Blockchain, Data Acquisition, Data Processing, Data Engineering, Data Management, WebBased Development, Scripting, Test Automation","scala, apache spark, apache arrow, sql, data warehouse, database optimization, query planner, query optimization, data routing, cluster management, open source, web3, blockchain, data acquisition, data processing, data engineering, data management, webbased development, scripting, test automation","apache arrow, apache spark, blockchain, cluster management, data acquisition, data engineering, data management, data processing, data routing, database optimization, datawarehouse, open source, query optimization, query planner, scala, scripting, sql, test automation, web3, webbased development"
Sr. Software Engineer - Database Systems,VERSES,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-software-engineer-database-systems-at-verses-3784960984,2023-12-17,Alhambra,United States,Mid senior,Remote,"100% Remote
USA or Canada Only
About Us
VERSES is a cognitive computing company specializing in next-generation Artificial Intelligence modeled after natural systems and the design principles of the human brain and the human experience. We are a distributed, diverse, and inclusive workforce that aspires to do our best work on important problems with exceptional people and we value:
AUDACITY, ASSERTIVENESS, ALIGNMENT, ACCOUNTABILITY, ABUNDANCE & AWESOMENESS.
We’re looking for trailblazers and problem solvers passionate about tackling important global-scale challenges so if that sounds appealing, let’s imagine a smarter world together!
Make An Impact
At Verses we are committed to building cutting-edge technology solutions that power the future. Our team is composed of dedicated professionals who share a passion for innovation. We are looking for a talented Senior Software Engineer to join our team and help us develop high-performance databases and data processing systems.
Essential Functions
Design, develop, and optimize high-performance database systems and data processing solutions.
Collaborate closely with product managers, architects, and other engineers to define and implement scalable and robust systems.
Write clean, efficient, and maintainable code in Rust and other relevant technologies.
Benchmark, profile, and optimize system performance.
Provide technical leadership and mentorship to other engineers, promoting best practices in software engineering and database design.
Engage in code and design reviews, ensuring the quality and reliability of our systems.
Stay updated with the latest advancements in database technologies and programming best practices.
Requirements
Bachelor's degree in Computer Science, Engineering, or a related field; Master's or PhD is a plus or equivalent experience
5+ years of experience in designing and building high-performance databases or similar
Experience building modern database technologies such as Graph, Vector, NoSQL, relational databases, and in-memory databases
Proficiency in the Rust programming language. Experience in other languages like C++, Python, or Web Assembly is a plus
Familiarity with distributed systems, concurrency models, and multi-threading
Experience with database storage mechanisms, query optimization, indexing strategies, database internals and/or data processing algorithms
Familiarity with cloud platforms like AWS, Azure, or GCP
Strong problem-solving skills and attention to detail
Excellent verbal and written communication skills
Great to Haves
Contributions to open-source database or data processing projects
Familiarity with database query languages like SQL, MQL, CQL, or others
Experience with database testing, benchmarking, and performance tuning
Benefits
In addition to fostering a culture of highly engaged, emotionally intelligent, and energetic people we offer:
Global virtual work environment (although some positions may need to operate within specific time zones)
100% Company paid medical, dental & vision benefits
Financial and Mental Health wellness programs
Responsible paid time off policy (RTO), sick time complying with all State and Federal guidelines, and of course... Company recognized Holidays!!
Vested interest in the success of individuals
Generous total rewards package which includes equitable base pay, potential for bonuses, stock options
Job Type
Regular, Full-time
Remote - USA & Canada
Starting at $160,000 USD (USA only)
Commitment to Diversity, Equity, Inclusion, and Belonging:
At VERSES, we value diverse creative practices and forms of knowledge. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions.
We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at recruiting@verses.io to request accommodation.
We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. VERSES will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose.
Show more
Show less","Artificial Intelligence, Cognitive Computing, Rust, SQL, MQL, CQL, AWS, Azure, GCP, NoSQL, Graph Database, Vector Database, Relational Database, InMemory Database, Distributed Systems, Concurrency Models, MultiThreading, Database Storage Mechanisms, Query Optimization, Indexing Strategies, Database Internals, Data Processing Algorithms, Contributions to OpenSource, Database Testing, Benchmarking, Performance Tuning","artificial intelligence, cognitive computing, rust, sql, mql, cql, aws, azure, gcp, nosql, graph database, vector database, relational database, inmemory database, distributed systems, concurrency models, multithreading, database storage mechanisms, query optimization, indexing strategies, database internals, data processing algorithms, contributions to opensource, database testing, benchmarking, performance tuning","artificial intelligence, aws, azure, benchmarking, cognitive computing, concurrency models, contributions to opensource, cql, data processing algorithms, database internals, database storage mechanisms, database testing, distributed systems, gcp, graph database, indexing strategies, inmemory database, mql, multithreading, nosql, performance tuning, query optimization, relational database, rust, sql, vector database"
"SR. Scala Engineer, Database Engineering",Experfy,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3684789209,2023-12-17,Alhambra,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Apache Spark, Apache Arrow, Scala, SQL, Data warehouse, Distributed systems, Query planner, Query optimization, Cluster computing, Big data, DevOps, Software engineering, Web3, Blockchain, Data engineering","apache spark, apache arrow, scala, sql, data warehouse, distributed systems, query planner, query optimization, cluster computing, big data, devops, software engineering, web3, blockchain, data engineering","apache arrow, apache spark, big data, blockchain, cluster computing, data engineering, datawarehouse, devops, distributed systems, query optimization, query planner, scala, software engineering, sql, web3"
Senior Data Engineer / Airflow / Python / Spark,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-airflow-python-spark-at-motion-recruitment-3784985658,2023-12-17,Alhambra,United States,Mid senior,Remote,"A software company that handles petabytes of data is hiring a Senior Data Engineer to join their growing team of 5. This company is in a growth phase so there is opportunity to grow from within as they expand. In this role you develop, optimize and maintain their ETL data pipelines that handle billions of records and has millions of monthly users. This team utilizes Python, AWS, S3, Airflow, EMR, DynamoDB, SQL, and prefer PySpark but can train someone into PySpark if you only have Spark. In this role you will help build and improve automation platform features and data pipeline orchestration tools. This is a Los Angeles based company offering a fully Remote position for SoCal residents.
Required Skills & Experience
5+ years professional Data Engineering Experience
5years of experience building ETL pipelines with Python
Experience with AWS, S3, Redshift, EMR, and Airflow
Proficient with Spark or PySpark
Experience working with large amounts of data
Desired Skills & Experience
Bachelors in STEM field
Excellent written and verbal communication skills
The Offer
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Sick Time
Paid Time Off
401(k) with match
Remote
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Cassi Benson
Show more
Show less","Data Engineering, ETL, Python, AWS, S3, Redshift, EMR, Airflow, Spark, PySpark, SQL, Automation, Data Pipeline Orchestration","data engineering, etl, python, aws, s3, redshift, emr, airflow, spark, pyspark, sql, automation, data pipeline orchestration","airflow, automation, aws, data engineering, data pipeline orchestration, emr, etl, python, redshift, s3, spark, sql"
Senior Data Engineer,Thrive Market,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-thrive-market-3782099426,2023-12-17,Alhambra,United States,Mid senior,Remote,"About Thrive Market
Thrive Market was founded in 2014 with a mission to make healthy living easy and affordable for everyone. As an online, membership-based market, we deliver the highest quality healthy and sustainable products at member-only prices, while matching every paid membership with a free one for someone in need. Every day, we leverage innovative technology and member-first thinking to help our over 1,000,000+ members find better products, support better brands, and build a better world in the process. We recently reached a significant milestone by becoming a Certified B Corporation, making us the largest grocer to earn this coveted qualification.
THE ROLE
Thrive Market’s Data Engineering team is seeking a Senior Data Engineer!
We are looking for a brilliant, dedicated, and hardworking engineer to help us build high-impact products alongside our Data Strategy Team. Our site sees millions of unique visitors every month, and our customer growth currently makes us one of the fastest-growing e-commerce companies in Los Angeles. We are looking for a Senior Data Engineer with hands-on experience working on structured/semi-structured/Complex data processing and streaming frameworks. We need your amazing software engineering skills to help us execute our Data Engineering & Analytics initiatives and turn them into products that will provide great value to our members. In this role, we are hoping to bring someone in who is equally excited about our mission, learning the tech behind the company, and can work cross-functionally with other engineering teams.
Responsibilities
Work across multiple projects and efforts to orchestrate and deliver cohesive data engineering solutions in partnership with various functional teams at Thrive Market
Be hands-on and take ownership of the complete cycle of data services, from data ingestion, data processing, ETL to data delivery for reporting
Collaborate with other technical teams to deliver data solutions which meet business and technical requirements; define technical requirements and implementation details for the underlying data lake, data warehouse and data marts
Identify, troubleshoot and resolve production data integrity and performance issues
Collaborate with all areas of data management as lead to ensure patterns, decisions, and tooling is implemented in accordance with enterprise standards
Perform data source gap analysis and create data source/target catalogs and mappings
Develop a thorough knowledge and understanding of cross system integration, interactions and relationships in order to develop an enterprise view of Thrive Market’s future data needs
Design, coordinate and execute pilots/prototypes/POC to provide validation on specific scenarios and provide implementation roadmap
Recommend/Ensure technical functionality (e.g. scalability, security, performance, data recovery, reliability, etc.) for Data Engineering
Facilitate workshops to define requirements and develop data solution designs
Apply enterprise and solution architecture decisions to data architecture frameworks and data models
Maintain a repository of all data architecture artifacts and procedures
Collaborate with IT teams, software providers and business owners to predict and devise data architecture that addresses business needs for collection, aggregation and interaction with multiple data streams
Qualifications
Hands on experience programming in Python, Scala or Java
Expertise with RDBMS and Data Warehousing (Strong SQL) with Redshift,Snowflake or similar
In-depth knowledge and experience with data and information architecture patterns and implementation approaches for Operational Data Stores, Data Warehouses, Data Marts and Data Lakes
Proficiency in logical/physical data architecture, design and development
Experience in Data lake / Big data analytics platform implementation either cloud based or on-premise; AWS preferred
Experience working with high volumes of data; experience in design, implementation and support of highly distributed data applications
Experience with Development Tools for CI/CD, Unit and Integration testing, Automation and Orchestration E.g. GitHub, Jenkins, Concourse, Airflow, Terraform
Experience with writing Kafka producers and consumers or experience with AWS Kinesis
Hands-on experience developing a distributed data processing platform with Big Data technologies like Hadoop, Spark etc
A knack for independence (hands-on) as well as team work
Excellent analytical and problem-solving skills, often in light of ill-defined issues or conflicting information.
Experience with streaming data ingestion, machine-learning, Apache Spark a plus
Adept in the ability to elicit, gather, and manage requirements in an Agile delivery environment
Excellent communication and presentation skills (verbal, written, presentation) across all levels of the organization. Ability to translate ambiguous concepts into tangible ideas.
Belong To a Better Company
Comprehensive health benefits (medical, dental, vision, life and disability)
Competitive salary (DOE) + equity
401k plan
12 Days of Observed Holiday
Flexible Paid Time Off
Subsidized ClassPass Membership with access to fitness classes and wellness and beauty experiences
Ability to work in our beautiful co-working space at WeWork in Playa Vista and other locations
Dog-Friendly Office
Free Thrive Market membership with exclusive employee discount
Coverage for Life Coaching & Therapy Sessions on our holistic mental health and well-being platform
We're a community of more than 1 Million + members who are united by a singular belief: It should be easy to find better products, support better brands, make better choices, and build a better world in the process.
At Thrive Market, we believe in building a diverse, inclusive, and authentic culture. If you are excited about this role along with our mission and values, we encourage you to apply.
Thrive Market is an EEO/Veterans/Disabled/LGBTQ employer
At Thrive Market, our goal is to be a diverse and inclusive workplace that is representative, at all job levels, of the members we serve and the communities we operate in. We’re proud to be an inclusive company and an Equal Opportunity Employer and we prohibit discrimination and harassment of any kind. We believe that diversity and inclusion among our teammates are critical to our success as a company, and we seek to recruit, develop, and retain the most talented people from a diverse candidate pool. If you’re thinking about joining our team, we expect that you would agree!
If you need assistance or accommodation due to a disability, please email us at eoe@thrivemarket.com and we’ll be happy to assist you.
Ensure your Thrive Market job offer is legitimate and don't fall victim to fraud. Thrive Market never seeks payment from job applicants. Thrive Market recruiters will only reach out to applicants from an @thrivemarket.com email address. For added security, where possible, apply through our company website at www.thrivemarket.com .
© Thrive Market 2024 All rights reserved.
JOB INFORMATION
Compensation Description - The base salary range for this position is $160,000 - $190,000/Per Year.
Compensation may vary outside of this range depending on several factors, including a candidate’s qualifications, skills, competencies and experience, and geographic location.
Total Compensation includes Base Salary, Stock Options, Health & Wellness Benefits, Flexible PTO, and more!
Show more
Show less","Data Engineering, Python, Scala, Java, Data Warehousing, RDBMS, Redshift, Snowflake, Data Architecture, Data Lakes, Big Data Analytics, Apache Spark, Hadoop, Kafka, AWS Kinesis, CI/CD, Unit and Integration Testing, Automation, Orchestration, GitHub, Jenkins, Concourse, Airflow, Terraform, MachineLearning, Agile Delivery, SQL","data engineering, python, scala, java, data warehousing, rdbms, redshift, snowflake, data architecture, data lakes, big data analytics, apache spark, hadoop, kafka, aws kinesis, cicd, unit and integration testing, automation, orchestration, github, jenkins, concourse, airflow, terraform, machinelearning, agile delivery, sql","agile delivery, airflow, apache spark, automation, aws kinesis, big data analytics, cicd, concourse, data architecture, data engineering, data lakes, datawarehouse, github, hadoop, java, jenkins, kafka, machinelearning, orchestration, python, rdbms, redshift, scala, snowflake, sql, terraform, unit and integration testing"
Senior Data Engineer / Pyspark & EMR,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-pyspark-emr-at-motion-recruitment-3756757530,2023-12-17,Alhambra,United States,Mid senior,Remote,"Our client is changing the way we can find our friends and family from around the world. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, EMR, Airflow, AWS, and have a STEM degree.
Basic Qualifications (Required Skills & Experience)
5+ years of experience, 8+ experience preferred
Python
Pyspark
Spark
AWS
Airflow
EMR
ETL work
SQL
Other Qualifications & Desired Competencies
Fully Remote
Equity and Bonuses involved
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k)
Posted By:
Julie Bennett
Show more
Show less","Python, PySpark, Spark, AWS, Airflow, EMR, ETL, SQL","python, pyspark, spark, aws, airflow, emr, etl, sql","airflow, aws, emr, etl, python, spark, sql"
Senior Data Engineer / Airflow / Python / Spark,Dice,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-airflow-python-spark-at-dice-3786297154,2023-12-17,Alhambra,United States,Mid senior,Remote,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!
A software company that handles petabytes of data is hiring a Senior Data Engineer to join their growing team of 5. This company is in a growth phase so there is opportunity to grow from within as they expand. In this role you develop, optimize and maintain their ETL data pipelines that handle billions of records and has millions of monthly users. This team utilizes Python, AWS, S3, Airflow, EMR, DynamoDB, SQL, and prefer PySpark but can train someone into PySpark if you only have Spark. In this role you will help build and improve automation platform features and data pipeline orchestration tools. This is a Los Angeles based company offering a fully Remote position for SoCal residents.
Required Skills & Experience
5+ years professional Data Engineering Experience
5years of experience building ETL pipelines with Python
Experience with AWS, S3, Redshift, EMR, and Airflow
Proficient with Spark or PySpark
Experience working with large amounts of data
Desired Skills & Experience
Bachelors in STEM field
Excellent written and verbal communication skills
The Offer
You will receive the following benefits:
Medical Insurance
Dental Benefits
Vision Benefits
Paid Sick Time
Paid Time Off
401(k) with match
Remote
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. Senior Data Engineer / Airflow / Python / Spark
Show more
Show less","Data Engineering, ETL, Python, AWS, S3, Redshift, EMR, Airflow, Spark, PySpark, SQL, Automation, Data Pipeline Orchestration","data engineering, etl, python, aws, s3, redshift, emr, airflow, spark, pyspark, sql, automation, data pipeline orchestration","airflow, automation, aws, data engineering, data pipeline orchestration, emr, etl, python, redshift, s3, spark, sql"
REMOTE DATA ANALYST,AppleOne Employment Services,"Los Angeles, CA",https://www.linkedin.com/jobs/view/remote-data-analyst-at-appleone-employment-services-3781184161,2023-12-17,Alhambra,United States,Mid senior,Remote,"Full Dental, Health and Vision
401k Matching
PTO and Extended Sick Leave
Great Pay up to $120K
Design, build, maintain, and optimize large scale data processing, and reporting modules on cloud and on-premise using combination of PostgreSQL, Elasticsearch databases, Spark, Data Lake and Event Driven solutions. Solid experience designing and developing high performance systems using big data processing & querying tools including PostgreSQL.
The right candidate will need to have professional experience reading existing legacy code and get the business logic from there specifically on the environment of Postgres, SQL queries, pgPL/SQL, python scripts, Spark and Data Lake. It is preferred to understand multi-language text data match processes (Transliteration, Translation and Fuzzy Match).
Participate in various requirements sessions with internal teammates and convert business / functional requirements into technical solutions, along with supporting materials such as use cases, designs, flowcharts, models, specifications, and reports.
Provide technical support to requirements, development, and testing.
Work directly with Project Management, Business Intelligence and Data Engineering to provide data insights and ad-hoc reports as necessary.
Draft standards and procedures affecting data analysis and process modeling, as well as large scale data design & processing, maintenance, and management.
Support initiatives related to scalability and performance.
Conduct performance tuning reviews for improvements and prepare written reports of findings, including recommendations.
Have experience working with GitLab and being able to build and implement CI/CD pipelines for continues development on different stages (dev, QA, production).
Basic Qualifications:
8+ years data analyst experience writing complex SQL queries on Postgres or Oracle. Including SQL, stored procedures using pgpl/sql or pl/sql programing languages
5+ years doing ETL processes. It will be a big plus doing it in a cloud environment specifically on AWS.
2+ years data analyst experience specifically using Spark, scala, pyspark, Athena on AWS
2+ python development
2+ years development experience on AWS environment(RDS, Lambda, Step functions, Kinesis, Athena, …)
2+ years working in an Object-Oriented programming language(Java, C, C++, …).
Must have Linux/Unix OS working experience.
Must have containerized development experience.
Must have CI/CD automated deployment experience.
Must have experience with performance tuning and optimization in a PostgreSQL or Oracle.
It is a plus to have Elasticsearch index build and querying experience.
It is a plus to have Databricks working experience.
It is plus to understand how web-based applications work.
#1091
AppleOne is proud to be an Equal Opportunity Employer. We believe in people, and we are committed to working with people of all backgrounds and connecting them with clients and companies who share our goals of diversity and inclusiveness. All qualified applicants will receive consideration for employment without regard to race, religion, ancestry, color, national origin, age, gender identity or expression, genetic information, marital status, medical condition, disability, protected veteran status, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable federal, state, or local laws.
For temporary assignments lasting 13 weeks or longer, the Company is pleased to offer major medical, dental, vision, 401k and any statutory sick pay where required.
If you believe you need a reasonable accommodation due to a disability or health condition in order to search for a job opening or to apply for a position, please contact your staffing representative who will reach out to our HR team.
AppleOne participates in the E-Verify program in certain locations as required by law. Learn more about the E-Verify program. https://e-verify.uscis.gov/web/media/resourcesContents/E-Verify_Participation_Poster_ES.pdf]]>
Show more
Show less","PostgreSQL, Elasticsearch, Spark, SQL, Data Lake, Python, pgPL/SQL, GitLab, CI/CD, AWS, Java, C, C++, Linux/Unix, Containerization, Performance tuning, Databricks, Web applications","postgresql, elasticsearch, spark, sql, data lake, python, pgplsql, gitlab, cicd, aws, java, c, c, linuxunix, containerization, performance tuning, databricks, web applications","aws, c, cicd, containerization, data lake, databricks, elasticsearch, gitlab, java, linuxunix, performance tuning, pgplsql, postgresql, python, spark, sql, web applications"
Staff Software Engineer - Datalake,Dremio,"Los Angeles, CA",https://www.linkedin.com/jobs/view/staff-software-engineer-datalake-at-dremio-3783887642,2023-12-17,Alhambra,United States,Mid senior,Remote,"Be Part of Building the Future
Dremio is The Easy and Open Data Lakehouse, providing self-service analytics with data warehouse functionality and data lake flexibility across all of your data. Dremio increases agility with a revolutionary data-as-code approach that adopts Git concepts to enable data experimentation, version control, and governance. In addition, Dremio breaks down data silos by simplifying ingestion into the lakehouse, and also allowing queries directly on databases and data warehouses. All of this is available through a fully managed service that not only eliminates the need to maintain infrastructure and software, but also automatically optimizes the data in the lakehouse to maximize performance for every workload.
Founded in 2015, Dremio is headquartered in Santa Clara, CA. Investors include Cisco Investments, Insight Partners, Lightspeed Venture Partners, Norwest Venture Partners, Redpoint Ventures, and Sapphire Ventures. For more information, visit www.dremio.com. Connect with Dremio on GitHub, LinkedIn, Twitter, and Facebook.
If you, like us, say “bring it on” to exciting challenges that really do change the world, we have endless opportunities where you can make your mark.
About The Role
We are looking for an experienced Staff Software Engineer to enhance Dremio’s data warehouse capabilities on top of the datalakes across all major table/file formats and object stores. These capability advancements will increase our competitive position in the market and enable Dremio adoption for a larger set of customers.
What You’ll Be Doing
Develop core components for Dremio’s query engine
Deliver key features and feature enhancements for our customers in the Datalake like DML operations, time travel, schema evolution along with performance and reliability improvements
Work with open source projects like Apache Iceberg, Parquet, Arrow and Calcite
Own design, implementation, testing, and support of next-generation features related to scalability, reliability, robustness, and performance of the product
Collaborate with Product Management to innovate and deliver on customer requirements and with Support and field teams to ensure customer success
Understand and reason about concurrency and parallelization to deliver scalability and performance in a multithreaded and distributed environment
Solve complex technical problems and customer issues while improving our telemetry and instrumentation to proactively detect issues before they arise and make debugging more efficient
Work with engineering leaders to establish solid designs/architecture for upcoming features.
Develop the future leaders of Dremio by providing continuous mentorship and coaching of junior software engineers, help with hiring and onboarding
What We’re Looking For
8+ year of industry experience
B.S./M.S/Equivalent in Computer Science or a related technical field or equivalent experience
Fluency in Java, C++ or another modern language
Strong database fundamentals including SQL, performance, and schema design and background in large scale data processing systems (e.g., Hadoop, Spark, etc.)
Understanding of distributed file systems such as S3, ADLS, or HDFS
Experience with Apache Iceberg, Parquet, AVRO and/or Delta Lake
Experience with Hive and AWS Glue
Ability to solve ambiguous, unexplored, and cross-team problems effectively
Interested and motivated to be part of a fast-moving startup with a fun and accomplished team
Big picture thinking, ability to scope and plan solutions for big problems and mentor others on the same
Bonus points if you have
Hands-on experience with distributed query engines, query processing or optimization, distributed systems, concurrency control, data replication, code generation, or storage systems
Hands on experience with AWS, Azure and Google Cloud Platform
What We Offer
Medical, dental and vision insurance
401(k) Plan
Short term / long term disability and life insurance
Pre-IPO stock options
Flexible PTO
16 hours of volunteer time off
12 company paid holidays, including Juneteenth
Remote work options
Paid parental leave
Employee Assistance Program (EAP)
Biannual swag surprise
Certain benefits are only allowed to full-time Dremio employees and may not be the same across all locations.
The base salary range for this position is $154,545 to $209,091 per year. The base salary actually offered to a successful candidate will take into account various relevant and non-discriminatory business factors including, without limitation, the candidate’s geographic location, job-related experience, knowledge, and skills, and education, as well as internal equity considerations. A successful candidate may also be eligible to earn additional compensation including commissions and/or bonuses.
What We Value
At Dremio, we hold ourselves to high standards when it comes to People, Thinking, and Action. Our Gnarlies (that's what we call our employees) communicate with clarity, drive accountability, and are respectful towards each other. We confront brutal facts and focus on results while operating with a sense of urgency and building a ""flywheel"". People who like to jump in and drive momentum will thrive in our #GnarlyLife.
Dremio is an equal opportunity employer supporting workforce diversity. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, age, marital status, protected veteran status, disability status, or any other unlawful factor.
Dremio is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request accommodation due to a disability, please inform your recruiter.
Dremio has policies in place to protect the personal information that employees and applicants disclose to us. Please click
here
to review the privacy notice.
Show more
Show less","Java, C++, Structured Query Language, Database design, Apache Iceberg, Parquet, Apache Arrow, Apache Calcite, Apache Hive, AWS Glue, Hadoop, Spark, Big data processing, S3, ADLS, HDFS, Distributed query engines, Query processing, Query optimization, Distributed systems, Concurrency control, Data replication, Code generation, Storage systems, AWS, Azure, Google Cloud Platform","java, c, structured query language, database design, apache iceberg, parquet, apache arrow, apache calcite, apache hive, aws glue, hadoop, spark, big data processing, s3, adls, hdfs, distributed query engines, query processing, query optimization, distributed systems, concurrency control, data replication, code generation, storage systems, aws, azure, google cloud platform","adls, apache arrow, apache calcite, apache hive, apache iceberg, aws, aws glue, azure, big data processing, c, code generation, concurrency control, data replication, database design, distributed query engines, distributed systems, google cloud platform, hadoop, hdfs, java, parquet, query optimization, query processing, s3, spark, sql, storage systems"
Data Analyst III - Healthcare - Office or Remote,Qlarant,"Los Alamitos, CA",https://www.linkedin.com/jobs/view/data-analyst-iii-healthcare-office-or-remote-at-qlarant-3787024037,2023-12-17,Alhambra,United States,Mid senior,Remote,"Qlarant is a not-for-profit corporation that partners with public and private sectors to create high quality, safe, and efficient delivery of health care and human services programs. We have multiple lines of business including utilization review, managed care organization quality review, and quality assurance for programs serving individuals with developmental disabilities. Qlarant is also a national leader in fighting fraud, waste and abuse for large organizations across the country. In addition, our Foundation provides grant opportunities to those with programs for under-served communities.
Looking to start or grow your career in healthcare data analytics? As a Data Analyst III working on our Unified Program Integrity Contractors (UPIC) team for the Western Jurisdiction, you can contribute to our efforts to make a positive difference in the future of our nation's healthcare programs. Our UPIC West team identifies and investigates fraud, waste and abuse in the Medicare and Medicaid programs covering 13 states and 3 territories. You will utilize your statistical analysis programming skills to detect patterns of potential fraud in large healthcare data sets. Experience in at least one of the following is required: Python, R and SAS in addition to SQL. This position is perfect for a recent Master's grad with academic project/internship experience using Python, R or SAS.
This position could be based in our Los Alamitos, CA office or home based in most states of the continental US.
This position is an entry level professional performing study design, data analysis, and report preparation. Studies originate from preliminary data analysis (trends), literature review, experience and expertise of the team, and mandated projects. Data analysis, including data preparation and presentation of findings is performed in conjunction with other analysts. Reports are drafted by teams with leadership of Data Scientists.
Essential Duties and Responsibilities
include the following. Other duties may be assigned.
Trend data to identify potential opportunities (e.g., variances, significant outliers, percentile ranked groups) for quality improvement or focused investigations.
Aid in design data analysis strategies to identify potential areas for quality improvement or focused investigation.
Analyze data, draw conclusions, and summarize into quality indicator values
Develop tabular and graphical presentations of data, which clearly and concisely illustrate current levels of care.
Populate tabular and graphical presentation of data.
Contribute to the development of interventions (i.e., develop educational materials for doctors and nurses) which will improve healthcare processes and outcomes.
Facilitate design re-measurement strategies (after intervention) of healthcare processes and outcomes to effectively quantify impact of interventions for improvement.
Analyze re-measurement data and summarize into quality indicator values.
Support development of reports concerning all of the above.
May mentor junior Data Analysts in technical aspects of their work.
Assist in preparing findings for publishing in peer reviewed journals.
Familiar with commonly used concepts, practices and procedures, relying on instructions and pre-established guidelines to perform the functions of the job.
Supervisory Responsibilities
This job has no supervisory responsibilities.
Salary Range: Offers are based on skill level, experience and geographic location. For example, the hiring range for Biloxi, MS would be up to $77,500 DOE; Chico, CA, Atlanta, Dallas and Salt Lake City would be up to $82,000/yr DOE and Los Angeles would be up to $90,000/yr DOE. In addition, internal equity is a major factor in our salary offers. We offer a complete compensation package that includes health, dental, vision, and long and short term disability insurance, generous leave accruals, tuition reimbursement and a retirement plan that contributes an amount equal to 10% of your earnings with no required employee contribution. We also offer promotional opportunities and a collaborative and inclusive work environment.
Required Skills
To perform the job successfully, an individual should demonstrate the following competencies:
Analytical- Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data.
Problem Solving- Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions.
Judgment- Exhibits sound and accurate judgment; Supports and explains reasoning for decisions.
Other Skills And Abilities
To perform this job successfully, an individual should have fluency in Python, R or SAS, SQL and MS Office.
Ability to work independently and in teams.
Must possess familiarity with statistical methodologies, and automation techniques for analytic tasks.
Ability to work with highly sensitive information while preserving the confidentiality of the information.
Working knowledge of healthcare systems, Medicare/Medicaid, healthcare databases and coding systems preferred.
Required Experience
Bachelor's degree (BA or BS) in Statistics, Biostatistics, Epidemiology, Public Health or related discipline (e.g., Economics, Mathematics, Engineering, Computer Science) required.
Minimum of 2 years of hands-on SAS or other statistical programming work experience, or demonstrated combination of education and experience.
6 months experience with health related analytic research and quality improvement methodology (ISO, CQI, TQM, Six Sigma, Lean, Etc.) preferred.
Qlarant is an Equal Opportunity Employer of Minorities, Females, Protected Veterans, and Individuals with Disabilities.
Show more
Show less","Python, R, SAS, SQL, MS Office, Statistical analysis, Data visualization, Data analysis strategies, Data preparation, Data presentation, Healthcare data sets, Healthcare systems, Medicare, Medicaid, Healthcare databases, Coding systems, Trend data, Outliers, Quality improvement, Focused investigation, Educational materials, Remeasurement strategies, Quality indicator values, Peer reviewed journals, Junior Data Analysts","python, r, sas, sql, ms office, statistical analysis, data visualization, data analysis strategies, data preparation, data presentation, healthcare data sets, healthcare systems, medicare, medicaid, healthcare databases, coding systems, trend data, outliers, quality improvement, focused investigation, educational materials, remeasurement strategies, quality indicator values, peer reviewed journals, junior data analysts","coding systems, data analysis strategies, data preparation, data presentation, educational materials, focused investigation, healthcare data sets, healthcare databases, healthcare systems, junior data analysts, medicaid, medicare, ms office, outliers, peer reviewed journals, python, quality improvement, quality indicator values, r, remeasurement strategies, sas, sql, statistical analysis, trend data, visualization"
Senior Software Data Engineer,Robert Half,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-software-data-engineer-at-robert-half-3782298105,2023-12-17,Alhambra,United States,Mid senior,Remote,"***FOR IMMEDIATE CONSIDERATION, PLEASE MESSAGE ALI SCOTT ON LINKEDIN WITH YOUR UPDATED RESUME***
Functional Role: Sr. Software Data Engineer
Location: Los Angeles *Fully Remote in US but flexible to work PST time zones*
Salary Range: $120,000-150,000 + benefits
Technical Skills: Python, Pyspark, .NET. SQL Server, C#, AWS, ETL, Dashboards, Data Pipelines
Summary:
Our client in the marketing and advertising industry is seeking a talented Senior Software Data Engineer to join their team. This role will be working on tons of billions of data a year, comprised with web data, ClicData, and impression data with exposure to data science and analytic applications. The ideal candidate is looking to be part of a small team where you have a voice and autonomy in what you do and are passionate about. If this is you, then look no further!
Top Requirements:
1. Ideally previous background working as a software engineer in .NET/C#, SQL Server or similar, and has moved into Data engineering in the most recent years
2. Senior level candidates
3. Python, Pyspark data programming
4. AWS, Data lakes, data warehousing, S3, ETL, ideally Redshift (flex)
5. Someone capable of building data pipelines, data analytics and understands basic statistics and modeling
6. Needs someone with a big breadth and depth of data and engineering/analytics
7. Nice to have relatable industry experience—marketing, advertising digital marketing, etc…
***FOR IMMEDIATE CONSIDERATION, PLEASE MESSAGE ALI SCOTT ON LINKEDIN WITH YOUR UPDATED RESUME***
Show more
Show less","Python, Pyspark, .NET, SQL Server, C#, AWS, ETL, Dashboards, Data Pipelines, Data Lakes, Data Warehousing, S3, Redshift, Statistics, Modeling, Data Analytics","python, pyspark, net, sql server, c, aws, etl, dashboards, data pipelines, data lakes, data warehousing, s3, redshift, statistics, modeling, data analytics","aws, c, dashboard, data lakes, dataanalytics, datapipeline, datawarehouse, etl, modeling, net, python, redshift, s3, spark, sql server, statistics"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-%24180k-%24220k-snowflake-coding-at-cybercoders-3766360819,2023-12-17,Alhambra,United States,Mid senior,Remote,"Permanently Remote in US
Job Title:
Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)
Salary:
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
Requirements:
Expert w/ Snowflake & Coding Ability
Based in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.
We are founded and owned by T.V.s largest publishers.
Our mission is to be bring simplicity & scale to audience based campaigns in television.
We're working with over 100 advertisers and anticipating another year of significant growth!
As a rapidly growing company
(founded in 2017 & up 140% year over year)
we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.
We have been in business for 7 years and have around 40 employees.
Due to growth, we are actively hiring a Senior Data Engineer with
Snowflake experience (ideally certified)
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!
Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.
This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.
If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.
Top Reasons to Work with Us
Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits
Raid Growth: Founded in 2017 & up 140% year over year
Culture: Fast paced, mission driven culture
Technology: Cutting edge technology
What You Will Be Doing
Building data pipelines from scratch
Data architecture via Snowflake
Data modeling
Technical review of everything this group builds.
Mange development velocity, team capacity, and backlogs
Partner closely with the product team
Take on key assignments and delegate as needed
Act as the main technical point of contact for engineering
Translate technical requirements to the rest of the engineering team
What You Need for this Position
Must Have Experience
Snowflake experience
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Some Experience With:
-
Fivetran and/or DBT
What's In It for You
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
401k
Vacation/PTO
Medical
Dental
Vision
Bonus
401k
Benefits
Vacation/PTO
Medical
Dental
Vision
Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!
Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-Pauly
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Nitu.Gulati-Pauly@cybercoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L568 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Snowflake, JavaScript, Python, Data pipelines, APIs, Fivetran, DBT, Data architecture, Data modeling, Product team","snowflake, javascript, python, data pipelines, apis, fivetran, dbt, data architecture, data modeling, product team","apis, data architecture, datamodeling, datapipeline, dbt, fivetran, javascript, product team, python, snowflake"
Data Protection Engineer,P1 Technologies,"Manhattan Beach, CA",https://www.linkedin.com/jobs/view/data-protection-engineer-at-p1-technologies-3775449187,2023-12-17,Alhambra,United States,Mid senior,Remote,"Why P1 Technologies?
At P1 you'll have an opportunity to work with a variety of customers across many industries, from media and entertainment, to aerospace, to healthcare and health sciences. You'll be part of an engineering team that is immersed in new technology and is constantly learning from our customers, our partners, and each other.
Who we're looking for:
A systems engineer who’s passionate about learning and capable of providing outstanding customer service. You will work directly with a variety of customers to understand their challenges and help them resolve issues. You'll have the opportunity to work on complex technical projects and develop a deep understanding of public cloud, identity management, virtualization, networking, storage, and data protection.
Where:
Wherever! Work from home. The greater Los Angeles area is a plus.
Responsibilities
:
Design, architect, and size large scale data protection (backup/recovery) systems.
Deploy data protection systems in the data center and public cloud.
Perform backup and recovery operations for a variety of workloads including large scale NAS, virtualized environments, and public cloud resources.
Troubleshoot and optimize backup and recovery processes.
Design, architect, and size of large scale virtualized systems in the data center and public cloud.
Design, architect, and size of large scale NAS systems in the data center and public cloud.
Perform general architect duties such as systems requirements gathering, performance analysis, design, and sizing.
Build and manage infrastructure using code.
Containerize applications and processes.
Configure access using a least privileged approach, audit, and ensure adherence to policies and procedures.
Communicate upgrade and rollback plans and execute upgrades.
Monitor performance, identify issues, troubleshoot, and resolve.
Maintain an internal wiki with technical documentation, manuals and IT policies.
Communicate with clients, understand their needs and build trust.
Project Examples:
Design protection for multi-PB NAS to efficiently store data on prem and in AWS S3.
Deploy vSphere cluster using vSAN in accordance with optimal and best practice storage design and layout.
Use Cohesity to protect vSphere VMs running databases to AWS S3, perform recovery in AWS, and platform convert to EC2. Automate the entire process using Teraform.
Automate metrics collection via API and create useful visualizations and dashboards that accurately detail the status of a platform.
Desired Technical Skills:
Cohesity
Data Protection (Backup/Recovery) Software (Rubrik, Commvault, Netbackup, Avamar, etc.)
VMware vSphere
AWS EC2, S3, IAM, RDS, and EKS
Azure
Windows
Linux
IP networks
Docker
Kubernetes
CI/CD
Python
Terraform
Packer
Ansible
Okta
Professional Skills:
Resourcefulness and problem-solving aptitude
Excellent communication skills
Ability to communicate technical concepts and the impact of technical issues.
Friendly and easy-going.
Teamwork oriented.
P1 Culture
One of our core tenets is making P1 a great place to work. You'll have access to and be able to learn from great engineers who are easy to work with.
Show more
Show less","Data Protection (Backup/Recovery) Software, VMware vSphere, AWS EC2, AWS S3, AWS IAM, AWS RDS, AWS EKS, Azure, Windows, Linux, IP networks, Docker, Kubernetes, CI/CD, Python, Terraform, Packer, Ansible, Okta, Cohesity, Rubrik, Commvault, Netbackup, Avamar","data protection backuprecovery software, vmware vsphere, aws ec2, aws s3, aws iam, aws rds, aws eks, azure, windows, linux, ip networks, docker, kubernetes, cicd, python, terraform, packer, ansible, okta, cohesity, rubrik, commvault, netbackup, avamar","ansible, avamar, aws ec2, aws eks, aws iam, aws rds, aws s3, azure, cicd, cohesity, commvault, data protection backuprecovery software, docker, ip networks, kubernetes, linux, netbackup, okta, packer, python, rubrik, terraform, vmware vsphere, windows"
"Looking for Big Data SDET - Santa Monica, CA (Hybrid) - Fulltime",Extend Information Systems Inc.,"Santa Monica, CA",https://www.linkedin.com/jobs/view/looking-for-big-data-sdet-santa-monica-ca-hybrid-fulltime-at-extend-information-systems-inc-3778290972,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Hi,
I hope you are doing well!
We have an opportunity for
Big Data SDET
with one of our clients for
Santa Monica, CA (Hybrid)
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Big Data SDET
Location:
Santa Monica, CA (Hybrid)
Terms:
Fulltime
Job Details
Must have Strong Python Experience along with the Big Data Testing
Basic Qualifications
The DEE Technology team in Santa Monica is seeking a Sr. Software Development in Test Engineer to join our Engineering Services team to build test automation and processes to ensure the quality of our advertising systems. When our systems work properly, they can reliably deliver relevant ads to all of our viewers; driving higher revenue for our business and helping our customers to discover brands and products that are relevant to their interests. Defects in our ad systems can negatively impact revenue and deliver advertisements that detract from the viewer experience due to irrelevance, repetition, and other factors. This is a unique opportunity to join an excellent team and have a meaningful impact on the QE and automation process and culture, as well as the products we release.
Preferred Qualifications
Responsibilities include:
Work closely with Software Engineers to understand the complex advertising ecosystem in place at DEE Technology
Develop automated test frameworks and suites on UI, API and Integration levels of testing using python or other OO language.
Participate in design discussions for our platform to help evolve the platform in a way that enables richer testing scenarios that simplify defect detection and prevention.
Assist with triage, diagnosis, and resolution of issues discovered across teams.
Contribute to end-to-end acceptance tests
Where necessary, develop and execute manual test cases to detect issues that cannot be detected through automated means
Drive the conversion of manual tests to automated whenever possible
Responsibilities And Duties Of The Role
Summarize job responsibilities, core deliverables and major duties. What is required for the position to exist?
Focus on major areas of work, typically 20% or more of role % of Time
Part of product teams in building architectures which are robust, fault-tolerant, and cloud- native. Builds solutions for problems of sizeable scope and complexity that have been successfully deployed to customers/users. Influences and drives software engineering best practices within the team 25%
Technically lead and deliver multiple projects utilizing an Agile methodology while reviewing team member’s code. Participates in developing technical and/or business approaches; and new/enhanced technical tools. 25%
Owns the design of software programs or systems within the team, and within the organization. Writes codes that establishes and enhances frameworks. Reviews code for the design, testability and clear usability. Builds solutions that scale and perform. Identifies opportunities to improve the system/product/services with each iteration. 50%
Required Education, Experience/Skills/Training
Minimum and Preferred. Inclusive of Licenses/Certs (include functional experience as well as behavioral attributes and/or leadership capabilities)
Minimum of 4 years of hands-on software test development experience, including both functional and non-functional test development
Passion around driving best practices in the testing space
Proficiency with Python or other OO language
Knowledge of software engineering practices and agile approaches
Strong desire for establishing and improving product quality.
Experience building or improving test automation frameworks .
Proficiency CICD integration and pipeline development in Jenkins, Spinnaker or other similar tools.
Proficiency in UI automation ( Selenium, Robot, Watir)
Experience in Gherkin ( BDD /TDD )
Willingness to take challenges head on while being part of a team
B.S. in Computer Science (or equivalent degree or work experience)
Required Education
BA/BS Degree
Thanks & Regards
Monika Singh
Extend Information System Inc
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Show more
Show less","Python, Agile, UI automation, Selenium, Jenkins, Spinnaker, Gherkin, BDD, TDD, Java","python, agile, ui automation, selenium, jenkins, spinnaker, gherkin, bdd, tdd, java","agile, bdd, gherkin, java, jenkins, python, selenium, spinnaker, tdd, ui automation"
Data Center and Cloud Solutions Engineer,"QuadraNet Enterprises, LLC.","Los Angeles, CA",https://www.linkedin.com/jobs/view/data-center-and-cloud-solutions-engineer-at-quadranet-enterprises-llc-3778493374,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Challenges You Will Solve
We are looking for a skilled Data Center and Cloud Sales Engineer to join our team and help drive our cloud solutions business. As a Cloud Sales Engineer, you will be responsible for providing technical expertise to sales teams, helping to identify and qualify opportunities, and delivering technical presentations and demos to prospective clients. You will also work closely with our product and engineering teams to ensure that our cloud solutions meet customer requirements and stay competitive in the market.
What You’ll Do
Collaborate with the sales team to understand client needs and identify opportunities to sell our cloud solutions
Deliver technical presentations and demonstrations to clients, showcasing the benefits and features of our cloud solutions
Develop and maintain strong relationships with clients, serving as a trusted technical advisor
Conduct product demonstrations and proof-of-concept (POC) projects to showcase our cloud solutions
Work with product and engineering teams to understand the technical details of our cloud solutions and how they fit into the market
Participate in trade shows, conferences, and other industry events to promote our cloud solutions and generate leads
Provide feedback from clients to product and engineering teams to improve our cloud solutions
Continuously stay up to date on industry trends and emerging technologies to stay competitive in the market
What You’ll Bring
Bachelor’s degree in computer science, Electrical Engineering, or a related field.
3-5 years of experience in technical sales, preferably in the data center or cloud solutions industry.
Strong technical knowledge of data center and cloud solutions, including virtualization, storage, networking, and security.
Excellent communication and interpersonal skills, with the ability to explain technical concepts to non-technical audiences.
Strong analytical and problem-solving skills.
Ability to work independently and as part of a team.
Willingness to travel as needed to meet with customers and attend industry events.
Experience with CRM systems, such as Salesforce, is a plus.
Flexible work from home options available.
Show more
Show less","Cloud Sales, Technical Expertise, Sales Teams, Technical Presentations, Demos, Product Teams, Engineering Teams, Customer Requirements, Client Needs, Sales Opportunities, Cloud Solutions, Technical Advisor, ProofofConcept, Product Demonstrations, Market Trends, Emerging Technologies, Technical Sales, Data Center Solutions, Virtualization, Storage, Networking, Security, CRM Systems, Salesforce","cloud sales, technical expertise, sales teams, technical presentations, demos, product teams, engineering teams, customer requirements, client needs, sales opportunities, cloud solutions, technical advisor, proofofconcept, product demonstrations, market trends, emerging technologies, technical sales, data center solutions, virtualization, storage, networking, security, crm systems, salesforce","client needs, cloud sales, cloud solutions, crm systems, customer requirements, data center solutions, demos, emerging technologies, engineering teams, market trends, networking, product demonstrations, product teams, proofofconcept, sales opportunities, sales teams, salesforce, security, storage, technical advisor, technical expertise, technical presentations, technical sales, virtualization"
Senior Data Engineer,System1,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-system1-3744704288,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"System1 is one of the largest customer acquisition companies in the world whose growth depends heavily on a very talented data engineering team. The
Sr.
Data Engineering
team at System1 is focused on building frameworks, processes, and automation to ensure smooth running data pipelines and infrastructure. We process billions of records per day, for the benefit of multiple business functions like business intelligence, data science & machine learning, traffic quality and analytics. You would be working in a fast-paced environment where system scalability, reliability, usability, efficiency are the goals. Come join us!
The Role You Will Have
Designing and developing data processing infrastructure.
Developing new and improving existing data pipelines, extracting from external API sources or internal events.
Developing self-serve data solutions, self-correcting robust ETL pipelines.
Continuously improving monitoring and alerting coverage.
As a Sr. Engineer, this role will provide mentorship to colleagues in/outside of the immediate team.
Self-driving proof of concept for new technologies, new patterns and writing technical specifications for data architecture projects.
Identifying complex scaling bottlenecks and how to prevent them, and communicating updates to stakeholders.
Performing maintenance of existing infrastructure, investigating issues and failures.
Conducting SQL data investigations, and optimizations.
Participate in peer code reviews and produce high quality documentation
What You Will Bring
Bachelor's or Master's degree in Computer Science/Engineering.
Programming expertise in Python is required. JVM lang like Scala, Kotlin are preferred.
6+ years’ experience in Cloud ecosystems like AWS is required. GCP, Azure are preferred.
SQL expertise, and preferably SQL query optimization experience.
Database design skills, both relational and non-relational, SQL and NoSQL.
Experience with Cloud data warehouses like BigQuery, Snowflake, Redshift preferred.
High level of proficiency with modern orchestration platforms such as Airflow.
Deep understanding of data engineering fundamentals, ETL experience and analytics skills required in order to solve complex challenges across the System1 environment.
Knowledge of data engineering mechanics, flow, distribution, optimization.
Data organization, distribution, latency, observability.
Distributed big data processing and storage systems.
Kubernetes, docker, containerization strategies, Linux/UNIX would be required.
Experience with Kafka is preferred.
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
100% covered Medical, Dental, Vision for employees
401k with match
Health & Dependent Care Flex Spending Account
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
The U.S. base salary range for this full-time position is
$159,000 - $222,000
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in U.S. role postings reflect the base salary only, and do not include bonus, equity, or benefits.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following approved locations: Arizona, Colorado, Georgia, Hawaii, Minnesota, Missouri, New Jersey, New York, North Carolina, Oklahoma, Oregon, Pennsylvania, Tennessee, Texas and Virginia. Prospective U.S. employees who live outside of any of these states will need to establish residency in one of the approved states prior to employment.
Show more
Show less","Python, Scala, Kotlin, AWS, GCP, Azure, SQL, NoSQL, BigQuery, Snowflake, Redshift, Airflow, ETL, Kubernetes, Docker, Kafka, Linux, UNIX","python, scala, kotlin, aws, gcp, azure, sql, nosql, bigquery, snowflake, redshift, airflow, etl, kubernetes, docker, kafka, linux, unix","airflow, aws, azure, bigquery, docker, etl, gcp, kafka, kotlin, kubernetes, linux, nosql, python, redshift, scala, snowflake, sql, unix"
Senior Data Analyst,Evite,"Glendale, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-evite-3782879235,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Curious. Innovative. Collaborative. Inclusive. Committed to bringing people together to celebrate their most important life moments. That's who we are at Evite. We work hard, move quickly, support each other, act with integrity and have a lot of fun along the way! Sound like a party you want to be a part of? We’re currently looking for a passionate and creative Sr. Data Analyst to join our team in Los Angeles.
The ideal candidate will be an analytically-minded and talented individual. Come join the party!
What We Value
Making a Difference: Never be afraid to act fast and be curious.
Transparency and Teamwork: Embrace collaboration and share those amazing ideas!
Excellence Without Attitude: Be passionate and positive while always remembering to have fun.
Core Capabilities
Demonstrated experience working with product and engineering teams to drive data-driven decision making
Proficiency in analyzing A/B tests, new features and roll-outs meant to drive product optimization and user experience improvements
In-depth understanding of ecommerce data metrics such as visits, sessions, conversion rates, and customer behavior
Ability to write, develop and iterate on complex SQL queries, reports and dashboards for both internal and external use
Solid grasp of statistical concepts and their application to data analysis in an ecommerce context
Expertise in data visualization and reporting tools to communicate insights effectively
Excellent communication skills to convey complex data insights to non-technical stakeholders
Understanding of data warehouse architecture and data design
Responsibilities & Requirements
Leverage your experience and skills to understand and share how our hosts and guests interact with and use our core business products across mobile, web, and app
Partner with Product to help define and analyze metrics that inform the success of new features and programs and to monitor product adoption and health across our party ecosystem
Select applicable statistical methods and synthesize results from complex A/B multivariate tests
Communicate via compelling, easy-to-use dashboards or analysis the state of our business and products, operational performance, experiment results, and potential opportunities for growth
Influence and inform product and design teams through write-ups, meeting and presentations of data-based recommendations
Champion the analytics process – from measurement strategies, data preparation, modeling, cleansing and evaluation
Bachelor's degree in a quantitative field, economics, or finance from a competitive school or 3+ years of experience using data to facilitate business or product decisions
Highly Proficient in SQL
Experience with data visualization tools such as Tableau, Power BI or Looker
Familiarity with Google Analytics
Fluency in either R or Python is desired but not required
Benefits & Perks At Evite
Healthcare & retirement:
Multiple medical plans to choose from
100% employer-paid dental & vision plans for employees and their families
Employer-matched 401(k) plan
24/7 access to coaches & mental health benefits
Vacation & Leaves
Unlimited trust-based vacation
Bereavement leave
Paid time off to volunteer
12 weeks leave for parents at 100% of your salary for new births, adoptions and foster children
11 paid company-wide holidays
Perks
Free parking at our Glendale office
Baby Bucks! An initial contribution to help cover some of those early parenting expenses
Donation matching
Volunteering and learning & development opportunities
Fully stocked kitchen (occasionally lunches and happy hours too)
And more!
Evite, Inc. is committed to equal employment opportunity and values a diverse workforce. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or perception, national origin, age, marital status, disability, veteran status, genetics or other legally protected characteristics.
Show more
Show less","Data Analysis, SQL, Data Visualization, Tableau, Power BI, Looker, Google Analytics, R, Python, Data Warehouse Architecture, Data Design, A/B Testing, Statistical Methods, Machine Learning, Probability, Linear Regression, Time Series Analysis, Decision Trees, Random Forests, Natural Language Processing","data analysis, sql, data visualization, tableau, power bi, looker, google analytics, r, python, data warehouse architecture, data design, ab testing, statistical methods, machine learning, probability, linear regression, time series analysis, decision trees, random forests, natural language processing","ab testing, data design, data warehouse architecture, dataanalytics, decision trees, google analytics, linear regression, looker, machine learning, natural language processing, powerbi, probability, python, r, random forests, sql, statistical methods, tableau, time series analysis, visualization"
Data Engineer / Hybrid in Los Angeles Area / Adtech,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-engineer-hybrid-in-los-angeles-area-adtech-at-motion-recruitment-3785897062,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Our client is in the Advertising space and looking for a Data Engineer to come in and migrate Data to the cloud using Big Data tools. They are looking for someone with Adtech experience and someone who can come in and make an immediate impact. Lastly this role will be a direct hire position on a hybrid model in the Los Angeles area.
Basic Qualifications (Required Skills & Experience)
4-6 years of experience
Data Engineering experience
Python, SQL, AWS, Spark, PySpark, Glue
Other Qualifications & Desired Competencies
Hybrid in the Los Angeles area
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k) matching
Posted By:
Casey Ryan
Show more
Show less","Data Engineering, Python, SQL, AWS, Spark, PySpark, Glue, Adtech","data engineering, python, sql, aws, spark, pyspark, glue, adtech","adtech, aws, data engineering, glue, python, spark, sql"
Staff Database Engineer,BlackLine,"Los Angeles, CA",https://www.linkedin.com/jobs/view/staff-database-engineer-at-blackline-3783318013,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Get to Know Us:
It's fun to work in a company where people truly believe in what they're doing!
At Blackline, we're committed to bringing passion and customer focus to the business of enterprise applications.
Since being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance.
Being a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers.
Work, Play and Grow at BlackLine!
Make Your Mark:
As a Staff Database Engineer at BlackLine, you will play a vital role in the performance, delivery, stability, and security of the databases we use, while continually driving forward improvements and optimizations at the database layer. As a member of the database team, you will be involved in the planning, development, and maintenance of the database, including troubleshooting issues, collaborating with other teams to define and build new features, optimize existing features, and collaborate in order to drive growth, improve controls and processes, and reduce overhead and complexity. In this role, we are looking for people who are team players, passionate about their areas of expertise, and constantly striving to learn and improve, not just in the sense of their own skills, but also in growing with peers whom they work with day-to-day. If you are someone who strives for excellence in all that they do, including helping those in your team, and someone who wants to ultimately deliver the best value and success you can, then we want to talk to you.
You'll Get To:
Participate in cross-functional teams and build relationships across the organization.
Build high-performance, massively-scalable, always-available Cloud-based systems.
Ensure data integrity and quality in database systems.
Maintain standard policies for database development activities.
Provide database solutions based on technical documents and business requirements.
Provide technical assistance to resolve all database issues related to performance, capacity and access.
Analyze issues holistically, from the application tier through the database, down to the storage.
Database Engineer will ensure that all deadlines are met and that quality is of the highest level throughout the Software Development Life Cycle.
What You'll Bring:
8 + years experience SQL 2008, 2012, 2014, 2017
4 or more years working in high-transaction environments is required
Advanced working knowledge of different index types and how they are used: columnstore, full-text, filtered, indexes with include columns
Advanced Execution Plan understanding
SSIS/SSRS experience is a strong plus
Experience with performance tuning and optimization, using native monitoring and troubleshooting tools and techniques, including complex queries as well as procedure and indexing strategies
Excellent written and verbal communication
Adaptable team-player with a focus on results and value delivery
Able to organize and plan work independently
Lead various technology POCs
Advance understanding of OLTP vs OLAP environments
Working knowledge of relational database internals (locking, consistency, serialization, recovery paths)
We’re Even More Excited If You Have:
NoSQL is a plus
Azure, GCP or AWS are a plus
MCTS, MCITP, and/or MVP certifications are a plus
Thrive at BlackLine Because You Are Joining:
A technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the world's most trusted name in Finance Automation!
A culture that is kind, open, and accepting. It's a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives.
A culture where BlackLiner's continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity.
BlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws.
BlackLine recognizes that the ways we work and the workplace itself has shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.
Salary Range:
USD $139,800.00 - USD $215,800.00
Pay Transparency Statement:
Placement within this range depends upon several factors, including the applicant’s prior relevant job experience, skill set, and geographic location. In addition to base pay, BlackLine also offers short-term and long-term incentive programs, based on eligibility, along with a robust offering of benefit and wellness plans.
Show more
Show less","SQL, SQL 2008, SQL 2012, SQL 2014, SQL 2017, Index, Columnstore, Fulltext, Filtered, Advanced Execution Plan, SSIS, SSRS, Performance tuning, Optimization, Azure, GCP, AWS, MCTS, MCITP, MVP, NoSQL, OLTP, OLAP, POC, Relational database","sql, sql 2008, sql 2012, sql 2014, sql 2017, index, columnstore, fulltext, filtered, advanced execution plan, ssis, ssrs, performance tuning, optimization, azure, gcp, aws, mcts, mcitp, mvp, nosql, oltp, olap, poc, relational database","advanced execution plan, aws, azure, columnstore, filtered, fulltext, gcp, index, mcitp, mcts, mvp, nosql, olap, oltp, optimization, performance tuning, poc, relational database, sql, sql 2008, sql 2012, sql 2014, sql 2017, ssis, ssrs"
Principal Data Engineer,Spotter,"Culver City, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-spotter-3761823198,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Overview
Spotter, named one of TIME100's Most Influential Companies this year, empowers top YouTube creators to accelerate their business and unleash their full creative potential by giving them access to the capital, knowledge, and community they need to succeed at scale. As the top provider of creator-friendly growth capital, Spotter tailors our investments to meet the unique needs of each creator we partner with, giving them the freedom to create without compromise.
Creators are free to reinvest their funds however they choose, from hiring a team, to building their own production studios, and everything in between, all while maintaining total control over their catalogs, their channels, and their future earnings. In addition to funding, Spotter provides creators with in-depth data insights into the performance of their existing content, enabling them to leverage the full value of their library, as well as the value of future uploads and how they can improve performance in the future.
Featured in Forbes, Fast Company, Variety, Axios, and more, Spotter has already deployed over
$850 million
to YouTube creators to reinvest in themselves and accelerate their growth. Spotter has licensed content that consists of over
725,000 videos
, which generate
88 billion
monthly watch-time minutes. With our curated premium video catalog, we deliver a unique scaled media solution to Advertisers and Ad Agencies that is transparent, efficient, and 100% brand safe.
What You’ll Do
Are you ready to help lead the charge in shaping the data-driven future of Spotter? We're in search of an exceptional Principal Data Engineer who will play a pivotal role in designing, building, and optimizing scalable data infrastructure. You will help us with data pipelines for acquisition and transformation of large datasets, storage and querying optimizations of varying data to support a large range of use cases from Analytics to Creator Products to Operations using traditional and ML focused access patterns. You will be a key player in empowering us to make data-informed decisions that will fuel our innovation and growth.
Develop and maintain scalable data pipelines, including:
ETL pipelines, both single and multi-node solutions
Build data quality assurance steps for new and existing pipelines
Create derived datasets with augmented properties
Work on analytics ready datasets to power internal and creator facing tools
Troubleshoot issues when they arise, working directly with internal data consumers
Automate pipeline runs with scheduling and orchestration tools
Work with large scale datasets
Work with/use various external APIs to enhance data
Setup database tables for analytics users to consume the data collected by the Data Engineering team
Work with big data technologies to improve data availability and data quality in the cloud (AWS)
Lead development of projects involving other team members and act as a mentor
Actively participate in team discussions about technology/architecture/solutions for new projects and to improve existing code and pipeline
Who You Are
Bachelor’s degree, preferably in Computer Science or Computer Information Systems
6+ years of software engineering experience
5+ years of data engineering experience with Apache Spark or Apache Flink
4+ years of experience running software and services in the cloud
Proficiency in working with DataFrame APIs (Pandas and Spark) for parallel and single node processing
Proficiency using advanced languages and techniques with Python, Scala, etc. with modern data optimized file formats such as Parquet and Avro
Proficiency with SQL on RDBMS and data warehouse solutions like Redshift
Hands on experience with Data Lake technologies like Delta Lake and Iceberg
Experience with data acquisition from external APIs at large scale / in parallel processing
Experience supporting ML/AI projects: deployed pipelines for computing features, using models for inference on large datasets
Additional Valued Skills
Experience with YouTube APIs
Experience with AWS Glue metastore
Experience with Data-Mesh approaches
Experience with data cataloging, data lineage and data governance tools and approaches
Experience with vector databases
Why Spotter
Medical and vision insurance covered up to 100%
Dental insurance
401(k) matching
Stock options
Complimentary gym access
Autonomy and upward mobility
Diverse, equitable, and inclusive culture, where your voice matters.
In compliance with local law, we are disclosing the compensation, or a range thereof, for roles that will be performed in Culver City. Actual salaries will vary and may be above or below the range based on various factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The overall market range for roles in this area of Spotter are typically: $100-$500K salary per year. The range listed is just one component of Spotter’s total compensation package for employees. Other rewards may include annual discretionary bonus and equity.
COVID-19 Vaccination Policy
Spotter requires proof of being fully vaccinated for COVID-19 as a condition of commencing employment.
Spotter is an equal opportunity employer. Spotter does not discriminate in employment on the basis of race, religion, creed, color, national origin, ancestry, citizenship, physical or mental disability, medical condition, genetic characteristics or information, marital status, sex (including pregnancy, childbirth, breastfeeding, and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, military status, veteran status, use of or request for family or medical leave, political affiliation, or any other status protected under applicable federal, state or local laws.
Equal access to programs, services and employment is available to all persons. Those applicants requiring reasonable accommodations as part of the application and/or interview process should notify a representative of the Human Resources Department.
Show more
Show less","Data Engineering, Apache Spark, Apache Flink, Python, Scala, SQL, RDBMS, Data Warehouse, Redshift, Delta Lake, Iceberg, Data Lake, Machine Learning, Artificial Intelligence, YouTube APIs, AWS Glue, Data Mesh, Data Cataloging, Data Lineage, Data Governance, Vector Databases, AWS","data engineering, apache spark, apache flink, python, scala, sql, rdbms, data warehouse, redshift, delta lake, iceberg, data lake, machine learning, artificial intelligence, youtube apis, aws glue, data mesh, data cataloging, data lineage, data governance, vector databases, aws","apache flink, apache spark, artificial intelligence, aws, aws glue, data cataloging, data engineering, data governance, data lake, data lineage, data mesh, datawarehouse, delta lake, iceberg, machine learning, python, rdbms, redshift, scala, sql, vector databases, youtube apis"
Senior Data Engineer - 462,Braintrust,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-462-at-braintrust-3786899486,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"About Us
Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.
About The Hiring Process
The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.
Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.
JOB TYPE:
Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION:
Work from anywhere - EST/EDT | partial day overlap
HOURLY RANGE
Our client is looking to pay $110.00 – $113.00/hr
ESTIMATED DURATION:
40/week - long term
EXPERIENCE:
5-9 years
BRAINTRUST JOB ID:
11491
The Opportunity
The ideal candidate is a Data Engineer with considerable experience in migrations and Big Data frameworks.
Must-Haves
Scala programming language expertise
Spark framework expertise
Experience working with BigQuery
Familiarity scheduling jobs in Airflow
Fluency with Google Cloud Platform, in particular GCS and Dataproc
Nice-to-Haves
Python programming language fluency
Scalding framework fluency
Pyspark framework fluency
Dataflow(Apache Beam) framework fluency
Details/Notes
This is a one-time project-based role and is unlikely to extend or convert.
This role is open to remote candidates across the US who can work standard hours with complete overlap to Etsy core business hours (EST 11am - 2pm).
Interview process: The interview process is two interviews, a 30 minute phone screen with the hiring manager and a 90 minute technical screen with engineers on the team.
What You'll Be Working On
About The Team
Batch Engines supports the batch Big Data tools for engineering teams at Etsy. As an infrastructure team, our goal is to ensure state of the art tools are easy to use and interface well within the Etsy engineering ecosystem to enable batch data processing. We are a 6 person team with expertise in Big Data and DevOps.
About The Role
The Data Engineer contractor role will be a project based role focused on migrating data pipelines from legacy infrastructure and frameworks such as Scalding to more modern infrastructure we support such as Spark Scala. This role will be responsible for:
Analyzing existing data pipelines to understand their architecture, dependencies, and functionality.
Working closely with data engineers to develop a migration strategy for converting pipelines from the current framework to the target framework.
Designing, building, and testing new data pipelines in the target framework, ensuring they meet or exceed existing performance and reliability standards.
Debugging and troubleshooting issues that arise during the migration process, working collaboratively with cross-functional teams to resolve them quickly.
Communicating progress, challenges, and timelines to stakeholders on a regular basis.
Apply Now!
Notes
Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.
Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.
Show more
Show less","Scala, Spark, BigQuery, Airflow, Google Cloud Platform, GCS, Dataproc, Python, Scalding, Pyspark, Dataflow(Apache Beam), Etsy, Big Data, DevOps, Data Engineer, Data pipeline, Legacy infrastructure","scala, spark, bigquery, airflow, google cloud platform, gcs, dataproc, python, scalding, pyspark, dataflowapache beam, etsy, big data, devops, data engineer, data pipeline, legacy infrastructure","airflow, big data, bigquery, data pipeline, dataengineering, dataflowapache beam, dataproc, devops, etsy, gcs, google cloud platform, legacy infrastructure, python, scala, scalding, spark"
Senior Software Engineer - Data Analytics,HiveWatch,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/senior-software-engineer-data-analytics-at-hivewatch-3744963118,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"ABOUT US:
HiveWatch is a tech-forward, inclusive organization fostering the evolution of the physical security industry. We are a diverse team of forward thinkers who empower each other to find creative and collaborative solutions in an industry ripe for modernization. We are passionate about the problems we’re solving for our customers and equally passionate about the company we’re building.
HiveWatch is here to help security teams pivot from chasing threats to preventing them. We protect organizations, people, and property through the intelligent orchestration of physical security programs. With better communication, more insights, and less “noise”, we are modernizing what it means for businesses and their employees to truly feel safe.
WHAT YOU'LL DO
:
HiveWatch is in search of a Senior Software Engineer for the Data Analytics team. This role will report directly to our Engineering Manager for Data Analytics. The Data Analytics team strives to organize and coalesce disparate sources of data into repositories that serve to provide conclusive insights to drive product development (Tailgate Detection, Noise Reduction) and business decisions.
The team designs modern cloud infrastructure at the intersection of data and product, monitors health and performance of our customer-facing data products (including our Computer Vision features), and delivers technical solutions to both edge and cloud environments. As a Senior Software Engineer you will:
Be a technical leader of Engineering projects including design, technical scoping and implementation of data products and features
Contribute to mission critical systems and processes including:
Building, optimizing and tuning data pipelines
Leveraging and extending our data lake in order to provide valuable business insights
Tuning and improving existing computer vision components
Contribute to team discussions and presentations, conveying complex technical concepts in a clear and understandable manner to both technical and non-technical audiences
TECH STACK:
Languages: Python, SQL, Java/Kotlin
Deployments: GitHub Actions,Terraform, Terragrunt, Docker, and Helm
Infrastructure: AWS S3, Athena, Glue, PostgreSQL, Kubernetes, Docker, Greengrass/IoT Core
PREFERRED QUALIFICATIONS
:
Expertise with object oriented programming, debugging bespoke software, and performance/diagnostics measurement skills
Experience with analytics data structures (data lakes, stream processing, data pipelines, etc.)
Understanding of Computer Vision core principles (classification, detection, tracking, etc.)
Understanding of Statistical Analysis to track performance of data/CV products
Experience with Infrastructure as Code (IaC) using Terraform and Terragrunt
Strong verbal and written communication skills, particularly for sharing information across non-technical audiences
MINIMUM QUALIFICATIONS
:
Expertise with a modern software development language such as Python, Kotlin, Java, C#, Rust
Experience troubleshooting custom applications in a service architecture
Experience with highly available Relational Databases, SQL
Experience building and operating application containers
Experience working in a professional development workflow, git, issue tracking, and collaboration tools and agile methodologies
ADDITIONAL INFO
:
Salary range for this position: $150,000 to $165,000 per year
Eligible to participate in HiveWatch Equity Incentive Plan
*Final offer will be at the company's sole discretion and determined by multiple factors, including years and depth of relevant experience and expertise, location, and other business considerations.
Show more
Show less","Python, SQL, Java, Kotlin, GitHub Actions, Terraform, Terragrunt, Docker, Helm, AWS S3, Athena, Glue, PostgreSQL, Kubernetes, Greengrass, IoT Core, Objectoriented programming, Debugging, Performance measurement, Analytics data structures, Data lakes, Stream processing, Data pipelines, Computer Vision, Classification, Detection, Tracking, Statistical Analysis, Infrastructure as Code (IaC), Verbal communication, Written communication, Git, Issue tracking, Agile methodologies","python, sql, java, kotlin, github actions, terraform, terragrunt, docker, helm, aws s3, athena, glue, postgresql, kubernetes, greengrass, iot core, objectoriented programming, debugging, performance measurement, analytics data structures, data lakes, stream processing, data pipelines, computer vision, classification, detection, tracking, statistical analysis, infrastructure as code iac, verbal communication, written communication, git, issue tracking, agile methodologies","agile methodologies, analytics data structures, athena, aws s3, classification, computer vision, data lakes, datapipeline, debugging, detection, docker, git, github actions, glue, greengrass, helm, infrastructure as code iac, iot core, issue tracking, java, kotlin, kubernetes, objectoriented programming, performance measurement, postgresql, python, sql, statistical analysis, stream processing, terraform, terragrunt, tracking, verbal communication, written communication"
Principal Spark Data Engineer | Series D Video Analytics (Up to $275k TC),Coda Search│Staffing,"Culver City, CA",https://www.linkedin.com/jobs/view/principal-spark-data-engineer-series-d-video-analytics-up-to-%24275k-tc-at-coda-search%E2%94%82staffing-3737510583,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"I'm recruiting a Principal Data Engineer for my client, a Series D start-up that has been rated one of TIME100’s most influential companies. They empower YouTube creators by providing capital, insights, and partnerships to scale their businesses and offer creator-friendly growth capital tailored to each creator's needs, allowing reinvestment such as hiring teams or building production studios, while creators maintain control over content and earnings. They also provide creators with detailed content performance insights, helping them maximize their library and future upload value.
As a Principal Data Engineer, you'll use Spark and software/data engineering expertise to scale their big data platform. Responsibilities include building scalable ETL pipelines, data quality checks, creating augmented datasets, utilizing analytics tools, troubleshooting, automating pipelines, setting up analytics tables, and enhancing AWS data quality. Additionally, you'll have the opportunity to mentor junior team members.
Required:
Around 10 years of total experience
5+ years of software engineering experience
3+ years of data engineering with Apache Spark (including the last 2 years)
3+ years of experience with cloud services
Proficiency in DataFrame APIs (Pandas and Spark) for parallel and single-node processing
Proficiency in languages like Python or Scala
Proficiency in SQL, particularly with Redshift
My client offers a comprehensive benefits package, including 100% medical, dental, and vision coverage, 401(k) matching, stock options, and complimentary gym access.
Apply now to learn more!
Show more
Show less","Data Engineering, Apache Spark, Cloud Services, DataFrame APIs, Pandas, Python, Scala, SQL, Redshift, ETL Pipelines, Data Quality Checks, Augmented Datasets, Analytics Tools, AWS, Data Quality Enhancement, Mentorship","data engineering, apache spark, cloud services, dataframe apis, pandas, python, scala, sql, redshift, etl pipelines, data quality checks, augmented datasets, analytics tools, aws, data quality enhancement, mentorship","analytics tools, apache spark, augmented datasets, aws, cloud services, data engineering, data quality checks, data quality enhancement, dataframe apis, etl pipelines, mentorship, pandas, python, redshift, scala, sql"
Principal Data Engineer,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-motion-recruitment-3779716460,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"We’re looking for a Principal Data Engineer to join our Data Platform team. We are a Series D startup in Los Angeles, CA, and have been listed as one of the most influential startups of 2023 by Time's Magazine.
Our main mission is to empower Creators by allowing them to receive cash for their video catalogs, through licensing their existing videos (and/or future video uploads) and receiving instant payouts. Creators then use the funds to fuel their growth through hiring resources, investing, or anyway they choose all while remaining independent.
In addition to funding, we provide creators with in-depth data insights into the performance of all existing content to further help educate the Creator on the value of their library, the value of future uploads and how they can improve performance in the future.
As a Principal Data Engineer you will be using Python & Spark/PySpark to develop and maintain scalable data pipelines. This will include ETL pipeline development for both single and multi-node solutions, as well as creating derived datasets with augmented properties. You will also play a pivotal role in optimizing big data frameworks, improving the availability & quality of data as it's integrated into the cloud (AWS).
This is a great role for someone that is eager to work with vast amounts of data. We have licensed content comprised of over 700,000 videos that are generating approximately 100 billion monthly watch-time minutes. Required Skills & Experience
Bachelor’s degree, preferably in Computer Science or Computer Information Systems
6+ years of software engineering experience
5+ years of data engineering experience with Spark or Flink
4+ years of experience running software and services in the cloud
DataFrame APIs (Pandas and Spark)
Proficiency in Python, Scala, with data optimized file formats such as Parquet and Avro
Proficiency with SQL on RDBMS and data warehouse solutions like Redshift
Experience with Data Lake technologies (Delta Lake, Iceberg, etc…)
The Offer You Will Receive The Following Benefits
Medical and vision insurance covered up to 100%
Dental insurance
401(k) matching
Stock options
Complimentary gym access
Autonomy and upward mobility
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Connor Hart
Show more
Show less","Python, Spark/PySpark, Spark, Flink, DataFrame APIs, Scala, Parquet, Avro, SQL, RDBMS, Redshift, Data Lake, Delta Lake, Iceberg","python, sparkpyspark, spark, flink, dataframe apis, scala, parquet, avro, sql, rdbms, redshift, data lake, delta lake, iceberg","avro, data lake, dataframe apis, delta lake, flink, iceberg, parquet, python, rdbms, redshift, scala, spark, sparkpyspark, sql"
Software Engineer - Data Analytics,HiveWatch,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/software-engineer-data-analytics-at-hivewatch-3747063810,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"ABOUT US:
HiveWatch is a tech-forward, inclusive organization fostering the evolution of the physical security industry. We are a diverse team of forward thinkers who empower each other to find creative and collaborative solutions in an industry ripe for modernization. We are passionate about the problems we’re solving for our customers and equally passionate about the company we’re building.
HiveWatch is here to help security teams pivot from chasing threats to preventing them. We protect organizations, people, and property through the intelligent orchestration of physical security programs. With better communication, more insights, and less “noise”, we are modernizing what it means for businesses and their employees to truly feel safe.
WHAT YOU'LL DO
:
HiveWatch is in search of a Software Engineer for the Data Analytics team. This role will report directly to our Engineering Manager for Data Analytics. The Data Analytics team strives to organize and coalesce disparate sources of data into repositories that serve to provide conclusive insights to drive product development (Tailgate Detection, Noise Reduction) and business decisions.
The team designs modern cloud infrastructure at the intersection of data and product, monitors health and performance of our customer-facing data products (including our Computer Vision features), and delivers technical solutions to both edge and cloud environments. As a Software Engineer you will:
Be a meaningful contributor of Engineering projects including design and implementation of data products and features
Contribute to mission critical systems and processes including:
Optimizing and tuning data pipelines
Leveraging and extending our data lake in order to provide valuable business insights
Contribute to team discussions and presentations, conveying complex technical concepts in a clear and understandable manner to both technical and non-technical audiences
TECH STACK:
Languages: Python, SQL, Java/Kotlin, Rust
Deployments: GitHub Actions,Terraform, Terragrunt, Docker, and Helm
Infrastructure: AWS S3, Athena, PostgreSQL, Kubernetes, Docker
PREFERRED QUALIFICATIONS
:
Experience with object oriented programming and debugging bespoke software
Experience with analytics data structures (data lakes, stream processing, data pipelines, etc.)
Experience with Infrastructure as Code (IaC) using Terraform and Terragrunt
Strong verbal and written communication skills, particularly for sharing information across non-technical audiences
MINIMUM QUALIFICATIONS
:
Experience with a modern software development language such as Python, Java, C#, Rust
Experience troubleshooting custom applications in a service architecture
Experience with Relational Databases, SQL
Experience building and operating application containers
Experience working in a professional development workflow, git, issue tracking, and collaboration tools and agile methodologies
Passionate for technology and drive to learn new tools, technologies, and concepts (e.g. Computer Vision, Statistical Analysis, etc.)
ADDITIONAL INFO
:
Salary range for this position: $120,000 to $140,000 per year
Eligible to participate in HiveWatch Equity Incentive Plan
*Final offer will be at the company's sole discretion and determined by multiple factors, including years and depth of relevant experience and expertise, location, and other business considerations.
Show more
Show less","Python, SQL, Java, Kotlin, Rust, GitHub Actions, Terraform, Terragrunt, Docker, Helm, AWS S3, Athena, PostgreSQL, Kubernetes, Objectoriented programming, Data analytics, Infrastructure as Code (IaC), Software development, Troubleshooting, Relational databases, Application containers, Agile methodologies, Computer Vision, Statistical analysis","python, sql, java, kotlin, rust, github actions, terraform, terragrunt, docker, helm, aws s3, athena, postgresql, kubernetes, objectoriented programming, data analytics, infrastructure as code iac, software development, troubleshooting, relational databases, application containers, agile methodologies, computer vision, statistical analysis","agile methodologies, application containers, athena, aws s3, computer vision, dataanalytics, docker, github actions, helm, infrastructure as code iac, java, kotlin, kubernetes, objectoriented programming, postgresql, python, relational databases, rust, software development, sql, statistical analysis, terraform, terragrunt, troubleshooting"
Sr. Data Engineer,Fisker,"Manhattan Beach, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-fisker-3760573082,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"About Fisker Inc.
California-based Fisker Inc. is revolutionizing the automotive industry by developing the most emotionally desirable and eco-friendly electric vehicles on Earth. Passionately driven by a vision of a clean future for all, the company is on a mission to become the No. 1 e-mobility service provider with the world’s most sustainable vehicles. To learn more, visit www.FiskerInc.com – and enjoy exclusive content across Fisker’s social media channels: Facebook, Instagram, Twitter, YouTube and LinkedIn. Download the revolutionary new Fisker mobile app from the App Store or Google Play store.
General Role Overview
Sr Data Engineer - Cloud Data Warehouse provides overall leadership to the team of data engineers, analysts, and architects to implement world-class data solutions that meet our diverse business objectives and needs. This role is both an internal customer facing and a hands-on programming/development role. Working with teams of internal data engineers and architects, this role will lead in the delivery on modernizing their current data infrastructure, including data warehouses, data lakes and data marts. This role will be responsible for leading and contributing to the team responsible for the architecture and development of data ingestion processes, performance, administration, and design of database structures.
Nature of Work
Duties And Responsibilities
Minimum of 10 years of Data Engineering and Migration experience in one or more of the following areas (individual areas or combined experience): Cloud Data Modernization, Data Engineering, Data Platforms, Data Integration, Data Migration, Data Governance, Data Strategy, Data Architecture, Master Data Management, Data Modeling,
Has strong solution architecture experience in large complex data management programs
Strong knowledge and solutioning experience in Azure are preferred - Microsoft Azure Architecture, DevOps, ETL, Datawarehouse, Data Lake architectures, Database platforms such as SQL, MS-SQL, PostgreSQL etc., legacy data system migration to cloud.
Broad experience across a variety of data stores (e. g., HDFS, Azure Data Lake Store, Azure Blob Storage, Azure SQL Data Warehouse, Apache HBase, Azure Document DB), messaging systems (e.g., Apache Kafka, Azure Event Hubs, Azure IoT Hub) and data processing engines (e. g., Apache Hadoop, Apache Spark, Azure Data Lake Analytics, Apache Storm, Azure HDInsight).
Develop processes for loading and transforming of large volumes of structured and semi-structured data
Responsible for data architecture, data modeling and data governance to implement high-performance, highly secure solutions
Participate in data warehouse development by defining metadata and security standards
Perform a variety of tasks to facilitate completion of projects including but not limited to coordinating communication with outside departments, writing documentation and specifications, testing, and consulting
Job Specification:
Minimum Qualifications
Bachelor’s degree in computer science or computer engineering.
10+ years of hands-on system software design and development experience on Data and Analytics solutions.
Marketing and Automotive Experience Preferred.
Knowledges, Skills, Abilities, Competencies
Strong stakeholder and client management skills and experience
Strong analytical, interpersonal and collaboration skills
Good written and communication skills
Strong verbal and written communication skills.
Must be comfortable with operating in a fast-paced, startup environment.
Additional Requirements
Must be willing to travel both domestically, as well as internationally, as required
Salary Range for Job Posting: 74,900.00 to 150,000.00
Additional compensation
: Salary is one part of total compensation which includes bonuses, equity awards as applicable, and benefits. An employee is eligible to participate in Fisker’s equity program, subject to the rules governing such programs.
Benefits:
Fisker provides comprehensive medical, prescription, dental, vision, and disability insurance packages for full-time employees, their spouse or domestic partner, and children up to age 26.
Coverage is effective on the first day of employment, and Fisker covers most of the premiums for employees.
The salary offered may vary depending on multiple individualized factors including market location, job-related knowledge, education/training, certifications, key skills, experience, internal peer equity as well as business considerations. Fisker reasonably and in good faith expects to pay for the position within the salary range listed, taking into account the wide variety of factors listed above.
Fisker Inc. is an Equal Opportunity Employer; employment at Fisker Inc. is governed based on merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Show more
Show less","Data Engineering, Data Migration, Cloud Data Modernization, Data Platforms, Data Integration, Data Governance, Data Strategy, Data Architecture, Master Data Management, Data Modeling, Hadoop, Apache Spark, Postgresql, Azure Data Lake Store, SQL, Apache Kafka, Microsoft Azure, Apache HBase, Microsoft SQL, Agile, Data Warehousing, ETL, Devops, Data Lake, Data Analytics","data engineering, data migration, cloud data modernization, data platforms, data integration, data governance, data strategy, data architecture, master data management, data modeling, hadoop, apache spark, postgresql, azure data lake store, sql, apache kafka, microsoft azure, apache hbase, microsoft sql, agile, data warehousing, etl, devops, data lake, data analytics","agile, apache hbase, apache kafka, apache spark, azure data lake store, cloud data modernization, data architecture, data engineering, data governance, data integration, data lake, data migration, data platforms, data strategy, dataanalytics, datamodeling, datawarehouse, devops, etl, hadoop, master data management, microsoft azure, microsoft sql, postgresql, sql"
Principal Data Engineer,Alldus,"Los Angeles, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-alldus-3762858266,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Principal Data Engineer
We are currently recruiting for a Principal Data Engineer to join an exciting Series D startup based in Los Angeles. They sit in the Media space and deal with huge amounts of data from the likes of YouTube.
They are looking for someone to come at Principal level to drive and build out the data infrastructure.
You as the ideal candidate will be responsible for:
Leading on building and developing ETL processes and data pipelines
Working with analytics to develop new tools
Automation and orchestration of data pipelines
Development data solutions in an AWS environment
Mentoring other members of the data team
You as the ideal candidate will require:
Experience in a similar fast paced data role
Spark experience – Must have
AWS experience
Strong Python experience
Experience with media API’s will be a benefit
If you feel this is a good fit. Please apply or send an updated resume to kieran@alldus.com
Show more
Show less","Data Engineering, ETL, Data Pipelines, Analytics Tools Development, Automation, Orchestration, AWS, Spark, Python, Media APIs","data engineering, etl, data pipelines, analytics tools development, automation, orchestration, aws, spark, python, media apis","analytics tools development, automation, aws, data engineering, datapipeline, etl, media apis, orchestration, python, spark"
Senior Data Engineer,FLYR,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-flyr-3748814684,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Our Vision
FLYR is focused on the relentless application of advanced and intuitive technologies that help transportation leaders unlock their ultimate potential.
FLYR is a technology company that is purpose-built for the travel industry. Leveraging deep learning, an advanced form of AI, FLYR is helping airlines, cargo, and hospitality businesses around the globe elevate their results. With FLYR, businesses are able to improve revenue performance and modernize the e-commerce experience through accurate forecasting, automation, and analytics.
Flight Itinerary (About The Role)
Our data-driven, deep-learning-based methodology enables any airline to truly maximize their revenue and accurately forecast outcomes, even under unprecedented conditions such as COVID-19. To achieve this vision, we’re looking for a talented Senior Data Engineer to join our leading-edge data engineering customer implementation team. This team delights new customers throughout the onboarding process, applying innovative, best-in-class tools and workflows to build performant, reliable data pipelines. The Data Engineer will play a key role in ensuring our customers see rapid time-to-value, and will build creative technical solutions for new and complex problems.
What Your Journey Will Look Like (Responsibilities)
Complete complex customer ingest and data warehousing projects
Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability
Engage with customers from data discovery, to ETL development, to data QA/QC metrics determination and delivery SLAs
Develop and implement best practices and standardized, repeatable processes for implementing enterprise-grade data transformation/ingest pipelines, with an eye toward scalability
Learn our customers’ business needs and apply business acumen to ensure success of highly technical projects
Collaborate closely with FLYR's data platform team to define tool requirements to support onboarding and ingest of new data sources
Defining platform data validation test suites
Provide mentorship and training to newer crew members
What To Pack For This Trip (Qualifications)
Quantitative work/education background (computer science or equivalent)
Ability to ship production-quality Python code
5+ years of hands-on experience with cloud computing services, Airflow and BigQuery
5+ years of experience building and operating data transformation pipelines
Experience with data warehouses, ETL automation, BI visualization tools, and cloud-based data management tools
Advanced SQL. You know your way around analytical and aggregate functions, complex joins, window functions, and are confident in wrangling all types of data in SQL
Can identify impediments to customer onboarding and propose improvements to processes. Can work with Product, Data Science and Data Platform teams to define product and platform capabilities to improve customer onboarding
Ability to clearly communicate status, blockers, risks, and dependencies to FLYR and customer stakeholders
First-Class Amenities
Equity in Series C startup with high growth potential
Comprehensive healthcare plans (Choice of PPO & HMO available)
Generous PTO policy and flexible working arrangements
401K with company match
Free breakfast/Lunch (in-office)
100% paid Parental Leave for 12 weeks
Annual educational fund
Compensation: The salary range for this role is commensurate with experience and is as follows:
$119,000 — $167,000 USD
Our Commitment to Equality
Here at FLYR, we’re committed to growing with intention, having our teams better reflect the world around us. We strive to create an environment of inclusion and even more importantly, belonging, where psychological safety, empathy, and human connection are at the center of our leadership principles. Not only does this enable us to create better products and have a better work environment, it’s good for the bottom line and it’s the right thing to do.
FLYR provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender identity, sex, sexual orientation, national origin, age, physical or mental disability, genetics, marital or veteran status. In addition to federal law requirements, FLYR complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company operates.
Privacy Policy
All applicants, including those based in California or the EU, are encouraged to review our Privacy and Cookie Policy .
Show more
Show less","SQL, Python, Cloud Computing Services, Airflow, BigQuery, Data Warehouses, ETL Automation, BI Visualization Tools, CloudBased Data Management Tools, Advanced SQL, Data Transformation Pipelines, Data Engineering, Customer Onboarding, Data Discovery, ETL Development, Data QA/QC Metrics, Data Validation, Platform Capabilities, Stakeholder Communication","sql, python, cloud computing services, airflow, bigquery, data warehouses, etl automation, bi visualization tools, cloudbased data management tools, advanced sql, data transformation pipelines, data engineering, customer onboarding, data discovery, etl development, data qaqc metrics, data validation, platform capabilities, stakeholder communication","advanced sql, airflow, bi visualization tools, bigquery, cloud computing services, cloudbased data management tools, customer onboarding, data discovery, data engineering, data qaqc metrics, data transformation pipelines, data validation, data warehouses, etl automation, etl development, platform capabilities, python, sql, stakeholder communication"
Data Integration Engineer,Insight Global,"Burbank, CA",https://www.linkedin.com/jobs/view/data-integration-engineer-at-insight-global-3783951746,2023-12-17,Alhambra,United States,Mid senior,Hybrid,"Title:
Data
Integration Engineer
Location:
Burbank, CA or ATL, GA (2 days per week onsite)
Duration:
6-month rolling contract (expected to be a 3-5yr project)
Openings:
3
Must haves:
5+ years of experience as a data integration architect specializing in ERP application integration
2+ years of experience with informatica (IICS/Intelligent Cloud Services)
2+ years of experience setting up, monitoring, and troubleshooting PowerCenter for SAP NetWeaver and SAP Idocs
3+ years of programming experience with Shell and Python (ability to write code, pass the data, and load)
Experience in API development (REST/SOAP)
Experience loading data into Snowflake data warehouse
Plusses:
Experience in Cloud API Gateway configuration
Experience in AWS, Salesforce, or MDM
Experience in data modelling, preferable knowledge with Erwin, ER Studio
Day to Day:
One of our largest Media & Entertainment clients is looking for three Sr. Integration Architects to assist in their integration with SAP S4. This role will play an important part in designing, creating, refining, deploying, and managing the organization's data structure including contributing to the end-to-end vision for how data will flow from system to system for multiple applications and across multiple territories. This individual will work alongside a business analyst and an offshore development partner on a hybrid schedule (2x per week) in Burbank, CA or Atlanta, GA. This engagement will be on a 6-month rolling contract until the project is complete (estimated 3-5 years).
Show more
Show less","Data Integration Architecture, Informatica IICS/Intelligent Cloud Services, PowerCenter, SAP NetWeaver, SAP Idocs, Shell, Python, API Development, REST, SOAP, Snowflake, Cloud API Gateway, AWS, Salesforce, MDM, Data Modelling, Erwin, ER Studio","data integration architecture, informatica iicsintelligent cloud services, powercenter, sap netweaver, sap idocs, shell, python, api development, rest, soap, snowflake, cloud api gateway, aws, salesforce, mdm, data modelling, erwin, er studio","api development, aws, cloud api gateway, data integration architecture, data modelling, er studio, erwin, informatica iicsintelligent cloud services, mdm, powercenter, python, rest, salesforce, sap idocs, sap netweaver, shell, snowflake, soap"
Software Engineer – Database,Intellectt Inc,"Irvine, CA",https://www.linkedin.com/jobs/view/software-engineer-%E2%80%93-database-at-intellectt-inc-3732620857,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Title
– Software Engineer – Database
Location
– Irvine CA
Customer Note:
Software engineer with Linux embedded system development experience in C/C++/Go, open sources and database design experience.
Job Description
Bachelor of Science Degree in Computer Sciences, Computer Engineering or Software Engineering, or equivalent experience
Minimum 5+ years of software development experience or an equivalent combination of IFE (Inflight Entertainment) systems/engineering experience combined with 3 plus years of software development experience.
Proficient with Time Series Database or other types of databases.
Proficient with PostgreSQL client applications and utilities.
Experience with high availability applications such as Patroni, HA Proxy and keepalived.
Experience with Timescale Database a plus.
Experience with network programming and application interfaces.
Proficient with writing application software, preferable Go Language, C or C++.
Proficient with the Linux environment and Open-Source software development tools.
Proficient skills using the collaboration tools, such as Microsoft Teams, JIRA and CI/CD Pipeline through GitLab.
Proficiency with the Go ecosystem (unit test framework, static analysis, package system, documentation etc.)
Linux developer at application.
Proficient with Agile Methodologies and Scrum practices.
Ability to provide technical direction to assigned project teams and perform as a technical expert and a source of information.
Highly proficient in project planning while addressing external/internal dependencies across multiple functions.
Use professional concepts and applies company policies and procedures to resolve a variety of issues.
Proficient with commonly used concepts, practices, and procedures within the Software Engineering field.
Demonstrated experience providing identifiable contributions to the success of a specific product/project.
Develop solutions to clearly defined problems of moderate to diverse scope. Analysis involves selecting the best alternative method or process from among several existing alternatives or based on evaluation of identifiable factors.
Use expertise, experience, and judgment to plan and accomplish goals..
Qualification
BE / MCA
Varun
Senior IT Recruiter
varun@intellectt.com| www.intellectt.com
Desk: 7323986579;Ex:261
Show more
Show less","Linux, PostgreSQL, Time Series Database, Timescale Database, Patroni, HA Proxy, Keepalived, Go, C, C++, Agile, Scrum, Microsoft Teams, JIRA, CI/CD, GitLab","linux, postgresql, time series database, timescale database, patroni, ha proxy, keepalived, go, c, c, agile, scrum, microsoft teams, jira, cicd, gitlab","agile, c, cicd, gitlab, go, ha proxy, jira, keepalived, linux, microsoft teams, patroni, postgresql, scrum, time series database, timescale database"
Senior Big Data Engineer (HYBRID),Luca Talent,"Irvine, CA",https://www.linkedin.com/jobs/view/senior-big-data-engineer-hybrid-at-luca-talent-3785893617,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Job Title:
Senior Big Data Engineer
LOCATION:
Irvine, CA (onsite). Monday through Thursday onsite, Fridays remote.
COMPENSATION:
$70-95 an hour. This is a contract that will convert to full-time within the time period.
ABOUT US:
We are on a mission to develop innovative AI solutions that will revolutionize our workforce. As we embark on an exciting new greenfield AI project, our data team is forward-thinking and data-driven dedicated to transforming data into actionable insights. We are seeking a skilled Data Engineer o join our team, responsible for leveraging Python, PostgreSQL, Snowflake and AWS to design, develop, and maintain data pipelines and systems that support our analytics and decision-making processes.
Job Description
Position Overview:
As a BIg Data Engineer and Architect, you will play a pivotal role in shaping and managing an organization's data infrastructure. This multifaceted position combines technical expertise with strategic vision to design, implement, and maintain data solutions that drive informed decision-making and support the organization's goals. You will be building a data warehouse, data lakes, and data pipelines, as well as managing data architecture and design.
Responsibilities
Data Strategy:
Develop and execute a comprehensive data strategy that aligns with the organization's objectives. This includes defining data requirements, governance policies, and data management best practices.
Data Modeling
Create and maintain data models that represent the organization's data assets and ensure data consistency, accuracy, and integrity. This involves designing conceptual, logical, and physical data models.
Data Integration
Implement data integration solutions to ensure data flows smoothly and securely between various systems and platforms. This may involve ETL (Extract, Transform, Load) processes, API integrations, or real-time data pipelines.
Database Management
Design and maintain PostgreSQL databases, including schema design, indexing, and performance optimization.
Monitor database performance, troubleshoot issues, and implement improvements as needed.
AWS Integration:
Utilize AWS services, such as AWS Glue, S3, Redshift, and EMR, for data storage, processing, and analytics.
Collaborate with AWS solutions architects to optimize data-related AWS infrastructure.
Data Quality And Governance
Establish data governance frameworks, policies, and procedures to manage data access, quality, security, and compliance with relevant regulations (e.g., GDPR, HIPAA).
Implement data quality checks and validation processes to ensure data accuracy and consistency.
Enforce data governance policies and security measures to protect sensitive data.
Define and implement data quality standards, data profiling, and data cleansing processes to ensure that data is accurate and reliable.
Data Security
Develop and enforce data security measures to protect sensitive and confidential information. This includes implementing encryption, access controls, and auditing mechanisms.
Scalability And Performance
Architect data solutions that can scale with the organization's growing data needs, ensuring that systems can handle large volumes of data efficiently.
Data Pipeline Development
Design, implement, and maintain data pipelines using Python, PostgreSQL, and AWS technologies.
Develop efficient and scalable ETL processes to extract, transform, and load data from various sources into our Snowflake data warehouses.
Performance Optimization
Continuously monitor and optimize data pipelines and database performance for scalability and efficiency.
Documentation And Collaboration
Maintain comprehensive documentation of data pipelines, database schemas, and ETL processes.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver actionable insights.
Qualifications
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Proven experience in data architecture, database design, and data modeling.
Proven experience (5+ years) as a data engineer with expertise in Python, PostgreSQL, and AWS.
Strong SQL skills and proficiency in data modeling and database design.
Hands-on experience with AWS services, particularly Glue, S3, Redshift, and EMR.
Knowledge of ETL processes and data integration techniques.
Excellent problem-solving skills and attention to detail.
Effective communication and collaboration skills to work within a team.
A commitment to staying current with industry trends and best practices in data engineering.
What Makes This Exciting
Be part of a dynamic team in a data-driven organization.
Technical ownership on greenfield projects
Contribute to projects that have a meaningful impact on our business and industry.
Competitive compensation and opportunities for professional growth.
Work in a collaborative and innovative environment.
If you are a skilled Data Engineer with expertise in Python, Java, PostgreSQL, and AWS and are passionate about turning data into insights, we encourage you to apply. Join us in our mission to harness the power of data!
Show more
Show less","Python, PostgreSQL, Snowflake, AWS, AWS Glue, S3, Redshift, EMR, Data architecture, Database design, Data modeling, Data integration, ETL, SQL, Data governance, Data security, Data pipelines, Data warehouses, Data lakes, Data quality, GDPR, HIPAA, Encryption, Access controls, Auditing, Scalability, Performance, Documentation, Collaboration, Data science, Data analytics","python, postgresql, snowflake, aws, aws glue, s3, redshift, emr, data architecture, database design, data modeling, data integration, etl, sql, data governance, data security, data pipelines, data warehouses, data lakes, data quality, gdpr, hipaa, encryption, access controls, auditing, scalability, performance, documentation, collaboration, data science, data analytics","access controls, auditing, aws, aws glue, collaboration, data architecture, data governance, data integration, data lakes, data quality, data science, data security, data warehouses, dataanalytics, database design, datamodeling, datapipeline, documentation, emr, encryption, etl, gdpr, hipaa, performance, postgresql, python, redshift, s3, scalability, snowflake, sql"
Senior Big Data Engineer/ Architect (Hybrid),Luca Talent,"Irvine, CA",https://www.linkedin.com/jobs/view/senior-big-data-engineer-architect-hybrid-at-luca-talent-3771296395,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Job Title:
Big Data Engineer and Architect
LOCATION:
Irvine, CA (onsite). Monday through Thursday onsite, Fridays remote.
COMPENSATION:
$70-95 an hour. This is a contract that will convert to full-time within the time period.
ABOUT US:
We are on a mission to develop innovative AI solutions that will revolutionize our workforce. As we embark on an exciting new greenfield AI project, our data team is forward-thinking and data-driven dedicated to transforming data into actionable insights. We are seeking a skilled Data Architect to join our team, responsible for leveraging Java, PostgreSQL, Snowflake and AWS to design, develop, and maintain data pipelines and systems that support our analytics and decision-making processes.
Job Description
Position Overview:
As a Data Architect, you will play a pivotal role in shaping and managing an organization's data infrastructure. This multifaceted position combines technical expertise with strategic vision to design, implement, and maintain data solutions that drive informed decision-making and support the organization's goals. You will be building a data war house, data lakes, and data pipelines, as well as managing data architecture and design.
Responsibilities
Data Strategy:
Develop and execute a comprehensive data strategy that aligns with the organization's objectives. This includes defining data requirements, governance policies, and data management best practices.
Data Modeling
Create and maintain data models that represent the organization's data assets and ensure data consistency, accuracy, and integrity. This involves designing conceptual, logical, and physical data models.
Data Integration
Implement data integration solutions to ensure data flows smoothly and securely between various systems and platforms. This may involve ETL (Extract, Transform, Load) processes, API integrations, or real-time data pipelines.
Database Management
Design and maintain PostgreSQL databases, including schema design, indexing, and performance optimization.
Monitor database performance, troubleshoot issues, and implement improvements as needed.
AWS Integration:
Utilize AWS services, such as AWS Glue, S3, Redshift, and EMR, for data storage, processing, and analytics.
Collaborate with AWS solutions architects to optimize data-related AWS infrastructure.
Data Quality And Governance
Establish data governance frameworks, policies, and procedures to manage data access, quality, security, and compliance with relevant regulations (e.g., GDPR, HIPAA).
Implement data quality checks and validation processes to ensure data accuracy and consistency.
Enforce data governance policies and security measures to protect sensitive data.
Define and implement data quality standards, data profiling, and data cleansing processes to ensure that data is accurate and reliable.
Data Security
Develop and enforce data security measures to protect sensitive and confidential information. This includes implementing encryption, access controls, and auditing mechanisms.
Scalability And Performance
Architect data solutions that can scale with the organization's growing data needs, ensuring that systems can handle large volumes of data efficiently.
Data Pipeline Development
Design, implement, and maintain data pipelines using Java, PostgreSQL, and AWS technologies.
Develop efficient and scalable ETL processes to extract, transform, and load data from various sources into our Snowflake data warehouses.
Performance Optimization
Continuously monitor and optimize data pipelines and database performance for scalability and efficiency.
Documentation And Collaboration
Maintain comprehensive documentation of data pipelines, database schemas, and ETL processes.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver actionable insights.
Qualifications
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Proven experience in data architecture, database design, and data modeling.
Proven experience (5+ years) as a data engineer with expertise in Java, PostgreSQL, and AWS.
Strong SQL skills and proficiency in data modeling and database design.
Hands-on experience with AWS services, particularly Glue, S3, Redshift, and EMR.
Knowledge of ETL processes and data integration techniques.
Excellent problem-solving skills and attention to detail.
Effective communication and collaboration skills to work within a team.
A commitment to staying current with industry trends and best practices in data engineering.
What Makes This Exciting
Be part of a dynamic team in a data-driven organization.
Technical ownership on greenfield projects
Contribute to projects that have a meaningful impact on our business and industry.
Competitive compensation and opportunities for professional growth.
Work in a collaborative and innovative environment.
If you are a skilled Data Engineer with expertise in Java, PostgreSQL, and AWS and are passionate about turning data into insights, we encourage you to apply. Join us in our mission to harness the power of data!
Show more
Show less","Java, PostgreSQL, Snowflake, AWS, Data Warehousing, Data Lakes, Data Pipelines, Data Architecture, Data Strategy, Data Modeling, Data Integration, Database Management, Data Quality, Data Governance, Data Security, Scalability, Performance Optimization, ETL, Data Science, Analytics, Cloud Computing, Big Data, Hadoop, AWS Glue, S3, Redshift, EMR","java, postgresql, snowflake, aws, data warehousing, data lakes, data pipelines, data architecture, data strategy, data modeling, data integration, database management, data quality, data governance, data security, scalability, performance optimization, etl, data science, analytics, cloud computing, big data, hadoop, aws glue, s3, redshift, emr","analytics, aws, aws glue, big data, cloud computing, data architecture, data governance, data integration, data lakes, data quality, data science, data security, data strategy, database management, datamodeling, datapipeline, datawarehouse, emr, etl, hadoop, java, performance optimization, postgresql, redshift, s3, scalability, snowflake"
Senior Data Engineer,Pacific Life,"Newport Beach, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-pacific-life-3761259730,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Job Description
Pacific Life is investing in bright, agile and diverse talent to contribute to our mission of innovating our business and creating a superior customer experience. We’re actively seeking a talented Senior Data Engineer to join our Workforce Benefits Technology team in Newport Beach, CA. This role can be on-site, hybrid or 100% remote.
This position will be responsible for the requirement gathering, design and delivery of integrated data marts, data translations, and data models in the newly formed Workforce Benefits Division. These data solutions will capture, manage, store and utilize structured and unstructured data from internal and external sources and provide integrated data for Sales, Actuarial, Pricing, Underwriting, Operations, Claims and other functions. The role will establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, apply appropriate security, and create data products. The position will work with functional data stewards to implement data governance and data management principles across Workforce Benefits, and may work with solution providers to ensure the integrity and automated flow of data to PL. This role will serve as a valued partner across Workforce Benefits functions to create a single-source of trusted data with fast, scalable access to data assets, to satisfy the analytical data needs of the division.
How You Will Make An Impact
Become a domain expert in WBD data by understanding complex data relationship between internal and external data sources, user interface source data and event data.
Act as a key liaison to Workforce Benefits Division stakeholders on data needs by translating business requirements to design, and develop efficient and reliable data products for business analytics.
Document requirements, link requirements to integrate common data elements, expand the WBD data model with new data elements, and create source to target mappings for destination tables.
Identify the appropriate microservice events from the policy admin system to capture required data, and transform source data into an operational data store by building third normal form tables.
Develop Workforce Benefit’s division integrated data layer by constructing data schemas for functional business analysis and reporting. Recommend best practices for data-driven reporting and analytics.
Build data products that are used to drive business growth, operational effectiveness, customer success, and manage expenses, like prospect and sales, pricing, enrollment, onboarding, and claims data schemas.
Build data management solutions to feed real-time data back to customer portals and vendor data exchanges.
Conduct proof-of-concepts, ideate options, and embrace new technologies to support business goals and align with division strategies.
Working with the Data Governance team, contribute to data governance standards, profiling, and glossaries using a data catalog and working with business data stewards. Utilize and maintain metadata related to Workforce Benefits data domains.
The Experience You Will Bring
6+ years of experience using T-SQL, PL/SQL, or Hive to analyze and manipulate large volumes of data
6+ years of experience in analysis, design, development, and delivery of relational databases, data marts, data load processes and data wrangling in support of business analytics
5+ years using dbt to transform source data into operational data stores and data warehouse layers
5+ years working with a microservices event data infrastructure, including Kafka, Amazon Kinesis, or other event orchestration tools
5+ years of experience building data warehouses using Snowflake, Redshift or comparable tools
Strong ability to build relationships across the organization and communicate and manage stakeholders at all levels
Ability to work with ambiguity, and drive efforts forward without fully complete information. Strong execution focused mindset.
Bachelor’s degree in Computer Science, Mathematics, Analytics or related field. Master’s degree preferred.
What Will Make You Stand Out
Knowledge of EIS administration system, including events and data structures
5+ years of experience designing and building data products in Group Benefits insurance data domains, including Sales, Actuarial, Operations, and Claims
Experience with cloud technologies such as AWS
Hands on experience using ETL/ELT tools like dbt or Matillion and analytics tools like Dataiku and Alteryx
You belong at Pacific Life
At Pacific Life we are committed to a culture of belonging, a space where all employees are empowered to be authentic. One way we cultivate an inclusive culture is through our employee connection groups. The purpose of these employee-led groups is to offer a place to build community, connection, camaraderie, and a sense of belonging. Each group can be active in education, advocacy, recruitment, and community building throughout our organization. Learn more about our employee connection groups at www.pacificlife.com.
Want to learn more about life at Pacific Life? Take an inside look at our company culture: Instagram.com/lifeatpacificlife.
Base Pay Range
The base pay range noted provides a basis to determine the appropriate offer dependent upon several factors including but not limited to geographic location, experience, skills, education and pay equity. Also, most employees are eligible for additional incentive pay.
$127,890.00 - $156,310.00
Your Benefits Start Day 1
Your wellbeing is important to Pacific Life, and we’re committed to providing you with flexible benefits that you can tailor to meet your needs. Whether you are focusing on your physical, financial, emotional, or social wellbeing, we’ve got you covered.
Prioritization of your health and well-being including Medical, Dental, Vision, and Wellbeing Reimbursement Account that can be used on yourself or your eligible dependents
Generous paid time off options including: Paid Time Off, Holiday Schedules, and Financial Planning Time Off
Paid Parental Leave as well as an Adoption Assistance Program
Competitive 401k savings plan with company match and an additional contribution regardless of participation
EEO Statement
Pacific Life Insurance Company is an Equal Opportunity /Affirmative Action Employer, M/F/D/V. If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access our career center as a result of your disability. To request an accommodation, contact a Human Resources Representative at Pacific Life Insurance Company.
Show more
Show less","TSQL, PL/SQL, Hive, dbt, Kafka, Amazon Kinesis, Snowflake, Redshift, AWS, ETL, ELT, Matillion, Dataiku, Alteryx, EIS, Data Governance, Data Wrangling, Microservices, Data Marts, Data Warehousing, Data Integration, Data Analytics, Data Modeling, Relational Databases","tsql, plsql, hive, dbt, kafka, amazon kinesis, snowflake, redshift, aws, etl, elt, matillion, dataiku, alteryx, eis, data governance, data wrangling, microservices, data marts, data warehousing, data integration, data analytics, data modeling, relational databases","alteryx, amazon kinesis, aws, data governance, data integration, data marts, data wrangling, dataanalytics, dataiku, datamodeling, datawarehouse, dbt, eis, elt, etl, hive, kafka, matillion, microservices, plsql, redshift, relational databases, snowflake, tsql"
Principal Data Engineer,Harnham,"Los Angeles County, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-harnham-3780822365,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Principal Data Engineer
Los Angeles, CA
$180,000 - 2000,000
An impressive opportunity for a Principal Data Engineer, to join a growing media startup that is a leader in its industry. If you want to make a big impact through your drive in data and leadership skills, this is the job for you!
THE COMPANY
This company is a media startup working to enhance the creator community and looking for a Principal Data Engineer to drive scalability and performance within data. This is a critical role for building out their big data pipelines and scalability for their high growth.
THE ROLE
This Senior Data Engineer will be responsible for building out scalable data pipelines, distributed computing, and working on very large data sets.
Work on large data set
Build scalable and performant pipelines
Software Engineering background
STRONG experience with Spark
Distributed computing
YOUR SKILLS & EXPERIENCE
High level experience with Spark (4-6 years)
Must have experience with Python or Scala
Must have experience with AWS, GCP, or Azure
Experience with Redshift or any data lake related technology
Experience with Media data is a +++
BENEFITS
180,000- 200,000 base salary
Bonus
Equity
Medical, Dental, Vision Insurance
HOW TO APPLY
Please register your interest by sending your resume to Alexandra Oechsle via the apply link on this page.
Show more
Show less","Spark, Python, Scala, AWS, GCP, Azure, Redshift, Data lake, Media data, Distributed computing, Data pipelines, Big data, Software Engineering","spark, python, scala, aws, gcp, azure, redshift, data lake, media data, distributed computing, data pipelines, big data, software engineering","aws, azure, big data, data lake, datapipeline, distributed computing, gcp, media data, python, redshift, scala, software engineering, spark"
Principal Data Engineer,Harnham,"Los Angeles County, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-harnham-3784645839,2023-12-17,Anaheim,United States,Mid senior,Onsite,"Principal Data Engineer
Los Angeles, CA- HYBRID
$200,000 - 230,000
An impressive opportunity for a Principal Data Engineer, to join a growing media startup that is a leader in its industry. If you want to make a big impact through your drive in data and leadership skills, this is the job for you!
THE COMPANY
This company is a media startup working to enhance the creator community and looking for a Principal Data Engineer to drive scalability and performance within data. This is a critical role for building out their big data pipelines and scalability for their high growth.
THE ROLE
This Senior Data Engineer will be responsible for building out scalable data pipelines, distributed computing, and working on very large data sets.
Work on large data set
Build scalable and performant pipelines
Software Engineering background
STRONG experience with Spark
Distributed computing
YOUR SKILLS & EXPERIENCE
High level experience with Spark (4-6 years)
Must have experience with Python or Scala
Must have experience with AWS, GCP, or Azure
Experience with Redshift or any data lake related technology
Experience with Media data is a +++
BENEFITS
200,000- 230,000 base salary
Bonus
Equity
Medical, Dental, Vision Insurance
HOW TO APPLY
Please register your interest by sending your resume to Alexandra Oechsle via the apply link on this page.
Show more
Show less","Spark, Python, Scala, AWS, GCP, Azure, Redshift, Data Lake, Big Data, Distributed Computing, Software Engineering","spark, python, scala, aws, gcp, azure, redshift, data lake, big data, distributed computing, software engineering","aws, azure, big data, data lake, distributed computing, gcp, python, redshift, scala, software engineering, spark"
Senior Data Engineer (Remote),First American,"Santa Ana, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-first-american-3787718725,2023-12-17,Anaheim,United States,Mid senior,Remote,"Who We Are
Join a team that puts its People First! As a member of First American's family of companies, National Commercial Services provides single-point service for simple to multi-property/multi-state national commercial real estate transactions. Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
What We Do
The Senior Data Engineer is a data expert and plays a key role in the development and deployment of innovative big data platforms for advanced
analytics and data processing. Supports building scalable and effective solutions using modern technology stack to grow and leverage the
companys vast data assets. Works with functional teams and supports software developers, database architects, data analysts, and data
scientists on data initiatives and ensures that optimal data delivery architecture is consistent throughout ongoing projects.
The Data Engineer is a data expert and plays a key role in the development and deployment of innovative big data platforms for advanced
analytics and data processing. Supports building scalable and effective solutions using modern technology stack to grow and leverage the
company's vast data assets. Works with functional teams and supports software developers, database architects, data analysts, and data
scientists on data initiatives and ensures that optimal data delivery architecture is consistent throughout ongoing projects.
What You'll Do
Viewed as a data expert; drives innovation and plays a key role in the department. Participates in highly visible initiatives that have broad impact.
Identify, design, and implement internal process improvements: automate manual processes, optimize data delivery, redesign infrastructure for greater scalability.
Design, develop, code, test, and document architectures and applications.
Work closely with team members and cross-functional teams to ensure design/architecture/deliverables support business requirements and align with best-practices.
Troubleshoot and resolve a wide range of data issues.
Makes innovative recommendations to improve data reliability, efficiency and quality.
Required to perform duties outside of normal work hours based on business needs.
What You'll Bring
Bachelor's degree in Computer Science/related field or equivalent combination of education and experience
5-8 years of directly related experience
Experience working in Agile SDLC methodology
Proficient with SQL and T-SQL programming skills
Working experience building data/ETL pipeline and data warehouse
Advanced to expert-level proficiency in Python
Advanced proficiency in Azure
General knowledge of machine learning models and their utilization.
Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server, MongoDB or other NoSQL databases
Exceptional analytical skills analyzing large and complex data sets
Perform thorough testing and data validation to ensure the accuracy of data transformations
Strong written and verbal communication skills, with precise documentation
Self-driven team player with ability to work independently and multi-task
Analytical, creative thinker and innovative problem solver
Working knowledge/proficient in modern cloud computing technology
Pay Range: $80,000 - $167,000. annually
This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location.
What We Offer
By choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.
Show more
Show less","SQL, TSQL, Python, Azure, Machine Learning, Data Modeling, Database Maintenance, MongoDB, NoSQL Databases, Data Analysis, Data Validation, Cloud Computing","sql, tsql, python, azure, machine learning, data modeling, database maintenance, mongodb, nosql databases, data analysis, data validation, cloud computing","azure, cloud computing, data validation, dataanalytics, database maintenance, datamodeling, machine learning, mongodb, nosql databases, python, sql, tsql"
Sr. Data Engineer,Motion Recruitment,"Irvine, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-motion-recruitment-3730276042,2023-12-17,Anaheim,United States,Mid senior,Remote,"Job Description | DOES NOT SUPPORT: C2C OR Sponsorship Our client is a people search engine that utilizes deep web crawlers to aggregate data. Their main goal is to reconnect friends, reunite families, prevent fraud, and their most notable usage of their platform has been connecting adoptees to their biological parents.
They are looking for a Data Engineer to work fully remote on their ‘Data Operations Team,’ and help optimize their platform, data pipelines, data extractions, and data preparations. This will involve working with infrastructure built in AWS, including Spark EMR, S3, and DynamoDB. Additionally, this role is going to help build statistical tools, develop unit & stress tests, and create automation in the orchestrating of ETL pipelines.
As the role grows you will help in the determining of data governance, data quality, data cleansing, and ETL processes. Overall, you will have huge impact for the only data company that serves millions of users directly. They bolster a transparent culture and are promoters of flexibility and work-life balance. Required Skills & Experience
7 yrs. of experience
5 yrs. of Python experience
5 yrs. of Spark OR PySpark experience
Airflow
Strong understanding of DAG
AWS
Computer Science Degree (Highly Preferred)
The Offer You Will Receive The Following Benefits
12-15% bonus (50% performance; 50% on company performance)
Stock Options
Unlimited PTO
Sign on Bonus
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Connor Hart
Show more
Show less","Python, Spark, PySpark, Airflow, DAG, AWS, Data Engineering, Data Operations, Data Pipelines, Data Extractions, Data Preparations, Infrastructure, Spark EMR, S3, DynamoDB, Statistical Tools, Unit Testing, Stress Testing, Automation, ETL Pipelines, Data Governance, Data Quality, Data Cleansing, Data Science","python, spark, pyspark, airflow, dag, aws, data engineering, data operations, data pipelines, data extractions, data preparations, infrastructure, spark emr, s3, dynamodb, statistical tools, unit testing, stress testing, automation, etl pipelines, data governance, data quality, data cleansing, data science","airflow, automation, aws, dag, data engineering, data extractions, data governance, data operations, data preparations, data quality, data science, datacleaning, datapipeline, dynamodb, etl pipelines, infrastructure, python, s3, spark, spark emr, statistical tools, stress testing, unit testing"
Data Engineer IV,Motion Recruitment,"Orange, CA",https://www.linkedin.com/jobs/view/data-engineer-iv-at-motion-recruitment-3779717347,2023-12-17,Anaheim,United States,Mid senior,Remote,"We are seeking a Data Engineer for a fully remote Contract to Hire opportunity.
Fortune 100 company, is one of the largest global pharmaceutical distributors and health care services companies. We provide access to life-saving therapies, medications that improve quality of life and adherence programs to keep patients on those medications, helping reduce overall health care costs and improve patient outcomes. We are seeking qualified candidates to join us in an enterprise-wide analytics initiative to help drive incremental business value from our broad range of data assets. Requires Bachelors Degree in Computer Science, Business Administration or a related field; requires 5+ years of responsible experience in developing business solutions. The experience should be with requirements gathering, SQL, Data Integration tools such as Informatica Powercenter, Microsoft SSIS, Azure ADF and Data Warehousing techniques.
Data Engineers and researchers are responsible to develop innovative data driven solutions that integrate distributed sources of data, perform large scale learning and reasoning, and integrate results in collaborative environments to support the business analytics and data provisioning needs of the enterprise. They must have a good understanding of data access requirements for business analytics and exploration. Also required are analytical skills, the ability to establish and maintain effective working relationships with team members, as well as an innate curiosity around wanting to understand business processes, business strategy and strategic business initiatives to help drive incremental business value from enterprise data assets.
Duration: 6 month Contract to Hire
Required Skills & Experience
5+ years of experience developing BI/DW solutions in Informatica Powercenter or similar tool like IICS, Microsoft ADF, Apache Spark, Talend, but not limited to architecture design/development with the ability to resolve issues quickly and efficiently.
5+ years of using Structured Query Language (SQL)
Knowledge and exposures to Azure SQL Data Warehouse, Databricks or other cloud or on premises MPP data warehousing systems (SAP HANA, Teradata, Snowflake)
Experience with Data Lake implementations and design patterns
Experience in using and tuning relational databases (MS SQL Server, Oracle, MySql)
Knowledge of Test Driven Development, Continuous Integration, Agile/Scrum
Experience with code versioning tools such as GitHub, Jenkins, etc., and a command of configuration management concepts and tools.
Demonstrated ability to quickly learn and adapt to new technologies and coding techniques.
Experience designing, developing and testing applications using proven or emerging technologies, in a variety of technologies and environments.
Motivated quick learner presenting a positive energetic and professional image able to work in a fast-paced, collaborative, and project-oriented environment.
Strong interpersonal and analytical skills including the ability to communicate effectively both verbally and in writing.
Good knowledge of Microsoft Word, Excel, Power Point and Outlook
Ability to lead other Data Engineers on small projects initiatives
Ensures that data is clean, consistent, and synchronized across platforms as needed.
Oversees the design and implementation of data cleansing procedures.
Leads the identification of areas affecting data integrity and coordinates corrective actions.
Develops solutions and recommendations for improving data integrity issues.
Analyzes data issues and works with development teams for problem resolutions.
Ensures that physical models align with the logical data model.
Reviews and approves the logical data models.
Desired Skills & Experience
Experience with Lambda and Kappa architecture implementations is a plus
Knowledge in cloud resource provisioning and management on Azure and/or AWS is a plus.
Familiar with Streaming and with streaming concepts and patterns is a plus
Scientific computing experience (R, Matlab, Python, etc.) is a plus
Some prior exposure to pharmaceutical distribution/health care services industry desired
What You Will Be Doing
Daily Responsibilities
Works with other team members and business stakeholders on complex projects and tasks to drive development of business analytics requirements and synthesize these into data integration and enterprise data access requirements that communicate a coherent message.
Works with data visualization specialists to facilitate technical design of complex data sourcing, transformation and aggregation logic ensuring business analytics requirements are met.
Drives development and adoption of and adherence to data engineering standards using best practice methods and techniques.
Provides support in execution and delivery of business reporting.
Leads small project teams or work streams.
Posted By:
VMS Sourcing
Show more
Show less","Business Analytics, Data Integration, Data Warehousing, Data Engineering, Data Migration, Data Cleansing, Data Governance, ETL, SQL, Informatica Powercenter, Microsoft SSIS, Azure ADF, Apache Spark, Talend, IICS, Azure SQL Data Warehouse, Databricks, SAP HANA, Teradata, Snowflake, Data Lake, Relational Databases, MS SQL Server, Oracle, MySql, Test Driven Development, Continuous Integration, Agile, Scrum, GitHub, Jenkins, Cloud Resource Provisioning, Cloud Management, Azure, AWS, Streaming, Lambda Architecture, Kappa Architecture, Scientific Computing, R, Matlab, Python, Pharmaceutical Distribution, Health Care Services","business analytics, data integration, data warehousing, data engineering, data migration, data cleansing, data governance, etl, sql, informatica powercenter, microsoft ssis, azure adf, apache spark, talend, iics, azure sql data warehouse, databricks, sap hana, teradata, snowflake, data lake, relational databases, ms sql server, oracle, mysql, test driven development, continuous integration, agile, scrum, github, jenkins, cloud resource provisioning, cloud management, azure, aws, streaming, lambda architecture, kappa architecture, scientific computing, r, matlab, python, pharmaceutical distribution, health care services","agile, apache spark, aws, azure, azure adf, azure sql data warehouse, business analytics, cloud management, cloud resource provisioning, continuous integration, data engineering, data governance, data integration, data lake, data migration, databricks, datacleaning, datawarehouse, etl, github, health care services, iics, informatica powercenter, jenkins, kappa architecture, lambda architecture, matlab, microsoft ssis, ms sql server, mysql, oracle, pharmaceutical distribution, python, r, relational databases, sap hana, scientific computing, scrum, snowflake, sql, streaming, talend, teradata, test driven development"
Senior Data Engineer,Insight Global,"Irvine, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-insight-global-3778858980,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"*HYBRID On-Site 3 days a week in Irvine, CA
Must Haves:
8+ years of experience as a Senior Data Engineer
Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server, MongoDB or other NoSQL databases
Expert level knowledge and programming skills with SQL. NoSQL, T-SQL
Strong experience with ETL processes to ingest large and complex amounts of data at regular intervals (they are working with over 150 million rows of large real estate data)
Strong knowledge of database performance optimization techniques, such as clustered, non-clustered, spatial, full-text indexing, etc.
MUST have a passion for data, an innovative spirit, and entrepreneurial mindset
Day to Day:
A Senior Data Engineer is wanted for a title insurance company’s new innovation group to speed up their digital transformation. This person will design and develop user-friendly software solutions to innovate a paper-based business and create a digital underwriting experience. They will also develop and deploy big data platforms for analytics and data processing, using modern technologies to leverage the company’s data assets. They will work with other teams and professionals on data projects and architecture. This is a direct hire position in Orange County, CA for three days a week.
Show more
Show less","SQL, NoSQL, TSQL, Data modeling, Database maintenance, Monitoring, Performance tuning, ETL processes, Data optimization, Big data platforms, Analytics, Data processing, Software development","sql, nosql, tsql, data modeling, database maintenance, monitoring, performance tuning, etl processes, data optimization, big data platforms, analytics, data processing, software development","analytics, big data platforms, data optimization, data processing, database maintenance, datamodeling, etl, monitoring, nosql, performance tuning, software development, sql, tsql"
Senior Data Engineer (Hybrid),First American,"Santa Ana, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-hybrid-at-first-american-3739280186,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"Who We Are
Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
What We Do
The Underwriting Innovation Team is looking for promising new Senior Data Engineer to help accelerate our digital transformation. We want diverse thinkers who know how to bring innovation to life by facilitating the design and development of simple, intuitive, and user-centered software solutions. You’ll help senior department leadership drive the transformation of a traditionally paper-based business by re-imagining underwriting as a digital experience, thinking through both the underwriter’s and customer’s lenses.
The Data Engineer is a data expert and plays a key role in the development and deployment of innovative big data platforms for advanced analytics and data processing. Supports building scalable and effective solutions using modern technology stack to grow and leverage the company’s vast data assets. Works with functional teams and supports software developers, database architects, data analysts, and data scientists on data initiatives and ensures that optimal data delivery architecture is consistent throughout ongoing projects.
Candidates will need to be local to Southern California and will get to work out of First American's Corporate HQ's in Santa Ana, CA for 2-3 days a week!
What You'll Do:
Viewed as a data expert; drives innovation and plays a key role in the department. Participates in highly visible initiatives that have broad impact.
Identify, design, and implement internal process improvements: automate manual processes, optimize data delivery, re-design infrastructure for greater scalability.
Design, develop, code, test, and document architectures and applications.
Work closely with team members and cross-functional teams to ensure design/architecture/deliverables support business requirements and align with best-practices.
Troubleshoot and resolve a wide range of data issues.
Makes innovative recommendations to improve data reliability, efficiency and quality.
Required to perform duties outside of normal work hours based on business needs.
What You'll Bring:
Strong database development skills
Strong knowledge of SQL and NoSQL databases
Strong experience with ETL processes to ingest large amounts of data at regular intervals
Strong knowledge of database performance optimization techniques, such as clustered, non-clustered, spatial, full-text indexing, etc.
Geospatial data knowledge preferred
Experience working in Agile SDLC methodology
Proficient with SQL and T-SQL programming skills
Working experience building data/ETL pipeline and data warehouse
Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server, MongoDB or other NoSQL databases
Exceptional analytical skills analyzing large and complex data sets
Perform thorough testing and data validation to ensure the accuracy of data transformations
Strong written and verbal communication skills, with precise documentation
Self-driven team player with ability to work independently and multi-task
Analytical, creative thinker and innovative problem solver
Working knowledge/proficient in modern cloud computing technology
Bachelor's degree in Computer Science/related field or equivalent combination of education and experience
5-8 years of directly related experience
Salary Range: $87,900.00 - $182,700 Annually
This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location.
#techreferral
What We Offer
By choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.
Show more
Show less","SQL, NoSQL, ETL, Database performance optimization, Geospatial data, Agile SDLC, TSQL, Data modeling, Data validation, Cloud computing, MongoDB","sql, nosql, etl, database performance optimization, geospatial data, agile sdlc, tsql, data modeling, data validation, cloud computing, mongodb","agile sdlc, cloud computing, data validation, database performance optimization, datamodeling, etl, geospatial data, mongodb, nosql, sql, tsql"
Hybrid Senior Data Engineer,First American,"Santa Ana, CA",https://www.linkedin.com/jobs/view/hybrid-senior-data-engineer-at-first-american-3779748706,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"Who We Are
Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for seven consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
What We Do
Candidates will need to be local to Southern California and will get to work out of First American's Corporate HQ's in Santa Ana, CA for 2-3 days a week!
What You'll Do:
Viewed as a data expert; drives innovation and plays a key role in the department. Participates in highly visible initiatives that have broad impact.
Identify, design, and implement internal process improvements: automate manual processes, optimize data delivery, re-design infrastructure for greater scalability.
Design, develop, code, test, and document architectures and applications.
Work closely with team members and cross-functional teams to ensure design/architecture/deliverables support business requirements and align with best-practices.
Troubleshoot and resolve a wide range of data issues.
Makes innovative recommendations to improve data reliability, efficiency and quality.
Required to perform duties outside of normal work hours based on business needs.
What You'll Bring:
Strong database development skills
Strong knowledge of SQL and NoSQL databases
Strong experience with ETL processes to ingest large amounts of data at regular intervals
Strong knowledge of database performance optimization techniques, such as clustered, non-clustered, spatial, full-text indexing, etc.
Geospatial data knowledge preferred
Experience working in Agile SDLC methodology
Proficient with SQL and T-SQL programming skills
Working experience building data/ETL pipeline and data warehouse
Demonstrated expertise in data modeling, database maintenance, monitoring and performance tuning on SQL Server, MongoDB or other NoSQL databases
Exceptional analytical skills analyzing large and complex data sets
Perform thorough testing and data validation to ensure the accuracy of data transformations
Strong written and verbal communication skills, with precise documentation
Self-driven team player with ability to work independently and multi-task
Analytical, creative thinker and innovative problem solver
Working knowledge/proficient in modern cloud computing technology
Bachelor's degree in Computer Science/related field or equivalent combination of education and experience
5-8 years of directly related experience
Salary Range: $87,900.00 - $13,700 Annually
This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location.
What We Offer
By choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.
Show more
Show less","SQL, NoSQL, ETL, Data modeling, Database maintenance, MongoDB, Data warehouse, Data validation, Cloud computing, Agile SDLC, TSQL","sql, nosql, etl, data modeling, database maintenance, mongodb, data warehouse, data validation, cloud computing, agile sdlc, tsql","agile sdlc, cloud computing, data validation, database maintenance, datamodeling, datawarehouse, etl, mongodb, nosql, sql, tsql"
