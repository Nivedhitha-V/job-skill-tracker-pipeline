job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Engineer 2,Cook Medical,"Bloomington, IN",https://www.linkedin.com/jobs/view/data-engineer-2-at-cook-medical-3526207414,2023-12-17,Bloomington,United States,Mid senior,Onsite,"Overview
The Data Engineer develops, implements and documents data systems that provide the technical solutions to meet specifications and business requirements defined by company objectives to promote effective, efficient, and compliant operations. Experience with Azure preferrable. Experience with relational SQL and NoSQL databases; experience with SQL Server, Azure, Oracle, and/or MongoDB preferred.
Hybird or Remote position with expectation of Eastern Time Zone Hours.
Responsibilities
Work as part of a project team to define and document data and data pipelines for a variety of solutions.
Technical responsibility for data and data pipelines to ensure compliance with data standards, architectural standards, and achievement of documented requirements.
Develop and maintain current state documentation and deliverables for data solutions.
Maintain existing and new data solutions to ensure that they continue to meet user needs.
Provide assistance to operational teams to ensure continuity of service.
Work with the Data Engineering community of practice to identify and implement continuous improvement opportunities.
Must have excellent analytical and problem solving skills
Performs other duties as assigned
Possesses solid working knowledge of subject matter.
May provide leadership, coaching, and/or mentoring to a subordinate group.
Ability to manage tasks independently and take ownership of responsibilities
Performs work under minimal supervision.
Demonstrated organizational, analytical and interpersonal skills
Must work and interact effectively and professionally with and for others throughout various levels of the global organization
Must have effective communication skills and ability to work in a collaborative and independent work situations and environments with minimal supervision
Ability to remain calm and receptive in fast paced situations
Ability to manage tasks independently; take ownership of responsibilities
Qualifications
Bachelors Degree in Computer Science or other related degree; or experience of such kind and amount as to provide a comparable background.
Experience with relational SQL and NoSQL databases; experience with SQL Server, Oracle, and/or MongoDB preferred.
Preferrable 4-6 years of experience as Data Engineer
Experience building data transformations, data structures, and data pipelines.
Experience with data integration tools.
Experience with object-oriented/object function scripting languages.
Demonstrated organizational, analytical and interpersonal skills
Ability to manage tasks independently; take ownership of responsibilities
Physical Requirements
Works under general office environment conditions
Sitting for extended periods, utilizes close visual acuity for working with computers, equipment, etc.
Must be able to perform the essential functions of the job, subject to reasonable accommodation requirements under the ADA
Requires occasional early morning or late evening teleconferences
International and domestic travel as required
For this position, qualified candidates must be legally authorized to be employed in the United States. Cook does not intend to provide sponsorship for employment visa status (e.g., H-1B or TN status) for this employment position.
Show more
Show less","Azure, SQL, NoSQL, SQL Server, Oracle, MongoDB, Data pipelines, Data integration tools, Objectoriented/object function scripting languages","azure, sql, nosql, sql server, oracle, mongodb, data pipelines, data integration tools, objectorientedobject function scripting languages","azure, data integration tools, datapipeline, mongodb, nosql, objectorientedobject function scripting languages, oracle, sql, sql server"
Staff Data Engineer,Recruiting from Scratch,"Bloomington, IN",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744389892,2023-12-17,Bloomington,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, automated testing, Kafka, Storm, SparkStreaming, dimensional data modeling, ETL, data management, data classification","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, etl, data management, data classification","airflow, automated testing, continuous integration, data classification, data management, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Bloomington, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828472,2023-12-17,Bloomington,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, ETL, TDD, Pair programming, Continuous integration, Software testing, Deployment, Streamprocessing systems, Data modeling, Schema design, Data warehouses, Dimensional data modeling, Legal compliance, Data management","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, etl, tdd, pair programming, continuous integration, software testing, deployment, streamprocessing systems, data modeling, schema design, data warehouses, dimensional data modeling, legal compliance, data management","airflow, continuous integration, data management, data warehouses, datamodeling, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, software testing, spark, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Bloomington, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392289,2023-12-17,Bloomington,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","TDD, Automation, Continuous delivery, Data engineering, Data science, Business intelligence, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile engineering, Pair programming, Continuous integration, Kafka, Storm, SparkStreaming, Data warehousing, ETL, Data classification, Data retention","tdd, automation, continuous delivery, data engineering, data science, business intelligence, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehousing, etl, data classification, data retention","agile engineering, airflow, automation, business intelligence, continuous delivery, continuous integration, data classification, data engineering, data retention, data science, datawarehouse, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Bloomington, IN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712024,2023-12-17,Bloomington,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data Ops, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Enrichment, Data Monitoring, Data Platforms, Data Frameworks, Big Data Technologies, Data Pipelines, Machine Learning, Natural Language Processing, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention, GenderAffirming Offerings, HR, Health, Dental, Vision","data engineering, ml data ops, data mining, data cleaning, data normalization, data modeling, data enrichment, data monitoring, data platforms, data frameworks, big data technologies, data pipelines, machine learning, natural language processing, large language models, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention, genderaffirming offerings, hr, health, dental, vision","airflow, applied machine learning, aws, azure, bash, big data technologies, data classification, data cleaning, data engineering, data enrichment, data frameworks, data management tools, data mining, data monitoring, data normalization, data platforms, data retention, datamodeling, datapipeline, dental, docker, etl, gcp, genderaffirming offerings, git, health, helm, hr, java, kafka, kubernetes, large language models, machine learning, ml data ops, natural language processing, nosql, python, snowflake, spark, sparkstreaming, sql, storm, vision"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Bloomington, IN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088678,2023-12-17,Bloomington,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, ML, Data Ops, Data Pipelines, Data Pre/post Processing, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Platforms, Data Frameworks, Big Data, Data Governance, Data Infrastructure, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Compliance, Data Classification, Data Retention, Data Projects","data engineer, ml, data ops, data pipelines, data prepost processing, data mining, data cleaning, data normalization, data modeling, data platforms, data frameworks, big data, data governance, data infrastructure, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data compliance, data classification, data retention, data projects","airflow, aws, azure, bash, big data, data classification, data cleaning, data compliance, data frameworks, data governance, data infrastructure, data mining, data normalization, data ops, data platforms, data prepost processing, data projects, data retention, dataengineering, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, ml, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Data Scientist - National Defense/Python/SQL/AWS/MLOps/Clearances,Motion Recruitment,"Laughlin AFB, TX",https://www.linkedin.com/jobs/view/data-scientist-national-defense-python-sql-aws-mlops-clearances-at-motion-recruitment-3732144864,2023-12-17,Del Rio,United States,Mid senior,Hybrid,"An Artificial Intelligence/National Defense start-up is searching for Data Scientists to join their team. You would be solving complex problems by being a systems-level thinker, all the while transforming the nation’s defense and national security. This team handles abstract problems regarding the public policy surrounding climate change, energy, and national security, among other pressing issues.
Some of your responsibilities would include discovering datasets that could help in solution development; curating data, analyzing, and performing quantitative modeling; validating the quality of data, models, and results; deploying and implementing solutions in collaboration with product team; and interacting with the product team on current and upcoming user requirements.
Required Skills & Experience
Masters or Ph.D. in Sciences, Mathematics or Engineering, especially numerical methods and simulations
At least 4 years of professional experience in a Data Scientist position
Python (pandas, numpy, scipy, sci-kit learn, etc)
SQL / MySQL
Cloud computing, preferably in an AWS environment
Large-scale data processing and implementing batch processing pipelines in HPC or cloud architecture
Active government clearance (secret, top secret, ts/sci - no Poly required)
What You Will Be Doing
Daily Responsibilities
70% Hands On
30% Team Collaboration
The Offer
Bonus eligible
You Will Receive The Following Benefits
Full medical, dental, vision coverage for employee and dependents
401k matching program
PTO and Holidays
Bonus and other incentive programs
Access to mental health program
Access to Flexible Spending Accounts for Health Care, Dependent and Commuter
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. Possession of at minimum a Secret Clearance is required.**
Posted By:
Lindsay Troyer
Show more
Show less","Data Science, Numerical Methods, Simulations, Python, Pandas, Numpy, Scipy, Scikit Learn, SQL, MySQL, Cloud Computing, AWS, LargeScale Data Processing, Batch Processing Pipelines, HPC, Government Clearance, HandsOn Work, Team Collaboration, Bonus Eligibility, Full Medical Dental Vision Coverage, 401k Matching Program, PTO, Holidays, Bonus and Incentive Programs, Mental Health Program, Flexible Spending Accounts","data science, numerical methods, simulations, python, pandas, numpy, scipy, scikit learn, sql, mysql, cloud computing, aws, largescale data processing, batch processing pipelines, hpc, government clearance, handson work, team collaboration, bonus eligibility, full medical dental vision coverage, 401k matching program, pto, holidays, bonus and incentive programs, mental health program, flexible spending accounts","401k matching program, aws, batch processing pipelines, bonus and incentive programs, bonus eligibility, cloud computing, data science, flexible spending accounts, full medical dental vision coverage, government clearance, handson work, holidays, hpc, largescale data processing, mental health program, mysql, numerical methods, numpy, pandas, pto, python, scikit learn, scipy, simulations, sql, team collaboration"
Senior ETL Data Quality Engineer,Wise Skulls,United States,https://www.linkedin.com/jobs/view/senior-etl-data-quality-engineer-at-wise-skulls-3748439902,2023-12-17,Inverell, Australia,Mid senior,Remote,"Title: Senior ETL Data Quality Engineer
Location: Austin, TX; Fortmill, SC (Remote for Now)
Duration: 12+ Months (Possibility of Extension)
Implementation Partner: Infosys
End Client: To be disclosed
Jd
Must have 9+ Years of Expereince.
Must have ETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD.
Good to have API.
Show more
Show less","ETL, SQL, PostgreSQL, Cloud, Snowflake, AWS, Python, Java, CI/CD, API","etl, sql, postgresql, cloud, snowflake, aws, python, java, cicd, api","api, aws, cicd, cloud, etl, java, postgresql, python, snowflake, sql"
"Senior Engineer, Data",Ensemble Health Partners,United States,https://www.linkedin.com/jobs/view/senior-engineer-data-at-ensemble-health-partners-3780820485,2023-12-17,Inverell, Australia,Mid senior,Remote,"Thank you for considering a career at Ensemble Health Partners!
Ensemble Health Partners is a leading provider of technology-enabled revenue cycle management solutions for health systems, including hospitals and affiliated physician groups. They offer end-to-end revenue cycle solutions as well as a comprehensive suite of point solutions to clients across the country.
Ensemble keeps communities healthy by keeping hospitals healthy. We recognize that healthcare requires a human touch, and we believe that every touch should be meaningful. This is why our people are the most important part of who we are. By empowering them to challenge the status quo, we know they will be the difference
The Opportunity:
As a Senior Data Engineer your role will be to implement the technology strategy for Ensemble data engineering team. You will focus on developing reusable software components, patterns, and tooling to address them. You will influence behaviors through code reviews and hands on design sessions. Your outcomes will be a primary contributor to achieving our long-term strategic goals.
Develop, test, deploy, monitor, maintain, and continuously improve scalable data pipelines and builds our new API integrations to support continuing increases in data volume and complexity.
Translate product concepts into project commitments that deliver incremental value to our customers frequently and with high quality.
Actively contribute to the engineering team through code reviews and hands on design sessions.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Review and recommend architectural patterns for data access and performance that align with the company’s best practices and platforms.
Required Minimum Education:
4 year/ Bachelors Degree or equivalent of working experience at the level
Preferred Education:
Graduate Degree (Masters)
Other Knowledge, Skills and Abilities Required:
5+ years of coding experience with Microsoft SQL.
3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones.
Experience and proficient understanding of MS Azure Data Bricks
Experience and proficiency in MS SQL- able to create and perform query and codes.
Excellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps
Experience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent
Experience with Delta tables as well as Parquet files stored in ADLS
Experience delivering applications using componentized and distributed architectures using event driven patterns
Demonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences
Solid foundations in formal architecture, design patterns and best practices
Understanding and experience working in a healthcare environment is strongly preferred.
Join An Award-winning Company
Three-time winner of “Best in KLAS” 2020-2022
2022 Top Workplaces Healthcare Industry Award
2022 Top Workplaces USA Award
2022 Top Workplaces Culture Excellence Awards
Innovation
Work-Life Flexibility
Leadership
Purpose + Values
Bottom line, we believe in empowering people and giving them the tools and resources needed to thrive. A few of those include:
Associate Benefits – We offer a comprehensive benefits package designed to support the physical, emotional, and financial health of you and your family, including healthcare, time off, retirement, and well-being programs.
Our Culture – Ensemble is a place where associates can do their best work and be their best selves. We put people first, last and always. Our culture is rooted in collaboration, growth, and innovation.
Growth – We invest in your professional development. Each associate will earn a professional certification relevant to their field and can obtain tuition reimbursement.
Recognition – We offer quarterly and annual incentive programs for all employees who go beyond and keep raising the bar for themselves and the company.
Ensemble Health Partners is an equal employment opportunity employer. It is our policy not to discriminate against any applicant or employee based on race, color, sex, sexual orientation, gender, gender identity, religion, national origin, age, disability, military or veteran status, genetic information or any other basis protected by applicable federal, state, or local laws. Ensemble Health Partners also prohibits harassment of applicants or employees based on any of these protected categories.
Ensemble Health Partners provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law. If you require accommodation in the application process, please contact TA@ensemblehp.com.
EEOC – Know Your Rights
FMLA Rights - English
La FMLA Español
Show more
Show less","SQL, Databricks, Apache Spark, Python, Azure Functions, Azure Data Bricks, MS SQL, Azure Service Bus, Azure Data Factory, Azure Dataflows, DevOps, Delta tables, AWS, Parquet files, Healthcare","sql, databricks, apache spark, python, azure functions, azure data bricks, ms sql, azure service bus, azure data factory, azure dataflows, devops, delta tables, aws, parquet files, healthcare","apache spark, aws, azure data bricks, azure data factory, azure dataflows, azure functions, azure service bus, databricks, delta tables, devops, healthcare, ms sql, parquet files, python, sql"
Senior Data Engineer,Attune,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-attune-3711772375,2023-12-17,Inverell, Australia,Mid senior,Remote,"Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.
We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.
Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.
Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.
Job Description
As a Senior Data Engineer joining our growing Data team, you will serve as a leader for data engineering - helping maintain our existing data pipelines and internal web applications while also providing guidance and support to other team members and looking for ways to improve our code and architecture to meet our growing data needs. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.
Responsibilities:
Independently drive improvements and optimization in ETLs and internal apps with a focus on future scalability and performance
Serve as a leader/mentor for data engineering, employing and encouraging good software and pipeline development practices
Develop automated checks to ensure data quality and integrity
Maintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvements
Work with our data engineers, BI, and other teams across the business to develop/refine ETL processes
Understand and answer questions on the data our team maintains & influence business decisions in the process, in partnership with Revenue, Marketing, and Underwriting
Qualifications:
5+ years experience in analytics, data science, or data engineering role building & maintaining ETL pipelines
Expert Python and Postgresql skills
Solid understanding of relational database design and basic query optimization techniques
Experience working with/optimizing for medium to large datasets in SQL
Experience working with git, and Gitlab or Github
Nice to have:
Experience with Linux CLI, shell programming
Understanding of CI/CD
Experience with AWS EC2 and S3
What we offer you:
150-190k per year
Flexible PTO
Generous parental and caregiver leave
401K match
Excellent medical, dental, and vision plans
Remote-first culture
And more!
Show more
Show less","Python, Pandas, PostgreSQL, Bash, AWS EC2, S3, SQL, Git, GitLab, Github, Linux, CLI, Shell programming, CI/CD","python, pandas, postgresql, bash, aws ec2, s3, sql, git, gitlab, github, linux, cli, shell programming, cicd","aws ec2, bash, cicd, cli, git, github, gitlab, linux, pandas, postgresql, python, s3, shell programming, sql"
Sr. Data Software Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/sr-data-software-engineer-at-steneral-consulting-3733275905,2023-12-17,Inverell, Australia,Mid senior,Remote,"Job
: Sr. Data Software Engineer
Location:
100% Remote
Duration:
6 month Contract to Hire (USC/ GC Holder ONLY)
Top Skills
SQL
Azure
Data Factory
DBT
ETL work
Proven ability to complete projects in a timely manner while clearly measuring progress
Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming)
Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.
Experience writing and maintaining frontend client applications, Angular preferred
Strong experience with revision control (Git)
Experience with cloud-based systems (Azure / AWS / GCP).
High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns
Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)
Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools
Strong communication skills
Strong experience with use of RESTful APIs
High level understanding of HL7 V2.x / FHIR based interface messages.
High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day
Communicate with business leaders to help translate requirements into functional specification
Develop broad understanding of business logic and functionality of current systems
Analyze and manipulate data by writing and running SQL queries
Analyze logs to identify and prevent potential issues from occurring
Deliver clean and functional code in accordance with business requirements
Consume data from any source, such a flat files, streaming systems, or RESTful APIs
Interface with Electronic Health Records
Engineer scalable, reliable, and performant systems to manage data
Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization
Build quality systems while expanding offerings to dependent teams
Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems.
Show more
Show less","SQL, Azure, Data Factory, DBT, ETL, Data structures, Algorithms, Async programming, Objectoriented design, Parallel programming, .NET, Java, Angular, Git, Cloudbased systems, Big data design, Data normalization, Queuing technologies, Metrics, Logging, Monitoring, Alerting, RESTful APIs, HL7 V2.x / FHIR, CI/CD Pipeline, K8s, Terraform, Electronic Health Records","sql, azure, data factory, dbt, etl, data structures, algorithms, async programming, objectoriented design, parallel programming, net, java, angular, git, cloudbased systems, big data design, data normalization, queuing technologies, metrics, logging, monitoring, alerting, restful apis, hl7 v2x fhir, cicd pipeline, k8s, terraform, electronic health records","alerting, algorithms, angular, async programming, azure, big data design, cicd pipeline, cloudbased systems, data factory, data normalization, data structures, dbt, electronic health records, etl, git, hl7 v2x fhir, java, k8s, logging, metrics, monitoring, net, objectoriented design, parallel programming, queuing technologies, restful apis, sql, terraform"
Data Analyst || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-analyst-remote-at-steneral-consulting-3673756434,2023-12-17,Inverell, Australia,Mid senior,Remote,"Can be 100% remote; if someone could come onsite some on the front end of the engagement, that would be ideal to help them get up to speed (could put the person up in a corporate hotel around them in Athens, GA)
Need LinkedIn
Candidates need to taker a video screen with prime vendor prior to end client submittal
Must Have
5+ years' experience Data Analysis & Modeling
Working knowledge of the Google Cloud Storage and Google BigQuery
Knowledge and familiarity of Google Looker (LookML) and/or Power BI (DAX)
Strong ANSI SQL experience (DDL, views, stored procedures)
Experience collaborating with data engineers and business users to create realistic solutions to business problems
Ability to understand business structure and process to determine what measures and KPI's can be gleaned from existing data
Strong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data management
Job Description
The Data Analyst will be responsible for requirements gathering, data modeling within new and existing components for our client's data ecosystem. He or she will have a thorough understanding of relational databases, as well as reporting & visualization technologies. There will be a focus on using raw and partially refined data to configure models and semantic layers within applicable reporting and business intelligence tools throughout the organization to support strategic initiatives. The candidate will need to be comfortable with engaging different areas of the business to gather requirements, evaluate circumstances, and produce results under the guidance of a senior data architect.
Essential Job Functions
Author and support data models within the semantic layers of relevant data visualization and BI tools
Author and support tables, views, and procedures within relational data warehousing technology to effectively ready data for consumption within relevant BI toolsets
Create, support, and distribute reports and visualizations using applicable tools available to our client's brand
Display a high level of competence with relevant BI tools (Looker, Power BI, etc.) in such a way that business analysts and other end users have the proper guidance on how to use the technology
Research and stay abreast of industry leading technologies that improve business intelligence and create highly usable data models
Participate in requirements gathering and brainstorming sessions, and be willing to suggest and provide innovative solutions to complex business problems
Partner with various groups throughout the organization to create collaborative solutions to meet business needs
Collaborate with other teams within IT to create a cohesive integration of systems and data
Validate, massage and clean data from all available sources to ensure the highest degree of accuracy and usability
Understand and perform to standards within project plans set forth by leaders of the organization and the Project Management Office
Qualifications
Working knowledge of the Google Cloud Storage and Google BigQuery
Knowledge and familiarity of Google Looker (LookML) and/or Power BI (DAX)
Strong ANSI SQL experience (DDL, views, stored procedures)
Experience collaborating with data engineers and business users to create realistic solutions to business problems
Ability to understand business structure and process to determine what measures and KPI's can be gleaned from existing data
Strong ability to articulate and explain situations, issues and solutions to members of the organization not regularly involved in data management
Ability to join and contribute effectively to large meetings with individuals from various departments within the organization and at various levels
Proficient in common offerings within Office 365 product suite (Word, PowerPoint, Excel)
Education And Experience Guidelines
Education: Bachelor's degree in Computer Science, Information Systems, Data Science, or related field
Experience: 5+ years of experience in Data Analysis and Modeling
Show more
Show less","Data Analysis, Data Modeling, Google Cloud Storage, Google BigQuery, Google Looker (LookML), Power BI (DAX), ANSI SQL, Relational Databases, Reporting & Visualization Technologies, Business Intelligence Tools, Data Visualization, Data Warehousing, Data Cleaning, Data Validation, Project Management, Office 365 (Word PowerPoint Excel), Business Requirements Gathering, Brainstorming, Problem Solving, Communication, Collaboration, Teamwork","data analysis, data modeling, google cloud storage, google bigquery, google looker lookml, power bi dax, ansi sql, relational databases, reporting visualization technologies, business intelligence tools, data visualization, data warehousing, data cleaning, data validation, project management, office 365 word powerpoint excel, business requirements gathering, brainstorming, problem solving, communication, collaboration, teamwork","ansi sql, brainstorming, business intelligence tools, business requirements gathering, collaboration, communication, data cleaning, data validation, dataanalytics, datamodeling, datawarehouse, google bigquery, google cloud storage, google looker lookml, office 365 word powerpoint excel, power bi dax, problem solving, project management, relational databases, reporting visualization technologies, teamwork, visualization"
"Job Opportunity :: Sr. Data Engineer :: 12 Months Contract to hire :: Fully Remote (Hybrid to Houston, TX)",Steneral Consulting,United States,https://www.linkedin.com/jobs/view/job-opportunity-sr-data-engineer-12-months-contract-to-hire-fully-remote-hybrid-to-houston-tx-at-steneral-consulting-3718993789,2023-12-17,Inverell, Australia,Mid senior,Remote,"Hi,
Hope you are doing well,
Please find the requirement below , If you find yourself comfortable with the requirement please reply with your
Updated Resume
and I will get back to you or I would really appreciate if you can give me a call back at my contact number
302-721-5174
Position:
Sr. Data Engineer
Location: 100% Remote or Hybrid (Houston, TX)
Duration: 12 Months Contract to hire
Work Authorization:
USC, GC Only
Interview:
Skype
Job Description
The Client has expressed a desire for the candidates to be able to work a hybrid 3days a week on-site, but they are willing to consider a remote candidate that has ""stellar"" skill set . This means , EOG would not have to cross train them into any of the tools being used in their environment. They would also need to have the ability of handling development independently as an individual contributor. As discussed before , this Client does not use large teams on big apps, they use data engineers on individual application issues.
If you find a candidate who needs to stay remote like Sai, they have to have everything requested in the description and
Over and above what the description requests , candidates would need to handle :
Experience in
PL/SQL
development work experience with
Oracle 19
Discuss working experience with tools like
Airflow, Git
Tell me about your
python
coding experience and
REST API developmen
t
In your career , do you have any experience with
accounting and financial data
Data Engineer Consultant for Houston Client,
and they can be under a VISA , Green Card or USC.
""Typically working on big data and cloud technologies
is not going to be helpful to us
such as; "" Hadoop ecosystem, Bigdata, PySpark, Spark, AWS, GCP, Big Query, Data Warehousing""
Please note
the following preferred skills as they truly need these to get the Manager's attention:
Expert In PL/SQL Development Work Experience With Oracle 19
Must have working experience with tools like Airflow, Git
Must have python coding experience and REST API development experience
Plus Would Be Experience With Accounting And Financial Data
Client would also l
ike to see 2 code samples of their best code
to gauge their coding style, and they can just show it to Client during the interview, They don't need copies of the code. Interview will be Teams based.
They have commented , they
don't want to see
resumes just with cloud technology experience (like with Azure, AWS, GCS etc), they are not using those at site.
Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities.
Remote or Hybrid:
We prefer someone local to Houston, who is willing to come to the office at least 3 days a week. But remote option is ok, if you are able to find great candidates.
Thank you and Regards
Gaurav Raj
Sr. Talent Acquisition Specialist -North America
Direct: +1
302-721-5174
gaurav@steneral.com
1007, N Orange St, 4th FL 329, Wilmington, DE 19801
Show more
Show less","PL/SQL, Oracle 19, Airflow, Git, Python, REST API, Accounting, Financial data","plsql, oracle 19, airflow, git, python, rest api, accounting, financial data","accounting, airflow, financial data, git, oracle 19, plsql, python, rest api"
Data Integration Developer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-integration-developer-at-steneral-consulting-3680019522,2023-12-17,Inverell, Australia,Mid senior,Remote,"Remote
Job Description
Oracle, PL/SQL, Azure, NoSQL data integration experience with good .Net framework experience
Description
Develop data integration process flows, distributed programs and data mappings using .Net framework.
Design, develop, document
Acomplex processes that consolidate and integrate data
Perform development, testing and implementation of Oracle PL/SQL, and miscellaneous Windows and Linux scripts to support data integration needs.
Implement data solutions using Azure Cloud data components like Azure Synapse, ADLS Gen 2, Azure ADF, Azure NoSQL Database, etc.
Support the business by facilitating the importing, exporting, and integration of data between our enterprise systems and those of our partners
Develop data integration processes and follow and contribute to best practices for data integration processes
Collaborate with the Data Architecture team to understand and implement load processes for reporting and analytic data structures (data warehouses and data marts)
Support the business by facilitating the importing, exporting, and integration of data between our enterprise systems and those of our partners
Work with the Data Architects to augment ERD's as changes are developed
Develop, maintain, and extend reusable data integration components, per design from senior data integration developers/architects.
Ensure all data integration processes are documented and recoverable as it pertains to disaster recovery.
Collaborate and be approachable to the team.
Provide timely project and task updates to all concerned parties
Monitor production data integration jobs and correct failures in a timely manner
Create and manage incident reports as they pertain to data integration processes
Perform all other duties and special projects as assigned.
Education And/or Training
Bachelor's degree in Computer Science or equivalent work experience
Relevant Work Experience
Minimum 4 years of work experience in data integration and/or data warehousing
Must have a working knowledge of.NET, C#, APIs, Windows Service, ORM, TDD, Windows scripting
Knowledge of implementing data solutions using Azure Cloud data components like Azure Synapse, ADLS Gen 2, Azure ADF, Azure NoSQL Database, etc. is preferred
Must have a knowledge of Object-Oriented architecture and Service-Oriented architecture
Must have a strong command of SQL, ANSI SQL, and PL/SQL for data integration relating to data warehousing and integration.
Must have a proven track record of developing well-structured, maintainable data integration solutions
Experience querying and loading dimensional and relational data models in a data warehouse environment is required
Strong analytical skills, able to effectively solve problems in a timely manner
Show more
Show less","Oracle, PL/SQL, Azure, NoSQL, .Net Framework, Data integration, Data mapping, Data warehousing, Data architecture, ERD, Data recovery, C#, APIs, Windows Service, ORM, TDD, Windows scripting, Azure Synapse, ADLS Gen 2, Azure ADF, Azure NoSQL Database, ObjectOriented architecture, ServiceOriented architecture, SQL, ANSI SQL, Data modeling, Data analysis","oracle, plsql, azure, nosql, net framework, data integration, data mapping, data warehousing, data architecture, erd, data recovery, c, apis, windows service, orm, tdd, windows scripting, azure synapse, adls gen 2, azure adf, azure nosql database, objectoriented architecture, serviceoriented architecture, sql, ansi sql, data modeling, data analysis","adls gen 2, ansi sql, apis, azure, azure adf, azure nosql database, azure synapse, c, data architecture, data integration, data mapping, data recovery, dataanalytics, datamodeling, datawarehouse, erd, net framework, nosql, objectoriented architecture, oracle, orm, plsql, serviceoriented architecture, sql, tdd, windows scripting, windows service"
Lead Azure Data Engineer,"Donato Technologies, Inc.",United States,https://www.linkedin.com/jobs/view/lead-azure-data-engineer-at-donato-technologies-inc-3707657523,2023-12-17,Inverell, Australia,Mid senior,Remote,"Job Title: Lead Azure Data Engineer
Location: 100% Remote
Duration: 6 Months Contract (Possibility for long term extension)
Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql
This is a lead level role, someone who has lead a team. Min 10-12+ years' experience.
Should be purely focused on Azure Cloud, Microsoft Stack, Data Warehouse experience.
Looking for someone who been a BI/Sql/ETL Developer etc.. and moved in to Azure cloud Data engineering.
Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts.
Description
Enterprise Data modelling / Design
Azure SQL Data Warehouse (Synapse)
T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..)
ADF / Any Enterprise ETL Tool
ADLS / any cloud storage
Should be able to understand the technical specifications and able to work independently with minimal or no supervision
Must Have Skills
Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric.
Proven experience with SQL, namely schema design and dimensional data modelling
Solid knowledge of data warehouse best practices, development standards and methodologies
Strong experience with Azure Cloud on data integration with Databricks
Be an independent self-learner with the ""let's get this done"" approach and ability to work in Fast paced and Dynamic environment
Nice-to-Have Skills
Basic understanding on ML Studio, AI/ML, MLOps etc.
Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge.
Good to have SAP Hana knowledge
Intermediate knowledge on Power BI
Good to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes.
Show more
Show less","Azure Cloud, SQL, TSQL, Data Warehouse, Azure Data Lake, Azure Data Factory, Azure Databricks, Azure Synapse, Enterprise Data Modelling, Azure SQL Data Warehouse (Synapse), ADF, ADLS, Data Migration, Data Processing, Schema Design, Dimensional Data Modelling, Data Warehouse Best Practices, Data Integration, Databricks, ML Studio, AI/ML, MLOps, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db, SAP Hana, Power BI, DevOps, CI/CD Deployments, Cloud Migration Methodologies, Cloud Migration Processes","azure cloud, sql, tsql, data warehouse, azure data lake, azure data factory, azure databricks, azure synapse, enterprise data modelling, azure sql data warehouse synapse, adf, adls, data migration, data processing, schema design, dimensional data modelling, data warehouse best practices, data integration, databricks, ml studio, aiml, mlops, event hub, iot hub, azure stream analytics, azure analysis service, cosmo db, sap hana, power bi, devops, cicd deployments, cloud migration methodologies, cloud migration processes","adf, adls, aiml, azure analysis service, azure cloud, azure data factory, azure data lake, azure databricks, azure sql data warehouse synapse, azure stream analytics, azure synapse, cicd deployments, cloud migration methodologies, cloud migration processes, cosmo db, data integration, data migration, data processing, data warehouse best practices, databricks, datawarehouse, devops, dimensional data modelling, enterprise data modelling, event hub, iot hub, ml studio, mlops, powerbi, sap hana, schema design, sql, tsql"
Data Engineer - Lead,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-lead-at-steneral-consulting-3750175286,2023-12-17,Inverell, Australia,Mid senior,Remote,"Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PA
Candidates need to take a video screen with prime vendor prior to end client submittal
Need 2 strong candidates on this role
Need strong knowledge of PySprak. 80% of the role will be using Pyspark.
They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool.
Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept.
Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team.
Top skills:
Pyspark
Azure Databricks and/or Data Factory
Experience leading and managing offshore teams.
ETL
Must Have
Azure Cloud
Azure Data Brick
ETL
High volumes of data
Pyspark
Python
Experience Leading or managing offshore teams
Data en
gineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.
Responsibilities
Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory
Working with event based streaming technologies to ingest and process data
Design and Implement Data Lakehouse
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Work with contractors to deliver products and services for the bottlers
Conduct Pilots with bottlers and measure value
Transition successful pilots into commercial applications
Ensure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics concepts
Review code/solutions developed by team prior to migration to production
What makes you a good fit?
Strong knowledge of data management principles
Experience in building ETL pipelines
Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience working with structured and unstructured data
Experience working in DevOps environment
3+ years of relevant experience
Ability to balance high customer orientation and service attitude with business priorities
Analytic thinking, and problem-solving skills
High energy, with strong will and ambition to learn and work on new things
Outstanding proven verbal, written and interpersonal communication skills
Ability to adapt quickly to changing product scope and priorities (demand driven)
Proven ability to influence and collaborate effectively with cross-functional teams
Demonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best work
Constant role model of culture: Integrity, Accountability, Passion, Collaboration, Innovation
Bachelor’s degree (or equivalent)
Show more
Show less","PySpark, Azure Databricks, Azure Data Factory, ETL, Azure Cloud, Python, Data Lakehouse, Agile, DevOps, Azure Storage, Azure Cosmos DB, Azure Stream Analytics, Structured Data, Unstructured Data","pyspark, azure databricks, azure data factory, etl, azure cloud, python, data lakehouse, agile, devops, azure storage, azure cosmos db, azure stream analytics, structured data, unstructured data","agile, azure cloud, azure cosmos db, azure data factory, azure databricks, azure storage, azure stream analytics, data lakehouse, devops, etl, python, spark, structured data, unstructured data"
Urgent Role - Senior Data Engineer || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/urgent-role-senior-data-engineer-remote-at-steneral-consulting-3696912005,2023-12-17,Inverell, Australia,Mid senior,Remote,"Job Description
Title: Senior Data Engineer
Location: Remote
Duration: 6+ Months
Position Details
Senior Data Engineer
Role: Contract
Duration:
Contract position approved for 6 months, with likely extension and/or conversion.
Location:
MN Preferred-Hybrid, would consider
100% Remote from following approved states; AR, AZ, DE, FL, GA, IA, IL, KS, KY, MD, ME, MI, MN, MO, ND, NE, OK, SD, TN, TX, VA, WI.
Description
The Senior Data Engineer supports ETL Development. They will be responsible for developing and maintaining ETL process and support multiple lines of Business and IT initiatives. This individual may work independently or collaborate with a smaller group of developers in similar efforts. This position is responsible for technical analysis, design, development, testing, and deployment of IT solutions as needed by business or IT. The senior developer works closely with other subject matter experts to help design and build solutions for a multitude of applications, including batch processing, OLTP, OLAP and data warehouse architecture.
The Senior Data Engineer will design, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of data sources. Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data. Write complex SQL queries to support analytics needs. Evaluate and recommend tools and technologies for data infrastructure and processing.
Minimum Knowledge and Skill
Complete working knowledge and skill with ability to be fully proficient in professional field.
Bachelor's degree or equivalent experience in related field, plus 5 years of related work experience beyond degree.
Requirements For All 3 Positions
Informatica ETL on premise and on cloud.
Master Data Management experience. Informatica MDM experience a plus.
Informatica IDQ/CDQ experience.
Snowflake, Python, Spark experience
Show more
Show less","Data Engineering, ETL Development, Data Storage Structures, Data Processing Pipelines, Data Extraction, Data Transformation, Data Loading, SQL, Informatica ETL, Informatica MDM, Informatica IDQ/CDQ, Snowflake, Python, Spark, Batch Processing, OLTP, OLAP, Data Warehouse Architecture","data engineering, etl development, data storage structures, data processing pipelines, data extraction, data transformation, data loading, sql, informatica etl, informatica mdm, informatica idqcdq, snowflake, python, spark, batch processing, oltp, olap, data warehouse architecture","batch processing, data engineering, data extraction, data loading, data processing pipelines, data storage structures, data transformation, data warehouse architecture, etl development, informatica etl, informatica idqcdq, informatica mdm, olap, oltp, python, snowflake, spark, sql"
Founding Data Engineer,Rogo,"New York, NY",https://www.linkedin.com/jobs/view/founding-data-engineer-at-rogo-3749759981,2023-12-17,Inverell, Australia,Mid senior,Remote,"Why Rogo?
Rogo will be the biggest Financial Services Artificial Intelligence company in the world. We're creating a category-defining AI company built on top of foundational AI models like GPT-4.
Exceptional early users: high-paying contracts with the world's largest investment banks, hedge funds, private equity firms, and consultants.
Massive demand: extensive waitlist of firms waiting for deployment.
World-class team: we take talent density very seriously. We like working with incredibly smart, driven people.
Cutting-edge technology: Work directly with the world's most advanced LLMs, AI, and RAG to build the future of generative AI and redefine finance.
Top-of-market cash and equity compensation.
Challenges:
We are building systems that can automate the most complex knowledge work in the world, e.g., financial analysis, research, due diligence, and more.
Creating financial research that's worth paying attention to: aggregating, analyzing, and producing insights from real-time information. Say goodbye to equity research.
Dealing with the most sensitive data in the world: client data from the largest financial services companies on earth.
Working past the edge of published AI research: tackling problems beyond the complexity of existing AI benchmarks.
Unsolved product, architectural, and business problems: natural language interfaces, prohibitively expensive evaluation of models, massive marginal costs, versioning/training/segregating models per task, client, and so on.
As a founding Data Engineer at Rogo, you will help build out our real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier.
Hard Requirements:
3+ years of industry experience as a data engineer
Highly proficient with Python and SQL, and an intuitive understanding of multi-threading, multi-processing, asyncio, and other concurrency primitives
Experience with at least one of: Postgres, Snowflake or Elasticsearch
Experience deploying and monitoring mission-critical ETL pipelines with large and heterogenous datasources
Experience working with Apache Airflow
Experience with AWS or other cloud environment
Bonus Requirements:
Experience at a hypergrowth startup
Financial Services work experience
Experience with Typescript
Experience with stream processing
Knowledge of Datadog and other Telemetry tooling
You'll fit in at Rogo if...
You have fun solving hard problems: we're tackling tech/product/business problems that are unsolved. It's super exciting.
You like to work hard: we feel lucky to work on these problems, and we enjoy pouring our all into solving them.
You care deeply about talent density: we care deeply about working with people who are super smart and motivated.
You have eclectic interests: whether you're a sci-fi aficionado, history buff, strategy game guru, policy wonk, or movie trivia expert, you'll find kindred spirits here.
Compensation Range: $120K - $160K
Show more
Show less","Python, SQL, Postgres, Snowflake, Elasticsearch, Apache Airflow, AWS, TypeScript, Stream Processing, Datadog, Telemetry, GPT4, Natural Language Processing, Machine Learning, Financial Analysis, Research, Due Diligence, RealTime Data Pipelines, Financial LLM, Data Engineering, Multithreading, Multiprocessing, Asynchronous Programming, Concurrency Primitives, ETL Pipelines, Large and Heterogeneous Data Sources, Cloud Environment","python, sql, postgres, snowflake, elasticsearch, apache airflow, aws, typescript, stream processing, datadog, telemetry, gpt4, natural language processing, machine learning, financial analysis, research, due diligence, realtime data pipelines, financial llm, data engineering, multithreading, multiprocessing, asynchronous programming, concurrency primitives, etl pipelines, large and heterogeneous data sources, cloud environment","apache airflow, asynchronous programming, aws, cloud environment, concurrency primitives, data engineering, datadog, due diligence, elasticsearch, etl pipelines, financial analysis, financial llm, gpt4, large and heterogeneous data sources, machine learning, multiprocessing, multithreading, natural language processing, postgres, python, realtime data pipelines, research, snowflake, sql, stream processing, telemetry, typescript"
Staff Data Engineer,"Nav Technologies, Inc.","Seattle, WA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-nav-technologies-inc-3775330460,2023-12-17,Inverell, Australia,Mid senior,Remote,"Nav is democratizing small business financing. In other words, we give small business owners access and control. Yes, this challenges the norm, but it means working with curious, purpose driven, dedicated, and inquisitively smart people who push themselves, our company and the community to the next level (and every level after that). We are the people behind the tech. And when it's good, we look for better. We don't over think the value we bring nor spend time trying to revamp mantras. We also do not come up with some crafty way to tell you who we are and what we offer. We are Nav! Here, you'll gain a wealth of experience, learn the tricks of the trade, and work with winners. All companies say people are connected to their mission but in our case our mission and our people are one – it is a way of being not just a cause you are committed to. And since 2013, Nav has holistically and organically developed its own ideology because Nav can only be Nav.
We are seeking an innovative and passionate Data Engineer to join our team. In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users. You'll also work with our business intelligence and data science teams to improve the data platform, ensuring that the entire company is able to make better data-driven decisions.
YOU WILL:
Building cool stuff by maintaining and fulfilling a technical data engineering roadmap and vision.
Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration.
Exploring data sets for insight and understanding that will help drive great product changes and priority.
Communicating results and findings to company audiences of varying technical ability in a clear, engaging manner. Breaking down complex technical concepts into digestible tid-bits.
Supporting self-service data pipeline management (ETL) and self-directed learning process.
Addressing competing explanations in result validity, and use formal reasoning approaches to avoid bias and miscommunication.
Collaboration with Engineering and Data Analysts to identify and design ways to solve critical business problems and ensure funnel optimization.
Ability to design, develop, and maintain scalable, reusable code.
WHO YOU ARE:
You are a driven data engineer with experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS.
You have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran
You love building data-driven products and have expertise in one or more programming languages (ideally Python).
You have strong SQL experience, with expert level skill in Postgres, Snowflake and /or AWS Redshift.
You're an Apache Airflow practitioner with experience managing cross-DAG dependencies in Airflow.
You know how to implement, advocate, and teach agile practices, and adapt them to your people and circumstances, while working in accordance with data ethics & privacy standards.
You can consistently evolve data models & data schema based on business and engineering needs.
You've developed and extended design patterns, processes, standards, frameworks and reusable components for use in various data engineering functions and areas.
Ability to implement systems for tracking data quality and consistency.
You're comfortable working directly with all internal data consumers and are eager to share your work through calibrations and organizational product demos.
You are committed to quality! And you're someone who's willing to tackle technical debt and constantly evolving our processes to help nav grow.
We'd love it if you have a high level understanding of credit scores, ecommerce, and B2B financing.
Inclusion at Nav:
At Nav, we celebrate what makes our employees unique because the businesses we serve are progressively diverse and distinctly original. Navericks are diverse, side hustlers, immigrants, veterans, queer, and we push generational boundaries. We are college dropouts, PhDs, special needs parents, allies, pet owners and community leaders. Navericks are human. We are committed to upholding a safe, supportive environment where everyone matters. We are committed to making a better future for all of us. We have created a workplace where people of all backgrounds can express their identities authentically. To put it simply, we want you to be proud to be you.
Our Compensation Philosophy is simple but powerful:
At Nav, we are transparent about our total rewards, including pay, across all levels and roles. We believe great, enduring relationships are grounded in trust and transparency. Compensation shouldn't be a distraction, and employees should understand how pay and career advancement decisions are made. Providing equal pay for equal work is table stakes for being a great place to work. Gender and ethnic inequity should only be something that our children read about in history books. We believe providing Navricks with company ownership, competitive pay, and a range of meaningful benefits is the start of creating a culture where people want to give the best they've got — not because they're simply making money, but because they've fallen in love with our vision, mission, values, and team.
During the interview process, your recruiter will be explaining how our rubrics work across all of our total rewards ( base, equity, bonus, perks, and benefit) offerings . The base salary for this role is targeted between $140K - $155K per year. Final offer amount is determined by your proficiencies within this level.
Our impact on you:
Competitive Pay. Company Ownership. Unlimited Vacation. Benefits Day One. 6 Weeks Paid Parental Leave. Work From Anywhere (yes we were distributed before it was cool). Flexible Work Arrangements. Free Telehealth and Telemental Health For All Employees. Employee Networking and Events. Community Network Groups (women's, PRIDE, culture). Meaningful Perks and Rewards. Learning and Development Opportunities. Pet Insurance.
A Naverick's DNA:
We look at the future and say ""why not""; we see possibilities where others see problems or routines. We show the way ahead and are committed to achieving ambitious goals.
We practice straight talk and listen generously to each other with empathy. We value different opinions and point of views. We ensure that we connect outside as well as inside to learn from others and inspire each other.
We hold ourselves accountable for delivering results. We choose to not to be a victim of circumstance. We make decisions & take responsibility so that we can act & support each other, rather than adopting defensive, and ""finger pointing"" behaviors.
As leaders we motivate & engage our teams to undertake beyond what they originally thought possible, by developing our teams & creating the conditions for people to grow and empower themselves through enabling & coaching.
If you are based in California, we encourage you to read our privacy notice for California residents linked here.
Show more
Show less","Docker, ECS, S3, Redshift, Kafka, RDS, dbt, Azure Data Factory, Matillion, Fivetran, Python, Postgres, Snowflake, Apache Airflow, Apache Kafka, Tableau, dbt, Fivetran, Matillion, Azure Data Factory, AWS Glue, AWS Redshift, AWS Athena, AWS EMR, AWS SageMaker, Spark, Hadoop, Hive, Pig, Scala, Java, Linux, Unix, Bash, SQL, NoSQL, MongoDB, Cassandra, HBase, Redis, ElasticSearch, Kibana, Logstash, Apache NiFi, Apache Hadoop, Apache Storm, Apache Spark, Apache Flink, Apache Airflow, Apache Hive, Apache Pig, Apache HBase, Apache Phoenix, Apache Cassandra, Apache Kafka, Apache ZooKeeper, Apache Flume, Apache Sqoop, Apache Parquet, Apache Avro, Apache Thrift, Apache Arrow, Apache Impala, Apache Drill, Apache HCatalog, Apache Hive LLAP, Apache Phoenix, Apache Spark SQL, Apache Flink SQL, Apache Beam, Apache Druid, Apache Kylin, Apache Pinot, Apache Superset, Apache Zeppelin, Apache Ambari, Apache Ranger, Apache Atlas, Apache Hive Metastore, Apache Spark Streaming, Apache Flink Streaming, Apache Storm Streaming, Apache Samza, Apache Apex, Apache NiFi Streaming, Apache Kafka Streams, Apache Pulsar, Apache RocketMQ, Apache FlinkCEP, Apache Storm Trident, Apache Samza Streams, Apache Apex Streams, Apache NiFi Streaming, Apache Kafka Streams, Apache Pulsar, Apache RocketMQ, Apache Spark MLlib, Apache Flink ML, Apache Mahout, Apache MxNet, Apache TensorFlow, Apache PyTorch, Apache Keras, Apache Theano, Apache CNTK, Apache MXNet, Apache TVM, Apache Ray, Apache Spark MLlib, Apache Flink ML, Apache Mahout, Apache MxNet, Apache TensorFlow, Apache PyTorch, Apache Keras, Apache Theano, Apache CNTK, Apache MXNet, Apache TVM, Apache Ray","docker, ecs, s3, redshift, kafka, rds, dbt, azure data factory, matillion, fivetran, python, postgres, snowflake, apache airflow, apache kafka, tableau, dbt, fivetran, matillion, azure data factory, aws glue, aws redshift, aws athena, aws emr, aws sagemaker, spark, hadoop, hive, pig, scala, java, linux, unix, bash, sql, nosql, mongodb, cassandra, hbase, redis, elasticsearch, kibana, logstash, apache nifi, apache hadoop, apache storm, apache spark, apache flink, apache airflow, apache hive, apache pig, apache hbase, apache phoenix, apache cassandra, apache kafka, apache zookeeper, apache flume, apache sqoop, apache parquet, apache avro, apache thrift, apache arrow, apache impala, apache drill, apache hcatalog, apache hive llap, apache phoenix, apache spark sql, apache flink sql, apache beam, apache druid, apache kylin, apache pinot, apache superset, apache zeppelin, apache ambari, apache ranger, apache atlas, apache hive metastore, apache spark streaming, apache flink streaming, apache storm streaming, apache samza, apache apex, apache nifi streaming, apache kafka streams, apache pulsar, apache rocketmq, apache flinkcep, apache storm trident, apache samza streams, apache apex streams, apache nifi streaming, apache kafka streams, apache pulsar, apache rocketmq, apache spark mllib, apache flink ml, apache mahout, apache mxnet, apache tensorflow, apache pytorch, apache keras, apache theano, apache cntk, apache mxnet, apache tvm, apache ray, apache spark mllib, apache flink ml, apache mahout, apache mxnet, apache tensorflow, apache pytorch, apache keras, apache theano, apache cntk, apache mxnet, apache tvm, apache ray","apache airflow, apache ambari, apache apex, apache apex streams, apache arrow, apache atlas, apache avro, apache beam, apache cassandra, apache cntk, apache drill, apache druid, apache flink, apache flink ml, apache flink sql, apache flink streaming, apache flinkcep, apache flume, apache hadoop, apache hbase, apache hcatalog, apache hive, apache hive llap, apache hive metastore, apache impala, apache kafka, apache kafka streams, apache keras, apache kylin, apache mahout, apache mxnet, apache nifi, apache nifi streaming, apache parquet, apache phoenix, apache pig, apache pinot, apache pulsar, apache pytorch, apache ranger, apache ray, apache rocketmq, apache samza, apache samza streams, apache spark, apache spark mllib, apache spark sql, apache spark streaming, apache sqoop, apache storm, apache storm streaming, apache storm trident, apache superset, apache tensorflow, apache theano, apache thrift, apache tvm, apache zeppelin, apache zookeeper, aws athena, aws emr, aws glue, aws redshift, aws sagemaker, azure data factory, bash, cassandra, dbt, docker, ecs, elasticsearch, fivetran, hadoop, hbase, hive, java, kafka, kibana, linux, logstash, matillion, mongodb, nosql, pig, postgres, python, rds, redis, redshift, s3, scala, snowflake, spark, sql, tableau, unix"
Healthcare Data Analyst,Pasito,United States,https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-pasito-3767238322,2023-12-17,Inverell, Australia,Mid senior,Remote,"About Pasito
Pasito uses data and AI to engage people with their health and financial employee benefits. We integrate payroll, financial, and claims data to personalize employee benefits support and proactively reach employees with benefits communications that drive cost savings for them and their employers.
Pasito is backed by Y Combinator and Google.
Healthcare Data Analyst
We are looking for a Healthcare Data Analyst consultant to collaborate closely with Pasito’s Product and Engineering teams. This is an exciting opportunity to work with leading AI engineers disrupting healthcare for the benefit of families and employees around the country. Pasito is open to structuring a full-time role for the right candidate.
As a Health Data Analyst, You Will
Review and analyze historical medical claims, accurately mapping CPT codes to specific services, procedures, and diagnoses.
Identify trends and patterns in claims data to understand population health, healthcare utilization and cost drivers.
Utilize predictive modeling and data analytics techniques to forecast future claims expenses and estimate the impact of various healthcare scenarios, and work closely with Pasito’s engineering team to scale your models.
Provide data-driven insights and recommendations for managing claims expenses effectively.
Monitor the effectiveness of cost-saving strategies.
Build-out real-time reporting (both tabular and visual)
Work with engineering teams to streamline data collection and data cleansing
Communicate complex healthcare data and findings to non-technical stakeholders in a clear and understandable manner.
Create and maintain documentation for data processes and analysis.
Support the company's sales and marketing efforts with data analytics.
Technical & Interpersonal Skills
Bachelor’s degree in Actuarial Science, Mathematics, other physical science, or related finance/business degree required
5+ years of relevant, professional work experience and/or education required
Comprehensive understanding of actuarial principles and methodologies and strong mathematical and analytical skills required
Comprehensive knowledge of the U.S. benefits landscape, US healthcare and/or US insurance landscape
Proficiency with relational databases, data architecture, data visualization tools and libraries.
Knowledge of machine learning techniques and algorithms.
Excellent English communication skills, both written and verbal.
Self-motivated and able to work both independently and collaboratively in a fast-paced environment.
Location
Remote
Show more
Show less","Data Analysis, Machine Learning, Actuarial Science, Forecasting, Predictive Modeling, Healthcare, Claims Data, CPT Codes, U.S. Benefits, U.S. Healthcare, U.S. Insurance, Data Architecture, Data Visualization, Relational Databases, Communication Skills, English","data analysis, machine learning, actuarial science, forecasting, predictive modeling, healthcare, claims data, cpt codes, us benefits, us healthcare, us insurance, data architecture, data visualization, relational databases, communication skills, english","actuarial science, claims data, communication skills, cpt codes, data architecture, dataanalytics, english, forecasting, healthcare, machine learning, predictive modeling, relational databases, us benefits, us healthcare, us insurance, visualization"
Data Engineer( Informatica power Center and Data Stage EXP),Innovee Consulting LLC,United States,https://www.linkedin.com/jobs/view/data-engineer-informatica-power-center-and-data-stage-exp-at-innovee-consulting-llc-3785705659,2023-12-17,Inverell, Australia,Mid senior,Remote,"Role: Sr.Data Engineer Informatica power Center and Data Stage EXP)
Location: Remote
Duration: Long Term
The Sr. data engineer will play a pivotal role in designing, building and operationalizing data pipelines that feed the enterprise data ecosystem for a variety of multi-faceted analytics created by data consumers. The data engineer needs to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing integrated and reusable data pipelines. The data engineer will be measured on their ability to create repeatable, efficient, high-quality code, automation-friendly processes, and delivering work products quickly.
General Tasks And Responsibilities Will Include
Lead construction, maintenance, & optimization of data pipelines.
Drive Automation through effective metadata management, using innovative and modern tools, techniques, and architectures to improve productivity.
Lead renovation of the data management infrastructure to drive automation in data integration and management.
Collaborate with multiple Analytics Center of Excellence teams and EIM team in refining their data requirements for various DnA initiatives and data consumption requirements.
Identify gaps in new data initiatives and how to address new data requirements.
Transfer knowledge of data and/or domain understanding in addressing new data requirements.
Propose appropriate (and innovative) data ingestion, preparation, integration and operationalization techniques to optimize data requirements.
Work with data governance teams to ensure data scientists and consumers use corresponding data responsibly through data governance and compliance initiatives.
Design and implement data quality monitoring systems, which includes source-to-target data validations as well as anomaly detection
Qualifications And Experience Requirements
Bachelor’s degree/master's degree/advanced degree in computer science, statistics, applied mathematics, data management, information systems, information science or related field
Expertise and excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies
Preferred experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms
Full Lifecycle Data Warehouse, Data Mart Development experience is a big plus
Agile Software development methodologies
Preferred experience with any automation tool such as AutoSys, Cntl-M.
Minimum 10 years of work experience in data management disciplines, including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Minimum 2 years designing and implementing a fully operational production grade large scale data solution on Snowflake Data Warehouse
Minimum 10 years of hands on experience designing and implementing production grade data warehousing solutions on large scale databases such as SQL Server, Oracle or DB2
5+ years data pipeline development experience with Informatica Powercenter and IBM DataStage
3+ years of experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, Java, C++, Scala, others
3+ years of experience with Shell/Perl Scripting
5+ years of experience with popular database programming languages including SQL, PL/SQL (required), others for relational databases and certifications on upcoming NoSQL/Hadoop oriented databases (preferred) like MongoDB, Cassandra, others for nonrelational databases
Show more
Show less","Data Engineering, Data Pipeline Design, Data Pipeline Construction, Data Pipeline Implementation, Data Analytics, Data Governance, Data Security, Reusable Data Pipelines, Automation, Metadata Management, Data Integration, Data Management, Snowflake, Data Science, Machine Learning, Agile Development, AutoSys, CntlM, Informatica Powercenter, IBM DataStage, R, Python, Java, C++, Scala, Shell Scripting, Perl Scripting, SQL, PL/SQL, MongoDB, Cassandra","data engineering, data pipeline design, data pipeline construction, data pipeline implementation, data analytics, data governance, data security, reusable data pipelines, automation, metadata management, data integration, data management, snowflake, data science, machine learning, agile development, autosys, cntlm, informatica powercenter, ibm datastage, r, python, java, c, scala, shell scripting, perl scripting, sql, plsql, mongodb, cassandra","agile development, automation, autosys, c, cassandra, cntlm, data engineering, data governance, data integration, data management, data pipeline construction, data pipeline design, data pipeline implementation, data science, data security, dataanalytics, ibm datastage, informatica powercenter, java, machine learning, metadata management, mongodb, perl scripting, plsql, python, r, reusable data pipelines, scala, shell scripting, snowflake, sql"
Job Opening for Senior Data Solutions Engineer - backfill - Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/job-opening-for-senior-data-solutions-engineer-backfill-remote-at-steneral-consulting-3657140007,2023-12-17,Inverell, Australia,Mid senior,Remote,"Hi,
Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""3025492448"".
Title:- Senior Data Solutions Engineer - backfill
Location:- Miami, FL
Duration:- 6+ Months
Visa:- Citizen, GC
Interview Mode:- Video
Description
remote role
Need last 6 of SSN, Full DOB and LinkedIn
Job Description
The Senior Data Solutions Engineer is responsible for building, managing, and optimizing complex reusable enterprise data pipelines effectively and in a timely manner through the development lifecycle to be used by internal consumers, such as business/data analysts and data scientists. Additionally, this role will operate in a fast pace environment working closely with the Data Science and Business teams on Revenue Management initiatives which concentrates in building data solutions, curated data to enable intelligent pricing automation, which relies on machine-learning and data-derived business rules. The Senior Engineer would use both technical and analytical skills to understand and solve business problems using available resources and current technology stack, while ensuring data governance and data security compliance.
Create and maintain technical design documentation
Gather and document business requirements, data mapping and designing
Create, build, and maintain complex data pipelines from disparate sources that meet functional / non-functional business requirements
Create, maintain, and reuse existing ETL/ELT processes, employing a variety of data integration and data preparation tools
Mentor engineers in finding optimal and efficient solutions for designing, preparing, and storing data for analytical and operational use cases.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing pipelines for greater scalability, etc.
Work with stakeholders including Product, Data and Business teams to assist with data-related technical issues and support their data needs
Create datasets for: (1) operational reports, key performance indicators/metrics, or other insights into current organizational activities, (2) analytics and data science to provide the ability to uncover the answers to major questions that help organizations make objective decisions and/or gain a competitive edge
Write, debug and implement complex queries involving multiple tables or databases across platform(s)
Collaborate with the Enterprise Architecture team to ensure alignment on data standards and processes
Work with data and analytics experts to strive for greater functionality in data systems
Manages and develops data processing and assigns customers to marketing segments in the CDP; using demographic data, behavioral data, and intent signals and works with data scientists to incorporate machine learning outcomes into the overall customer segmentation model
Required Skills
Significant experience in using best practices in designing, building and managing data pipelines that require data transformations as well as metadata and workload management
Significant experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional and new data integration technologies (such as ETL, ELT, data replication, change data captures, message-oriented data movement, API design, stream data integration and data virtualization)
Significant experience with streaming technologies (Kafka, Pubsub, Kinesis) and log-based architectures and experience writing batch and stream processing jobs (i.e. Apache Beam, Google Cloud DataFlow, Apache Spark, Apache Storm)
Significant experience in performing root cause analysis on internal and external data and processes to identify issues and opportunities for improvement
Expert level knowledge with programming languages including SQL, PL/SQL, T-SQL
Expert level knowledge with relational SQL databases such as Oracle and SQL Server
Significant experience with a scripting language & streaming technologies: Python, Java, Scala, Kafka, etc.
Experience with NoSQL databases are a plus
Experience supporting and working with cross-functional teams in a dynamic environment
Required Education
Bachelor of Science in Computer Science, Information Technology or equivalent
Required Years Of Experience
5+ years of experience in a data/cloud engineering role
5+ years of experience working and creating datasets for a data warehouse
5+ years of experience with ETL development tools, Informatica or Azure Data Factory (ADF) preferred
3+ years of cloud experience, Azure preferred
Desired Skills
Clear understanding of data modeling patterns (relational and dimensional)
Proven ability to collaborate with technical peers
Capable of working independently as well as part of a team
Strive to provide orientation and direction to junior engineers requiring their expertise
Experienced with continuous integration and continuous deployment practices
Ability to approach complex problems with creativity and display analytical and problem-solving skills
Display curiosity in understanding the data for the specific area of responsibility
Knowledge and experience working with agile methodologies and tools (such as Jira) a plus
Additional Information
Number Of Allowed Submittals Per Vendor
3
Project assignments, allocation % each project, capital % for capital projects:
100% C23-001 - RM Platform and Automation
1 resource will work on CEL RM Automation
1 resource will work on RCI RM Automation
Gaurav Verma
Talent Acquisition -North America
Direct:+1 3025492448
gaurav.verma@steneral.com
In my absence please reach out to Mr. Harish Sharma at harish@steneral.com &
302-721-6151
Show more
Show less","Data Pipelines, Data Integration, Data Preparation, ETL, ELT, Data Warehousing, Data Modeling, SQL, PL/SQL, TSQL, Oracle, SQL Server, Python, Java, Scala, Kafka, Pubsub, Kinesis, Apache Beam, Google Cloud DataFlow, Apache Spark, Apache Storm, Informatica, Azure Data Factory, Jira, Agile Methodologies","data pipelines, data integration, data preparation, etl, elt, data warehousing, data modeling, sql, plsql, tsql, oracle, sql server, python, java, scala, kafka, pubsub, kinesis, apache beam, google cloud dataflow, apache spark, apache storm, informatica, azure data factory, jira, agile methodologies","agile methodologies, apache beam, apache spark, apache storm, azure data factory, data integration, data preparation, datamodeling, datapipeline, datawarehouse, elt, etl, google cloud dataflow, informatica, java, jira, kafka, kinesis, oracle, plsql, pubsub, python, scala, sql, sql server, tsql"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751644354,2023-12-17,Inverell, Australia,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Data Pipelines, Cloud Technologies, ETL / Data Orchestration, Data Warehousing, Data Lake Solutions, SQL, Database Design, Data Structure, Programming Languages, DevOps Tools, Big Data, Open Source, Data Streaming, Architectural Components, Technical Leadership, Mentoring, Cloud Certification","data engineering, data pipelines, cloud technologies, etl data orchestration, data warehousing, data lake solutions, sql, database design, data structure, programming languages, devops tools, big data, open source, data streaming, architectural components, technical leadership, mentoring, cloud certification","architectural components, big data, cloud certification, cloud technologies, data engineering, data lake solutions, data streaming, data structure, database design, datapipeline, datawarehouse, devops tools, etl data orchestration, mentoring, open source, programming languages, sql, technical leadership"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Richmond, VA",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751641509,2023-12-17,Inverell, Australia,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Cloud Data Engineering, ETL, Data Orchestration, SQL, NoSQL, Data Warehouse, Data Lake, Snowflake, Redshift, Databricks, DevOps, Git, Jenkins, CI/CD, Jira, Python, Java, R, C, C#, C++, Shell, Linux, Unix, Windows","data engineering, cloud data engineering, etl, data orchestration, sql, nosql, data warehouse, data lake, snowflake, redshift, databricks, devops, git, jenkins, cicd, jira, python, java, r, c, c, c, shell, linux, unix, windows","c, cicd, cloud data engineering, data engineering, data lake, data orchestration, databricks, datawarehouse, devops, etl, git, java, jenkins, jira, linux, nosql, python, r, redshift, shell, snowflake, sql, unix, windows"
Senior Data Engineer,Hopscotch Primary Care,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-hopscotch-primary-care-3770481737,2023-12-17,Inverell, Australia,Mid senior,Remote,"About Hopscotch Primary Care
At Hopscotch Primary Care, we believe great healthcare should be accessible to all people across all communities. Today, almost 20% of Americans live in a rural community, yet only 11% of physicians practice in those same communities. We are on a mission to transform healthcare in rural America. We provide high-quality primary care tailored to meet the needs of our patients through our robust care model and comprehensive care team, delivering care in our clinics, and across settings, and wrapping resources around the patients who need them most.
Our patients and the care teams who serve them sit at the center of everything we do at Hopscotch. Hopscotch Primary Care takes a team approach to serve patient needs and provide the best care possible. Our goal is to provide the care each of us would want for ourselves or for our family members, in the right setting, and at the right time.
Today, we are serving thousands of patients in our value-based care model and the number is growing every day. If you want to bring your experience, skill and passion to make a lasting impact in healthcare, we’d like to meet you.
About The Role
We are looking for a mission-driven
Senior Data Engineer
to join our early team at a critical and exciting time for the company. In this position, you will play a pivotal role in building out Hopscotch’s data and analytics platform and data team.  You will have an opportunity to own end-to-end execution of data products and initiatives that directly impact the care delivery process. You will be part of a skilled, passionate, and collaborative team committed to working together to realize Hopscotch’s ambitious and important vision.
Responsibilities
Near-term focus areas for this role will include, but are not limited to:
Build out and maintenance of the data infrastructure that supports end-user workflows, 3rd party integrations, and analytics needs
Support integration of new data (e.g. HIE, ADT feeds) as needed to enhance quality and breadth of information available to clinical care teams
Build out and maintenance of data pipelines that drive actionable intelligence
Requirements scoping and documentation with key stakeholders to continually improve the design of data products across the company
Monitor and maintain data pipeline health from ingest to final data products
Contribute to our organizational ontology (i.e. data modeling)
Collaborate with product teams to design and build out the backing data and infrastructure for organizational workflows
Generate analytic insights, build backend KPIs and other metrics to help provide visibility into company performance towards critical goals
ABOUT YOU:
You would be a great fit for this position if you have 4+ years of experience in data engineering, preferably in healthcare services, and you are:
Knowledgeable about healthcare, preferably in the services/provider sector
Results-driven and focus time/resources against the most important priorities
Creative in finding solutions to arrive at mutually beneficial outcomes
Strategic and can bring a structured, proactive approach to get things done
Can quickly develop insights from data, and translate findings into action
Demonstrate first-principles approach to understanding and solving new problems
Can translate complex concepts, verbally and in writing, and use synthesized communications to collaborate across an organization
From a cultural perspective, you are:
Thoughtful and can work effectively in a fast-paced, ever-changing environment
Constantly seeking ways to simplify and improve how things are done
Accountable, holding yourself and others to a high standard
Willing to roll up your sleeves to support the work required and collaborate effectively with people of all backgrounds
Concise and articulate, drive towards clarity
Collaborative, assuming positive intentions
Mission-driven, with a passion for serving patients and communities
Intellectually curious, with a desire to learn new skills and explore new spaces
ADDITIONAL QUALIFICATIONS:
Experience shipping production level code and pipelines for data purposes
Familiarity with healthcare data sources, interfaces and software including EHR, population health tools, patient portals, and clinical decision support technology
Comfortable writing clear, testable python code, with pyspark experience preferred
Experience with tools to build re-usable, testable, and maintainable data models (e.g. ontologies)
Experience with Airflow, Prefect, or other task execution platforms
Experience with data lake solutions such Palantir Foundry, Databricks
Comfortable in Docker and on the Unix command line
Experience with cloud infrastructure (e.g. AWS) and working with serverless computing services (e.g. Lambda)
At Hopscotch Primary Care, we embrace diversity, invest in a culture of inclusion and positivity and encourage all to apply to join our team. You will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Show more
Show less","Data engineering, Healthcare data, Data infrastructure, Data pipelines, Data analytics, Data modeling, Python, Pyspark, Airflow, Prefect, Palantir Foundry, Databricks, Docker, Unix, AWS, Lambda, EHR, Population health tools, Patient portals, Clinical decision support technology","data engineering, healthcare data, data infrastructure, data pipelines, data analytics, data modeling, python, pyspark, airflow, prefect, palantir foundry, databricks, docker, unix, aws, lambda, ehr, population health tools, patient portals, clinical decision support technology","airflow, aws, clinical decision support technology, data engineering, data infrastructure, dataanalytics, databricks, datamodeling, datapipeline, docker, ehr, healthcare data, lambda, palantir foundry, patient portals, population health tools, prefect, python, spark, unix"
Remote Work - Need Sr Data Software Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/remote-work-need-sr-data-software-engineer-at-steneral-consulting-3735082431,2023-12-17,Inverell, Australia,Mid senior,Remote,"Job
: Sr. Data Software Engineer
Location:
100% Remote
Duration:
6 month Contract to Hire (USC/ GC Holder ONLY)
Top Skills
SQL
Azure
Data Factory
DBT
ETL work
Proven ability to complete projects in a timely manner while clearly measuring progress
Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming)
Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.
Experience writing and maintaining frontend client applications, Angular preferred
Strong experience with revision control (Git)
Experience with cloud-based systems (Azure / AWS / GCP).
High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns
Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)
Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools
Strong communication skills
Strong experience with use of RESTful APIs
High level understanding of HL7 V2.x / FHIR based interface messages.
High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day
Communicate with business leaders to help translate requirements into functional specification
Develop broad understanding of business logic and functionality of current systems
Analyze and manipulate data by writing and running SQL queries
Analyze logs to identify and prevent potential issues from occurring
Deliver clean and functional code in accordance with business requirements
Consume data from any source, such a flat files, streaming systems, or RESTful APIs
Interface with Electronic Health Records
Engineer scalable, reliable, and performant systems to manage data
Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization
Build quality systems while expanding offerings to dependent teams
Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems.
Show more
Show less","SQL, Azure, Data Factory, DBT, ETL, Angular, Git, Data structures, Algorithms, Async programming patterns, Objectoriented design, Parallel programming, .NET, Java, Queuing technologies, Kafka, SNS, RabbitMQ, Metrics, Logging, Monitoring, Alerting, RESTful APIs, HL7 V2.x, FHIR, System deployment, CI/CD Pipeline, K8s, Terraform","sql, azure, data factory, dbt, etl, angular, git, data structures, algorithms, async programming patterns, objectoriented design, parallel programming, net, java, queuing technologies, kafka, sns, rabbitmq, metrics, logging, monitoring, alerting, restful apis, hl7 v2x, fhir, system deployment, cicd pipeline, k8s, terraform","alerting, algorithms, angular, async programming patterns, azure, cicd pipeline, data factory, data structures, dbt, etl, fhir, git, hl7 v2x, java, k8s, kafka, logging, metrics, monitoring, net, objectoriented design, parallel programming, queuing technologies, rabbitmq, restful apis, sns, sql, system deployment, terraform"
"Software Engineer, Data Warehouse Analytics",Stripe,United States,https://www.linkedin.com/jobs/view/software-engineer-data-warehouse-analytics-at-stripe-3785927124,2023-12-17,Inverell, Australia,Mid senior,Remote,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
The team is responsible for offering data warehouse analytics capabilities for Stripe and the stack is supported by a collection of internally developed services and several popular open source technologies like Trino/Presto, Hive Metastore, etc. The systems we own support all of the data warehouse workloads initiated by both services and by individual Stripes across the company to support various business analytics use cases. We process hundreds of thousands of queries on petabytes of data per day and form the key data serving layer for one of the largest financial data lakes in the world. The team is distributed across the USA.
What you’ll do
You shall be a key contributor to the evolution of our platform and how Stripe analyzes its vast and ever growing data warehouse, to power ever evolving use-cases around analytics, reporting, fraud, AI/ML training, etc at scale. As a Software Engineer, you’ll be empowered to make decisions with a significant impact on Stripe while making our systems reliable, secure, and a delight to use.
Responsibilities
Scope and lead large technical projects with enormous impact for Engineers within Stripe
Build and maintain the infrastructure which powers the core of Stripe.
Directly contribute to core interface design and write code.
Work closely with the open source community to identify opportunities for adopting new open source features as well contribute back to the OSS.
Plan for the growth of Stripe’s infrastructure. Unblock, support and communicate with internal partners to achieve results.
Ensure operational excellence and enable a highly available, reliable and secure Data Warehouse Analytics platform.
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
BS or MS in Computer Science or equivalent field and interest in Data.
2-7 years of professional experience writing high quality production level code or software programs.
Have experience with distributed SQL query engines like Trino.
Experience developing, maintaining and debugging distributed systems built with open source tools.
Experience building infrastructure as a product centered around user needs.
Experience optimizing the end to end performance of distributed systems.
Experience with scaling distributed systems in a rapidly moving environment.
Preferred Qualifications
Experience coding in Java, Scala, Golang.
Experience working with distributed SQL query engine space.
Familiarity designing APIs or building developer platforms.
Familiarity with cloud-based environments such as AWS.
Pay and benefits
The annual US base salary range for this role is $172,800 - $233,800. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","Data Warehouse, Data Analytics, Trino/Presto, Hive Metastore, SQL, Java, Scala, Go, AWS, Cloud Computing, Distributed Systems, Open Source, APIs, Developer Platforms","data warehouse, data analytics, trinopresto, hive metastore, sql, java, scala, go, aws, cloud computing, distributed systems, open source, apis, developer platforms","apis, aws, cloud computing, dataanalytics, datawarehouse, developer platforms, distributed systems, go, hive metastore, java, open source, scala, sql, trinopresto"
"(Level III Database Engineer- ETL- SSIS, ADF, C#)",Steneral Consulting,United States,https://www.linkedin.com/jobs/view/level-iii-database-engineer-etl-ssis-adf-c%23-at-steneral-consulting-3742301276,2023-12-17,Inverell, Australia,Mid senior,Remote,"Job
: Level III Database Engineer
Location:
Remote
Term:
6+ months Contract to Hire (USC/ GC Holer ONLY)
Responsibilities
Build and maintain ETL processes in SSIS and Azure Data Factory
Engineer scalable, reliable and performant systems to manage data
Develop, implement and optimize stored procedures and functions
Independently research and analyze data issues and provide automated solutions
Implement new technologies to enhance the optimization of current practices
Provide valuable suggestions regarding new ideas and technologies
Top Skills
7+ years of experience as a Data Engineer
5+ years of experience with ETL tools, both SSIS and Azure Data Factory
3+ years of experience with C# (preferably .NET Core)
1+ years of experience with PowerShell or other demonstrated scripting experience
Excellent problem-solving skills.
Thorough understanding of T-SQL scripting including but not limited to stored procedures, functions and triggers
Proven ability to analyze and provide complex solutions based on business requirements
Sense of ownership and pride in your performance and its impact on company success
Bachelor’s degree in Computer Science, Management Information Systems, Healthcare-related discipline or related field or equivalent work experience
Preferred Skills
1+ years of experience with Snowflake
1+ years of experience with Databricks
Master’s degree in above mentioned fields
Show more
Show less","SSIS, Azure Data Factory, SQL, TSQL, C#, PowerShell, Snowflake, Databricks","ssis, azure data factory, sql, tsql, c, powershell, snowflake, databricks","azure data factory, c, databricks, powershell, snowflake, sql, ssis, tsql"
Staff Data Engineer,AuditBoard,United States,https://www.linkedin.com/jobs/view/staff-data-engineer-at-auditboard-3732150210,2023-12-17,Inverell, Australia,Mid senior,Remote,"Who We Are
Having surpassed $150M ARR and continuing to grow rapidly, AuditBoard is the leading audit, risk, and compliance platform on the market. More than 40% of the Fortune 500, including 6 of the Fortune 10, leverage our award-winning technology to move their businesses forward with greater clarity and agility. And our customers love us: AuditBoard is top-rated on G2.com and Gartner Peer Insights.
At AuditBoard we inspire each other to innovate and are proud of what we are producing. We spend each day thinking of new ways to help our customers and contribute to the greater good of our company and our surrounding communities. We are all about assisting each other and breaking through barriers to create the most loved audit, risk, and compliance platform by our customers. This is how we have become one of the 500 fastest-growing tech companies in North America for the fourth year in a row as ranked by Deloitte!
Why This Role is Exciting
As a Senior Engineer on the Data Platform team, you will play a crucial role in building and operating the data infrastructure that serves AuditBoard. Your responsibilities will include delivering high-quality results to ensure the data infrastructure's security, compliance, optimization, and automation. You will contribute to our architecture while maintaining a high architectural standard.
Key Responsibilities
Best practices and guardrails: Champion the best practices for using the data platform. Optimize for cost and performance
Data architecture: Evaluate data platform architecture. Build and optimize data services to improve compliance, security, reliability, performance, and resource utilization.
Infrastructure automation: Increase reliability and employee productivity. Eliminate human errors through the standardization of processes and Infrastructure as Code (Iac)
Data infrastructure monitoring: Monitor the platform's health, reliability, cost, and usage.
Operational support: Support the platform users for troubleshooting, configuration, version updates, and feature adoptions.
Compliance: Build processes and tools to ensure that the data platform is compliant
Security: Identify infrastructure security-relevant actions, tests, and key performance indicators. Enhances the security posture of the data platform, ensuring the protection of data, systems, and assets.
Attributes for a Successful Candidate
8+ years of development experience with a focus on architecting cloud data platforms, setting up, managing, and automating the infrastructure of cloud data platforms, including Airflow, Databricks, Spark, and Kafka
8+ years of experience and proficient programming skills in Python or Java/C#. 3 years of programming experience in SQL.
7+ years of developer experience using Infrastructure as Code (IaC) technologies, such as AWS Cloud Formation and Terraform
5+ years of experience maintaining and improving OE of data platforms - service alerting, monitoring, and automation.
3+ years of experience with data compliance and governance. Demonstration of collaboration with the internal compliance team and external auditors in planning and execution of all phases of compliance including risk assessment & scoping. Standardize and automate the compliance process.
B.S., M.S., or Ph.D. in Computer Science or equivalent
Nice to Have
Subject matter expert (SME) in architecting cloud data platforms using Databricks, Snowflake, AWS Lake Formation, or other relevant technologies.
SME in big data technologies including Spark, Presto, Airflow, Kafka
Work in a startup-like environment building agile products and services
Create clear and concise documentation, including diagrams, service descriptions, decisions, and runbooks.
Deep knowledge of cloud platforms such as AWS, GCP, Azure
Perks*
Launch a career at one of the fastest-growing SaaS companies in North America!
Live your best life (LYBL)! $200/mo for anything that enhances your life
Remote and hybrid work options, plus lunch in the office
Comprehensive employee health coverage (all locations)
401K with match (US) or pension with match (UK)
Competitive compensation & bonus program
Flexible Vacation (US exempt & CA) or 25 days (UK)
Time off for your birthday & volunteering
Unlimited access to LinkedIn Learning
Employee resource groups
Stock options
Opportunities for team and company-wide get togethers!
perks may vary based on eligibility
Show more
Show less","Apache Airflow, Apache Kafka, Apache Spark, AWS, AWS Cloud Formation, AWS Lake Formation, Azuer, Big Data, Cloud Platforms, Cloud Services, Compliance, Databricks, Data Governance, GCP, Infrastructure as Code, Java, Kafka, MySQL, Python, Security, Snowflake, SQL, Terraform","apache airflow, apache kafka, apache spark, aws, aws cloud formation, aws lake formation, azuer, big data, cloud platforms, cloud services, compliance, databricks, data governance, gcp, infrastructure as code, java, kafka, mysql, python, security, snowflake, sql, terraform","apache airflow, apache kafka, apache spark, aws, aws cloud formation, aws lake formation, azuer, big data, cloud platforms, cloud services, compliance, data governance, databricks, gcp, infrastructure as code, java, kafka, mysql, python, security, snowflake, sql, terraform"
Sr. Data Engineer - Remote,Get It Recruit - Information Technology,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-data-engineer-remote-at-get-it-recruit-information-technology-3777842745,2023-12-17,Inverell, Australia,Mid senior,Remote,"Join a recognized leader in promoting health and well-being! We are currently seeking a skilled and experienced professional to fill a remote position based in the beautiful state of Arizona. The remote work opportunity requires residency within the State of Arizona.
Role Overview
As a Data Engineer, you will play a crucial role in designing and implementing business intelligence and extract, transform, and load (ETL) solutions. The ideal candidate will bring a wealth of experience in advanced SQL techniques, relational database performance, Big Data technologies, and scripting languages like Python.
Key Responsibilities
Mentor team members in ETL processes and standards, participating in code reviews.
Develop and maintain ETL processes, ensuring data accuracy and quality.
Design and implement automated tests, including unit tests and integration tests.
Collaborate with other technology teams to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Support database clustering, mirroring, and replication among other SQL Server technologies.
Participate in the Software Development Life Cycle and adhere to best practices.
Analyze, map data, and contribute to the team's knowledge base.
Qualifications
Required:
4+ years of experience in computer programming, query design, and databases.
High-School Diploma or GED in a general field of study.
Preferred
Bachelor's Degree in Information Technology or related field.
4+ years of experience building and managing complex Data Integration solutions.
MS SQL Certification or other certification in current programming languages.
Essential Competencies
Proficiency in spreadsheet, database, and word processing software.
Strong knowledge of business intelligence, programming, and data analysis software.
Excellent troubleshooting skills and working knowledge of PowerShell.
Effective interpersonal skills and ability to prioritize tasks.
Our Commitment
We are an equal opportunity employer, fostering a diverse and inclusive workplace. We do not discriminate in hiring or employment based on race, ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or any other protected group.
Thank you for considering this exciting opportunity!
Employment Type: Full-Time
Show more
Show less","SQL, Python, ETL, Data Integration, Big Data, Software Development Life Cycle, Data Analysis, Relational Database, PowerShell, MS SQL, Spreadsheets, Databases, Word Processing","sql, python, etl, data integration, big data, software development life cycle, data analysis, relational database, powershell, ms sql, spreadsheets, databases, word processing","big data, data integration, dataanalytics, databases, etl, ms sql, powershell, python, relational database, software development life cycle, spreadsheets, sql, word processing"
Senior Data Engineer / Remote,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-motion-recruitment-3754633912,2023-12-17,Inverell, Australia,Mid senior,Remote,"Our client is changing the way we can find our friends and family from around the world. Founded in 2008, they have millions of users per month visiting their platform for various reasons.
They are looking for a Senior Data Engineer to join their Data Ops team to come in and make an impact right away. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, Airflow, AWS, and have a computer Science Degree. If this is you APPLY NOW!
Basic Qualifications (Required Skills & Experience)
6-8 years of experience
Python – 5 years of experience
Spark – 5 years (Prefer Pyspark)
AWS – 2+ years of experience
Airflow – 2+ years of experience
EMR
ETL work
SQL
Technology degree is important as well! (ex: CS degree)
Other Qualifications & Desired Competencies
Fully Remote!!!
Equity and Bonuses involved
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k)
Posted By:
Casey Ryan
Show more
Show less","Python, PySpark, Airflow, AWS, EMR, ETL, SQL, Computer Science Degree","python, pyspark, airflow, aws, emr, etl, sql, computer science degree","airflow, aws, computer science degree, emr, etl, python, spark, sql"
REMOTE: Data Engineer //Pay rate: $43.35/hr,Stellar Professionals,United States,https://www.linkedin.com/jobs/view/remote-data-engineer-pay-rate-%2443-35-hr-at-stellar-professionals-3704005282,2023-12-17,Inverell, Australia,Mid senior,Remote,"Applicant must have 2 years of relevant experience with the following:
Hands-on experience with data engineering design and implementation
Experience with data modeling design and implementation
Hands-on industry experience programming in SQL on relational database platforms (T-SQL and PL/SQL preferred)
Hands-on industry experience working with enterprise ETL/ELT tools (Azure Data Factory and Databricks preferred)
Hands-on experience with modern programing languages like Python, C#, JavaScript, etc (Python preferred)
Hands-on experience with Azure, AWS, and/or GCP cloud platforms (Azure preferred)
Bachelor's degree in Computer Science, Data Science, Software Engineering, Information Technology or a similar field
Progressive mindset particularly around deployment models and emerging technologies
Collaborative team player who is detailed oriented, focused on solution quality and execution
Experience with Docker for containerization and Kubernetes for orchestration
Show more
Show less","Data engineering, Data modeling, SQL, TSQL, PL/SQL, ETL/ELT, Azure Data Factory, Databricks, Python, C#, JavaScript, Azure, AWS, GCP, Docker, Kubernetes","data engineering, data modeling, sql, tsql, plsql, etlelt, azure data factory, databricks, python, c, javascript, azure, aws, gcp, docker, kubernetes","aws, azure, azure data factory, c, data engineering, databricks, datamodeling, docker, etlelt, gcp, javascript, kubernetes, plsql, python, sql, tsql"
Senior Data Warehouse Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/senior-data-warehouse-engineer-at-steneral-consulting-3718978112,2023-12-17,Inverell, Australia,Mid senior,Remote,"Share only 2 profiles
Indiana local candidates only with local DL
LinkedIn is must
Senior Data Warehouse Engineer
Indianapolis, IN.
Location of candidates: Indiana residents ONLY, no relocation.
12+ Month Contact
Remote work schedule
Responsibilities
Collaborate with various IT teams (data, database, and infrastructure) to analyze the agency's utilization of the data warehouse.
Provide guidance and education on warehouse environments using industry standards and best practices.
Research leading-edge capabilities within the Indiana Office of Technology's scope and guidelines to determine the optimal solution for the overall environment.
Define the technological architectural direction and deliver assessment recommendations.
Develop a project/implementation plan for establishing the warehouse, including designing ideal data pipelines and other data and visual interfaces.
Oversee the setup of the agency's warehouse environment in alignment with the agency's initiative.
Configure and set up Azure Synapse with the correct structure.
Collaborate with Data Analyst(s) as necessary to define minimal viable data assets in support of visual needs.
Review and provide architectural guidance for all analytic solutions.
Advise management on any deficiencies in current tools and approaches related to business initiatives.
Required Skills
Extensive knowledge of information systems design principles and new systems design techniques.
A very strong understanding of current industry standards and best practices used in data and data warehouse development.
Extensive knowledge of system and business data applications.
Sizable experience in setting up other data warehouse environments.
Ability to evaluate the business needs and objectives.
Extensive knowledge of policies, standards, procedures, and techniques used for data and application development.
Ability to perform problem-solving and analytical analysis on complex issues.
High technical competency level in all phases of data analysis and development activities.
Willingness to train/educate data team members, DBAs, infrastructure support, and management in effective data warehouse designs/approaches.
Proficient in developing project plans, including discovery, development, and implementation.
Experience in using ETL tools - Azure Synapse and Azure Data Factory.
Ability to manage multiple data development assignments and priorities.
Ability to communicate effectively, both orally and in writing.
Working knowledge in the above skills must be within the last 6 months.
Show more
Show less","Data Warehousing, Data Analysis, Azure Synapse, Azure Data Factory, ETL Tools, Data Pipelines, Data Visualization, Business Intelligence, Project Management, Problem Solving, Analytical Analysis, Training and Education, Communication Skills, Data Development, Data Application Development, Data and Application Development Policies, Data and Application Development Standards, Data and Application Development Procedures, Data and Application Development Techniques, Information Systems Design Principles, New Systems Design Techniques, Industry Standards and Best Practices, System and Business Data Applications","data warehousing, data analysis, azure synapse, azure data factory, etl tools, data pipelines, data visualization, business intelligence, project management, problem solving, analytical analysis, training and education, communication skills, data development, data application development, data and application development policies, data and application development standards, data and application development procedures, data and application development techniques, information systems design principles, new systems design techniques, industry standards and best practices, system and business data applications","analytical analysis, azure data factory, azure synapse, business intelligence, communication skills, data and application development policies, data and application development procedures, data and application development standards, data and application development techniques, data application development, data development, dataanalytics, datapipeline, datawarehouse, etl tools, industry standards and best practices, information systems design principles, new systems design techniques, problem solving, project management, system and business data applications, training and education, visualization"
Senior Data Engineer - Remote,Get It Recruit - Information Technology,"San Mateo, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-get-it-recruit-information-technology-3774809543,2023-12-17,Inverell, Australia,Mid senior,Remote,"We are actively searching for a talented Senior Data Engineering Developer to play a pivotal role in shaping the design, development, scaling, and maintenance of our cutting-edge SaaS infrastructure. Join our dynamic team where you'll collaborate closely with cross-functional experts in Data Science, Engineering, and Product/Business Technology. Your mission: construct robust data infrastructure, streamline processes, and enhance our toolset.
Responsibilities
Visionary Leadership:
Spearhead the vision for Business Intelligence (BI) and Data Warehousing, steering the strategic plan to fruition.
Assemble a high-caliber BI and Data Warehouse team, fostering their growth and skill development.
Collaboration
Cultivate collaborative relationships with Product Managers, Analysts, and Software Engineers to decipher data requirements and deliver impactful solutions.
Infrastructure Excellence
Architect, construct, oversee, and optimize foundational data infrastructure.
Implement a monitoring infrastructure for real-time insights into the status of our data pipelines.
Implement and supervise processes that enhance implemented solution performance.
Optimize schemas, including partitions, compression, and distribution, to balance costs and performance.
Craft bespoke data infrastructure solutions not readily available off-the-shelf.
Create and sustain custom data ingestion pipelines and seamless integrations with third-party platforms.
Data Quality And Dashboards
Champion Data Quality and the creation of high-impact dashboards.
Management
Define and manage Service Level Agreements (SLAs) for all production datasets and processes.
Provide guidance and support to our data team, assisting with design decisions and performance optimization strategies.
Qualifications
Education:
Bachelor's degree in a technical and/or quantitative field of study—e.g., computer science, mathematics, physics, statistics, or equivalent and/or substantial related experience.
Experience
Remarkable track record of 8+ years in the realm of distributed data technologies.
Demonstrable experience in ETL and ELT in cloud SaaS/PaaS infrastructures.
Proficiency in serverless Microservices like GCP Cloud Function, AWS Lambda.
Hands-on experience on streaming and batch data pipeline.
Expertise in databases such as Bigquery, MS SQL on data/domain architecture.
Expertise in SQL language to be able to transform raw source data into SQL columns.
Experience with GCP solutions such as DataFlow and Pubsub are a huge plus.
Understanding and experience in AI/ML platform and pipeline, such as Vertex AI.
Employee Benefits
Competitive compensation packages including bonus and options.
Medical, dental, and vision benefits.
Matching 401(K).
Paid time off.
Telecommuting and remote-work options.
Support for continuing education.
Team off-sites, social events, annual company events, and frequent extracurricular activities.
Unlimited snacks and drinks.
Remote work.
Employment Type: Full-Time
Show more
Show less","Data Engineering, Business Intelligence, Data Warehousing, Data Infrastructure, Data Pipelines, Data Quality, Dashboards, Data Integration, Cloud SaaS/PaaS, Microservices, GCP Cloud Function, AWS Lambda, Streaming Data, Batch Data, Bigquery, MS SQL, Data Architecture, SQL, DataFlow, Pubsub, Vertex AI, AI/ML","data engineering, business intelligence, data warehousing, data infrastructure, data pipelines, data quality, dashboards, data integration, cloud saaspaas, microservices, gcp cloud function, aws lambda, streaming data, batch data, bigquery, ms sql, data architecture, sql, dataflow, pubsub, vertex ai, aiml","aiml, aws lambda, batch data, bigquery, business intelligence, cloud saaspaas, dashboard, data architecture, data engineering, data infrastructure, data integration, data quality, dataflow, datapipeline, datawarehouse, gcp cloud function, microservices, ms sql, pubsub, sql, streaming data, vertex ai"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-%24180k-%24220k-snowflake-coding-at-cybercoders-3766365237,2023-12-17,Inverell, Australia,Mid senior,Remote,"Permanently Remote in US
Job Title:
Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)
Salary:
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
Requirements:
Expert w/ Snowflake & Coding Ability
Based in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.
We are founded and owned by T.V.s largest publishers.
Our mission is to be bring simplicity & scale to audience based campaigns in television.
We're working with over 100 advertisers and anticipating another year of significant growth!
As a rapidly growing company
(founded in 2017 & up 140% year over year)
we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.
We have been in business for 7 years and have around 40 employees.
Due to growth, we are actively hiring a Senior Data Engineer with
Snowflake experience (ideally certified)
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!
Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.
This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.
If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.
Top Reasons to Work with Us
Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits
Raid Growth: Founded in 2017 & up 140% year over year
Culture: Fast paced, mission driven culture
Technology: Cutting edge technology
What You Will Be Doing
Building data pipelines from scratch
Data architecture via Snowflake
Data modeling
Technical review of everything this group builds.
Mange development velocity, team capacity, and backlogs
Partner closely with the product team
Take on key assignments and delegate as needed
Act as the main technical point of contact for engineering
Translate technical requirements to the rest of the engineering team
What You Need for this Position
Must Have Experience
Snowflake experience
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Some Experience With:
-
Fivetran and/or DBT
What's In It for You
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
401k
Vacation/PTO
Medical
Dental
Vision
Bonus
401k
Benefits
Vacation/PTO
Medical
Dental
Vision
Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!
Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-Pauly
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Nitu.Gulati-Pauly@cybercoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L569 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Snowflake, JavaScript, Python, Data pipelines, APIs, Fivetran, DBT, Data architecture, Data modeling, Technical review, Mange development velocity, Team capacity, Backlogs, Partner closely, Product team, Key assignments, Delegate assignments, Main technical point of contact, Engineering, Translate technical requirements","snowflake, javascript, python, data pipelines, apis, fivetran, dbt, data architecture, data modeling, technical review, mange development velocity, team capacity, backlogs, partner closely, product team, key assignments, delegate assignments, main technical point of contact, engineering, translate technical requirements","apis, backlogs, data architecture, datamodeling, datapipeline, dbt, delegate assignments, engineering, fivetran, javascript, key assignments, main technical point of contact, mange development velocity, partner closely, product team, python, snowflake, team capacity, technical review, translate technical requirements"
Sr. Data Engineer (REMOTE),Syrinx Consulting,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-engineer-remote-at-syrinx-consulting-3648834182,2023-12-17,Inverell, Australia,Mid senior,Remote,"Who We Are
Working with the leader in digital performance solutions, improving the impression quality and audience impact of digital advertising. Built on best practices, our solutions create value for media buyers and sellers by bringing transparency and accountability to the market, ensuring ad viewability, brand safety, fraud protection, accurate impression delivery and audience quality across campaigns to drive performance.
This is a great opportunity to get in a remote role!! Please apply directly to ldavis@syrinx.com
Headquartered in New York City, DoubleVerify’s investors include JMI Equity, Institutional Venture Partners, Blumberg Capital, First Round Capital and Genacast Ventures. Learn more at doubleverify.com.
Overview
DoubleVerify is seeking a data slayer, a one in a thousand DBA that loves to ask questions and be on the cutting edge of Big Data technology. The team is responsible for all technical data aspects of the company’s products. The company’s system is comprised of an extremely scalable, highly-available and rapidly-developing architecture, and includes big-data stores of all types – relational, massively-parallel-processing, and NoSQL. This individual will join a small yet highly capable and motivated team in order to maintain the system’s databases, and develop, integrate and deploy complex new modules – quickly and at scale.
What You Will Do
Develop and maintain Big Data system (Hadoop, Kafka, Hive, Spark)
Maintain the system’s databases (Vertica, SQL Server, MongoDB)
Develop, integrate and deploy complex new modules (TSQL, Python, Splunk)
Design and support Database infrastructure, with consideration for performance, availability, and specific application requirements.
Work closely with other departments on the implementation and deployment of new initiatives
Who You Are
5+ years of database development and/or administration.
5+ Experience with Hadoop Big Data eco system and experience with Vertica is a must
Software development experience is preferred
Fast learner, creative thinker, problem solver. Must love BIG data!
Accountable, dedicated and willing to be on-call as needed
AdTech experience is a plus!
Show more
Show less","Hadoop, Kafka, Hive, Spark, Vertica, SQL Server, MongoDB, TSQL, Python, Splunk, Big Data, Relational Database, MassivelyParallelProcessing, NoSQL, Database Infrastructure, Performance, Availability, Application Requirements","hadoop, kafka, hive, spark, vertica, sql server, mongodb, tsql, python, splunk, big data, relational database, massivelyparallelprocessing, nosql, database infrastructure, performance, availability, application requirements","application requirements, availability, big data, database infrastructure, hadoop, hive, kafka, massivelyparallelprocessing, mongodb, nosql, performance, python, relational database, spark, splunk, sql server, tsql, vertica"
AWS Data Engineer,Extend Information Systems Inc.,United States,https://www.linkedin.com/jobs/view/aws-data-engineer-at-extend-information-systems-inc-3652274467,2023-12-17,Inverell, Australia,Mid senior,Remote,"Hi
Partner
,
I hope you are doing well!
We have an opportunity
Information Security/ Cyber Security Application Project Manager
with one of our clients for
Mc Lean, VA.
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Job Title:
Information Security/ Cyber Security Application Project Manager
Location:
Mc Lean, VA
Duration: C2C
Experience level :
10
hands on experience on financials / budget analysis, tracking actuals, working with leads to address the gaps
Job Description
Identify business unit requirements, create project and process specifications, facilitate testing, coordinate with project teams and ensure that projects are delivered on-time and within budget
Identify process improvements and implement
Take ownership of end-to-end project delivery
Responsible for acting as the primary interface between a specific business/functional area and their IT partners.
Represent the organization as the principal customer contact for projects, light enhancements, and other change initiatives
Interact with senior customer personnel on significant technical matters frequently requiring coordination across organizational lines
Identify and resolve operational issues with business areas.
Develops detailed work plans, schedules, project estimates, resource plans, and status reports.
Ability to work independently and effectively with staff at all levels and from multiple divisions.
Ability to manage multiple priorities and deadlines.
Proven ability to learn new business concepts supporting product and/or business process development.
Strong organization and presentation skills.
Required Skills And Knowledge
At least 9+ years of experience in all aspects of the development and implementation of assigned projects and provides a single point of contact for those projects.
At least 9+ years of experience in taking projects from original concept through final implementation. Interfaces with all areas affected by the project including end users, computer services, and client services.
At least 6+ years of experience in defining project scope and objectives.
PMP is a plus
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell:
- 571 - 386 - 2431
Email:
Anoop@extendinfosys.com
Address:
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Web:
www.extendinfosys.com
Show more
Show less","Information Security, Cyber Security, Project Management, Financials, Budget Analysis, Project Specifications, Testing, Project Delivery, Interface, Customer Contact, Technical Matters, Coordination, Operational Issues, Work Plans, Schedules, Project Estimates, Resource Plans, Status Reports, Communication, Organization, Presentation, PMP","information security, cyber security, project management, financials, budget analysis, project specifications, testing, project delivery, interface, customer contact, technical matters, coordination, operational issues, work plans, schedules, project estimates, resource plans, status reports, communication, organization, presentation, pmp","budget analysis, communication, coordination, customer contact, cyber security, financials, information security, interface, operational issues, organization, pmp, presentation, project delivery, project estimates, project management, project specifications, resource plans, schedules, status reports, technical matters, testing, work plans"
GCP Data Engineer,United Consulting Hub,United States,https://www.linkedin.com/jobs/view/gcp-data-engineer-at-united-consulting-hub-3689044361,2023-12-17,Inverell, Australia,Mid senior,Remote,"Role: GCP Data Engineer
Location: Pleasanton, USA (Hybrid) or Remote
Duraction : long term
Required Past Experience
Exp
: 4-6Years
1+ years of overall experience in architecting, developing, testing & implementing Data Platform projects using GCP Components (e.g. BigQuery, Dataflow, Dataproc,DLP,BigTable,Pub/Sub,Compos etc..).
Good Understanding of Data Structures.
Worked with large datasets and solving difficult analytical problems.
Experience working with GIT for Source Code Management
Worked with Structured and Unstructured data
E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
Worked with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform
Automating manual processes to speed up delivery.
Good Understanding of Data Pipeline (Batch and Streaming) and Data Governance
Experience in code deployment from lower environment to production.
Good communication skills to understand business requirements.
Work closely with offshore team in India
Required Skills And Abilities
Mandatory Skills
- BigQuery ,Composer, Python, GCP Fundamentals.
Secondary Skills
Snowflake, DLP, Pub/Sub, Dataflow,Shell Scripting,SQL, Security(Platform & Data) concepts.
Knowledge of ETL Migration from On-Premises to GCP Cloud
SQL Performance Tuning
Batch/Streaming Data Processing
Fundamentals of Kafka,Pub/Sub to handle real-time data feeds.
Good To Have - Certifications in any of the following: GCP Professional Cloud Architect, GCP Professional Data Engineer
Ability to communicate with customers, developers, and other stakeholders.
Mentor and guide team members
Good Presentation skills
Strong Team Player
Show more
Show less","GCP Fundamentals, BigQuery, Composer, Python, Snowflake, DLP, Pub/Sub, Dataflow, Shell Scripting, SQL, Security concepts, ETL Migration, SQL Performance Tuning, Batch/Streaming Data Processing, Kafka, Realtime data feeds","gcp fundamentals, bigquery, composer, python, snowflake, dlp, pubsub, dataflow, shell scripting, sql, security concepts, etl migration, sql performance tuning, batchstreaming data processing, kafka, realtime data feeds","batchstreaming data processing, bigquery, composer, dataflow, dlp, etl migration, gcp fundamentals, kafka, pubsub, python, realtime data feeds, security concepts, shell scripting, snowflake, sql, sql performance tuning"
Big Data Developer (Azure),Experfy,United States,https://www.linkedin.com/jobs/view/big-data-developer-azure-at-experfy-3719412375,2023-12-17,Inverell, Australia,Mid senior,Remote,"Big Data and Cloud Architecture
Design big data batch processing and interactive solutions
Design big data real-time processing solutions
Expertise in Azure technology stack
Operationalize end-to-end cloud analytics solutions
#INDEXP
Requirements
Technical Qualifications:
4+ years of hands-on experience with Cloud using Microsoft Azure
4+ years of relevant technology architecture consulting or industry experience to include experience in Information delivery, Analytics and Business Intelligence based on data from SQL and different features of SQL; SQL DW, SQL DB, PolyBase, U-SQL, etc
At least 3+ years of hands-on working experience with Microsoft Azure technologies; Azure ADF, Event Hub, Nifi, HDInsight, Blob Storage, Redis Cache, etc
2+ years experience with Snowflake cloud DW
2+ years of hands-on experience with data lake implementations, core modernization, and data ingestion
Bachelor's Degree or equivalent professional experience
Hadoop Certification or Spark Certification
Experience with Cloud using Microsoft Azure
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Preferred Qualifications:
Experience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like Microsoft Azure
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data, and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Show more
Show less","Big Data Architecture, Cloud Architecture, Batch Processing, Interactive Solutions, Realtime Processing, Azure, Cloud Analytics, SQL, SQL DW, SQL DB, PolyBase, USQL, Azure ADF, Event Hub, Nifi, HDInsight, Blob Storage, Redis Cache, Snowflake, Data Lake Implementations, Core Modernization, Data Ingestion, Hadoop Certification, Spark Certification, Reporting, Visualization, Unstructured Data, Structured Data, Scalable Systems, Distributed Systems, Cloud Computing, Data Cleansing, Data Quality, Standardization, Transformation, Rationalization, Linking, Matching, Data Standards, Master Data, Metadata, MultiTerabyte Data","big data architecture, cloud architecture, batch processing, interactive solutions, realtime processing, azure, cloud analytics, sql, sql dw, sql db, polybase, usql, azure adf, event hub, nifi, hdinsight, blob storage, redis cache, snowflake, data lake implementations, core modernization, data ingestion, hadoop certification, spark certification, reporting, visualization, unstructured data, structured data, scalable systems, distributed systems, cloud computing, data cleansing, data quality, standardization, transformation, rationalization, linking, matching, data standards, master data, metadata, multiterabyte data","azure, azure adf, batch processing, big data architecture, blob storage, cloud analytics, cloud architecture, cloud computing, core modernization, data ingestion, data lake implementations, data quality, data standards, datacleaning, distributed systems, event hub, hadoop certification, hdinsight, interactive solutions, linking, master data, matching, metadata, multiterabyte data, nifi, polybase, rationalization, realtime processing, redis cache, reporting, scalable systems, snowflake, spark certification, sql, sql db, sql dw, standardization, structured data, transformation, unstructured data, usql, visualization"
Sr. Data Analyst,"Data Ideology, LLC","Pittsburgh, PA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-data-ideology-llc-3776640712,2023-12-17,Inverell, Australia,Mid senior,Remote,"Data Ideology
At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious. For more information about Data Ideology, visit www.dataideology.com
Data Analyst - Full-time
We are looking for Data Analytics Consultants to join our growing team. A Data Analytics Consultant will leverage their business and technical knowledge to organize and manage deliverables in a data warehouse consolidation and business intelligence implementation project, while also working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.
Key Responsibilities
Data Analysis and Interpretation:
Conduct in-depth analysis using the data warehouse to identify trends, patterns, and insights.
Interpret complex data sets and communicate findings to both technical and non-technical stakeholders.
Data Extraction and Transformation:
Assist with ETL processes to extract, transform, and load data from various sources.
Collaborate with IT teams to create seamless data integrations.
Quality Assurance and Testing
Perform thorough data validation and testing to ensure the highest possible data quality.
Clearly define and communicate when quality standards are not being met.
Problem Solving
Perform root cause analysis to identify the underlying or fundamental causes of an issue or problem.
Demonstrate ability to offer suggestions and solutions to remedy issues and promote sustainability.
Collaboration and Communication:
Work closely with cross-functional teams, including IT and business stakeholders, to align on goals and key deliverables.
Effectively prioritize tasks to meet established team goals.
Documentation:
Accurately document data mappings and definitions.
Outline standard processes and procedures of core activities
Qualifications
Education and Experience:
Bachelor’s degree in healthcare or computer science related field preferred, but not necessary.
5+ years’ experience with any programming language (SQL, PLSQL, R, Python, etc)
5+ years’ experience in data analytics on any data warehouse (Oracle, SQL, Snowflake)
Healthcare experience a plus
Consulting experience a plus
Agile/SAFe experience a plus
Work Environment:
Remote work from home.
Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands:
Must be able to remain in a stationary position 50% of the time.
The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits:
Unlimited Discretionary Time Off Policy
Insurance (medical, dental, vision) for employees
100% company paid - short and long-term disability insurance for employees
100% company paid - life insurance and AD&D insurance for employees
100% company paid – employee assistance program
Retirement plans with company match
Training and Certification Reimbursement annually
Performance-based incentive program
Commission incentive program
Profit Sharing Plan
Referral Bonuses
Data Ideology is an EEO Employer
Show more
Show less","Data Analytics, SQL, PLSQL, R, Python, Data Warehouse, Data Integration, Data Validation, Data Testing, Data Mapping, Data Definition, Agile, SAFe, Oracle, SQL, Snowflake, Healthcare","data analytics, sql, plsql, r, python, data warehouse, data integration, data validation, data testing, data mapping, data definition, agile, safe, oracle, sql, snowflake, healthcare","agile, data definition, data integration, data mapping, data testing, data validation, dataanalytics, datawarehouse, healthcare, oracle, plsql, python, r, safe, snowflake, sql"
Senior Big Data Developer-US,Zortech Solutions,United States,https://www.linkedin.com/jobs/view/senior-big-data-developer-us-at-zortech-solutions-3661724035,2023-12-17,Inverell, Australia,Mid senior,Remote,"Role: Senior Big Data Developer
Location: Austin TX (Remote)
Duration: 6+ Months
Job Description
10+ years of hands-on and strong experience in python programming
10+ years of experience in developing test automation utilities for data reconciliation validation using python data science libraries
Hands-on experience in pandas, numpy, pyspark, big data technology
8+ years of experience in handling resources like AWS glue, ec2 and lambda
Proficient in writing complex SQL queries
10+ years of experience in handling ETL testing for various data sources (on-prem & and cloud)
QE experience is a plus
Show more
Show less","Python, Data Science, Pandas, Numpy, Pyspark, AWS Glue, EC2, Lambda, SQL, ETL, QE","python, data science, pandas, numpy, pyspark, aws glue, ec2, lambda, sql, etl, qe","aws glue, data science, ec2, etl, lambda, numpy, pandas, python, qe, spark, sql"
Azure Data Engineer,"Global Soft Systems, Inc.",United States,https://www.linkedin.com/jobs/view/azure-data-engineer-at-global-soft-systems-inc-3779845878,2023-12-17,Inverell, Australia,Mid senior,Remote,"Role: Azure Data Engineer
Duration: 6+ months
Remote (Prefer candidates in and around Kansas City, MO)
Designs, develops, implements, and supports technical solutions to meet the business needs. Leading Microsoft Solution Framework and may serve as Project Lead on a technical development team. Provides technical leadership in addressing complex systems solutions. Plans and coordinates processes used in data management including reporting, normalization, field mapping, marrying data with emerging technologies and tooling to support the overall Data Sharing Lifecycle. Works closely with the business staff and technical staff to analyze business problems, develop system solutions, and plan training requirements for application implementation. Provides input related to the technical performance evaluation for the Development staff.
Skills
Consults with users and Information Services departments, determines needs, and designs highly complex technology-based solutions. Suggests alternative approaches to meet user requirements.
Develops and implements business solutions. May fill the roles of Product, Program or Project Lead on a development team. Acts as technical mentor for the Development and Business Analyst teams.
Develops and maintains documentation of programs, user guides, policies and procedures.
Implement technologies that supports the data strategy while developing and implement the Data Lake/Lakehouse architecture while providing guidance on best practices.
Define and implement data pipelines that enable analytics and operational dashboards by designing and develop high quality batch and real time data pipelines at a senior level.
Define and Implement Data Engineering standards and best practices that improve quality and efficiency and define/promote data engineer standards, best practices, and design patterns.
Define and Implement Data Engineering workflows that improve quality and reduce the time to release new features along with designing and implement CI/CD workflows and pipelines.
Ensure the team can support ongoing development and operations of new technology and mentor other members of the team in new technology and skills.
Communicates project progress and status to the Manager and other development team members.
Evaluates existing applications and systems. Recommends improvements or modifications utilizing current technologies and techniques.
Evaluates employee performance and deliverables when managing a development team project
May determine training needs, mentor and recommend selection, termination, promotion, and reassignment of subordinates to their manager.
Must maintain knowledge of current and upcoming technologies keeping an eye on external competitive trends and best practices.
Required to create business solutions using knowledge and application of multiple technologies and applications.
Other duties as assigned
Required Experience
Requires a bachelor’s degree in a Computer Information Services or a related field.
Requires 6+ years’ experience in project analysis, application development and/or application programming.
Additional years of qualifying work experience may be considered in lieu of formal education.
Development experience with Microsoft Azure and experience implementing Azure Data Factory Pipelines using latest technologies and techniques
Requires knowledge of Microsoft Software (SSIS, MS Visual Studio and MS Server Products), MS Office Products, SQL server products, VB Scripting or related scripting languages.
Requires effective communication and presentation skills and the ability to lead multi-disciplinary development teams.
Ability to move into projects and immediately apply knowledge and add value.
Has the ability to explain theories and reasoning behind decision making while seeing the big picture and risks
Experience leading a team of more junior employees providing guidance and expertise to help them learn
Bachelor’s degree in Computer Science or related field or equivalent combination of industry-related professional experience and education
Show more
Show less","Azure, Data Factory Pipelines, Microsoft Software, SSIS, Visual Studio, Server Products, Office Products, SQL Server, VB Scripting, Data Lake, Lakehouse, Data Pipelines, Data Engineering, CI/CD, Cloud Computing, Data warehousing, Data modeling, Data analysis, Data visualization, Agile Development, Project Management, Communication, Presentation, Leadership","azure, data factory pipelines, microsoft software, ssis, visual studio, server products, office products, sql server, vb scripting, data lake, lakehouse, data pipelines, data engineering, cicd, cloud computing, data warehousing, data modeling, data analysis, data visualization, agile development, project management, communication, presentation, leadership","agile development, azure, cicd, cloud computing, communication, data engineering, data factory pipelines, data lake, dataanalytics, datamodeling, datapipeline, datawarehouse, lakehouse, leadership, microsoft software, office products, presentation, project management, server products, sql server, ssis, vb scripting, visual studio, visualization"
TEST DATA ENGINEER-REMOTE-EST or CST only,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/test-data-engineer-remote-est-or-cst-only-at-steneral-consulting-3661805618,2023-12-17,Inverell, Australia,Mid senior,Remote,"GenRocket is MUST have. Additionally, I would like to see minimum of 1+ yr. of implementation experience.
Must required: GenRocket, scripting, PostGres/DB ability, API work
Determine best practice(s) around synthetic, masked data in DCE/Platform
Analyze and understand how data flows in data hub
Determine best way to manage and create synthetic data in Data Hub; deliver solution
Figure out a solution to mock massive accounts coming through APIs
Understanding data models, table structures and dependencies.
Guide teams in generating new synthetic data, creating more, and maintaining over time.
Create PoC for syntheitc data use cases working with Product team
Agile Mindset, familiarity with Azure Cloud
ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API work
Determine best practice(s) around synthetic, masked data in DCE/Platform
Analyze and understand how data flows in data hub
Determine best way to manage and create synthetic data in Data Hub; deliver solution
Figure out a solution to mock massive accounts coming through APIs
Understanding data models, table structures and dependencies.
Guide teams in generating new synthetic data, creating more, and maintaining over time.
Create PoC for syntheitc data use cases working with Product team
Agile Mindset, familiarity with Azure Cloud
ADO experience, python or powershell scripting experience
Show more
Show less","GenRocket, Scripting, PostgreSQL, SQL, API development, Synthetic data, Data masking, Data hub, Data management, Data modeling, Agile methodology, Azure Cloud, ADO, Python, PowerShell","genrocket, scripting, postgresql, sql, api development, synthetic data, data masking, data hub, data management, data modeling, agile methodology, azure cloud, ado, python, powershell","ado, agile methodology, api development, azure cloud, data hub, data management, data masking, datamodeling, genrocket, postgresql, powershell, python, scripting, sql, synthetic data"
Data Engineer - Lead,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-lead-at-steneral-consulting-3742583023,2023-12-17,Inverell, Australia,Mid senior,Remote,"Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PA
Candidates need to take a video screen with prime vendor prior to end client submittal
Need 2 strong candidates on this role
Need strong knowledge of PySprak. 80% of the role will be using Pyspark.
They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool.
Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept.
Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team.
Top skills:
Pyspark
Azure Databricks and/or Data Factory
Experience leading and managing offshore teams.
ETL
Must Have
Azure Cloud
Azure Data Brick
ETL
High volumes of data
Pyspark
Python
Experience Leading or managing offshore teams
Data en
gineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.
Responsibilities
Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory
Working with event based streaming technologies to ingest and process data
Design and Implement Data Lakehouse
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Work with contractors to deliver products and services for the bottlers
Conduct Pilots with bottlers and measure value
Transition successful pilots into commercial applications
Ensure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics concepts
Review code/solutions developed by team prior to migration to production
What makes you a good fit?
Strong knowledge of data management principles
Experience in building ETL pipelines
Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience working with structured and unstructured data
Experience working in DevOps environment
3+ years of relevant experience
Ability to balance high customer orientation and service attitude with business priorities
Analytic thinking, and problem-solving skills
High energy, with strong will and ambition to learn and work on new things
Outstanding proven verbal, written and interpersonal communication skills
Ability to adapt quickly to changing product scope and priorities (demand driven)
Proven ability to influence and collaborate effectively with cross-functional teams
Demonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best work
Constant role model of culture: Integrity, Accountability, Passion, Collaboration, Innovation
Bachelor’s degree (or equivalent)
Show more
Show less","PySpark, Azure Databricks, Data Factory, ETL, Python, Azure Cloud, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, DevOps, Data pipelines, Data ingestion, Data processing, Data Lakehouse, Data management principles, Structured data, Unstructured data","pyspark, azure databricks, data factory, etl, python, azure cloud, azure data lake, azure cosmos db, azure stream analytics, devops, data pipelines, data ingestion, data processing, data lakehouse, data management principles, structured data, unstructured data","azure cloud, azure cosmos db, azure data lake, azure databricks, azure stream analytics, data factory, data ingestion, data lakehouse, data management principles, data processing, datapipeline, devops, etl, python, spark, structured data, unstructured data"
Installation Supervisor (Data Cables / International Travel),Ernest Gordon Recruitment,"Northampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/installation-supervisor-data-cables-international-travel-at-ernest-gordon-recruitment-3781181401,2023-12-17,Northampton, United Kingdom,Mid senior,Onsite,"£40,000 - £45,000 + Company Van + (£47k - £52k) OTE + Autonomy
Northampton, Northamptonshire
Do you have hands on experience installing / removing Data Cables and are looking for a days based, Monday to Friday role covering the globe boasting a no two days are the same, field based role where you will have the autonomy to manage a team of installation engineers and projects?
On offer is a days based role where you will be visiting sites nationally and internationally supervising installation projects and engineers within a variety of blue chip telecommunication clients. There is plenty of optional overtime available to boost your earnings and you will be required to stay away occasionally during international install projects.
This company are renowned within the telecommunications industry for their professional approach and strong customer service skills. Since their start in 2008 this company has grown exponentially and have gained a sterling reputation for their staff retention and professionality.
This role would suit an Installation Engineer or similar with supervisory experience looking for a highly autonomous, days based role travelling the globe with plenty of overtime to boost your earnings and the opportunity to further progress into senior managerial positions.
The Role
Supervising installation projects and engineers globally
Travelling across the world with occasional staying away required
Optional overtime available paid at 1.33 on Saturday and 1.5 on Sunday
Monday to Friday, 8am - 5pm core hours, 40 hours a week
The Person
Installation Supervisor / Engineer or similar
Experience with data cables or similar
Full right to work in the UK
Reference Number: BBBH11294
Service, Engineer, Installation, Data, Cables, Mechanical, Leicester, Birmingham, Northampton, Nottingham, Supervisor, Field, Junior, Cambridge, Peterborough, Milton Keynes
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV.
We are an equal opportunities employer and welcome applications from all suitable candidates. The salary advertised is a guideline for this position. The offered remuneration will be dependent on the extent of your experience, qualifications, and skill set.
Ernest Gordon Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job, you accept the T&C's, Privacy Policy and Disclaimers which can be found at our website.
Contact: Arthur Garvey
Reference: Totaljobs/BBBH11294
Job ID: 101700309
Show more
Show less","Data Cables, Installation, Engineering, Project Management, Supervisory Experience, Field Service, Technical Service, Customer Service, Telecommunications, Mechanical, Installation Supervisor, Installation Engineer","data cables, installation, engineering, project management, supervisory experience, field service, technical service, customer service, telecommunications, mechanical, installation supervisor, installation engineer","customer service, data cables, engineering, field service, installation, installation engineer, installation supervisor, mechanical, project management, supervisory experience, technical service, telecommunications"
Data Scientist,"Techaxis, Inc","Banbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-techaxis-inc-3774785189,2023-12-17,Northampton, United Kingdom,Mid senior,Onsite,"Our client is a multinational technology company specializing in IT services and consulting. It offers a wide range of solutions, including digital transformation, cloud computing, and cybersecurity.
Position: Data Scientist
Location: Banbury, England
Engagement Type: Full-time
Skills:
Proficiency in Python with experience in data manipulation and analysis using libraries such as Pandas, Numpy, or Matplotlib.
Proficiency in SQL and database management systems like PostgreSQL.
Knowledge of AI methodologies.
Strong problem-solving skills and ability to work with large datasets and complex models.
Strong numeracy.
Responsibilities:
Utilize advanced statistical and machine learning techniques to analyze timing and car data to extract meaningful insights.
Apply AI methodologies to enhance data analysis processes and automate repetitive tasks.
Design, implement, and optimize algorithms for data mining, machine learning, and predictive modelling.
Collaborate with cross-functional teams to integrate Data Science and AI solutions into existing systems and workflows.
Gather requirements from race and performance engineers.
Collaborate with other data scientists, software and motorsport engineers to drive projects from conception to completion.
Writing and maintaining technical documentation, including user manuals and API documentation
Contact Details:
babita.hazarika@techaxisinc.com | +1 (571) 655-3890
About us:
Techaxis Inc. is headquartered in Northern Virginia, USA. We are ranked #3294 in Inc 5000 and #105 in Inc 5000 DC Metro Series, SWaM, and WBE Certified technology talent search firm.
Equal Opportunities Employers:
Our clients provide equal opportunities to all its employees and all qualified applicants for employment, without regard to their race, caste, religion, color, ancestry, marital status, sex, age, nationality, disability, and veteran status. Employees of our clients are treated with dignity and in accordance with their policy to maintain a work environment free of sexual harassment, whether physical, verbal, or psychological. Employee policies and practices are administered in a manner that would ensure that in all matters equal opportunity is provided to those eligible and the decisions are merit-based.
Show more
Show less","Python, Pandas, Numpy, Matplotlib, SQL, PostgreSQL, AI, Machine learning, Data mining, Predictive modeling, Data analysis, Technical documentation, User manuals, API documentation","python, pandas, numpy, matplotlib, sql, postgresql, ai, machine learning, data mining, predictive modeling, data analysis, technical documentation, user manuals, api documentation","ai, api documentation, data mining, dataanalytics, machine learning, matplotlib, numpy, pandas, postgresql, predictive modeling, python, sql, technical documentation, user manuals"
HR Systems and Data Analyst - 12 months FTC,Wickes,"Northamptonshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/hr-systems-and-data-analyst-12-months-ftc-at-wickes-3782766198,2023-12-17,Northampton, United Kingdom,Mid senior,Hybrid,"We are recruiting a HR Systems and Data Analyst to join our wider HR team in Northampton on a 12 months FTC basis.
This role will involve administering and configuring our different HR Systems (e.g. user set up, structure changes), act as a HR systems specialist stakeholder when dealing with escalated support tickets, or managing regular auditing processes and high volume data entry.
Key Responsibilities:
Administer & maintain governance for HR systems e.g. user set up, hierarchy changes, process user requests in line with HR governance, Supporting with data extraction and distribution.
Act as an escalation point to problem solve user issues as part of wider HR Shared Services team
Provide HR data and systems specialist knowledge to the wider business, Being solely responsible for audits across our HR Systems and maintaining data integrity.
Provide support with processes that impact the full employee life-cycle, such as new joiners.
Proactively recognise trends in support tickets and escalating system issues immediately to the HR Systems Senior Analyst. Identify system improvements to increase efficiencies.
Complete accurate Data Entry in good time, spot inconsistencies in audits and correcting/managing those as soon as they arise.
Follow the correct process for accessing and distributing HR data, liaising with the wider team on their reporting requirements.
Improve the Line Manager’s experience by identifying training needs, and maintaining/creating user guides/supporting documentation.
What are we looking for:
An understanding of the HR employee life-cycle.
Experience with Data Entry/Customer Service
Experience of working with different HR Systems (HRe ideal but not essential)
Experience of working with multiple data sources
Awareness of HR processes and practices.
Analytical and problem-solving skills
High attention to detail
Strong Organisation Skills
Experience with Pivot Tables/basic formula within Excel/Google Sheets
Excellent communication skills to effectively present and explain complex systems processes in a clear and simple way
Ability to maintain confidentiality and exercise discretion
Committed to delivering good customer service
Willing to adapt to change
Thrives in a busy environment
Maintains high quality of detail during repetitive tasks.
What can we offer you?
You’ll be supported with fantastic learning and development and have the opportunity to grow and develop your career with us
We’ll also equip you with a benefits package that includes
Annual bonus
Save-as-you-earn scheme
Contributory pension scheme
Colleague discount
Discount platform including savings and cash back at numerous retailers, savings on gym membership, cycle to work scheme
Our widely recognised wellbeing strategy is something we’re proud of at Wickes. As part of this, we offer a range of health and wellbeing benefits and support, including an Employee Assistance Programme, financial education & loans, and access to parental, menopause and fertility support.
We recognise the value of bringing our teams together to collaborate, support each other and build on our amazing culture. We are also encouraging our teams to work flexibly, with a blend of remote / office working.
About Us:
Wickes is a multi-channel retailer operating in the home improvement market. With 40 years in industry, Wickes now generates revenue in excess of £1.6Bn across 230 stores delivered by 8,000+ colleagues.
But it is the Wickes’ culture that is considered its best kept secret; it’s a collaborative, down to earth, fun and inclusive environment where people feel part of a winning team. All our colleagues come from different backgrounds, but what we all have in common is a determination to succeed and a passion for being the best we can be. If that sounds like you, we’ll make you feel right at home.
Please note: All offers of employment are subject to DBS / background checks
Vacancy Reference #
Please contact us here if you require any adjustments within the application process. If you require any reasonable adjustments at the interview stage you will have an opportunity to inform us when we invite you to interview. Please note, this link is only for reasonable adjustments required - general enquiries, or direct CV applications cannot be accepted via this form.
Show more
Show less","HR Systems Administration, Data Extraction, Data Distribution, Troubleshooting, Data Integrity, Pivot Tables, Microsoft Excel, Google Sheets, Confidentiality, Customer Service, HR Processes, Data Entry","hr systems administration, data extraction, data distribution, troubleshooting, data integrity, pivot tables, microsoft excel, google sheets, confidentiality, customer service, hr processes, data entry","confidentiality, customer service, data distribution, data entry, data extraction, data integrity, google sheets, hr processes, hr systems administration, microsoft excel, pivot tables, troubleshooting"
Senior Data Architect | TS/SCI,YO HR Consultancy,"Springfield, MO",https://www.linkedin.com/jobs/view/senior-data-architect-ts-sci-at-yo-hr-consultancy-3767417101,2023-12-17,Spokane,United States,Mid senior,Onsite,"Company Overview
YO HR CONSULTANCY is a leading HR consulting firm operating in the None industry. We specialize in providing comprehensive human resources solutions to companies of all sizes, helping them achieve their talent acquisition and management goals.
Role And Responsibilities
We are currently seeking a highly skilled Senior Data Architect who will be responsible for the design and implementation of data architectures that support our clients' business goals. In this role, you will be working on various projects, researching and evaluating sources of information, applying sampling techniques, analyzing statistical information, and utilizing statistical methods to solve industry-specific problems. You will also be responsible for preparing detailed reports, training team members, and designing computer code to improve software and applications.
Candidate Qualifications
Bachelor's degree in mathematics, statistics, computer science or a related field
Minimum 8 years of related data engineering architecture and design
Comfort with Linux/Windows command line
Minimum 8 years of system administration and/or DevOps environment experience
TS/SCI Clearance
Required Skills
Research and analytical skills
Statistical analysis and problem-solving
Data interpretation and reporting
Training and mentoring
Software development and coding
Skills: automation solutions,data engineer,statistics,architecture,computer science,data engineering architecture,research,voip phone setup,mathematics,device apis,economics,devops,engineering,linux/windows command line,design,data architecture,statistical methods,computer code,operational metrics,data engineering,data pipelines,sampling techniques,system administration,devops environment
Show more
Show less","Linux, Windows, Command line, Statistical analysis, Problemsolving, Data interpretation, Reporting, Software development, Coding, Automation, Data engineering, Research, Architecture, Computer science, Mathematics, Economics, DevOps, System administration, Data engineering architecture, Data pipelines, Sampling techniques, Operational metrics","linux, windows, command line, statistical analysis, problemsolving, data interpretation, reporting, software development, coding, automation, data engineering, research, architecture, computer science, mathematics, economics, devops, system administration, data engineering architecture, data pipelines, sampling techniques, operational metrics","architecture, automation, coding, command line, computer science, data engineering, data engineering architecture, data interpretation, datapipeline, devops, economics, linux, mathematics, operational metrics, problemsolving, reporting, research, sampling techniques, software development, statistical analysis, system administration, windows"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087716,2023-12-17,Cincinnati,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Pandas, R, Spark, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, ML, LLMs, NLP","python, java, bash, sql, pandas, r, spark, airflow, kubeflow, snowflake, kubernetes, docker, helm, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, ml, llms, nlp","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, helm, java, kafka, kubeflow, kubernetes, llms, ml, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759710405,2023-12-17,Cincinnati,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, NLP, Pandas, R","python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, nlp, pandas, r","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Data Analyst,AAA Club Alliance,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-at-aaa-club-alliance-3774283756,2023-12-17,Cincinnati,United States,Mid senior,Hybrid,"The
Senior Data Analyst
will support our Insurance Sales, Service, and Operations teams. This individual will utilize data analytics, analysis, & reporting to provide key stakeholders with the information needed to make strategic business decisions and develop unique solutions to enhance our Agency's growth.
The primary duties of the Senior Insurance Analyst, Digital Transformation are to:
Establish data analytics processes, roles, quality criteria, and performance metrics to support Agency leadership in achieving performance objectives.
Identify, develop, and implement scalable data strategy, self-service tools, and workflow automation.
Analyze and report data associated with sales, service and operational effectiveness, member retention, and project/product performance.
Identify and evaluate the business value of new and existing data sources and uncover significant trends that will affect the organization in optimizing business decisions.
Ensure the quality and accuracy of data results and analytic methods through the establishment of testing and validation procedures and best practices.
Participate in the evaluation of new technology, and vendor selection processes.
Prioritize, scope, and recommend enterprise integrations and key performance indicators (KPIs) for projects across multiple business lines.
Participate in project plans, timelines, and deliverables, and manage Agency resources.
Advocate for data analytics practices and technology to senior stakeholders.
Select and implement (or oversee implementation) all data analytics tools and frameworks.
Execute quality controls and testing for ensuring project deliverables meet user expectations.
Working with Enterprise counterparts to assure the Agency complies with the data governance framework.
To The Qualified Candidate, We Offer
A competitive base salary, depending on experience
Annual Bonus eligibility
8 Paid Company Holidays
Nearly 4 weeks of Paid Time Off
401(k) plan with company match up to 7%
Medical, Dental, Vision & Prescription Coverage
Tuition reimbursement
Complimentary AAA Membership
FREE Life Insurance
Minimum Qualifications
Bachelor's degree (BS/BA) or equivalent in Insurance, Business Administration, or related field, with at least 4 years experience in an analytics or technical project management environment, or the equivalent combination of education and work experience.
Proficient in analytics tools such as SQL, Excel, R, Tableau or PowerBI, Python, Adobe, or Google Analytics.
Proficiency with call center data analytics and analysis (speech, text, cross-channel, predictive, and performance).
Conduct in-depth analyses of data to identify trends, patterns, and relationships, and summarize findings clearly and concisely for both technical and non-technical audiences.
Exceptional analytical, communication, critical thinking, and problem-solving capabilities
Work with project teams to understand the business objectives of each project and ensure that data analytics is used to its fullest potential to achieve these goals
Develop detailed reports and presentations to communicate results of data analysis to key stakeholders, providing recommendations for further action based on findings
AAA Club Alliance (ACA) is an equal opportunity employer.
Our Investment In Diversity, Equity, And Inclusion
At ACA, we are committed to cultivating a welcoming and inclusive workplace of team members with diverse backgrounds and experiences to enable us to meet our goals and support our values while serving our Members and customers. We strive to attract and retain candidates with a passion for their work and we encourage all qualified individuals, regardless of race, color, gender, identity, veteran status, sexual orientation, physical ability or national origin, to apply.
Job Category
Admin - Clerical
Show more
Show less","SQL, Excel, R, Tableau, PowerBI, Python, Adobe, Google Analytics, Data analytics, Speech analytics, Text analytics, Crosschannel analytics, Predictive analytics, Performance analytics, Data analysis, Trend analysis, Pattern analysis, Relationship analysis, Data visualization, Reporting, Communication, Critical thinking, Problemsolving, Project management","sql, excel, r, tableau, powerbi, python, adobe, google analytics, data analytics, speech analytics, text analytics, crosschannel analytics, predictive analytics, performance analytics, data analysis, trend analysis, pattern analysis, relationship analysis, data visualization, reporting, communication, critical thinking, problemsolving, project management","adobe, communication, critical thinking, crosschannel analytics, dataanalytics, excel, google analytics, pattern analysis, performance analytics, powerbi, predictive analytics, problemsolving, project management, python, r, relationship analysis, reporting, speech analytics, sql, tableau, text analytics, trend analysis, visualization"
Senior Business Intelligence Developer - Data & Application Engineering,Jobs via eFinancialCareers,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-business-intelligence-developer-data-application-engineering-at-jobs-via-efinancialcareers-3777111285,2023-12-17,Cincinnati,United States,Mid senior,Hybrid,"Job Description Summary
The Senior Business Intelligence Analyst for the Business Systems Operations (BSO) team is responsible for analyzing large, fragmented, and complex datasets that will be integrated into enterprise applications spanning GE Aerospace Finance and Supply Chain organizations. The applications and analytics developed by the Senior Business Intelligence Analyst will empower our business leaders to achieve annual business P&L targets, and establish the capability needed to achieve BSO's 3-5-year HK strategy. The strategy, capabilities, and analytical products will need to be developed jointly with other P&Ls and functions for scalability and integration across the business/matrix. This role will partner with our Data Science organization to integrate advanced analytical features into our data models.
Scope of the work will be for (but not limited to): Commercial Engines and Services, Defense and Systems, Supply Chain (Internal Manufacturing + Sourcing), and Indirect Sourcing. Tactically, the Senior Business Intelligence Analyst will partner with functional teams (i.e. cost leaders, finance, engineers, digital technology, etc.) to implement developed analyses and drive action.
Job Description
Essential Responsibilities:
Experienced cross functional analyst with a background in supply chain, finance, and process improvement.
Own data modeling and automation efforts for Prodigy Cost Management and Prodigy Demand Management modules.
Expertise with Business Intelligence tools and SQL. Proven ability to utilize BI tools, such as Alteryx to model data and automate processes that align with business outcomes.
Experience in extracting and analyzing data from ERP, MES, and MRP systems.
Proactively identify functional process optimization opportunities and deploy solutions in partnership with business stakeholders.
Comfortable with big data, analytics, distilling complex problems, and recommending changes in a clear and concise manner.
Proven ability to influence, present, and communicate across multiple stakeholders and drive alignment and consensus.
Must have great cross functional skills and proven track record in enabling outcomes strategically as well as tactically.
Comfortable with ED+ interactions/presentations and communication
Qualifications/ Requirements:
Bachelors degree from an accredited college or university and 5+ years of business intelligence analysis experience (or a minimum high school diploma / GED with an additional 4+ years of business intelligence analysis experience).
2+ years in a functional role focused in supply chain or finance.
Proven expertise utilizing Alteryx and Tableau.
Desired Characteristics:
Alteryx Core Certification desired
Training in Lean Six Sigma methodology, greenbelt or above preferred.
Experience in developing complex SQL queries to prep and blend data.
Problem solver: analytical-minded, challenges existing processes, critical thinker
Demonstrated ability to analyze and resolve problems.
Ability to document, plan, market, and execute projects.
Established project management skills.
Strong oral and written communication skills.
Humble: respectful, receptive, agile, eager to learn
Transparent: shares critical information, speaks with candor, contributes constructively
Focused: quick learner, strategically prioritizes work, committed
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer . Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided:
No
Show more
Show less","Business Intelligence, Data Modeling, SQL, Alteryx, Tableau, ERP, MES, MRP, Lean Six Sigma, Project Management, Communication, Problem Solving, Data Analysis","business intelligence, data modeling, sql, alteryx, tableau, erp, mes, mrp, lean six sigma, project management, communication, problem solving, data analysis","alteryx, business intelligence, communication, dataanalytics, datamodeling, erp, lean six sigma, mes, mrp, problem solving, project management, sql, tableau"
Data Center Building Operating Engineer (2nd Shift),JLL,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-2nd-shift-at-jll-3748261882,2023-12-17,Garland,United States,Associate,Onsite,"General Description:
The Data Center Operations Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
Show more
Show less","UPS, HVAC, Chillers, Crac, Crah, Plumbing, Electrical, ATS, STS, PDU, Refrigeration, Air conditioning, Boilers, Water heaters, Pumps, Valves, Piping, Filters, CMMS, EPA 608, NFPA70E","ups, hvac, chillers, crac, crah, plumbing, electrical, ats, sts, pdu, refrigeration, air conditioning, boilers, water heaters, pumps, valves, piping, filters, cmms, epa 608, nfpa70e","air conditioning, ats, boilers, chillers, cmms, crac, crah, electrical, epa 608, filters, hvac, nfpa70e, pdu, piping, plumbing, pumps, refrigeration, sts, ups, valves, water heaters"
Data Center Building Operating Engineer (2nd Shift),JLL,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-2nd-shift-at-jll-3748265361,2023-12-17,Garland,United States,Associate,Onsite,"General Description:
The Data Center Operations Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
Show more
Show less","Data center operations, Electrical systems, Mechanical systems, HVAC, Chillers, CRAC, CRAH, Plumbing, Controls, UPS, ATS, STS, PDU, Generators, Switchgear, Power distribution, Transformers, Hot water systems, Refrigeration, Chilled water, Air conditioning, Boilers, Ventilating, Water heaters, Pumps, Valves, Piping, Filters, CMMS, Vendor management, Customer facing tickets, Emergency escalation procedures, EPA 608, NFPA70E, Corrigo, MCIM, Salesforce, Zendesk, ServiceNow, Word, Excel","data center operations, electrical systems, mechanical systems, hvac, chillers, crac, crah, plumbing, controls, ups, ats, sts, pdu, generators, switchgear, power distribution, transformers, hot water systems, refrigeration, chilled water, air conditioning, boilers, ventilating, water heaters, pumps, valves, piping, filters, cmms, vendor management, customer facing tickets, emergency escalation procedures, epa 608, nfpa70e, corrigo, mcim, salesforce, zendesk, servicenow, word, excel","air conditioning, ats, boilers, chilled water, chillers, cmms, controls, corrigo, crac, crah, customer facing tickets, data center operations, electrical systems, emergency escalation procedures, epa 608, excel, filters, generators, hot water systems, hvac, mcim, mechanical systems, nfpa70e, pdu, piping, plumbing, power distribution, pumps, refrigeration, salesforce, servicenow, sts, switchgear, transformers, ups, valves, vendor management, ventilating, water heaters, word, zendesk"
Sr. Data Analyst,Aston Carter,"Dallas, TX",https://www.linkedin.com/jobs/view/sr-data-analyst-at-aston-carter-3788861259,2023-12-17,Garland,United States,Mid senior,Remote,"Description:*
Essential Job Functions:*
Support clients: Work and communicate closely with their clients throughout the report/dashboard development process. Provide status updates. Help clients understand their report/dashboard. Provide on-going maintenance and support.
Support projects: Participate in outcome evaluation, program evaluation, and quality improvement studies. Attend meetings, answer data-related questions, and offer suggestions.
Create reports and dashboards: Create ad-hoc and routine reports. Design and develop dashboards to display key metrics and trends.
Manage data: Collect, organize, store, and share a wide variety of data.
Transform data: Clean and optimize data for analyses.
Ensure data quality: Audit data, data transformation processes, workflow, deliverables and outputs.
Perform analyses: Perform statistical analyses (descriptive and inferential analyses).
Present findings: Present data and findings in a clear and concise manner, using appropriate reporting and data visualization tools.
Create and maintain documentation: Create FDD, document report requirement, business logic and workflow. Create data dictionaries. Ensure documentation is up-to-date.
Maintain up-to-date knowledge on information management systems, processes and data.
Manage compliance reporting: Maintain up-to-date knowledge of CMS, DHCS and internal compliance reporting requirement. Translate reporting requirement into reports. Work with clients to ensure accuracy of data. Submit report to external and internal agencies in a timely manner. Attend compliance trainings, meetings, and data validation webinars.
Support system enhancement/implementation: Perform data-related research and testing. Stay informed of system and process changes. Identify impact on existing reports and dashboards. Modify existing reports and dashboards accordingly.
Prioritize work and keep supervisor informed: Work on multiple projects at the same time. Organize and prioritize work effectively. Inform management when requirement or due date cannot be met.
Adheres to all quality, compliance and regulatory standards to achieve organization outcomes.
Required Education: *
Bachelor’s or Master’s degree in Analytics, Healthcare Informatics, Statistics, Computer Science, or related field.
Required Experience:*
At least 5 years of experience analyzing and compiling data, preferably in a health plan setting.
Required Skills/Abilities:*
Ability to manipulate and analyze data to produce accurate results. Present results in data visualizations, dashboards, and reports.
Knowledge in CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), and CMS Special Needs Plan (SNP), and DHCS Medi-Cal Managed Care reporting requirements.
Knowledge in authorization, claims, and encounter data. Clinical code knowledge (ICD, CPT, etc) related to utilization data.
Advanced skills in Microsoft Office, SQL Transactional SQL (T-SQL), SQL Server Reporting Services (SSRS), and Tableau.
Experience in SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA).
Must have analytical, communication, documentation, interpersonal, planning, presentation, problem-solving and research skills.
About Aston Carter:
Please Note: Scammers are posing as Aston Carter. We'll never contact you via Gmail, Telegram, or WhatsApp and we'll never solicit money from you.
At Aston Carter, we’re dedicated to expanding career opportunities for the skilled professionals who power our business. Our success is driven by the talented, motivated people who join our team across a range of positions – from recruiting, sales and delivery to corporate roles. As part of our team, employees have the opportunity for long-term career success, where hard work is rewarded and the potential for growth is limitless. Established in 1997, Aston Carter is a leading staffing and consulting firm, providing high-caliber talent and premium services to more than 7,000 companies across North America. Spanning four continents and more than 200 offices, we extend our clients’ capabilities by seeking solvers and delivering solutions to address today’s workforce challenges. For organizations looking for innovative solutions shaped by critical-thinking professionals, visit [AstonCarter.com.](AstonCarter.com) Aston Carter is a company within Allegis Group, a global leader in talent solutions. The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please call 888-237-6835 or email [astoncarteraccommodation@astoncarter.com](mailto:%20astoncarteraccommodation@astoncarter.com) for other accommodation options. However, if you have questions about this position, please contact the Recruiter located at the bottom of the job posting. The Recruiter is the sole point of contact for questions about this position.
Show more
Show less","Healthcare Informatics, Statistics, Computer Science, Data Analytics, Data Visualization, Data Management, Data Transformation, Data Auditing, Statistical Analysis, Reporting, Data Documentation, CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), CMS Special Needs Plan (SNP), DHCS MediCal Managed Care reporting, Authorization Data, Claims Data, Encounter Data, Clinical Code Knowledge, ICD, CPT, Microsoft Office, SQL Transactional SQL (TSQL), SQL Server Reporting Services (SSRS), Tableau, SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA), Analytical Skills, Communication Skills, Documentation Skills, Interpersonal Skills, Planning Skills, Presentation Skills, ProblemSolving Skills, Research Skills","healthcare informatics, statistics, computer science, data analytics, data visualization, data management, data transformation, data auditing, statistical analysis, reporting, data documentation, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, dhcs medical managed care reporting, authorization data, claims data, encounter data, clinical code knowledge, icd, cpt, microsoft office, sql transactional sql tsql, sql server reporting services ssrs, tableau, sql server integration services ssis, visual basic for applications vba, analytical skills, communication skills, documentation skills, interpersonal skills, planning skills, presentation skills, problemsolving skills, research skills","analytical skills, authorization data, claims data, clinical code knowledge, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, communication skills, computer science, cpt, data auditing, data documentation, data management, data transformation, dataanalytics, dhcs medical managed care reporting, documentation skills, encounter data, healthcare informatics, icd, interpersonal skills, microsoft office, planning skills, presentation skills, problemsolving skills, reporting, research skills, sql server integration services ssis, sql server reporting services ssrs, sql transactional sql tsql, statistical analysis, statistics, tableau, visual basic for applications vba, visualization"
AWS Data Engineer,Novateur Research Solutions,"Ashburn, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-at-novateur-research-solutions-3759788603,2023-12-17,Silver Spring,United States,Mid senior,Onsite,"Job Description
Novateur is looking for a AWS data engineer with experience of processing large quantities of data on the cloud. Experience in machine learning and/or geospatial data is a plus but not required.
RESPONSIBILITIES
Design, develop, and deploy scalable data pipelines.
Leverage big data tools like Apache Spark to handle large-scale datasets.
Work closely with data scientists and machine learning engineers to deliver actionable insights from complex data structures.
Use AWS services (such as S3, EC2, RDS, Lambda) to build and maintain cloud-based data storage and computing infrastructure.
Optimize data queries for performance and scalability.
Implement ETL (Extract, Transform, Load) processes.
Assist in setting up data warehousing solutions and analytics dashboards.
Participate in code reviews and contribute to team's best practices.
REQUIRED
• Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.
• Minimum 3 years of experience working with distributed processing systems like Hadoop, Spark, GeoMesa (or Sedona), and Kafka.
• Minimum 2 years of experience with AWS.
• Excellent programming skills, preferably in Python.
• Strong SQL skills and experience with databases like Postgres, MySQL, or MongoDB.
• Familiarity with data visualization tools like Tableau or Power BI is a plus.
• Excellent problem-solving skills and attention to detail.
• Strong communication skills, both verbal and written.
•
US Permanent Resident / US Citizen.
DESIRED
Experience with Kubernetes ecosystem and containerization.
Familiarity with AWS EMR.
Expertise in geospatial data technologies such as PostGIS, QGIS, and GeoJSON.
Familiarity with development and deployment of deep learning and machine learning training and inference pipeline on large databases.
Experience with machine learning libraries, such as PyTorch.
COMPANY BENEFITS
Novateur offers competitive pay and benefits including a wide choice of healthcare options with generous company subsidy, 401(k) with generous employer match, paid holidays and paid time off increasing with tenure, and company paid short-term disability, long-term disability, and life insurance.
Show more
Show less","Apache Spark, AWS, Linux, S3, EC2, RDS, Lambda, Hadoop, Kafka, Postgres, MySQL, MongoDB, Python, SQL, Tableau, Power BI, Kubernetes, EMR, PostGIS, QGIS, GeoJSON, PyTorch","apache spark, aws, linux, s3, ec2, rds, lambda, hadoop, kafka, postgres, mysql, mongodb, python, sql, tableau, power bi, kubernetes, emr, postgis, qgis, geojson, pytorch","apache spark, aws, ec2, emr, geojson, hadoop, kafka, kubernetes, lambda, linux, mongodb, mysql, postgis, postgres, powerbi, python, pytorch, qgis, rds, s3, sql, tableau"
Data Engineer (2) - REMOTE - Active TS/SCI Clearance with Security Clearance,ClearanceJobs,"Alexandria, VA",https://www.linkedin.com/jobs/view/data-engineer-2-remote-active-ts-sci-clearance-with-security-clearance-at-clearancejobs-3770706955,2023-12-17,Silver Spring,United States,Mid senior,Remote,"Candidates must hold an active TS/SCI clearance and live in MD, DC or VA We are currently seeking a Data Engineer with Cloud Expertise for a full-time, permanent position located in Washington, DC. This is a hybrid role that is primarily remote work. Candidates must live in VA, MD or DC in order to work onsite when needed. The Data Engineer with Cloud Expertise will support a new Zero Trust Project that falls within our support of the Chief Data Officer at DHS' Office of Intelligence and Analysis. This individual will help the team implement Zero Trust maturity recommendations across the portfolio of application development and data analytics projects. Duties: The team member will develop a plan and complete all data architecture and cloud engineering tasks to harden I&A's architecture environments (microservices and data layer) specifically related to current and future data stores and data catalog in the Top-Secret Data Hub (TSDH) and implement Zero Trust measures that deliver the desired outcomes for each of the seven pillars. System One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan. System One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law.
Show more
Show less","TS/SCI clearance, Data Engineer, Cloud Expertise, Zero Trust, Data Architecture, Cloud Engineering, Microservices, Data Layer, TopSecret Data Hub (TSDH), System One, Joulé, ALTA IT Services, CM Access, MOUNTAIN LTD, Equal Opportunity Employer","tssci clearance, data engineer, cloud expertise, zero trust, data architecture, cloud engineering, microservices, data layer, topsecret data hub tsdh, system one, joul, alta it services, cm access, mountain ltd, equal opportunity employer","alta it services, cloud engineering, cloud expertise, cm access, data architecture, data layer, dataengineering, equal opportunity employer, joul, microservices, mountain ltd, system one, topsecret data hub tsdh, tssci clearance, zero trust"
IT ENGINEERING ( DATA ENGINEER),IVY TECH SOLUTIONS INC,"Vienna, VA",https://www.linkedin.com/jobs/view/it-engineering-data-engineer-at-ivy-tech-solutions-inc-3787776281,2023-12-17,Silver Spring,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
IT ENGINEERING ( DATA ENGINEER)
Location: Remote or Vienna, VA
Duration: 12+Months
Please send the resume to
or 847- 350-1008
DATAWAREHOUSE, DB2, ETL, ORACLE, POWER BI, SQL
We will only accept the following:
US Citizens, Green Card Holders, TN Visa
Description:
Below are the primary skills required for the role:
SQL, Oracle, DB2 server
Hands on experience of building and maintaining artifacts in Datawarehouses, Data lakes etc for both on-prem (Teradata) and on-cloud (preferably on Azure)
Hands on experience of using ETL tools such as Informatica
Hands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOps
Business analytics and Intelligence tools such as Tableau, Power BI etc
Experience with Agile frameworks
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
vqS80RNybq
Show more
Show less","SQL, Oracle, DB2 Server, Datawarehouses, Data Lakes, Teradata, Azure, Informatica, Azure Data Factory, Azure Data Bricks, Synapse, Azure DevOps, Tableau, Power BI, Agile","sql, oracle, db2 server, datawarehouses, data lakes, teradata, azure, informatica, azure data factory, azure data bricks, synapse, azure devops, tableau, power bi, agile","agile, azure, azure data bricks, azure data factory, azure devops, data lakes, datawarehouses, db2 server, informatica, oracle, powerbi, sql, synapse, tableau, teradata"
Lead Data SDET,ASCENDING Inc.,"Fairfax, VA",https://www.linkedin.com/jobs/view/lead-data-sdet-at-ascending-inc-3787774323,2023-12-17,Silver Spring,United States,Mid senior,Remote,"Location:
Remote
anywhere in the continental US (lower 48 states). Cannot work outside of the U.S.
Contract to Hire, Long term Contract
Job Description:
We are supporting a Senior Director in the Quality Assurance space, who is looking for a talented,
Lead Data SDET
.
Our client works in a Big Data department that processes north of 250 Billion market events/day. This groups handles massive amounts of financial data and more projects and work are appearing in the Crypto/Blockchain space. Our client is looking for a Lead Data SDET that has a data engineering background mixed in with Quality assurance.
Requirements:
7+ years of experience.
Bachelor of Science in Computer Science or related field.
Very strong programming skills in Scala, Java and/or Python.
Strong experience and knowledge of Spark/Spark SQL.
Experience writing automated tests and building test frameworks from scratch.
AWS experience is a must EMR, S3, DynamoDB.
Experience with Data pipeline tools like Airflow or DBT.
Data engineering/ETL experience.
Strong communication skills.
Proactive and strong sense of ownership and self-reliance.
Preferred Experience:
Crypto/Blockchain QA Experience.
Exposure to Behaviour Driven Development and experience in programming and testing.
The ability to recognize risks and errors in an application.
Excellent verbal and written communication skills.
Good time management and organizational skills.
The ability to keep current with the constantly changing technology industry.
A passion for testing, development, and design.
Powered by JazzHR
L6OZrEZ8QE
Show more
Show less","Lead Data SDET, Scala, Java, Python, Spark, Spark SQL, AWS, EMR, S3, DynamoDB, Airflow, DBT, Data engineering, ETL, Crypto, Blockchain, Behaviour Driven Development, Quality Assurance","lead data sdet, scala, java, python, spark, spark sql, aws, emr, s3, dynamodb, airflow, dbt, data engineering, etl, crypto, blockchain, behaviour driven development, quality assurance","airflow, aws, behaviour driven development, blockchain, crypto, data engineering, dbt, dynamodb, emr, etl, java, lead data sdet, python, quality assurance, s3, scala, spark, spark sql"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Stamford, CT",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783185698,2023-12-17,Bridgeport,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Stamford-DataResearchAn.010
Show more
Show less","Generative AI, Python, JavaScript, JSON, OOP, Data Science, Machine Learning, Natural Language Processing, R, Algorithms, Product Development, Education Technology, Data Analytics, HighPerformance Coaching, Remote Work, Independent Contractor Agreement","generative ai, python, javascript, json, oop, data science, machine learning, natural language processing, r, algorithms, product development, education technology, data analytics, highperformance coaching, remote work, independent contractor agreement","algorithms, data science, dataanalytics, education technology, generative ai, highperformance coaching, independent contractor agreement, javascript, json, machine learning, natural language processing, oop, product development, python, r, remote work"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Stamford, CT",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783191135,2023-12-17,Bridgeport,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Stamford-DataScientist.017
Show more
Show less","Python, JavaScript, JSON, R, OOP, Generative AI, AI, Data Science, Technology, Programming, SQL, Technical Writing, Communication, Stakeholder Management, Project Management, Data Analysis, Coaching, Agile","python, javascript, json, r, oop, generative ai, ai, data science, technology, programming, sql, technical writing, communication, stakeholder management, project management, data analysis, coaching, agile","agile, ai, coaching, communication, data science, dataanalytics, generative ai, javascript, json, oop, programming, project management, python, r, sql, stakeholder management, technical writing, technology"
Data Engineer,SID Global Solutions,"Exton, PA",https://www.linkedin.com/jobs/view/data-engineer-at-sid-global-solutions-3780000797,2023-12-17,Exeter,United States,Mid senior,Onsite,"Position
: Data Engineer
Exp level
: 4+ years
Location :
Bellevue, Washington
Skillset
: SQL, AWS Stack, Python, Redshift, MYSQL
Roles & Responsibilities:
Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift
Hands on Python programming.
Working knowledge on S3
AWS certification is a nice to have.
Must be punctual and follow deadlines and deliver on time.
Must be able to clearly communicate with stakeholders.
Show more
Show less","Data Engineering, SQL, Python, AWS, Redshift, MySQL, S3, Cloud Computing, Data Warehousing, Data Analysis, Data Visualization, Communication","data engineering, sql, python, aws, redshift, mysql, s3, cloud computing, data warehousing, data analysis, data visualization, communication","aws, cloud computing, communication, data engineering, dataanalytics, datawarehouse, mysql, python, redshift, s3, sql, visualization"
"Data Scientist (Hybrid- Exton, PA)",Spherix Global Insights,"Exton, PA",https://www.linkedin.com/jobs/view/data-scientist-hybrid-exton-pa-at-spherix-global-insights-3787798881,2023-12-17,Exeter,United States,Mid senior,Onsite,"Overview
The
Data Scientist
will apply statistical theory and methods to collect, interpret, and summarize data, as well as make predictions and recommendations based on complex data sets including solutions for process improvement and new product development. Responsibilities include data extraction, evaluation and use of machine learning tools, analysis of proprietary data for patterns, and development of predictive models. A successful candidate will be versed in effective data visualization techniques with 10 years of relevant experience in the life sciences industries.
Key Responsibilities
Identifies new, strategic requirements for data and leads process of acquisition and productization
Develops and implements statistical models and algorithms to analyze complex data sets
Performs statistical analysis of data to identify trends and patterns and to build reports for stakeholders
Works with cross-functional teams to identify and define business problems and develop analytical solutions
Defines and steers implementation of processes and standards for Machine Learning tools, code, testing, and product deployment
Trains and mentors others with respect to advanced Data Science/AI/Machine Learning techniques and tools, mathematical and statistical understanding of Machine Learning, and industry best practices for Data Science development and solutions
Evaluates new technologies to drive best practices
Industry and Academic Awareness: Stays up-to-date with the latest developments in around AI and associated technologies in both academia and industry, integrating relevant advancements into our Spherix strategy
Education & Experience
Bachelor’s degree in computer science, statistics, applied math or related field required
Extensive background in data mining and statistical analysis
Experience with SQL and statistical software packages (Python, R, SAS) required
Experience using business intelligence tools and data frameworks, working with AI and on product innovation
Experience with data science methods related to data architecture, data munging, data and feature engineering, and predictive analytics
Experience with data de-identification and synthetic data generation/upscaling
Experience with large-scale healthcare data sets, including electronic health records, claims data, and patient-generated data
10+ years experience as data scientist
Generous Benefits Package
Competitive pay and an annual performance-based bonus
20 Days PTO per year and Christmas to New Years paid vacation
Summer Fridays Memorial Day to Labor Day
Competitive 401k with company match
Interested candidates should submit a resume with cover letter stating why this role is of interest and what attributes you will bring to the Spherix team.
About Spherix
At Spherix, we are a hyper-focused market intelligence firm that leverages our proprietary network of Health Care Practitioners (HCPs) to provide actionable insights to our clients so that they can make better decisions. We specialize in select dermatology, gastroenterology, nephrology, neurology, ophthalmology, and rheumatology markets.
Our leadership team has extensive client-side experience in biopharmaceutical brand management and strategic marketing, coupled with a passion for the specialty markets that they cover. Our therapy teams intimately understand each therapeutic market and effectively partner with our clients. In addition, Spherix Global Insight’s proprietary network allow us to not only rapidly turnaround research, but it also ensures a high-quality respondent base that is engaged in the research process.
Spherix Global Insights has a passionate team of professionals that is driven by helping customers turn information into insights. Our proprietary approach and our passionate team have allowed the company to grow rapidly over the past five years.
Recognized by the Philadelphia Business Journal as one of the fastest growing businesses in Philadelphia- Award: Soaring 76 – 2019, 2020, 2021, 2022, 2023
Recognized by the Entrepreneurs’ Forum as one of the fastest growing privately held business in the region- Award: Philadelphia 100- 2019, 2020, 2021, 2022
Powered by JazzHR
ywItEY8qyK
Show more
Show less","Data Scientist, Statistics, Machine Learning, Predictive Analytics, Data Visualization, Data Extraction, Data Mining, SQL, Python, R, SAS, Business Intelligence, Data Architecture, Data Munging, Data Engineering, Data DeIdentification, Synthetic Data Generation, Healthcare Data, Electronic Health Records, Claims Data, PatientGenerated Data","data scientist, statistics, machine learning, predictive analytics, data visualization, data extraction, data mining, sql, python, r, sas, business intelligence, data architecture, data munging, data engineering, data deidentification, synthetic data generation, healthcare data, electronic health records, claims data, patientgenerated data","business intelligence, claims data, data architecture, data deidentification, data engineering, data extraction, data mining, data munging, data scientist, electronic health records, healthcare data, machine learning, patientgenerated data, predictive analytics, python, r, sas, sql, statistics, synthetic data generation, visualization"
Data Scientist,Smart TechLink Solutions Inc.,"Pennsburg, PA",https://www.linkedin.com/jobs/view/data-scientist-at-smart-techlink-solutions-inc-3747470902,2023-12-17,Exeter,United States,Mid senior,Remote,"Location: Remote (This position is based in Hartford, Connecticut. UST will provide relocation assistance)
Description: UST is looking for talented Data Scientists who have experience building data-based solutions powered by the advancement of Machine Learning algorithms and Deep Learning networks. As a Data Scientist (Stars Business Intelligence, Medicare) you will play a critical role in developing analytical capabilities to improve and optimize member experience.
Responsibilities
As a Data Scientist you:
Should have the ability to communicate data insights to all organizational levels, concluding, defining recommended actions, and reporting results across stakeholders.
Should work on integrating data from different data sources.
Should be working on pre-processing large datasets to build machine learning models, automating, deploying, and maintaining them into production.
Should be able to understand how the deployed models run correctly.
Should develop, test, and deploy data structures using Entity-Relationship Diagramming, and data modeling tools.
Requirements
1+ years of hands-on experience on Flask and Rest-API, model deployment.
3+ years of hands-on experience with Python, MySQL, and SAS (SAS Enterprise), R, Tableau, SPSS, STATA.
5+ years of experience in data science specialization, including statistical data analysis and/or machine learning in an enterprise-scale environment.
Deep understanding of common database technologies, such as SQL Database/Server, SQL Data Warehouse, Oracle, DB2, Netezza, MySQL, and other data sources, such as Azure Data Lake Storage and Azure Blob Storage.
Experience working with distributed computing tools (Hadoop, Hive, Spark, etc.)
Experience with Cloud Platforms using GCP/Azure/AWS.
Experience with Linux and experience with NLP.
Healthcare Experience is a huge plus.
Expert in Docker, CI/CD deployment, writing YMAL files to implement code and functions as service.
Hands-on experience with real-time streaming processing as well as high volume batch processing, and skilled in Advanced SQL, Amazon S3, Apache Kafka, Data-Lakes, etc.
Experience with Tableau is a plus.
Experience with large scale data mining tools such as Spark
Advanced understanding of best practices for structuring and organizing Data Lake file systems for large volumes of data.
Experience with ML models automation and deployment to production.
Experience performing advanced data pipelines, data structure and modeling, data processing, data extraction, joining, manipulation cleaning, analysis, and presentation for medium to large datasets.
Experience developing models for forecasting, classification, clustering, regression analysis, recommendations, variable selections, and natural language processing.
Experience with scientific computing and analysis packages such as NumPy, Pandas, Scikit-Learn, SciPy, and ggplot2.
Experience with Deep Learning frameworks like PyTorch, TensorFlow, and Keras.
Experience with automated feature engineering/feature extraction and reduction. Experience with data visualization libraries such as Matplotlib, Seaborn Pyplot, ggplot2.
Strong grasp of experimental design, A/B testing, and advanced statistical analysis
Experience with Git, GitHub, and Linux administration.
Experience leading end-to-end data science project implementation including training, testing, and deploying machine learning models in production environments.
Show more
Show less","Machine Learning, Deep Learning, Flask, RestAPI, Python, MySQL, SAS, R, Tableau, SPSS, STATA, SQL Database/Server, SQL Data Warehouse, Oracle, DB2, Netezza, Azure Data Lake Storage, Azure Blob Storage, Hadoop, Hive, Spark, GCP/Azure/AWS, Linux, NLP, Docker, CI/CD, YMAL, Advanced SQL, Amazon S3, Apache Kafka, DataLakes, Spark, NumPy, Pandas, ScikitLearn, SciPy, ggplot2, PyTorch, TensorFlow, Keras, Matplotlib, Seaborn Pyplot, Git, GitHub","machine learning, deep learning, flask, restapi, python, mysql, sas, r, tableau, spss, stata, sql databaseserver, sql data warehouse, oracle, db2, netezza, azure data lake storage, azure blob storage, hadoop, hive, spark, gcpazureaws, linux, nlp, docker, cicd, ymal, advanced sql, amazon s3, apache kafka, datalakes, spark, numpy, pandas, scikitlearn, scipy, ggplot2, pytorch, tensorflow, keras, matplotlib, seaborn pyplot, git, github","advanced sql, amazon s3, apache kafka, azure blob storage, azure data lake storage, cicd, datalakes, db2, deep learning, docker, flask, gcpazureaws, ggplot2, git, github, hadoop, hive, keras, linux, machine learning, matplotlib, mysql, netezza, nlp, numpy, oracle, pandas, python, pytorch, r, restapi, sas, scikitlearn, scipy, seaborn pyplot, spark, spss, sql data warehouse, sql databaseserver, stata, tableau, tensorflow, ymal"
Data Reporting Team Lead,Piper Companies,"Oaks, PA",https://www.linkedin.com/jobs/view/data-reporting-team-lead-at-piper-companies-3782803682,2023-12-17,Exeter,United States,Mid senior,Hybrid,"Piper Enterprise Solutions
is searching for a
Data Reporting Team Lead
for a financial services client in the King of Prussia area for a
hybrid, long term contract.
Responsibilities for the Reporting Lead:
Responsible for leading a team of 10+ reporting team members, creating reports using SQL and Crystal Reports
Work in a hands-on, report development role, leading team to complete all reporting needs
Qualifications for the Reporting Lead:
10+ years of experience in a business intelligence, analytics, or similar role
Leadership experience in an analytics-focused position
Crystal Report development experience
Hands-on background in SQL server, and ability to write complex queries
Financial services data is preferred
Strong communication and project management skills
Compensation/Benefits for the Reporting Lead:
targeting $65/hr, W2 Contract (long-term)
hybrid work schedule (3 days/wk on-site)
Comprehensive benefit package; Medical, Dental, Vision, 401k
Keywords: crystal reports, Microsoft sql, sql, sql server, crystal, obiee, cognos, powerbi, analytics, business intelligence, leadership, developer, quality assurance, engineer, bi engineer, business intelligence lead, manager, contract, remote, work from home, hybrid, hands on, project management, tableau, visual, reporting lead, reporting, lead, database, data warehouse, analytical, dashboards, pl/sql, oracle, azure, visual reports, importing data, etl, presentations, client, requirement gathering, requirements, medical, dental, vision, benefits, PTO
Show more
Show less","SQL, Crystal Reports, Leadership, Communication, Project Management, OBIee, Cognos, PowerBI, Analytics, Business Intelligence, PL/SQL, Oracle, Azure, Data Visualization, Data Warehousing, ETL, Presentations, Requirement Gathering","sql, crystal reports, leadership, communication, project management, obiee, cognos, powerbi, analytics, business intelligence, plsql, oracle, azure, data visualization, data warehousing, etl, presentations, requirement gathering","analytics, azure, business intelligence, cognos, communication, crystal reports, datawarehouse, etl, leadership, obiee, oracle, plsql, powerbi, presentations, project management, requirement gathering, sql, visualization"
Senior Data Engineer,hackajob,"Dundee, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-hackajob-3780132480,2023-12-17,Dundee, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with Social Security Scotland helping them to hire the best talent and build the future.
In order to be matched you have to fill out the form by clicking
""Apply""
DDaT Pay Supplement
This post attracts a £5000 Digital, Data and Technology (DDaT) pay supplement after a 3-month DDaT competency qualifying period. The payment will be backdated to your start date in the role. Pay supplements are temporary payments designed to address recruitment and retention issues caused by market pressures and are subject to regular review. This post is part of the Scottish Government DDaT profession. As a member of the profession, you will join the professional development system, currently BCS RoleModelplus.
Joining the team at Social Security Scotland as a Senior Data Engineer offers the opportunity to play a pivotal role in driving the organization's data initiatives. The successful candidate, as a data enthusiast, will have the chance to leverage cutting-edge AWS services to harness the power of data, ensuring its accuracy, reliability, and security. Contributing significantly to the design, development, and optimization of the data infrastructure, the Senior Data Engineer will enable the delivery of essential services to the people of Scotland.
As part of the Cloud and Engineering branch, this role involves collaboration with colleagues from various technical teams and the wider organization. The Senior Data Engineer will be responsible for designing and leading the implementation of data flows that connect operational systems and data for analytics and BI systems.
Within the engineering team, the Senior Data Engineer will take on a leadership role, managing and mentoring junior staff members. Close collaboration with Developers, Business Analysts, and Quality Assurance Analysts is essential for success in this role. Working closely with other Senior Data Engineers, there will be opportunities to learn from and support each other, contributing to the achievement of business objectives.
Social Security Scotland, as an executive agency of the Scottish Government, is undergoing the largest and most complex IT and digital change program since devolution. With a lifetime budget exceeding £300m, the organization is dedicated to delivering a social security system that will support the people of Scotland for decades to come.
The organization is committed to fostering a positive and inclusive culture within Social Security Scotland, where individuals are supported to flourish. This involves creating a working environment where everyone is treated with dignity and respect, and where contributions are recognized and valued.
Show more
Show less","AWS, Data Engineering, Data Infrastructure, Data Analytics, Business Intelligence, Data Architecture, Data Security, Cloud Computing, Python, Java, SQL, NoSQL, Linux, Hadoop, Spark, Docker, Kubernetes, Git, Agile, Scrum, Jira, Confluence","aws, data engineering, data infrastructure, data analytics, business intelligence, data architecture, data security, cloud computing, python, java, sql, nosql, linux, hadoop, spark, docker, kubernetes, git, agile, scrum, jira, confluence","agile, aws, business intelligence, cloud computing, confluence, data architecture, data engineering, data infrastructure, data security, dataanalytics, docker, git, hadoop, java, jira, kubernetes, linux, nosql, python, scrum, spark, sql"
"Analyst, Data Operations",Digitas North America,"Boston, MA",https://www.linkedin.com/jobs/view/analyst-data-operations-at-digitas-north-america-3784804382,2023-12-17,Beverly,United States,Associate,Onsite,"Digitas is the Networked Experience Agency, built on the vision that we create magnetic experiences that earn the right for brands to exist in human networks. Today, and tomorrow. We deliver Networked Experiences by leveraging comprehensive data, technology, creative, media and strategy capabilities. Digitas delivers ambitious outcomes via unique solutions that include Creative Experiences, Integrated Media, Addressable Relationships, Social Marketing and Total Commerce. Celebrated by Ad Age as Data and Insights Agency of the Year, U.S Campaign’s Brand Experience Agency of the Year, Media Network of the Year and celebrated by Forrester and Gartner, Digitas serves the world’s leading brands through a global network comprised of more than 5,500 employees across over 65 offices in 43 countries.
Job Description
Analyzing data and consulting with subject matter experts to design and develop business rules for data processing
Setting up and/or maintaining any existing dataflows in data wrangling tools like Alteryx or Google Dataprep
Creating and/or maintaining SQL scripts
Where applicable, working with data engineers to assure specifications are understood and followed
Monitoring, troubleshooting, and remediating data quality across marketing data systems ensuring full understanding of client deliverables
Designing and executing data quality checks
Keeping up to date on digital media operations – new partners, buy changes, etc.
Maintaining ongoing management and stewardship of data governance, processing, and reporting
Governing taxonomy additions, application, and use
Serving as a knowledge expert for operational processes and identify areas of improvement to ensure appropriate turnaround times and data quality standards are being met
Evaluating opportunities for simplification and/or automation for reporting and various processes
We’re looking for strong, impactful experience, which typically includes:
1-2 years of applicable experience
A four-year college degree; advanced degree is a plus
Strong attention to detail
Excellent oral and written communication skills
Passion for problem solving, data automation and finding efficiencies in current processes
Passion for digital marketing, eagerness to learn in a constantly changing space, and a natural curiosity
Proficient with SQL and data visualization tools (i.e. Tableau, PowerBI, Google Data Studio)
Knowledge and experience with Python and/or R a plus
Experience with tools like Alteryx a plus
Experience working with Google Cloud and AWS a plus
Comfortable in a fast-paced and deadline-driven environment
Got what it takes? We’d love to hear from you!
Show more
Show less","Data Analysis, Data Wrangling, SQL, Data Quality Assurance, Data Governance, Data Stewardship, Taxonomy Management, Process Improvement, Automation, Tableau, PowerBI, Google Data Studio, Python, R, Alteryx, Google Cloud, AWS","data analysis, data wrangling, sql, data quality assurance, data governance, data stewardship, taxonomy management, process improvement, automation, tableau, powerbi, google data studio, python, r, alteryx, google cloud, aws","alteryx, automation, aws, data governance, data quality assurance, data stewardship, data wrangling, dataanalytics, google cloud, google data studio, powerbi, process improvement, python, r, sql, tableau, taxonomy management"
"Senior Software Engineer, Data Platform","1upHealth, Inc.","Boston, MA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-platform-at-1uphealth-inc-3654520720,2023-12-17,Beverly,United States,Mid senior,Remote,"1upHealth is powering the digital transformation of the healthcare industry with our industry leading platform. By making healthcare data easily accessible and computable, we are on a mission to improve the patient experience, improve the quality of care, and reduce the overall cost of care.
We're looking for a bold Senior Software Engineer to join our newly formed Data Platform team. As a member of the Data Platform team, you'll be working together to design and build the platform that will be the foundation of 1up's next-generation products. You'll partner with teams and functions across the company to create a secure, scalable, data-centric platform, based on open source technologies, that meets the needs of today's market and is ready to take on the challenges of the future.
At 1up, we believe in building an open and collaborative environment, where every member of the team is able to bring their whole self to work. We celebrate diversity and consider the development of the members of our team to be a top priority. We take our mission to improve healthcare for everyone very seriously, and we're proud to work at 1up. We think you will be, too.
You will:
Help drive the technical vision of the Data Platform team
Design and develop data systems that lay the foundation for the future of 1up
Be a hands on contributor for a product that tells a compelling and complete story
Participate in code discussions, reviews, and refactoring
Collaborate with members across the organization to improve our products and services
Advocate for engineering best practices
Be a mentor and role model for team members
We are looking for people who have:
5+ years of software engineering experience building highly performant and scalable applications in a modern language (e.g., Java, Scala, Python, Typescript)
3+ years experience working with data technologies, such as Apache Kafka, Apache Spark, Apache Nifi, Apache Iceberg, Apache Airflow, Apache Hadoop, Apache Hive, Redshift, etc.
2+ years experience in a modern cloud infrastructure (e.g., AWS, GCP)
Expert understanding of securely designing and consuming RESTful APIs
Ability to communicate clearly with various stakeholders of technical and non-technical backgrounds
You may also have
Previous healthcare technology experience (e.g., HL7, FHIR)
Container/Orchestration experience (e.g., Docker, Kubernetes)
Hands-on experience working in a CI/CD environment
Messaging system background-kafka
Security Alert:
1upHealth only uses email domains of First Name. Last Name@1up.health or no-reply@1up.health to communicate with prospects. You will never receive an email from a third-party email service such as gmail. In addition, we will never ask a candidate for employment to share personal information (such as banking information, social security numbers, passport, etc), purchase their own equipment, or pay to apply to an open position.
About 1upHealth
At 1upHealth, our mission is to unlock health data and improve industry outcomes. As leaders in FHIR® interoperability, our platform makes it easier for partners to access, integrate, aggregate, and share data across a variety of systems. 1upHealth is building a data ecosystem to promote the digital transformation of the industry and encourage insight-driven healthcare.
We are proud to announce that we have been named 2022 Best Places to Work in the Small Company and Best Paying Company categories by Built In Boston.
Benefits
100% Paid BCBS Medical and Dental Insurance for Employees
Vision Insurance
Unlimited PTO
Equity
401(k)
Home Office Stipend
Commuter Stipend
Wellness Reimbursement
Parental Leave (16 weeks for birthing parents, 6 weeks for non-birthing parents)
Company Meetings with Free Lunch
Show more
Show less","Java, Scala, Python, Typescript, Apache Kafka, Apache Spark, Apache Nifi, Apache Iceberg, Apache Airflow, Apache Hadoop, Apache Hive, Redshift, AWS, GCP, RESTful APIs, HL7, FHIR, Docker, Kubernetes, CI/CD, Kafka","java, scala, python, typescript, apache kafka, apache spark, apache nifi, apache iceberg, apache airflow, apache hadoop, apache hive, redshift, aws, gcp, restful apis, hl7, fhir, docker, kubernetes, cicd, kafka","apache airflow, apache hadoop, apache hive, apache iceberg, apache kafka, apache nifi, apache spark, aws, cicd, docker, fhir, gcp, hl7, java, kafka, kubernetes, python, redshift, restful apis, scala, typescript"
Senior Data Engineer - 462,Braintrust,Greater Boston,https://www.linkedin.com/jobs/view/senior-data-engineer-462-at-braintrust-3786553397,2023-12-17,Beverly,United States,Mid senior,Hybrid,"About Us
Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.
About The Hiring Process
The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.
Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.
JOB TYPE:
Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION:
Work from anywhere - EST/EDT | partial day overlap
HOURLY RANGE
Our client is looking to pay $110.00 – $113.00/hr
ESTIMATED DURATION:
40/week - long term
EXPERIENCE:
5-9 years
BRAINTRUST JOB ID:
11491
The Opportunity
The ideal candidate is a Data Engineer with considerable experience in migrations and Big Data frameworks.
Must-Haves
Scala programming language expertise
Spark framework expertise
Experience working with BigQuery
Familiarity scheduling jobs in Airflow
Fluency with Google Cloud Platform, in particular GCS and Dataproc
Nice-to-Haves
Python programming language fluency
Scalding framework fluency
Pyspark framework fluency
Dataflow(Apache Beam) framework fluency
Details/Notes
This is a one-time project-based role and is unlikely to extend or convert.
This role is open to remote candidates across the US who can work standard hours with complete overlap to Etsy core business hours (EST 11am - 2pm).
Interview process: The interview process is two interviews, a 30 minute phone screen with the hiring manager and a 90 minute technical screen with engineers on the team.
What You'll Be Working On
About The Team
Batch Engines supports the batch Big Data tools for engineering teams at Etsy. As an infrastructure team, our goal is to ensure state of the art tools are easy to use and interface well within the Etsy engineering ecosystem to enable batch data processing. We are a 6 person team with expertise in Big Data and DevOps.
About The Role
The Data Engineer contractor role will be a project based role focused on migrating data pipelines from legacy infrastructure and frameworks such as Scalding to more modern infrastructure we support such as Spark Scala. This role will be responsible for:
Analyzing existing data pipelines to understand their architecture, dependencies, and functionality.
Working closely with data engineers to develop a migration strategy for converting pipelines from the current framework to the target framework.
Designing, building, and testing new data pipelines in the target framework, ensuring they meet or exceed existing performance and reliability standards.
Debugging and troubleshooting issues that arise during the migration process, working collaboratively with cross-functional teams to resolve them quickly.
Communicating progress, challenges, and timelines to stakeholders on a regular basis.
Apply Now!
Notes
Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.
Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.
Show more
Show less","Data Engineer, Scala, Spark, BigQuery, Airflow, Google Cloud Platform (GCP), GCS, Dataproc, Python, Scalding, Pyspark, Apache Beam","data engineer, scala, spark, bigquery, airflow, google cloud platform gcp, gcs, dataproc, python, scalding, pyspark, apache beam","airflow, apache beam, bigquery, dataengineering, dataproc, gcs, google cloud platform gcp, python, scala, scalding, spark"
"Principal Data Engineer (Boston, MA)",Huxley,"Boston, MA",https://www.linkedin.com/jobs/view/principal-data-engineer-boston-ma-at-huxley-3780198526,2023-12-17,Beverly,United States,Mid senior,Hybrid,"Principal Data Engineer
Boston, MA (Hybrid - 3x/week onsite)
An industry-leading GreenTech company is looking to bring on a
Principal Data Engineer
to the team. They offer a product suite of SaaS and AI tools used by companies around the globe to assess and improve their carbon footprint.
In this role, you will be responsible for development, design and architecture of greenfield Big Data solutions within a cloud-native environment. This person will be highly visible across the firm, being a critical thought-leader to ensure delivery on highly-performant, scalable data solutions.
Qualifications:
6-8+ years of professional data engineering experience
Expertise developing Big Data systems centered on AWS
Highly skilled using Snowflake, Redshift, S3, Lambda in a data engineering context
Proficient with Python programming, PostgreSQL database and REST APIs
Passionate about complex problem solving, delivering innovative solutions and working on meaningful projects
BS or MS in Computer Science (or related STEM field)
Desired Skills and Experience
Python, SQL, Postgres, Big Data, AWS, Snowflake, Redshift, S3, Lambda, Spark, Hadoop
Show more
Show less","Python, SQL, PostgreSQL, Big Data, AWS, Snowflake, Redshift, S3, Lambda, Spark, Hadoop, Cloudnative environment, Big Data systems","python, sql, postgresql, big data, aws, snowflake, redshift, s3, lambda, spark, hadoop, cloudnative environment, big data systems","aws, big data, big data systems, cloudnative environment, hadoop, lambda, postgresql, python, redshift, s3, snowflake, spark, sql"
Principal Data Engineer - Green Energy,Huxley,"Boston, MA",https://www.linkedin.com/jobs/view/principal-data-engineer-green-energy-at-huxley-3779831166,2023-12-17,Beverly,United States,Mid senior,Hybrid,"Principal Data Engineer opportunity with a global green energy company that has a great mission and is a leader in the space! The company is looking to expand and therefore the demand for more big data solutions means they are looking for a Principal Data Engineer to lead their team from the front.
The role is based in Boston and offers a hybrid 3 day in office work life.
Key Skills/ Requirements
Strong Python Programming skills, building highly scalable data software
6+ years industry experience in Data or Software Engineering
AWS and Postgres experience to build new big data solutions
Data Integration experience working with large volumes of data.
Data Lake experience
If you are interested in this opportunity or would like to learn more please submit your resume or feel free to contact Frazer Spackman.
Show more
Show less","Python, Data Software Development, AWS, Postgres, Data Integration, Data Lake","python, data software development, aws, postgres, data integration, data lake","aws, data integration, data lake, data software development, postgres, python"
Principal Snowflake Data Engineer,Motion Recruitment,"Boston, MA",https://www.linkedin.com/jobs/view/principal-snowflake-data-engineer-at-motion-recruitment-3760765752,2023-12-17,Beverly,United States,Mid senior,Hybrid,"Are you a skilled Data Engineer seeking an exciting opportunity to work on cutting-edge projects? Our client, located in Boston, is actively looking to hire a full-time Principal Data Engineer. Our client is a leading organization in the [Industry] sector, committed to leveraging the latest technology for data analytics and reporting. In this role, you'll be at the forefront of innovation, working with technologies such as Snowflake and Tableau.
Second Paragraph
Here's what makes this opportunity stand out: our client is seeking a data expert like you to become their Principal Data Engineer, leading the charge in designing and implementing best-in-class analytics solutions. This is not just a job; it's an opportunity for personal and professional growth. You'll be at the helm of a dynamic team, mentoring junior members, and collaborating on exciting, innovative projects that make a real impact in the financial services domain. With a hybrid work environment, you can enjoy flexibility and work-life balance, with in-office work on Tuesdays, Wednesdays, and Thursdays and remote work availability on Mondays and Fridays.
Required Skills & Experience
Bachelor's or Master's degree in computer science or related field
5+ years of software and data engineering experience
Desired Skills & Experience
Strong implementation experience with Snowflake ecosystem
Expertise in modeling and building data warehouses
Proficiency in Snowflake using warehouses, stored procedures, streams, snow pipes, tasks, stages, and more
Experience with ETL/ELT technologies, such as Informatica, Snaplogic
Cloud-native application development using AWS or Azure
Hands-on experience with scheduling tools like Airflow, Control M, Autosys
Excellent SQL skills and experience with BI tools like Tableau or Power BI
Leadership experience and Agile methodologies proficiency
Strong API knowledge (REST, API Gateway)
Outstanding technical and analytical skills in financial services domain
Tech Breakdown
What You Will Be Doing:
100% Python
Daily Responsibilities
100% Hands-On
100% Team Collaboration
The Offer
Bonus OR Commission eligible
Medical, Dental, and Vision Insurance
Vacation Time
Stock Options
If you're an ambitious data engineer with a passion for innovation and a desire to be a key player in an exciting organization, don't miss this opportunity. Our client welcomes applicants authorized to work in the US on a full-time basis and offers an environment that fosters growth, learning, and work-life balance. Join us and make a meaningful impact in the world of data engineering.
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Austin Getzloff
Show more
Show less","Data Engineering, SnowFlake, Tableau, SQL, Airflow, Control M, Autosys, Python, Cloudnative application development, AWS, Azure, Informatica, Snaplogic, BI, REST, API Gateway, Agile","data engineering, snowflake, tableau, sql, airflow, control m, autosys, python, cloudnative application development, aws, azure, informatica, snaplogic, bi, rest, api gateway, agile","agile, airflow, api gateway, autosys, aws, azure, bi, cloudnative application development, control m, data engineering, informatica, python, rest, snaplogic, snowflake, sql, tableau"
Principal Data Platform Engineer,CarGurus,"Cambridge, MA",https://www.linkedin.com/jobs/view/principal-data-platform-engineer-at-cargurus-3366858682,2023-12-17,Beverly,United States,Mid senior,Hybrid,"Who We Are
At CarGurus (NASDAQ: CARG), our mission is to give people the power to reach their destination. We started as a small team of developers determined to bring trust and transparency to car shopping. Since then, our history of innovation and go-to-market acceleration has driven industry-leading growth. In fact, we’re the largest and fastest-growing automotive marketplace, and we’ve been profitable for over 15 years.
What We Do
The market is evolving, and we are too, moving the entire automotive journey online and guiding our customers through every step. That includes everything from the sale of an old car to the financing, purchase, and delivery of a new one. Today, tens of millions of consumers visit CarGurus.com each month, and ~30,000 dealerships use our products. But they're not the only ones who love CarGurus—our employees do, too. We have a people-first culture that fosters kindness, collaboration, and innovation, and empowers our Gurus with tools to fuel their career growth. Disrupting a trillion-dollar industry requires fresh and diverse perspectives. Come join us for the ride!
Role Overview
The Data Engineering team is looking for a highly motivated Principal Data Pipeline Engineer to evolve our modern data warehousing platform to world-class levels! As CarGurus enters the next phase of dramatic growth with new data sources, new business lines, and business acquisitions, we are looking for a candidate who is interested in revamping how we do data at the company. You will act as a technical mentor for the other data engineers in the team.
What You'll Do
Gain a deep understanding of our evolving data and analytics stack and provide guidance in enhancing those
Define and drive tactical decisions around Data Platform domain while providing input into larger strategic decisions around data infrastructure, data quality, data observability, and data governance
Build robust and scalable data integration (ETL/ELT) pipelines using Python, SQL and Airflow
Build and deliver high quality data architecture to support our team of Analytics Engineers, Data Analysts, Data Scientists, and various stakeholders throughout the company
Be a technical mentor to other data and sr data engineers and expose the team to new technologies and techniques
Maintain the data infrastructure layer using IaaS and keep enhancing the framework.
What You'll Bring
7+ years of experience in data engineering, designing and building scalable and reliable data pipelines and infrastructure
Extensive knowledge in programming languages like Python
Advanced knowledge of SQL, Snowflake preferred
Expert in system architecture – with the ability to understand the context of a business need and build a system that addresses it
3+ years experience with cloud-based data platforms like AWS or GCP
Excellent problem-solving and communication skills, and ability to work collaboratively with cross-functional teams including product managers, data scientists, and software engineers
Proficient with data streaming frameworks like Kafka or Kinesis
Extensive knowledge of working with workflow management tools like Airflow.
Proficiency in performance tuning the ETLs/ELTs and database solutions for large data sets
Extensive knowledge in data governance.
Proficiency with modern Agile development methodologies.
Extensive experience in requirement gathering, analysis, design, code review, and unit and integration testing
Deliver maintenance and improvements for existing applications.
The ability to be an exceptional team player, act as a technical mentor, interested in sharing knowledge with other team members, and passionate about learning new technologies
Experience on managing and maintaining the data infrastructure with the use of Terraform, Cloudform, Ansible or other tools.
Experience in building monitoring and alerting techniques.
Nice to have
Experience in managing and maintaining warehouses in Snowflake.
Experience working with NoSQL databases.
Worked building event driven streaming pipelines.
Familiarity with dbt.
Working at CarGurus
We reward our Gurus’ curiosity and passion with best-in-class benefits and compensation, including equity for all employees, both when they start and as they continue to grow with us. Our career development and corporate giving programs, as well as our employee resource groups (ERGs) and communities, help people build connections while making an impact in personally meaningful ways. A flexible hybrid model and robust time off policies encourage work-life balance and individual well-being. Thoughtful perks like daily free lunch, a new car discount, meditation and fitness apps, commuting cost coverage, and more help our people create space for what matters most in their personal and professional lives.
We welcome all
CarGurus strives to be a place to which people can bring the ultimate expression of themselves and their potential—starting with our hiring process. We do not discriminate based on race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation. We foster an inclusive environment that values people for their skills, experiences, and unique perspectives. That’s why we hope you’ll apply even if you don’t check every box listed in the job description. We want to know what only
you
can bring to CarGurus.
Show more
Show less","Python, SQL, Snowflake, Airflow, Kafka, Kinesis, Terraform, Cloudform, Ansible, Agile, ETL, ELT, dbt","python, sql, snowflake, airflow, kafka, kinesis, terraform, cloudform, ansible, agile, etl, elt, dbt","agile, airflow, ansible, cloudform, dbt, elt, etl, kafka, kinesis, python, snowflake, sql, terraform"
"Senior Data Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Fort Collins, CO",https://www.linkedin.com/jobs/view/senior-data-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783185446,2023-12-17,Fort Collins,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Senior Data Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-FortColl-SeniorDataAnal.002
Show more
Show less","Python, JavaScript, JSON, OOP, Generative AI, Data Science, Machine Learning, Education Technology, EdTech, Natural Language Processing, NLP, AI Prompts, Data Analytics, Research, Product Development, Product Engineering, Software Engineering, Git, SQL, NoSQL, Cloud Computing","python, javascript, json, oop, generative ai, data science, machine learning, education technology, edtech, natural language processing, nlp, ai prompts, data analytics, research, product development, product engineering, software engineering, git, sql, nosql, cloud computing","ai prompts, cloud computing, data science, dataanalytics, edtech, education technology, generative ai, git, javascript, json, machine learning, natural language processing, nlp, nosql, oop, product development, product engineering, python, research, software engineering, sql"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Fort Collins, CO",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783184520,2023-12-17,Fort Collins,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-FortColl-DataScientist.004
Show more
Show less","Python, JavaScript, JSON, R, OOP languages, Data science, Generative AI, AI training models, Algorithms, Natural language processing, Machine learning, Data analytics, Communication skills, Project management, Time management, Problemsolving skills, Research skills","python, javascript, json, r, oop languages, data science, generative ai, ai training models, algorithms, natural language processing, machine learning, data analytics, communication skills, project management, time management, problemsolving skills, research skills","ai training models, algorithms, communication skills, data science, dataanalytics, generative ai, javascript, json, machine learning, natural language processing, oop languages, problemsolving skills, project management, python, r, research skills, time management"
Senior Data Engineer,"Geo Owl - GIS, Geospatial, and UAS Experts","Georgia, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-geo-owl-gis-geospatial-and-uas-experts-3767911467,2023-12-17,Macon,United States,Mid senior,Onsite,"Geo Owl is currently looking for a motivated and qualified Senior Data Engineer to support our Department of Defense contract opportunity. To be qualified, you need knowledge of Army structure and defense level intelligence, intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products, and meet the requirements listed below. If interested, apply now, or contact one of our recruiters. Location: Fort Gordon, GA Clearance: TS/SCI Requirements: Must meet all the requirements listed below. Excellent written & oral communication, research, and analytic skills Expert ability to manage personnel, requirements, and coordination of projects Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions Desired Requirements: Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products Knowledge and experience with intelligence operations and in assisting with drafting expert assessments across operations priorities on behalf of the stakeholder Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc) Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community; knowledge of private sector data science/analytics, machine learning, and data visualization communities Education Requirements: MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND
2 years
CURRENT Intelligence Analysis experience;
OR BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND
5 years
CURRENT Intelligence Analysis experience;
OR Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics, Statistics, or related field AND at least
10 years of Intelligence Analysis experience
Benefits: Health Insurance (Geo Owl pays 80%+ of the premium). 401k matching. Dental, Vision, and other supplemental insurance plans available. Company-paid short-term and long-term disability and life insurance. Peer-to-Peer spot bonuses. 120 hours of PTO per year plus federal holidays. Joining the Geo Owl Team | What to Expect At Geo Owl, we highly value our team members. We offer challenging but rewarding opportunities for those who want to work hard to provide a great experience for the customer and strive to reach their professional goals. As a member of the Geo Owl family, you will be working alongside people who share this work ethic and are aiming to be the best partner for our customer. We are all proud to be a part of this company and we want you to be too. Our Mission
Provide high quality solutions to our mission partners in the United States through our expert analysts.
Be recognized as the best at what we do by our customers.
Be a team our team members are proud and excited to be a part of.
Continually strive for excellence and seek to tackle the most difficult challenges our industry has to offer. About Us Geo Owl is a premiere provider of Full-Motion Video (FMV), Geospatial, ISR, Intelligence and IT services to the Department of Defense and Intelligence Community. We are vitalized by our engaged team of professionals that truly value each other and the important missions we support. Equal Opportunities Geo Owl is an equal opportunity employer and does not discriminate on the basis of race, color, religion, creed, sex, age, sexual orientation, national origin, disability, marital status, military status, genetic predisposition, or any other basis protected by law. To stay up to date about new career opportunities: Follow us on
Twitter Follow us on
Instagram Follow us on
LinkedIn
Powered by JazzHR
Show more
Show less","SAS, R, Java, C, MATLAB, Scala, Python, Pandas, Scikit, TensorFlow, Gensim, SQL, Machine learning, Data science, Data visualization, Data analytics","sas, r, java, c, matlab, scala, python, pandas, scikit, tensorflow, gensim, sql, machine learning, data science, data visualization, data analytics","c, data science, dataanalytics, gensim, java, machine learning, matlab, pandas, python, r, sas, scala, scikit, sql, tensorflow, visualization"
Senior Data Protection Operations Engineer,BECU,"Georgia, United States",https://www.linkedin.com/jobs/view/senior-data-protection-operations-engineer-at-becu-3749162582,2023-12-17,Macon,United States,Mid senior,Remote,"As the nation's largest community credit union, we begin every day focused on delivering superior financial products and services for our 1.3 million members and more than $30 billion in managed assets. Our work has an economic impact as we support our members' financial goals. We are unapologetic about being devoted to our members and the communities we serve. Our business is guided by our people helping people philosophy – which includes our team members.
BECU has been in business for more than 85 years, driven by unwavering core values and a dedication to improving the communities we serve. While we have a rich history, the future of our company, accelerated by business and technology transformation, is even brighter. There's never been a better time to work for BECU.
To learn more visit becu.org/careers.
PAY RANGE
The Target Pay Range for this position is $130,200-$159,200 annually. The full Pay Range is $101,100-$188,300 annually. At BECU, compensation decisions are determined using factors such as relevant job-related skills, experience, and education or training. Should an offer for employment be made, we will consider individual qualifications. In addition to your salary, compensation incentives are available for the hired applicant. Incentives are performance based and targets vary by role.
Benefits
Employees and their eligible family members have access to a wide array of employee benefits, such as medical, dental, vision and life insurance coverage. Employees have access to disability and AD&D insurance. We also offer health care and dependent care flexible spending accounts, as well as health savings accounts, to eligible employees. Employees are able to enroll in our company’s 401k plan and employer-funded retirement plan. Newly hired employees accrue 6.16 hours of paid time off (PTO) on a per pay period basis based on hours worked (up to a maximum of 160 PTO hours per year) and receive ten paid holidays throughout the calendar year. Additional details regarding BECU Benefits can be found here.
Impact You’ll Make
Are you looking to take your data protection career to a new level? Monitor. Detect. Analyze. Could you be our next guardian on our forefront team of defense as a Senior Data Protection Operations Engineer? In this role, you'll safekeep our digital realms, proactively monitoring and swiftly analyzing events in real time to keep our data fortress unbreachable.
You’ll use our tools and craft scripts crucial in rapid threat detection and response. You'll also sculpt and maintain systems ensuring top-tier data protection. Collaborate with fellow tech enthusiasts, sharing knowledge and elevating our protective expertise. You’ll maintain the defenses that will shape the future of our data safety.
What You’ll Do
Compliance Captain & Relationship Builder:
Uphold BECU standards and stay compliant while handling data protection. Collaborate and guide others, such as data owners and data analysts. Use your exceptional problem solving and interpersonal skills to effectively communicate the complex in simple terms as you provide customer service.
Design Lead:
Take the lead in shaping and rolling out data protection patterns for vendor solutions, ensuring sensitive info is always safeguarded. Stay updated on tech threats and emerging security trends. Review existing cloud security architecture and recommend improvement road map.
Systems Steward & Data Privacy:
Seamlessly manage and recover systems that track sensitive data, focusing on a user-friendly experience. Champion for enhancing our data privacy practices, boosting team efficiency in the process.
Risk Analyst & Incident Partner:
Dive deep into risk assessments, considering asset value, potential threats, and existing controls. Engage in incident responses, directing vital alerts to the Security Operations Center.
Tech Advisor & Vendor Liaison:
Guide Tech teams on best practices and tools within our tech stack to shield crucial info. Quick learner, Adjunct IT professor, team leadership experience, Gov tenant. Bridge the gap with our data protection vendors, ensuring we're always in good hands.
And yes, there might be a few extra tasks thrown in, and maybe some afterhours work because cybersecurity is always an ongoing threat, but you've got this! If this sounds like the next step in your career, let’s connect.
Basic Qualifications
Bachelor’s degree in a relevant technology or business management area or equivalent work experience.
Minimum of seven years working with security tools and/or information technology operations
Minimum of four years working with data tools, including performing deployment and configuration, and maintaining operations and content development
Implementation knowledge of Microsoft defender for endpoint, Microsoft defender for Identity, Microsoft, cloud App Security, Microsoft defender for O365
Preferred Qualifications
Advanced scripting skills (e.g., PowerShell, Python, JAVA) preferred.
Configured Microsoft Cloud App Security (MCAS) for customer cloud workload discovery, MCAS integrations with AWS, ServiceNow, Workday, Data loss prevention policies and integrated with AIP labels, Office 365 Advanced threat protection, Azure logs for AIP label analytics, and endpoint solutions for antivirus, antimalware, host-based intrusion prevention, and EDR.
Cloud security, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Identity mgmt., Azure MFA,
eDiscovery, Compliance experience
Perform hands-on activities involved in creating cloud infrastructure components.
Deployed AIP/MIP labels using Crawl, Walk, Run
Deployed and configured on-premises AIP scanner to discover data
High level knowledge and configuration of Microsoft Defender ATP, Microsoft Threat Experts, EDR
Advanced understanding of Cyber Security Operations (monitoring, detection, incident response, forensics) required.
Advanced scripting skills (e.g., PowerShell, Python, JAVA)
Knowledge of TCP/IP and OSI network protocol stack required.
EEO Statement
BECU is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Show more
Show less","Cybersecurity, Data protection, Compliance, Risk assessment, Incident response, Vendor management, Microsoft defender for endpoint, Microsoft defender for Identity, Microsoft cloud App Security, Microsoft defender for O365, PowerShell, Python, JAVA, Microsoft Defender ATP, Microsoft Threat Experts, EDR, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Identity mgmt., Azure MFA, eDiscovery, Cloud infrastructure, AIP/MIP labels, Onpremises AIP scanner, TCP/IP, OSI network protocol stack","cybersecurity, data protection, compliance, risk assessment, incident response, vendor management, microsoft defender for endpoint, microsoft defender for identity, microsoft cloud app security, microsoft defender for o365, powershell, python, java, microsoft defender atp, microsoft threat experts, edr, azure sentinel, splunk, cyberark, iaas, paas, identity mgmt, azure mfa, ediscovery, cloud infrastructure, aipmip labels, onpremises aip scanner, tcpip, osi network protocol stack","aipmip labels, azure mfa, azure sentinel, cloud infrastructure, compliance, cyberark, cybersecurity, data protection, ediscovery, edr, iaas, identity mgmt, incident response, java, microsoft cloud app security, microsoft defender atp, microsoft defender for endpoint, microsoft defender for identity, microsoft defender for o365, microsoft threat experts, onpremises aip scanner, osi network protocol stack, paas, powershell, python, risk assessment, splunk, tcpip, vendor management"
Senior Data Analyst,ClickJobs.io,"Creve Coeur, MO",https://www.linkedin.com/jobs/view/senior-data-analyst-at-clickjobs-io-3788843900,2023-12-17,Defiance,United States,Mid senior,Onsite,"Job no: 492867 Work type: Full-time Location: Remote Categories: Information Technology, Project Management Posting Details Job Summary/Basic Function The Senior Data Analyst will lead and manage all external reporting and survey completion activities. This position will develop processes and queries to streamline and automate all external surveys and reporting requirements. This position will help ensure the institution is in compliance with all federal, state, and local reporting requirements and all national reports and surveys. The position will assist the school and colleges in academic accreditation reporting. The position will develop institutional strategic reports and provide support to subject matter experts (SMEs) in building out data models, reports and dashboards. This position will be key in training and upskilling SMEs in dashboard creation and report writing. The position will partner with data engineering in ensuring the data infrastructure is well architected for current and historic analysis and will aid in data slice prioritization for reporting and compliance. This position will partner with data stewards across the institution to define, catalog, validate, and audit the data within the customer data platform (CDP), and master data management (MDM) system for more efficient reporting, analysis, and to continually expand the institution’s single source of truth and underlying documentation. The position will partner with SMEs in data quality initiatives to help ensure data integrity in the CDP and MDM and in operational systems as well. This position will collaborate with SMEs and appropriate departments to identify potential areas to grow data collection efforts, eliminate data silos, and provide recommendations for changes to better respond to survey and reporting requirements when appropriate. Essential Responsibilities:
Lead all institutional external reporting activities, including preparation of required federal, state, and national reports/surveys, census, and university fact book. Partner with schools and colleges for accreditation reporting. Ensure accuracy, timeliness, and compliance with reporting requirements.
Partner with data stewards in academic and institutional departments to systematically collect, catalog, and warehouse data in the master data management system. This position will train and upskill data stewards in dashboards and reporting.
Create new data modeling and querying processes from the data lake / warehouse to automate the completion of external surveys for more efficient and accurate reporting. Collaborate with data engineering to help ensure the data architecture is built for current and historical analysis and reporting.
This position will collaborate with data stewards and appropriate departments to identify potential areas to grow data collection efforts, eliminate data silos, and provide recommendations for changes in order to better respond to survey and reporting requirements when appropriate.
Working with a team of people to ensure data integrity and quality, develop and maintain a dictionary of common data definitions, and ensure institutional data collection and reporting processes are reliable, valid, and ethical. Please note, due to the nature of this work, the successful candidate will need to possess experience in higher education institutional research data and/or higher education data. See below qualifications for additional details. Minimum Qualifications
5+ years previous experience in higher education institutional research including experience with federal, state, and national reporting and surveys for higher education, such as IPEDS, Common Data Set, US News, Petersons, etc.
Demonstrated experience in creating automation scripts and queries to streamline survey and reporting completion
10+ years demonstrated proficiency with Excel, including creating complex equations, pivot tables, and advanced visualizations
5+ years with data querying in SQL Server or similar relational database
3+ years dashboarding and visualization experience with Tableau or other dashboarding tool
Ability to generate the most appropriate visualizations, dashboards, and KPIs for data storytelling and to solve strategic business questions
Ability to create data models using MDX
Ability to understand complex processes, anticipate how changes impact other areas, and make appropriate suggestions and create changes for process improvements where appropriate
Strong interpersonal skills and ability to collaborate effectively with stakeholders and colleagues
Strong experience in data storytelling and clearly articulating findings and results to decision makers
Ability to document and validate data catalog definitions and collaborate with various constituents on change management Preferred Qualifications
3+ years experience with Python or R programming languages (or similar programming language) and their associated visualization and data wrangling libraries (e.g. dplyr, ggplot2, tidyverse, matplotlib, pandas, numpy, seaborn, etc)
Experience with data lake and data warehouse applications in Amazon Web Services in particular: Amazon RedShift, S3, and AWS Glue
Experience with Mulesoft or other data integration tools
Experience writing quality assurance rules with Drools (or other Business Rules Management System (BRMS))
Experience migrating / converting dashboards from Power BI to Tableau
Experience with master data management / data catalog applications (e.g. Informatica MDM, EBX, etc)
Knowledge of statistical and descriptive analysis methods Physical Demands Open Until Filled Yes Special Instructions to Applicants An offer of employment is contingent upon successful completion of a background screening. Applicants requiring University sponsorship to obtain employment authorization will not be considered for this position. Maryville University is committed to a policy of equal opportunity and prohibits discrimination on the basis of age, disability, gender, genetic information, marital status, national origin, race/color, religion, sex, sexual orientation, veteran status, or any other status protected by law. This extends to all aspects of the employment relationship, including recruiting, hiring, training, on-the-job treatment, promotion, layoff, and termination. Advertised: November 29, 2023 Applications close: Open until filled
Show more
Show less","Data Analysis, Reporting, Survey Design, Data Integration, Data Warehousing, Data Quality, Data Visualization, SQL, Tableau, Python, R, AWS, Amazon RedShift, S3, AWS Glue, Drools, Power BI, Informatica MDM, Data Governance, Data Dictionary, Dashboarding, Data Modeling, Data Migration, Master Data Management, Data Catalog, Data Wrangling, Data storytelling, Statistical Analysis","data analysis, reporting, survey design, data integration, data warehousing, data quality, data visualization, sql, tableau, python, r, aws, amazon redshift, s3, aws glue, drools, power bi, informatica mdm, data governance, data dictionary, dashboarding, data modeling, data migration, master data management, data catalog, data wrangling, data storytelling, statistical analysis","amazon redshift, aws, aws glue, dashboard, data catalog, data dictionary, data governance, data integration, data migration, data quality, data storytelling, data wrangling, dataanalytics, datamodeling, datawarehouse, drools, informatica mdm, master data management, powerbi, python, r, reporting, s3, sql, statistical analysis, survey design, tableau, visualization"
Senior Data Analyst- Deposits,Bank OZK,"Little Rock, AR",https://www.linkedin.com/jobs/view/senior-data-analyst-deposits-at-bank-ozk-3768671421,2023-12-17,Little Rock,United States,Mid senior,Onsite,"Job ID: 13407
Position Description
Job Purpose and Scope:
Serves as the key deposits data expert within the organization, and provides thought leadership on deposit pricing, campaign and marketing initiatives, customer and product analytics, and sales reporting requirements.
Essential Job Functions
Analyzes product, customer, pricing, financial, sales, and deposit data to evaluate trends and synthesize information.
Collaborates with technology partners to develop and enhance the Deposit Management team’s data capabilities.
Interprets and provides recommendations for performance and profitability enhancements.
Oversees regular and ad-hoc analytical efforts for the Bank’s Deposit Management team.
Conducts research on the Bank’s various and wide range of products and services.
Analyzes pricing tests to determine customer response to different initiatives and strategies.
Develops regular reporting to monitor performance of the Bank’s deposit portfolios.
Partners with various teams across the Bank to coordinate report distribution and knowledge sharing across the organization, including to key stakeholders for executive meetings.
Presents information and analysis to various working groups, councils, committees, and executive management as needed.
Regularly exercises discretion and judgment in the performance of essential job functions.
Maintains good punctuality and attendance to work.
Follows Bank policy, procedures and guidelines.
Knowledge, Skills & Abilities
Knowledge of relational databases.
Ability to analyze complex financial data and turn it into actionable analysis, insights, and strategic recommendations.
Ability to communicate effectively, both verbally and in writing.
Ability to demonstrate business acumen and attention to detail.
Ability to create and own reports distributed to large groups throughout organization.
Ability to work with various cross-functional teams in a rapidly moving and changing work environment.
Ability to demonstrate initiative in managing work and accomplishing objectives.
Ability to demonstrate excellent and effective customer service skills.
Skill in using computers, including Microsoft Word, Excel and PowerPoint.
Equipment Used In Job Performance/Working Environment
Computer
Telephone
Multi-function device
Major Job Demands (Physical/Mental)
Decision-making
Problem-solving
Note:
This description is not an exhaustive list of all job functions, duties, skills and job standards required. Other job functions, duties, skills, and standards may be added. Management reserves the right to add or change the job requirements at any time.
Position Requirements
Basic Qualifications:
Bachelor’s degree in finance, marketing, economics, business, computer science, statistics, or related field, or commensurate work experience, required.
MBA or Master’s degree in a quantitative field preferred.
Minimum three (3) years of work experience in a pricing, modeling, database, programming, or analytics role, or commensurate postsecondary academic experience required..
Minimum two (2) years of experience with SAS EG, Base SAS, SQL, or a scripting language (e.g., R, Python) in a professional or academic setting required.
Experience with SAS Visual Analytics or Power BI preferred.
Bank OZK is an equal opportunity employer and give consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, disability status, protected veteran status, or any other characteristic protected by federal, state, and local law. Member FDIC.
Show more
Show less","Relational databases, Financial data analysis, Data visualization, SAS EG, SAS Base, SQL, Python, SAS Visual Analytics, Power BI, Microsoft Word, Excel, PowerPoint","relational databases, financial data analysis, data visualization, sas eg, sas base, sql, python, sas visual analytics, power bi, microsoft word, excel, powerpoint","excel, financial data analysis, microsoft word, powerbi, powerpoint, python, relational databases, sas base, sas eg, sas visual analytics, sql, visualization"
Senior Data Engineer,Globant,"Arkansas, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-globant-3715675144,2023-12-17,Little Rock,United States,Mid senior,Hybrid,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.
Right now, we are looking for
Data Engineers
to join our team at Globant!
You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.
What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh
At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.
Are you ready?
Job Segment:
Data Analyst, Data Modeler, Database, Cloud, SQL, Data, Technology
Show more
Show less","Data Engineering, Data Platforms, Data Pipelines, Data Extraction, Data Transformation, Data Loading, Data Analysis, Data Modeling, Data Quality, Data Profiling, Data Cleansing, SQL, NoSQL, MongoDB, CosmosDB, Azure Databricks, Azure Data Factory, Azure SQL, Azure Synapse, Java, Python, Scala, Spark, Snowflake, AWS Cloud, Data Mesh, Data Analyst, Data Modeler, Database, Cloud","data engineering, data platforms, data pipelines, data extraction, data transformation, data loading, data analysis, data modeling, data quality, data profiling, data cleansing, sql, nosql, mongodb, cosmosdb, azure databricks, azure data factory, azure sql, azure synapse, java, python, scala, spark, snowflake, aws cloud, data mesh, data analyst, data modeler, database, cloud","aws cloud, azure data factory, azure databricks, azure sql, azure synapse, cloud, cosmosdb, data engineering, data extraction, data loading, data mesh, data modeler, data platforms, data profiling, data quality, data transformation, dataanalytics, database, datacleaning, datamodeling, datapipeline, java, mongodb, nosql, python, scala, snowflake, spark, sql"
Staff Data Engineer,Recruiting from Scratch,"Germantown, WI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399207,2023-12-17,Germantown,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data classification, Data retention, TDD, Pair Programming, Continuous Integration, Agile engineering practices","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, data classification, data retention, tdd, pair programming, continuous integration, agile engineering practices","agile engineering practices, airflow, continuous integration, data classification, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Germantown, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828483,2023-12-17,Germantown,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Germantown, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744396123,2023-12-17,Germantown,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Legal Compliance, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Germantown, WI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709537,2023-12-17,Germantown,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML pipelines, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Legal compliance, Data management tools, Data classification, Retention, 401K plan, Company bonus, Equity programs, Genderaffirming offerings, Included Health, Monthly stipends for HRT, Flexible vacation policy, Monthly stipends for cell phone internet wellness and food, Homeoffice setup stipend","data engineering, ml pipelines, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management tools, data classification, retention, 401k plan, company bonus, equity programs, genderaffirming offerings, included health, monthly stipends for hrt, flexible vacation policy, monthly stipends for cell phone internet wellness and food, homeoffice setup stipend","401k plan, airflow, applied machine learning, aws, azure, bash, company bonus, data classification, data engineering, data management tools, docker, dynamodb, equity programs, etl, flexible vacation policy, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, included health, java, kafka, kubernetes, legal compliance, ml pipelines, monthly stipends for cell phone internet wellness and food, monthly stipends for hrt, python, retention, snowflake, spark, sparkstreaming, sql, storm"
Project Data Analyst,Fessler & Bowman,"Flushing, MI",https://www.linkedin.com/jobs/view/project-data-analyst-at-fessler-bowman-3782302826,2023-12-17,Flushing,United States,Mid senior,Onsite,"Founded in 1963 by Don Fessler & George Bowman, Fessler & Bowman, Inc. has grown from a residential flatwork & basement wall contractor to a top 100 Commercial Concrete Contractor recognized across the nation. Keeping customer satisfaction as our fundamental core value has allowed Fessler & Bowman, Inc. to prosper for nearly six decades.
Our Culture & Philosophy:
Fessler & Bowman is a fast-paced and culture-focused industry leader in specialty concrete and civil earthwork. When we talk about culture at F&B, it is not just a plaque hanging in our lobby. Our values are our driving force and have allowed us to build a phenomenal team delivering best-in-class products to our customers.
Open & Constructive
Take Pride in Our Product
Relentless Commitment
Care About Our Customers
Team Success
Summary:
The Project Data Analyst is an integral part of Fessler & Bowman’s Estimating function. Working collaboratively with Project Management, Estimating, Field, and Finance Teams, the Project Data Analyst will ensure projects perform as expected and are accurately documented. This role will also analyze past projects and available construction data to ensure the strategic execution of pre-construction bids. The descriptions below are representative of, but not limited to, the duties and responsibilities of this position.
Five Key Roles of the Project Data Analyst:
Data Analysis Accuracy:
Ensure that data analysis is precise and reliable, resulting in well-informed project decisions and improvements in project performance.
Project Efficiency:
Measure the efficiency of project coordination activities; assist in facilitating project handoff alongside Estimating, Project Management and Field Teams.
Quality Assurance Score:
Maintain regional metrics and evaluate the adherence to quality control measures, striving for the highest possible quality standards on all projects.
Cost Control Savings:
Monitor cost control measures and report on cost-saving opportunities that enhance the company's profitability.
Documentation Compliance:
Assess the completeness and accuracy of project documentation such as budget set up and production quantities and ensuring that all records adhere to industry standards.
Essential Duties & Responsibilities:
Clean up raw data through the collection, analysis, and interpretation of construction project data to identify trends, potential issues, and areas for improvement.
Create reports and dashboards to communicate project performance, cost analysis, and other relevant metrics to the management team.
Collaborate with estimators, project managers, subcontractors, and other stakeholders to ensure seamless project execution.
Assist in the development and maintenance of project schedules, historical costs, and resource allocation.
Track project costs, including materials, labor, and equipment expenses, and ensure adherence to budgetary constraints.
Identify cost-saving opportunities and propose recommendations to optimize project cost efficiency.
Identify inaccurate project records and take steps to correct them.
Establish and build project schedules during the estimating process to place crews in alignment with availability.
Maintain and modify accurate material and subcontracting costs.
Communicating and upholding the Company’s commitment to Integrity and Code of Ethics, as well as ensuring the same for the entire team.
Other relevant tasks as assigned
Education, Experience & Qualifications:
Bachelor's degree in construction management, civil engineering, data analysis, or a related field is preferred.
Prior experience in construction project coordination and data analysis is highly preferred.
Proficiency in data analysis tools and software, such as Excel, Tableau, or similar applications.
Strong organizational and communication skills with the ability to work collaboratively in a fast-paced construction environment.
Knowledge of construction industry standards, practices, and regulations preferred.
Proficiency in construction management software, including Procore, SalesForce, Bid2Win, Project Schedule, or similar scheduling tools and pdf programs.
Travel:
Travel to and from job sites and office locations may be required for this position. Fessler & Bowman will compensate for travel when applicable.
Work Environment:
As a Project Data Analyst, you will be subject to various work environments. These environments include office and field settings. Employee must be comfortable with sitting for an extended period
Benefits:
Fessler & Bowman is proud to provide eligible Team Members with the following benefits:
Medical, dental and vision insurance
401k with company contributions
Paid Holidays and Paid Time Off
Recruitment Process:
The recruitment process will include a phone screen, in-person meeting, and a pre-employment background check and drug test.
Fessler & Bowman is an Equal Opportunity Employer
Show more
Show less","Construction Project Coordination, Data Analysis, Procore, SalesForce, Bid2Win, Project Schedule, Excel, Tableau, pdf, Construction Management Software, Data Analysis Tools, Scheduling Tools","construction project coordination, data analysis, procore, salesforce, bid2win, project schedule, excel, tableau, pdf, construction management software, data analysis tools, scheduling tools","bid2win, construction management software, construction project coordination, data analysis tools, dataanalytics, excel, pdf, procore, project schedule, salesforce, scheduling tools, tableau"
Senior Data Analyst,Jam City,"Carlsbad, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-jam-city-3701056393,2023-12-17,Oceanside,United States,Mid senior,Hybrid,"As a leading mobile games developer, Jam City is looking to “level up” our talent. We’re searching for innovators who consider themselves dynamic, collaborative, and thrive in a fast-paced environment.
Perks & Benefits
Unlimited Vacation, Paid Sick Days, Recharge Days & Holidays
100% Employee Covered Medical, Dental, Vision Plan Base Plan
Life Insurance, 401k, Flexible Spending Accounts, Commuter Benefits & More
Happy Hours
Company Events
About The Role
Jam City is on the hunt for a Senior Data Analyst to work on one of our highly successful titles, Cookie Jam. You will be responsible for providing both reactive and original insight at a project and studio level.
The ideal candidate will be an expert in their field and have a solid understanding of the F2P market. This candidate applies and seeks out industry knowledge to develop creative approaches to analyzing data and analytics improvements.
Responsibilities
Apply industry knowledge to come up with analytical approaches to analyze data and make project and studio improvements based on statistical methods.
Extract complex data sets to understand game metrics in order to inform hypotheses, and create analytical experiments with clear and measurable success goals.
Create reports summarizing business intelligence data for review by executives and managers; collaborate with senior managers and decision makers to identify and solve a variety of problems to clarify management objectives.
Lead complex analytics projects applying mathematical approaches such as Multivariable Regression, Clustering, Time Series Forecast, etc.
Identify and propose improvements on analytics pipeline and operational process.
Work cross functionally with other teams to provide business insight for decision making.
Qualifications
5+ years of experience performing analysis, preferably in the mobile games industry.
Excellent quantitative and analytical skills; the ability to derive conclusions from data and clearly communicate findings is critical.
Highly proficient in SQL, must be able to write queries to extract raw data and also load data into Tableau for visualization.
Skillful in forecasting and manipulating data in Excel and/or Google Sheets.
Master degree in mathematics, computer science, business or related fields strongly preferred.
NICE TO HAVE
Passion for games and empathetic towards the players.
Curious, self-driven and thorough in analysis studies.
Performs well with ambiguous tasks and ability to breakdown complex problems into actionable recommendations.
In compliance with local law, we are disclosing the compensation for this role. The range listed is just one component of Jam City's total compensation package for employees, which may also include annual bonuses, short- and long-term incentives, and program-specific awards. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. Starting pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, certifications, location, business needs, or market demands. This range is based on a full-time position. This range may be modified in the future. In addition, Jam City provides a variety of benefits to employees, including but not limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan, life, disability, and accident insurance. Pay Range: $102,000— $150,00 salary per year.
OUR COMMITMENT TO EQUITY, DIVERSITY, & INCLUSION
We believe in creating games that unite people across the world and that showcase our commitment to providing an environment that is both inclusive and diverse for our players and employees. We strive to create a workforce that is reflective of our global player community as we know that we are stronger and better when we play together. To help promote an inclusive culture, we celebrate the visible and invisible diversity of our Jam Citizens through initiatives including Employee Resource Groups, cultural events, trainings, speaker series, and more.
Jam City is an equal opportunity employer. We enthusiastically accept our responsibility to make employment decisions without regard to race, age, sex (including pregnancy), national origin, ancestry, religion, ethnicity, marital, or domestic partnerships status, disability, genetic information (including the refusal to submit to genetic testing), predisposing genetic characteristics, military status, veteran status, domestic violence victim status, sexual orientation, gender identity or expressions, or any other classification protected by federal, state, and local laws. Our management is committed to following this policy with respect to hiring, placement, promotion, transfer, demotion, layoff, termination, recruiting, pay, and other forms of compensation, training, and general treatment during employment.
About Jam City
Jam City is an award-winning mobile entertainment studio providing unique and deeply engaging games that appeal to a broad, global audience.
Led by CEO Josh Yguado, former 20th Century Fox executive, Jam City is the creative powerhouse behind some of the highest-grossing and most enduring mobile games. Jam City’s global franchise Cookie Jam has generated more than half a billion dollars, and Panda Pop has more than 120 million downloads to date.
The company also is the go-to studio for Hollywood, having developed immersive, narrative-rich mobile games around iconic entertainment brands. The company’s popular RPG game Harry Potter: Hogwarts Mystery was the #1 game in more than 40 countries at its launch in April 2018.
Jam City has nine studios located in Los Angeles (HQ), Berlin, Buenos Aires, Bogotá, Burbank, Cedar Falls, San Diego, San Francisco, and Toronto.
Show more
Show less","Data Analysis, Data Visualization, Business Intelligence, Data Mining, Forecasting, Machine Learning, Statistics, Time Series Analysis, Clustering, Multivariable Regression, SQL, Tableau, Excel, Google Sheets, Mathematics, Computer Science, Business","data analysis, data visualization, business intelligence, data mining, forecasting, machine learning, statistics, time series analysis, clustering, multivariable regression, sql, tableau, excel, google sheets, mathematics, computer science, business","business, business intelligence, clustering, computer science, data mining, dataanalytics, excel, forecasting, google sheets, machine learning, mathematics, multivariable regression, sql, statistics, tableau, time series analysis, visualization"
Senior Data Engineer,Scout,"Hammersmith, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-scout-3766649874,2023-12-17,High Wycombe, United Kingdom,Mid senior,Hybrid,"🧑🏻‍💻 Senior Data Engineer
📍 Location: Hammersmith, London
🤸🏻‍♀️ Flexible working: 1-2 days per week in the office
Scout is proud to be partnered and embedded within
Otto Car 🚗, to
help support their growing UK Product and Engineering Team.
🌐 Who are Otto Car?
We're helping to change the fabric of how we move in our cities one car at a time. Otto Car is a fast growing fleet operator and enabler, helping passengers and drivers get around safely and sustainably. We help private hire drivers and Uber partners get on the road so they can achieve their goals.
🚗 Our Story
Otto Car started with just one car in a garage, 5 years ago. We're proud to say that we are now the biggest fleet of PCO cars in Europe, with over 5,000+ cars on the road in London alone, and for the last 3 years, one of the UK’s fastest growing companies! We've helped over 20,000 drivers get on the road, and the numbers are growing every day. Owning a car is often taken for granted, but for many people, the simple act of providing an affordable and sustainable means of car ownership represents the path to financial and social freedom!
💻 The Role
We are a passionate team with huge ambition. We're growing fast, and we want to build an inclusive, healthy and sustainable business. We are looking for a
Senior Data Engineer
that shares our passion for sustainable and safe mobility. You will be working with a team of specialists, leading the technology concept creation and delivery for autonomous fleet management applications, leveraging the latest AI technology, working to bring better mobility, safer and cleaner streets to our cities.
👀 You'll be a great fit, if you:
• Are passionate about sustainable mobility
• Are motivated to improve road safety, specifically by improving driver standards
• Aren’t afraid to share your opinion
• Like to figure out how to fix it, instead of who to blame
• Are down to earth, enjoy being part of something bigger
• Care about the environment
• Devote yourself to what you love, and never want to stop
💡 Your responsibilities:
• Design, build and maintain batch and real-time data pipelines in production
• Maintain and optimize the data infrastructure required for accurate extraction, transformation, and loading of data from a wide variety of data sources
• Develop ETL (extract, transform, load) processes to help extract and manipulate data from multiple sources
• Design, develop and maintain data models that support Otto Cars products, data and processes
• Automate data workflows such as data ingestion, aggregation, and ETL processing
• Prepare raw data in Data Warehouses into a consumable dataset for both technical and non-technical stakeholders
• Partner with data scientists, analysts and functional leaders in sales, marketing, and product to deploy machine learning models in production
• Build, maintain, and deploy data products for analytics and data science teams on cloud platforms, AWS
• Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures
• Monitor data systems performance and implement optimisation strategies
• Leverage data controls to maintain data privacy, security, compliance, and quality for allocated areas of ownership
• Analysing and finding any impacts that technical changes may have on business and technical processes
• Evaluating any project constraints and devising alternatives to reduce risks, whilst performing process re-engineering
• Being transparent with the team about challenges, failures, and successes
🤝 Experience required:
• Extensive experience as a Data Engineer
• Advanced SQL skills and experience with relational databases and database design
• Experience working with cloud Data Warehouse solutions, preferably AWS (e.g., Snowflake, Redshift, BigQuery, Azure, etc.)
• Experience working with AWS Cloud-based solutions
• Experience building and deploying machine learning models and reporting solutions in production
• Familiar with DataOps
• Strong proficiency in object-oriented languages: e.g. Python, Java, C++, Scala
• Strong proficiency in scripting languages like Bash
• Strong proficiency in data pipeline and workflow management tools (e.g., Airflow, Azkaban)
• Great numerical and analytical skills
• Strong project management and organisational skills
• Excellent problem-solving, communication, and organisational skills
• Proven ability to work independently and with a team
We are a small but passionate team delivering something completely revolutionary. We believe a great workplace is one that represents the wonderfully diverse world we live in. All you need is a passion for what you do, strong team spirit and a desire to be part of a fast-growing start-up in an inspiring space
📈 Working with us:
• You will join a passionate team with a high-performance but a friendly culture of open communication, collaboration and diversity
• You will have the chance to influence how we deliver our products and services from customer operations to design
• You will shape the technology roadmap and culture for our company for years to come
• Clear career progression and access to mentorship
We believe in embracing diversity in all forms and fostering an inclusive environment for all people to do their best work with us. We're an equal opportunity employer - Where all applicants will be considered for employment without attention to ethnicity, religion, sexual orientation, gender identity, family or parental status, national origin, veteran, neurodiversity or disability status. Diversity and inclusion are a priority for us – if we want to solve problems for people worldwide, our team has to represent our customers
If you require any reasonable adjustments during the application or interview process, please let us know and we’ll do our best to accommodate
Show more
Show less","SQL, Python, Java, C++, Scala, Bash, Airflow, Azkaban, DataOps, ETL, Data Warehousing, Machine Learning, AWS, Snowflake, Redshift, BigQuery, Azure, Data Pipelines, Data Modeling, Data Analytics, Data Science, Data Integration, Data Quality, Data Governance, Data Security, Data Privacy","sql, python, java, c, scala, bash, airflow, azkaban, dataops, etl, data warehousing, machine learning, aws, snowflake, redshift, bigquery, azure, data pipelines, data modeling, data analytics, data science, data integration, data quality, data governance, data security, data privacy","airflow, aws, azkaban, azure, bash, bigquery, c, data governance, data integration, data privacy, data quality, data science, data security, dataanalytics, datamodeling, dataops, datapipeline, datawarehouse, etl, java, machine learning, python, redshift, scala, snowflake, sql"
Data Engineer,Nominet,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-nominet-3768204137,2023-12-17,High Wycombe, United Kingdom,Mid senior,Hybrid,"Job description:
About the role
This is a hybrid role that will be based in Oxford 1-2 times per week.
Nominet is looking for a Data Engineer to join our team. This team is tasked with designing, building and operating our Threat Intelligence Platform. We’re making a difference by detecting, blocking, and analysing cyber security threats amongst billions of DNS queries. Our mission critical Protective DNS (PDNS) product and adherence to strict security and compliance controls mean you will have many challenging problems to solve.
As a Data Engineer you’ll utilise our Lakehouse platform to enhance our Threat Intelligence capabilities. You will be responsible for designing, developing, and maintaining scalable data pipelines and workflows on the Databricks platform.
The essential bits
You have demonstrable experience as a Data Engineer, and you can work confidently with AWS (this is what we’re using) or another cloud platform. You also have strong experience with Databricks.
About you and your experience
You have a strong problem-solving mindset, and experience as a Data Engineer managing end-to-end ETL pipelines using Databricks for seamless data extraction, transformation, and loading. Security is always a strong consideration throughout your work, and ideally you also have an interest in gaining exposure to cybersecurity/threat intelligence.
Show more
Show less","Data Engineering, AWS, Hadoop, Databricks, Data Pipelines, Software Development, Data Security, Cybersecurity, Threat Intelligence, ETL, Data Extraction, Data Transformation, Data Loading","data engineering, aws, hadoop, databricks, data pipelines, software development, data security, cybersecurity, threat intelligence, etl, data extraction, data transformation, data loading","aws, cybersecurity, data engineering, data extraction, data loading, data security, data transformation, databricks, datapipeline, etl, hadoop, software development, threat intelligence"
Lead Data Analyst,hackajob,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-analyst-at-hackajob-3784539660,2023-12-17,High Wycombe, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with
AWE
helping them to hire the best talent and build the future. To get the chance to get matched to this role and other similar roles, click on Apply to set up your free profile.
AWE plays a crucial role in the defense of the United Kingdom, by providing and maintaining the warheads for the country’s nuclear deterrent. They are a centre of scientific, engineering and technological excellence, with some of the most advanced research, design and production facilities in the world. Their unique expertise also assists the UK Government in developing and delivering a range of innovative and integrated support services, including national nuclear security and counter-terrorism solutions.
Role: Statistics Team Leader
AWE
(one of the best 25 big companies to work for in the UK)
offers an attractive array of benefits which include:
Time to recharge your batteries with 270 hours of annual leave (plus every other Friday off work)
Consideration for flexible working arrangements so that your work may fit in with your lifestyle. Just let us know on your application if you wish to work part time
Opportunities for Professional Career Development that include funding for the annual membership of a relevant professional body, access to mentors and training
Employee Assistance Programme and Occupational Health Services
A generous defined contribution Group Personal Pension (they will pay between 9% and 13% of your pensionable pay depending on your own contribution)
Life Assurance
Discounts – access to savings on a wide range of everyday spending
Special Leave Policy including paid time off for volunteering, public service (including reserve forces) and caring for your family
A host of voluntary & core benefits to suit your health and wellbeing – more information available on their careers site
Relocation package available (terms and conditions apply)
Potential opportunities to travel with work for conferences and collaboration
The Role:
This role would suit an experienced statistics professional who is looking to make the next step in their career and who has experience of liaising with customers and training/ mentoring of junior team members. The successful candidate would be responsible for functional management of the team, ensuring delivery of work to customers on-time and on-budget, leading development of best practises and methods, and engaging with other teams across AWE and their collaborators.
What you would be responsible for:
Management of team members welfare, performance, and development
Ensuring delivery of highly specialised technical work to customers
Developing team capability in terms of individual staff development as well as tools, methods, and processes
Building and maintaining links with other stakeholders across AWE, the UK enterprise, and international collaborators
Recruitment, coaching and mentoring of staff
Whilst not to be considered a check list, it would be beneficial if you could demonstrate some of the following:
Have an appropriate qualification in statistics/ mathematics/ data analysis or a related subject
Be able to demonstrate relevant experience within the public or private sector
Be a member of a relevant Professional Institution and be Chartered or close to achieving Charted status
Have experience of leading a team and/or mentoring & developing others
Excellent interpersonal skills, with an ability to interact with staff at all levels and specialisms
Expertise in a statistical analysis package (e.g. R, Python, MATLAB, etc)
Show more
Show less","Statistics, Data Analysis, Machine Learning, R, Python, MATLAB, Team Leadership, Mentoring, Project Management, Budget Management, Stakeholder Engagement, Recruitment, Coaching","statistics, data analysis, machine learning, r, python, matlab, team leadership, mentoring, project management, budget management, stakeholder engagement, recruitment, coaching","budget management, coaching, dataanalytics, machine learning, matlab, mentoring, project management, python, r, recruitment, stakeholder engagement, statistics, team leadership"
Data Architect m/f,HP,Greater Reading Area,https://uk.linkedin.com/jobs/view/data-architect-m-f-at-hp-3785306049,2023-12-17,High Wycombe, United Kingdom,Mid senior,Hybrid,"Who we are?
HP creates new possibilities for technology to have a meaningful impact on people, businesses, governments and society. The world’s largest technology company and ranked 10 on the Fortune 500 list for 2012, HP brings together a portfolio that spans printing, personal computing, software, services and IT infrastructure to serve more than 1 billion customers in over 170 countries on six continents.
HP invents, engineers, and delivers technology solutions that drive business value, create social value, and improve the lives of our clients. At HP, we know that our people and values are the most important elements in this success. And we want you to add your voice and imagination to our mission.
Diversity drives our innovation engine
Diverse teams create transformative solutions that better serve our customers and advance how the world works and lives.
We’re reinventing the standard for diversity and inclusion — in how we operate as a company and impact society. Diversity is embedded in all we do, and every HP employee at every level plays a part. By valuing differences, we engage top industry talent to drive our company’s long-term success.
What can we do together?
In the GTM advanced analytics COE, our mission is to deliver impact by building machine learning (ML) products to optimize pricing, marketing investments and provide guidance to sales and other HP teams.
We are looking for a Data Architect / Senior Data Engineer to join our data engineering team. If you are passionate about solving problems and making improvements through data and technology, we are eager to talk to you. The ideal candidate has both a willingness and desire to work in a dynamic environment, is able to apply Agile methodologies in day-to-day activities, and is a self-motivated developer who also enjoys working in a team environment. Our people have a creative, innovative, fun, and collaborative attitude, and are dedicated to creating new and valuable solutions for HP.
The data engineering architect applies developed subject matter knowledge to solve common and complex business issues within established guidelines and recommends appropriate alternatives.
Works on problems of diverse complexity and scope.
May act as a team or project leader providing direction to team activities and facilitates information validation and team decision making process.
Exercises independent judgment within generally defined policies and practices to identify and select a solution.
Ability to handle most unique situations.
May seek advice in order to make decisions on complex business issues.
Who you are?
3+ years of experience in software or data engineering architecture.
Proficient in data modeling, data integration and processing of structured and unstructured data.
Proficient in one or more programming languages (Python preferred).
Strong SQL proficiency (experience with NoSQL – advantage).
Experienced with Apache Spark / pyspark
Excellent communication skills; mastery in English and local language.
Ability to effectively communicate product architectures, design and change proposals.
Additional Preferred Qualifications Advantages
Databricks
Pandas
Familiar with best practices of the data and software engineering lifecycle and/or best practices of the above platforms and tools.
Dataiku DSS
Why joining HP France?
At HP France, you will be immersed in an inspiring workplace where all our people love to be, collaborate well together and where they can learn, grow and develop professionally as well as personally. We offer empowering responsibilities. You will have the opportunity to evolve through your commitment, your skills and your sense of initiative! This position can be the 1st step in a great career within our teams!
Joining us also means a great work-life balance and benefits such as meal vouchers, discounts on HP products and more! Driven by a strong corporate culture, our employees are involved in numerous initiatives through internal networks (Next Gen, Women@hp, workplace well-being team, ect.) and external volunteering (Nos Quartiers ont du Talent, Hour of Code).
HP is also committed to an equal opportunities policy, from recruitment to career development. Job offers are open to all, including people with disabilities.
Our promise: To create a diverse and inclusive working environment! So, what are you waiting for?
This role is based in Grenoble (38) within our international skills and expertise center. We offer a collaborative, stimulating and ultra-connected working environment in the heart of the mountains and just a 10 minute walk from the train station.
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Machine learning, Python, SQL, NoSQL, Spark, pyspark, Databricks, Pandas, Dataiku DSS, Data modeling, Data integration, Software architecture, Data engineering architecture, Agile methodologies","machine learning, python, sql, nosql, spark, pyspark, databricks, pandas, dataiku dss, data modeling, data integration, software architecture, data engineering architecture, agile methodologies","agile methodologies, data engineering architecture, data integration, databricks, dataiku dss, datamodeling, machine learning, nosql, pandas, python, software architecture, spark, sql"
Mobile Data Services Engineer,Randstad Sourceright,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/mobile-data-services-engineer-at-randstad-sourceright-3779848935,2023-12-17,High Wycombe, United Kingdom,Mid senior,Hybrid,"Job title
-
Principal Mobile Data Services Engineer
Location:
Slough office 2 days pw, remote 3 days pw
Hours:
normal work hours - some flexibility on hours is required, to get the job done
Contract length:
monthly rolling contract (likely to last for 6 -12 months)
Rate:
£373pd Umbrella - Inside IR35
Randstad Sourceright, a leading provider of RPO & MSP Recruitment Services is currently recruiting for a Principal Mobile Data Services Engineer to work for a leading mass media and telecoms organisation.
The Role:
To provide 3rd line support in an area which supports live operations for the company’s mobile database. Fault fixing on issues affecting customers, though this is not a customer facing role.
The majority of the role is on a helpdesk for dealing with all elements and packet core.
There may sometimes be requirements to aid the project delivery and aid preparations in the company’s delivery of the packet core.
Job Purpose:
This is a services and systems engineering role:
To protect the company’s revenue and brand name by providing 3rd Line Technical Support for Packet Core GPRS/3G/4G,5G + PCRF.
To Lead, coordinate, manage and investigate faults escalated to the MDS group, escalating where necessary to the equipment Vendors.
To provide workarounds and processes for 1st, 2nd level and field engineers. To manage, coach and mentor support engineers to increase their Technical and Managerial skills.
Responsibilities:
Provide Third Level Support for the NFVI infrastructure and Packet core applications detailed above – VM and Container based.
Provide Helpdesk Support for BAU cases from 1st/2nd line during the day on a rota basis
Taking responsibility, ownership and coordinating a team of engineers where necessary to fix faults that are raised against the supported elements, networks or applications, managing and driving through the resolution within the agreed SLA, following the escalation process where this is not possible
For service outages caused by a failure lead the investigation into the RCA producing the required reports and co-ordinating any fix activities required to prevent any further failures. Also providing assistance to other teams to identify root cause for failures on their supported elements where able.
Ensure all processes and procedures are documented and controlled in accordance with the Quality process.
Undertake technical coding, configuration and database related support tasks when requested of MDS other than day to day support problems.
This role will need to include prioritisation, team leading and progress reporting
Identify any inadequacies with existing infrastructure and processes and propose recommendations to improve the existing infrastructure and work practices.
To represent MDS in project definition workshops, Platform Change Control, Operational reviews and meetings with other departments as required.
To advise and participate during the introduction of new network elements and applications, including features liaising as necessary with other departments and Vendors, providing on-site technical support when required.
To provide operational consultancy to projects that are being worked on
To Safeguard the operational network and the customer experience at all times.
Key Skills and experience required:
Mobile telecoms provider background
Ability to lead on high priority incidents, represent the team on incident calls
Good stakeholder communications - able to converse with senior management
Ability to manage vendors to deliver results
3rd line support experience of end to end systems infrastructure in a Telecommunications environment.
Basic IP and net knowledge
Packet core knowledge
VoLTE and VoWIFI knowledge
Essential Experience:
Basic IP and net knowledge is essential
Packet core knowledge - MME, EPG, SGW, WMG, AMF, SMF, UPF, PCRF, PCF, DNS.
VoLTE and VoWIFI knowledge
Strong experience in actively supporting end to end systems infrastructure in a Telecommunications environment.
Strong experience in analysing and troubleshooting service, application and network performance issues for business critical systems
Ability to troubleshoot complex Packet call flows - MME, EPG, SGW,WMG, AMF, SMF, UPF, PCRF, PCF.
The candidate needs to be able to work under pressure and provide clear and concise instructions and timely reports on any service disruptions, to aid restoration of the service and management escalations as and when necessary
The candidate needs to have a grasp of telecommunication systems from an end to end perspective.
Familiarity with openstack to support NFVI infrastructure
Familiarity with Kubernetes to support container based applications and environments.
We are committed to providing equal employment opportunities and encourage all qualified candidates to apply. While the hiring process may not be expedited, we urge all interested candidates to submit their applications promptly to ensure their consideration.
To apply, please follow the instructions on our application portal. We look forward to receiving your application
If this isn’t the role you’re looking for right now, please visit our contractor portal below where you will see all of our live roles and communities to join:
https://contractortalent.gr8people.eu/
Show more
Show less","Mobile telecoms, Incident management, Stakeholder communications, Vendor management, 3rd line support, IP networking, Packet core, MME, EPG, SGW, AMF, SMF, UPF, DNS, PCF, PCRF, VoLTE, VoWIFI, Endtoend systems infrastructure, Service performance troubleshooting, Application performance troubleshooting, Network performance troubleshooting, Complex packet call flows, Telecommunication systems, Openstack, Kubernetes","mobile telecoms, incident management, stakeholder communications, vendor management, 3rd line support, ip networking, packet core, mme, epg, sgw, amf, smf, upf, dns, pcf, pcrf, volte, vowifi, endtoend systems infrastructure, service performance troubleshooting, application performance troubleshooting, network performance troubleshooting, complex packet call flows, telecommunication systems, openstack, kubernetes","3rd line support, amf, application performance troubleshooting, complex packet call flows, dns, endtoend systems infrastructure, epg, incident management, ip networking, kubernetes, mme, mobile telecoms, network performance troubleshooting, openstack, packet core, pcf, pcrf, service performance troubleshooting, sgw, smf, stakeholder communications, telecommunication systems, upf, vendor management, volte, vowifi"
Lead Data Engineer - Hybrid,Incendia Partners,"Boston, MA",https://www.linkedin.com/jobs/view/lead-data-engineer-hybrid-at-incendia-partners-3724314055,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Description
:
We are searching for a Lead Data Engineer to implement data engineering and analytics solutions .
Primary responsibilities include full implementation and maintenance of data ingestion, data maintenance, data validation and data delivery of investment data.
We are looking for someone who thrives in an agile, collaborative, team-based environment, working closely with technology peers across the organization, investment professionals and key vendor partners.
This position offers the opportunity to shape the future of investment data here.
Duties:
Design, develop, and implement data pipelines to maintain unified data platform for the Investment Data Management group
Lead and participate in all development activities, develop and implement solutions to meet business requirements that align with program strategic objectives
Responsible for new and on-going development of data pipelines sourcing from internal and external sources
Drive continuous improvement of data quality, resiliency, control, efficiency, and monitoring
Troubleshooting complex system interactions to find the root cause to problems
Partner with platform lead to design, develop, implement and deploy new software components to investment data platform
Partner with data architect to evaluate and finalize the unified data model
Partner with integration architect to upgrade and integrate data ingestion and data delivery tools with the unified data platform
Upgrade and integrate transformation tool, data validation tool and orchestration tools with the unified data platform to implement data engineering, analytical engineering and data maintenance capabilities.
Provide support during unexpected outages
Qualifications:
Bachelor’s degree in Computer Science or related disciplines.
Minimum of 5 years of experience in design, development and building data oriented complex applications.
Deep understanding of Agile SDLC, DevOps and Cloud technologies required, in addition to exposure to multiple, diverse technologies, platforms, and processing environments.
Experience in data integration, data warehouse, data modeling and data analytics architecture and design principles. Knowledge of and experience with Snowflake and other cloud native databases is highly preferred.
Knowledge about various architectures, patterns such as unified data management architecture (UDM), data mesh architecture, event-driven architecture, real-time data flows, non-relational repositories, data virtualization, tc.
Experience with building solutions in the financial services domain with an understanding of financial instruments, transactions, and positions, is desired.
Good interpersonal and communication skills with the ability to lead cross-team collaboration and partnerships across a variety of internal and external constituencies.
#ZR
Show more
Show less","Data Engineering, Agile, SDLC, DevOps, Cloud, Data Integration, Data Warehouse, Data Modeling, Data Analytics, Snowflake, Cloud Native Databases, Unified Data Management Architecture, Data Mesh Architecture, EventDriven Architecture, RealTime Data Flows, NonRelational Repositories, Data Virtualization","data engineering, agile, sdlc, devops, cloud, data integration, data warehouse, data modeling, data analytics, snowflake, cloud native databases, unified data management architecture, data mesh architecture, eventdriven architecture, realtime data flows, nonrelational repositories, data virtualization","agile, cloud, cloud native databases, data engineering, data integration, data mesh architecture, data virtualization, dataanalytics, datamodeling, datawarehouse, devops, eventdriven architecture, nonrelational repositories, realtime data flows, sdlc, snowflake, unified data management architecture"
Senior Data Engineer,Syrinx Consulting,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-syrinx-consulting-3682867988,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Senior Data Engineer
Boston, MA
U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. No Corp to Corp.
Apply direct to: creposa@syrinx.com
Summary
The Data team is responsible for leading efforts to ingest, structure, and model our data to support critical decision making. Our enterprise data warehouse is based on AWS Redshift, supporting Looker analytical dashboards, and is backed by an AWS data lake. In addition to analytics, the data warehouse also supports an active and growing data science practice.
The ideal candidate will have experience building and supporting data platforms that help end users make sound business decisions. This includes data pipeline creation, data quality management, system reliability, job orchestration and recovery aka “Data Ops”. In additional to technical competency, they should be truly motived to empower end users.
Responsibilities
Implement data pipelines using agile, iterative processes to deliver value quickly
Treat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines
Collaborate with the team to make the best technology choices
Deliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling
Create and maintain data models, data catalogues, and data security.
Qualifications
3-5 years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background
2-3 years experience with ETL technologies (e.g. Talend, Informatica, Matillion)
2-3 years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)
3-5 years of experience with SQL
Experience in optimizing queries and tuning database performance
Familiarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus
Python or Java experience is a plus
Show more
Show less","Data Engineering, Data Warehousing, Data Pipeline Creation, Data Quality Management, System Reliability, Job Orchestration, Data Ops, Data Modeling, Data Catalogs, Data Security, ETL Technologies, Talend, Informatica, Matillion, AWS Technologies, Redshift, S3, IAM, Kinesis, Lambda, EMR, Spark, Hive, SQL, Looker, Python, Jupyter, R, Java","data engineering, data warehousing, data pipeline creation, data quality management, system reliability, job orchestration, data ops, data modeling, data catalogs, data security, etl technologies, talend, informatica, matillion, aws technologies, redshift, s3, iam, kinesis, lambda, emr, spark, hive, sql, looker, python, jupyter, r, java","aws technologies, data catalogs, data engineering, data ops, data pipeline creation, data quality management, data security, datamodeling, datawarehouse, emr, etl technologies, hive, iam, informatica, java, job orchestration, jupyter, kinesis, lambda, looker, matillion, python, r, redshift, s3, spark, sql, system reliability, talend"
Senior Data Engineer (Sportsbook),DraftKings Inc.,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-sportsbook-at-draftkings-inc-3782382751,2023-12-17,Ipswich,United States,Mid senior,Onsite,"We’re defining what it means to build and deliver the most extraordinary sports and entertainment experiences. Our global team is trailblazing new markets, developing cutting-edge products, and shaping the future of responsible gaming.
Here, “impossible” isn’t part of our vocabulary. You’ll face some of the toughest but most rewarding challenges of your career. They’re worth it. Channeling your inner grit will accelerate your growth, help us win as a team, and create unforgettable moments for our customers.
The Crown Is Yours
As a Senior Data Engineer (Sportsbook), you will act as a creative contributor, leader, and mentor in our data engineering organization. You’ll leverage your experience and expertise with data modeling and transformation to produce data sets and solutions used to make business critical decisions. You’ll collaborate with business partners to understand and incorporate complex domain knowledge into analytics data models to support our core products and immediately improve the user experience for our customers.
What you’ll do as a Senior Data Engineer (Sportsbook)
Design and create data sets to support stakeholders across Draftkings.
Work with teams across the organization to design data models that allow our stakeholders to answer their questions and derive key business insights.
Build, orchestrate, and test data transformations of application data into analytics models.
Design, interpret, and maintain technical documentation for both upstream software engineering and downstream analytics, reporting, and data science stakeholder consumption.
Write, maintain, and optimize SQL code with cloud MPP data warehouse platforms (e.g. Snowflake).
Mentor junior team members to ensure best data practices are implemented company-wide.
What you’ll bring
At least 3 years of hands-on experience working with SQL, ELT pipelines, data transformation, and modeling.
Ability to apply software engineering best practices to analytics code using tools like Bitbucket, Github, Octo, etc.
Experience working in an agile environment with a mix of in-person and remote teammates and stakeholders.
Knowledge of mobile sportsbook product functionality and features, prior experience working in the sports betting industry is strongly preferred.
Excellent communications skills and interpersonal skills to effectively communicate with both business and technical teams.
Experience with any of the following tools and technologies: Snowflake, Airflow, Python, AWS S3, AWS Lambda, AWS Batch, Terraform, Kafka, MySQL, Tableau, PagerDuty, DataDog, Jira, Confluence, Bitbucket, Alation
Familiarity with working in a SOX controlled Software Development Life Cycle.
Join Our Team
We’re a publicly traded (NASDAQ: DKNG) technology company headquartered in Boston. As a regulated gaming company, you may be required to obtain a gaming license issued by the appropriate state agency as a condition of employment. Don’t worry, we’ll guide you through the process if this is relevant to your role.
The US base salary range for this full-time position is $116,800.00 - $175,200.00, plus bonus, equity, and benefits as applicable. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range and how that was determined during the hiring process.
Show more
Show less","Data Modeling, Data Transformation, Business Intelligence, Analytics, Data Warehousing, SQL, ELT Pipelines, Agile Development, Software Engineering, Version Control, Cloud Computing, AWS, Snowflake, Airflow, Python, Kafka, MySQL, Tableau, PagerDuty, DataDog, Jira, Confluence, Bitbucket, Alation, SOX Compliance","data modeling, data transformation, business intelligence, analytics, data warehousing, sql, elt pipelines, agile development, software engineering, version control, cloud computing, aws, snowflake, airflow, python, kafka, mysql, tableau, pagerduty, datadog, jira, confluence, bitbucket, alation, sox compliance","agile development, airflow, alation, analytics, aws, bitbucket, business intelligence, cloud computing, confluence, data transformation, datadog, datamodeling, datawarehouse, elt pipelines, jira, kafka, mysql, pagerduty, python, snowflake, software engineering, sox compliance, sql, tableau, version control"
Lead Data Engineer,Zelis,"Boston, MA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zelis-3767529298,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Summary
Build High level technical design both for Streaming and batch processing systems
Design and build reusable components, frameworks and libraries at scale to support analytics data products
Perform POCs on new technology, architecture patterns
Design and implement product features in collaboration with business and Technology stakeholders
Anticipate, identify, and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Drive the implementation of new data management projects and re-structure of the current data architecture
Implement complex automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development and production deployment frameworks
Drive collaborative reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues
Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products.
Partner closely with product management to understand business requirements, breakdown Epics,
Partner with Engineering Managers to define technology roadmaps, align on design, architecture, and enterprise strategy
Requirements
Minimum of 8+ years experience with the following:
Snowflake (Columnar MPP Cloud data warehouse)
DBT (ETL tool)
Python
Experience designing and implementing Data Warehouse
Preferred Skills
Azure/AWS cloud technology
SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations
Understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Design and development of Azure/AWS Data Factory Pipelines preferred.
Design and development of data marts in Snowflake preferred
Working knowledge of Azure/AWS Architecture, Data Lake, Data Factory
Business analysis experience to analyze data to write code and drive solutions
Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management and/or business analysis skills.
Attention to detail and concern for impact is essential
As a leading payments company in healthcare, we guide, price, explain, and pay for care on behalf of insurers and their members. We’re Zelis in our pursuit to align the interests of payers, providers, and consumers to deliver a better financial experience and more affordable, transparent care for all. We partner with more than 700 payers, including the top-5 national health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, over 4 million providers, and 100 million members, enabling the healthcare industry to pay for care, with care. Zelis brings adaptive technology, a deeply ingrained service culture, and a comprehensive navigation through adjudication and payment platform to manage the complete payment process.
Commitment to Diversity, Equity, Inclusion, and Belonging
At Zelis, we champion diversity, equity, inclusion, and belonging in all aspects of our operations. We embrace the power of diversity and create an environment where people can bring their authentic and best selves to work. We know that a sense of belonging is key not only to your success at Zelis, but also to your ability to bring your best each day.
Equal Employment Opportunity
Zelis is proud to be an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
We encourage members of traditionally underrepresented communities to apply, even if you do not believe you 100% fit the qualifications of the position, including women, LGBTQIA people, people of color, and people with disabilities.
Accessibility Support
We are dedicated to ensuring our application process is accessible to all candidates. If you are a qualified individual with a disability or a disabled veteran and require a reasonable accommodation with any part of the application and/or interview process, please email TalentAcquisition@zelis.com
SCAM ALERT: There is an active nationwide employment scam which is now using Zelis to garner personal information or financial scams. This site is secure, and any applications made here are with our legitimate partner. If you’re contacted by a Zelis Recruiter, please ensure whomever is contacting you truly represents Zelis Healthcare. We will never asked for the exchange of any money or credit card details during the recruitment process. Please be aware of any suspicious email activity from people who could be pretending to be recruiters or senior professionals at Zelis.
Show more
Show less","Streaming processing, Batch processing, Reusable components, Frameworks, Libraries, POCs, Data management, Data quality, Data optimization, Workflow scheduling, Continuous integration, Testdriven development, Production deployment, Data engineering, Data structures, Design patterns, Performance tuning, Troubleshooting, Root cause analysis, Team leadership, Mentoring, Agile development, Git, Azure DevOps, Jira, Confluence, SQL, TSQL, Indexes, Stored procedures, Triggers, Functions, Views, Data warehouse design, Data warehouse implementation, Data marts, Data Lake, Data Factory, Business analysis, Project management, Attention to detail","streaming processing, batch processing, reusable components, frameworks, libraries, pocs, data management, data quality, data optimization, workflow scheduling, continuous integration, testdriven development, production deployment, data engineering, data structures, design patterns, performance tuning, troubleshooting, root cause analysis, team leadership, mentoring, agile development, git, azure devops, jira, confluence, sql, tsql, indexes, stored procedures, triggers, functions, views, data warehouse design, data warehouse implementation, data marts, data lake, data factory, business analysis, project management, attention to detail","agile development, attention to detail, azure devops, batch processing, business analysis, confluence, continuous integration, data engineering, data factory, data lake, data management, data marts, data optimization, data quality, data structures, data warehouse design, data warehouse implementation, design patterns, frameworks, functions, git, indexes, jira, libraries, mentoring, performance tuning, pocs, production deployment, project management, reusable components, root cause analysis, sql, stored procedures, streaming processing, team leadership, testdriven development, triggers, troubleshooting, tsql, views, workflow scheduling"
Data Analyst/BI Engineer #: 23-19298,"TalentBurst, an Inc 5000 company","Boston, MA",https://www.linkedin.com/jobs/view/data-analyst-bi-engineer-%23-23-19298-at-talentburst-an-inc-5000-company-3773596325,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Title: Data Analyst/BI Engineer
Location: Boston, MA (Hybrid)
Duration: 6 Month Renewable Contract
Locals Only
Hours: 37.5 Weekly hours
Position Summary
The BEST Program Data Analyst/BI Engineer (DA/BI) will work with a Team comprised of BEST Program staff, agency business and technical SMEs and the new solution system integrator (SI) and product vendor to design and implement the data information management and reporting strategies, tools, and outputs to provide a robust reporting and data analytics capacity for users of the new Financials solution. The implementation of the new Financials solution will provide users with the ability to run queries and reports on the live data in this application directly rather than waiting for data to be available through warehouse reports. Data will be integrated across modules permitting more robust, real-time reporting and analytics capabilities.
The BEST Program Data Analyst/BI Engineer will work with the BEST Financials Data Lead to identify reporting, dashboard and business analytics needs that will drive the configuration of standard reports and dashboards delivered through the new Financials solution. The DA/BI Lead will assist in leading requirements sessions with BEST staff, agency staff and the system integrator and product vendor to review the reporting and analytics capabilities of the new solution and to identify available configuration options. The DA/BI Lead will develop and maintain a list of selected configurations for each report and dashboard and work with reports/dashboard developers during configuration to assist in translating business requirements to the configured reports. The DA/BI Lead will also participate in testing configured reports and dashboards and in organizing user testing of these items.
Whenever feasible, the BEST Program intends to adopt delivered reports and dashboards, but in instances where reporting requirements cannot be met by the delivered solutions, the DA/BI Lead will work with business and the system integrator and product vendor to identify requirements for future report development and participate in testing of these items when they are developed. The DA/BI Lead will work with business and the system integrator and product vendor to identify opportunities for improving the availability of data for reporting and analytics by understanding the solution's capacities in this area, sharing that information with business and documenting implementation options with owners of other Commonwealth business intelligence and data analytics resources.
Financials
The DA/BI Lead will support data access for users in the following areas, working to improve data quality and access in the new solution:
Acquire to Retire (Capital Asset Management)
Cash Management
Cost Allocation
Debt Management
Grants Financial Management
Operating Budget Management
Order to Cash (Revenue/Accounts Receivable)
Procure to Pay (Accounts Payable)
Program Management
Record to Report (Financial Reporting)
Reporting/Analytics
Sub-fund Management
Vendor Management and Self Service
Operations
Maintenance and Warranty Management
Incident Management
Capacity Management
Availability Management
Performance Management
Security Management
Nightly Batch Jobs Management
Software Change Management and Testing
Disaster Recovery Testing and Management
Specific Duties
Contribute to the development of data reporting and data access through database queries, reports, and dashboards.
Gather information from agency business users and technical staff regarding the types of data, frequency of reports or dashboards, distribution of reports or dashboards, and other information as needed to understand the current reporting needs of agencies.
Participate in developing an approach to identifying reporting and data analytics needs across the new solution user community.
Document configuration decisions for each standard report and dashboard and provide guidance to report developers during the configuration stage.
Participate in testing of configured reports and dashboards and assist in coordinating user testing of these materials.
Support and manage enterprise architecture's data operations (optimize data processing, query performance and data quality) and data pipelines.
Integrate data from diverse sources using ETL processes and transform raw data into datasets used for analysis.
Implement robust security measures, monitor data access and develop access policies.
Provide technological expertise to capture, store and employ data catalogue and lineage to support governance initiatives.
Understand business requirements in the BI context and design data models to convert raw data to meaningful insights.
Perform SQL, DAX queries and functions in Power BI.
Receive training and hands on guidance from the system integrator and product vendor in how to build queries, reports, and dashboards in order to assist reports developers in their work.
Assist in the creation of job aids and other training materials to support report writing by agency users.
Required Skills
Experience in building complex queries, reports and dashboards using a range of web-based tools.
Knowledge of structured data, such as entities, classes, hierarchies, relationships, and metadata.
Proven ability to collaborate with business owners, information architects, content architects and other stakeholders to support common goals and approaches including:
Interviewing managers and other stakeholders to understand their data needs.
Interviewing users to understand what they need from data systems to boost their performance.
Translating business goals, user needs and process improvements into data management functions and requirements.
Explaining the capabilities of data systems to business managers and users as the new solution is designed and configured.
Providing guidance on project scoping to meet stakeholders' requirements.
Assessing whether data is fit for use by performing initial validation of data delivered as part of the project.
Exhibiting an excellent understanding of how data are used within business processes and its impact.
In-depth exposure to data quality concepts, best practices, and tools.
Experience in designing, developing and deploying business analytics dashboards using PowerBI.
Experience in Cloud Data warehouses like Snowflake, Redshift.
Familiarity with Snowflake Data sharing, Snowpipe and Snowpark.
Experience in ETL tools such as Pentaho/Informatica and proficient in SQL.
Well-developed system analysis skills.
Understanding and knowledge of IT standards and controls.
Proficient written and verbal communication and interpersonal skills.
Preferred Qualifications
Experience with Software as a Service cloud implementations particularly those in which legacy on premise applications have been migrated to cloud delivery options.
Minimum Entrance Requirements
Bachelor's degree in computer science, system analysis or a related study, or equivalent experience.
Minimum of 5 years of design and implementation experience in the areas of business intelligence/reporting and data warehousing.
Job #: 23-19298
Show more
Show less","Data Warehousing, Business Intelligence, Reporting, Analytics, Dashboards, SQL, DAX, Power BI, Snowflake, Redshift, Snowpipe, Snowpark, Pentaho, Informatica, ETL, System Analysis, IT Standards and Controls","data warehousing, business intelligence, reporting, analytics, dashboards, sql, dax, power bi, snowflake, redshift, snowpipe, snowpark, pentaho, informatica, etl, system analysis, it standards and controls","analytics, business intelligence, dashboard, datawarehouse, dax, etl, informatica, it standards and controls, pentaho, powerbi, redshift, reporting, snowflake, snowpark, snowpipe, sql, system analysis"
Quantitative Data Engineer,Jobs for Humanity,"Boston, MA",https://www.linkedin.com/jobs/view/quantitative-data-engineer-at-jobs-for-humanity-3786351825,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Quantitative Data Engineer
Quantitative Investment Platforms and Developer Operations Team
Location: Boston, MA, Springfield, MA, or New York, NY
The Opportunity
We are looking for a Data Engineer to join our fast-paced and innovative team. As a Data Engineer, you will be responsible for solving complex problems and communicating effectively with both technical and non-technical stakeholders. It is preferred that you have a strong background in Data Engineering, workflow orchestration, and CI/CD pipelines. Experience with cloud platforms and system architectures is also a plus.
The Team
As a Quantitative Operations Engineer, you will be part of the Quantitative Investment & Developer Operations team at MassMutual. This team supports our portfolio managers by providing collaborative and portfolio risk solutions. We foster a collaborative and cross-functional team culture that values high performance and work-life balance.
The Impact
We are looking for a passionate Data Engineer who enjoys building data projects and automating workflows. Your responsibilities will include designing and maintaining complex ETL jobs, translating business requirements into technical specifications, ingesting data from various sources, cleansing and enriching data, and providing insight and direction for the development of our data platform. In addition, you will collaborate with other developers, mentor junior team members, and evaluate and recommend tools and technologies.
The Minimum Qualifications
Bachelor’s degree in Computer Science, Finance, Business or a related field.
2+ years of experience in the IT and/or finance industry.
Understanding of ETL methodologies and proficiency in Python tools.
Hands-on experience with Python and advanced data processing using Python libraries and AWS services.
Experience working in a cloud environment, particularly AWS.
Experience in modeling and tuning relational databases (SQL Server/PostgreSQL) and NoSQL databases (Mongo).
Experience with GIT and code review/deployment & scheduling.
Experience in Agile/SCRUM development.
The Ideal Qualifications
Master’s degree in computer science, engineering or a related field.
4+ years of experience in the IT and/or quantitative, investment or finance industry.
Experience with troubleshooting and root cause analysis.
Good knowledge of orchestration and scheduling tools.
Experience with data reporting tools (e.g. MicroStrategy, Tableau, Looker) or Python libraries.
Entrepreneurial mindset and ability to work in a rapid and iterative development environment.
Experience delivering mobile apps and app store ecosystems.
Excellent communication, problem-solving, organizational, and analytical skills.
Data-driven mindset and ability to analyze unstructured feedback to inform solutions development.
Adaptability to changing business priorities and strong work ethic.
Curiosity about emerging digital and technology trends.
Proven ability to collaborate cross-functionally and influence outcomes.
What to Expect as Part of MassMutual and the Team
Regular meetings with the Quantitative and ETX project teams.
Focused one-on-one meetings with your manager.
Access to mentorship opportunities.
Networking opportunities including access to various Business Resource Groups.
Access to learning content on Degreed and other platforms.
Valued ethics and integrity with industry-leading pay and benefits.
If you need an accommodation to complete the application process, please contact us.
MassMutual is an Equal Employment Opportunity employer. We welcome applications from all individuals, including minorities, females, LGBTQIA+, veterans, and individuals with disabilities. Note: Veterans are welcome to apply, regardless of their discharge status.
Show more
Show less","Python, AWS, SQL server, PostgreSQL, Mongo, GIT, Agile, Scrum, MicroStrategy, Tableau, Looker, Data engineering, Workflow orchestration, CI/CD pipelines, Cloud platforms, System architecture, ETL methodologies, Data processing, Relational Database, NoSQL, Code review, Deployment, Scheduling, Mobile apps, App store ecosystem","python, aws, sql server, postgresql, mongo, git, agile, scrum, microstrategy, tableau, looker, data engineering, workflow orchestration, cicd pipelines, cloud platforms, system architecture, etl methodologies, data processing, relational database, nosql, code review, deployment, scheduling, mobile apps, app store ecosystem","agile, app store ecosystem, aws, cicd pipelines, cloud platforms, code review, data engineering, data processing, deployment, etl methodologies, git, looker, microstrategy, mobile apps, mongo, nosql, postgresql, python, relational database, scheduling, scrum, sql server, system architecture, tableau, workflow orchestration"
MDM Data Engineer,Jobs for Humanity,"Boston, MA",https://www.linkedin.com/jobs/view/mdm-data-engineer-at-jobs-for-humanity-3786353539,2023-12-17,Ipswich,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Ad: Data Engineer - Informatica Power Center and MDM** **Objectives of the role:** - Design, build, and measure complex data processes using Informatica Power Center and MDM tools. - Work on various projects to develop data pipelines and solutions to meet specific business needs. **Daily and Monthly Responsibilities:** - Create and evaluate data integration tasks to bring together data from different sources into a reliable data repository. - Develop Master Data Management (MDM) solutions for different domains within the company. - Support MDM solutions, including data ingestion, creating master records, data extracts, and integration with other systems. - Implement data modeling policies, procedures, and standards. - Document technical information and system flows. - Ensure data accuracy and quality. - Analyze source system data and collaborate with teams to understand emerging data patterns. - Convert business needs into technical specifications. - Support the development and implementation of MDM solutions and other initiatives. **Basic Qualifications:** - Bachelor's degree in computer science or engineering. - 5+ years of experience with Informatica Power Center. - 5+ years of experience with data analytics, data modeling, and database design. - Coding and scripting experience in languages like Python, Java, or Scala. - 3+ years of experience with Informatica MDM platform. - ELT methodologies and tools knowledge. - Strong SQL tuning and troubleshooting skills. - Experience with Oracle database and AWS. - Knowledge of basic UNIX commands and shell scripts. - Experience with 3rd party job schedulers and RESTful APIs. - Willingness to provide support outside business hours. - Excellent communication, problem-solving, and organizational skills. - Authorized to work in the USA. **Preferred Qualifications:** - Master's degree in computer science or engineering. - Familiarity with agile project delivery process. - SQL knowledge for data access and analysis. - Ability to manage multiple projects and identify data issues. - Adaptability in fast-changing environments. - Additional experience with Python, Kafka, and database technologies. - Experience with automated ETL processes. MassMutual is an equal opportunity employer, and we welcome applications from all individuals, including elderly, refugees, people with visible and invisible disabilities, LGBTQIA+, and veterans. We encourage all qualified candidates to apply. Veterans are welcome to apply, regardless of discharge status. If you need any accommodations or assistance to complete the application process, please contact us and provide specific details of the support you require. **To apply, please fill out the form below:** - Name: - Email: - Phone: - Do you have a Bachelor's degree in computer science or engineering? - How many years of experience do you have with Informatica Power Center? - How many years of experience do you have with data analytics, data modeling, and database design? - Do you have coding and scripting experience? If yes, please specify the languages. - How many years of experience do you have with Informatica MDM platform? - Do you have knowledge of ELT methodologies and tools? - Are you proficient in SQL tuning and troubleshooting? - Do you have experience with Oracle database and AWS? - Are you familiar with UNIX commands and shell scripts? - Have you worked with 3rd party job schedulers and RESTful APIs? - Are you willing to provide support outside business hours? - Are you authorized to work in the USA? - Do you have a Master's degree in computer science or engineering? - Are you familiar with the agile project delivery process? - Do you have knowledge of SQL for data access and analysis? - Can you manage multiple projects and identify data issues? - Are you adaptable in fast-changing environments? - Do you have any additional experience with Python, Kafka, or database technologies? - Have you worked with automated ETL processes?
Show more
Show less","Informatica Power Center, MDM, Data Pipelines, Data Integration, Data Modeling, Python, Java, Scala, Informatica MDM, ELT, Data Quality, AWS, SQL, Oracle, UNIX, Shell Scripts, Job Schedulers, RESTful APIs, Data Analysis, ProblemSolving","informatica power center, mdm, data pipelines, data integration, data modeling, python, java, scala, informatica mdm, elt, data quality, aws, sql, oracle, unix, shell scripts, job schedulers, restful apis, data analysis, problemsolving","aws, data integration, data quality, dataanalytics, datamodeling, datapipeline, elt, informatica mdm, informatica power center, java, job schedulers, mdm, oracle, problemsolving, python, restful apis, scala, shell scripts, sql, unix"
Senior Data Engineer,Harnham,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3785223114,2023-12-17,Ipswich,United States,Mid senior,Remote,"To Apply for this Job Click Here
Senior Data Engineer
6-month contract (with potential extensions)
Fully Remote
$70-$85/ Hr
The Role
Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
Design and implement data model changes that align with warehouse dimensional modeling standards.
Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model.
Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments.
Design and develop SQL stored procedures, functions, views, and triggers
Design, code, test, document and troubleshoot deliverables
Collaborate with others to test and resolve issues with deliverables
Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding.
Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs.
Perform Continuous Integration and deployment using Azure Devops and Git
Technical Skills / Knowledge
Has demonstrated proficiency in designing and developing Azure Data Factory Pipelines. (2 Years)
Strong Experience designing and implementing Data Warehouse.
Experience working with Microsoft BI stack (SSIS/SSRS/SSAS) and Microsoft SQL server. (5 Years)
Must have Experience with at least one Columnar MPP Cloud data warehouse
(Snowflake /Azure Synapse / Redshift) (2+ years) * Working knowledge managing data in the Data Lake.
Experience in ETL tools like Fivetran and DBT. (2 Years)
Experience with Git and Azure Devops.
Experience in Agile, Jira and Confluence.
Solid understanding of programming SQL objects (procedures, triggers, views, functions) in SQL Server. Experience optimizing SQL queries a plus.
Working Knowledge of Azure Architecture, Data Lake
Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
To Apply for this Job Click Here
Show more
Show less","Azure Data Factory Pipelines, Microsoft SQL Server, SSIS, SSRS, SSAS, Snowflake, Azure Synapse, Redshift, Fivetran, DBT, Git, Azure Devops, TSQL, Agile, Jira, Confluence, Data Warehouse, Data Lake, Data Model, ETL, Data Quality, Data Analysis, SQL, Programming, Dimensional Data Model, Continuous Integration, Deployment","azure data factory pipelines, microsoft sql server, ssis, ssrs, ssas, snowflake, azure synapse, redshift, fivetran, dbt, git, azure devops, tsql, agile, jira, confluence, data warehouse, data lake, data model, etl, data quality, data analysis, sql, programming, dimensional data model, continuous integration, deployment","agile, azure data factory pipelines, azure devops, azure synapse, confluence, continuous integration, data lake, data model, data quality, dataanalytics, datawarehouse, dbt, deployment, dimensional data model, etl, fivetran, git, jira, microsoft sql server, programming, redshift, snowflake, sql, ssas, ssis, ssrs, tsql"
"Senior Data Scientist, Sales",Pearl Health,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-scientist-sales-at-pearl-health-3781763781,2023-12-17,Ipswich,United States,Mid senior,Remote,"Who We Are. . .
Pearl Health is restoring the independence and joy of practicing medicine for doctors in the United States. Our healthcare system has burdened our doctors with too much process, while removing them from what they do best: caring for those we love most. We are a team of physicians and public health experts (Stanford, Dartmouth, Harvard, Mass. General), technologists (athenahealth, Apple, Amazon, Meta, Flatiron, WalMart Health), healthcare innovators (Oscar, Aledade, VillageMD, Stellar, Arcadia) and experienced risk management professionals (CVS/Aetna, Humana, Cigna, United) who believe that primary care providers (PCPs) are the key to the United States’ future health. Pearl recently announced a strategic partnership to proliferate value-based care with Walgreens, one of the largest healthcare companies in the country. Our investors include Andreessen Horowitz, Viking Global Investors, AlleyCorp, and SV Angel.
Pearl Health helps PCPs who are overwhelmed by the demands of an aging population and the unpredictable business model inherent to Traditional Medicare. Over time, this solution will empower PCPs to provide better care to their entire patient population. We believe there is a need to bring humanism back to healthcare by giving physicians the ability to better allocate their time with patients, increasing their autonomy, creating incentives that are aligned with achieving better outcomes through clinical decision-making powered by actionable intelligence, and making risk more available and more readily optimized through an intelligent marketplace. The Center for Medicare and Medicaid (CMS) is launching a new value-based care program to disrupt Traditional Medicare, and we believe that this represents an opportunity to achieve the meaningful change we envision.
What We Hope You Can Do...
We are seeking a sales-oriented Data Scientist — think Sales Engineer, but for Data Science — with a track record of managing cross-functional projects and communicating quantitative results to a lay audience. You will function as a bridge between our Data Science and Growth teams, giving a voice to analyses that can delve into the arcane and helping close sales by articulating the value of Pearl’s offering. You should be an effective and compelling communicator and a strategic thinker with a penchant for understanding the needs of internal and external clients. The role will leverage and extend our analytic platform to predict financial outcomes for healthcare entities, aligning those research projects with an evolving and highly seasonal go-to-market motion, metabolizing third party analyses in order to compare them to internally-generated reports, and providing feedback to your colleagues about how we can further differentiate and elevate Pearl’s offerings.
What a Senior Data Scientist, Sales means to us during the sales season...
Engage with salespeople and sales leads as a “human explicability layer.” Although you might have direct contact with leads during our inside sales motion, automating explicability through technical processes would certainly be within the scope of the job.
Manage a complex and dynamic collaboration between the Data Science and Growth teams, which will require requisitioning and organizing resources in order to ensure that SLAs are met or exceeded throughout the sales motion.
Help us refine our product-market fit by providing strategic guidance as to what aspects of the sales offerings are compelling and which aren’t working.
Ensure quality and availability of various data sources: lead information, sales touchpoint info, evolution of financial analyses shared with a lead, and deal terms.
What a Senior Data Scientist, Sales means to us during the off-cycle…
Research payment models that resonate with providers and power our go-to-market motion.
Enable the support of the growth motion through design and implementation of technical systems.
Deepen knowledge of the healthcare models and programs in which Pearl is operating (e.g. ACO REACH, MSSP, Medicare Advantage).
Evaluate vendors to help us better understand our customers and the market.
Who You Are…
8+ years of experience in performing results-driven quantitative analysis in a healthcare management or professional consulting environment, although strong candidates with less experience will certainly be considered.
A degree in statistics, applied math, computer science, engineering or a related field (e.g., healthcare policy) is strongly preferred.
SQL expertise and strong foundation in at least one of the main data science programming languages (Python, R, Julia, Scala, Matlab). Python is our current primary language and is preferred.
A thoughtful collaborator who is able to design and execute quantitatively-driven business processes in a way that balances deep insight and fast turnaround.
Eloquent communicator of quantitative analyses, in particular those relating to financial estimates or healthcare, to a lay audience.
A proven ability to apply research and analytic models to inform growth operations including product-market fit, the development of sales collateral, and the evolution of RevOps (“GrowthOps” in Pearl-speak) workflows.
Relish synthesizing the complexity of the healthcare/financial ecosystem into actionable insights that provide value to providers.
Our Values
We are an Equal Opportunity Employer and our employees are people with different strengths, experiences and backgrounds, who share a passion for improving people's lives. Our definition of diversity not only includes race and gender identity, but also age, disability status, veteran status, sexual orientation, religion and many other parts of one’s identity. We believe all of our colleague’s points of view are integral to our success, and that inclusion is everyone's responsibility and a cause of beautiful things.
We welcome candidates from all backgrounds and are committed to a fair hiring process free from discrimination and focused around problem solving, improvement, and mutual empowerment.
Remote Work
We are a remote-first company, with a concentration of team members at our NYC headquarters and in the Boston area.
Compensation And Benefits
The salary range Pearl Health expects to pay for this position is between $130,000-$170,000 per year. Full time employees are also eligible for annual discretionary bonus. Where a given candidate falls within that range will depend on a variety of factors, including, but not limited to, the candidate’s relevant skills, experience and location, labor market conditions and participation, if any, in other compensation arrangements. Pearl Health provides its employees a competitive benefit package - for more information please review our benefits page.
Agency Submissions
We are not currently working with contingency search firms. If a resume is submitted to any Pearl Health employee by a third party without a valid written and signed search agreement, it will become the property of Pearl Health and no fee will be paid, irrespective of whether the candidate is hired.
Show more
Show less","Sales, Data Science, Quantitative Analysis, Communication, SQL, Python, R, Julia, Scala, Matlab, Healthcare, Financial Analysis, ProductMarket Fit, RevOps, Problem Solving, Improvement, Mutual Empowerment","sales, data science, quantitative analysis, communication, sql, python, r, julia, scala, matlab, healthcare, financial analysis, productmarket fit, revops, problem solving, improvement, mutual empowerment","communication, data science, financial analysis, healthcare, improvement, julia, matlab, mutual empowerment, problem solving, productmarket fit, python, quantitative analysis, r, revops, sales, scala, sql"
Data scientist,Experfy,"Boston, MA",https://www.linkedin.com/jobs/view/data-scientist-at-experfy-3683302706,2023-12-17,Ipswich,United States,Mid senior,Remote,"The successful candidate will have a strong background in remote sensing, with experience in Google Earth Engine using remotely-sensed imagery for mapping applications and analysis. The successful candidate will have strong programming skills, including proficiency with Python, R and Google Earth Engine (GEE). Candidate is required to have proficiency with GIS and image processing software ArcGIS.
Requirements
Acquire and pre-process HLS data. UseClient's available R code for automating HLS geotiff extraction from Client's Land Processes Distributed Active Archive Center (LP DAAC) archives, based on region of interest and specified temporal range
Generate HLS NDVI time series to identify peak greenness from cloud-masked data
Further identify dates of peak greenness across various elevation bands of interest. These NDVI time series will be compared to concurrent 250m daily MODIS NDVI time series for additional validation of ESI phenological peaks. Generate code to automate the extraction of HLS image granule representing peak greenness for regions of interest. Similarly run process to identify optimal late-season imagery
Show more
Show less","Remote Sensing, Google Earth Engine (GEE), Image Processing, Python, R, ArcGIS, GIS, Geospatial Analysis, HLS, NDVI, MODIS, Land Processes Distributed Active Archive Center (LP DAAC), Peak Greenness, Time Series Analysis, Machine Learning","remote sensing, google earth engine gee, image processing, python, r, arcgis, gis, geospatial analysis, hls, ndvi, modis, land processes distributed active archive center lp daac, peak greenness, time series analysis, machine learning","arcgis, geospatial analysis, gis, google earth engine gee, hls, image processing, land processes distributed active archive center lp daac, machine learning, modis, ndvi, peak greenness, python, r, remote sensing, time series analysis"
Senior Data Analyst,Traackr,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-traackr-3772606898,2023-12-17,Ipswich,United States,Mid senior,Remote,"About Traackr
Traackr is a global SaaS technology company providing a data-driven influencer marketing platform that marketers use to optimize investments, streamline campaigns, and scale programs. Our customers range from some of the world’s largest companies in the beauty and personal care space to digitally native indie brands, which have all made influencer management and engagement a critical practice of their marketing and advertising programs. We are a remote-first company, and for the folks that like to meet in person, we have offices in San Francisco, New York, Boston, Paris, and London.
At Traackr, we’re lucky to have a team of kind, driven, and respectful humans from around the world. We operate on a culture of mutual respect, with core value pillars including:
Trust.
We earn the trust of our team, customers, creators, and partners through transparency, predictability, and integrity.
Diversity.
We believe that bringing diverse perspectives to the table results in stronger outcomes. All are welcome.
Value.
We strive to create tangible value for our customers and peers through our words and actions. We only succeed when our community succeeds.
Ownership.
We lead with action. We take pride in solving the hardest challenges and feel accountable towards our commitments.
Mutual success.
We share goals with each other and with our clients. Alignment, collaboration, and empathy are the cornerstones of our success.
Our team also loves to have fun together, with lots of cool employee-led initiatives like the Traackr Summer Olympics annual workout challenge!
About the role
Traackr is seeking a
Senior Data Analyst
to join their Professional Services team.
We have been organically developing our professional services offering over the last few years to support the growth of the business. The Professional Services team includes data analysts, who bring strong analytical and technical expertise, and consultants, who bring business expertise; both are responsible for delivering projects to Customers. The Senior Data Analyst reports to the Professional Services Director.
The objectives of Traackr’s Professional Services team are:
Become a strategic advisor to Traackr’s Enterprise Customers to help them optimize influencer programs by sharing best practices and developing advanced measurement solutions
Generate insights and thought leadership in the influencer marketing space that will contribute to Traackr’s innovation capabilities
Develop new data analysis methodologies and visualizations that can be prototyped as services and influence our future product developments
Help Customers make the most of their Traackr subscription by analyzing performance and offering strategic recommendations backed by data
The
Senior Data Analyst
will be the most senior team member with a strong technical background and will be responsible for leveraging Traackr’s extensive dataset, which includes millions of influencer performance and demo/psychographic data points, to help our clients optimize their influencer investment and performance. Key focus areas include performance measurement, spend optimization, and return on investment.
What you will do:
Develop new and innovative ways for our clients to leverage data to understand the ROI of their investments with influencers
Collaborate with the Consultants and Client Partner teams to understand client needs and develop a methodology for fulfilling project requests
Coach other members of the data analytics team, helping them develop their scripting and data visualization skills
Extract, aggregate, and visualize data from Traackr’s data lake to identify trends and insights
Document processes and identify areas to streamline workflow and make the team more efficient
Collaborate with the Data and Engineering teams to optimize our ability to access data and contribute to the product roadmap
Qualifications
Strong analytical capabilities
Experience with applied statistics (regression, segmentation, ANOVA, cluster analysis)
Ability to create pivot tables and run regression models
Experience with R or Python–statistical programming
Proficiency with programming languages like Python, Shell, and SQL
Experience running scripts/programs from the Linux/Unix/OSX command line (terminal)
Experience using data visualization to effectively communicate key findings to stakeholders (Excel, Tableau, Data Studio, or R/Python)
Experience with text analysis and building word clouds
Project management experience
Leadership skills and the ability to coach a team of junior analysts
An understanding of digital and social media marketing and measurement needs and goals for global enterprise brands
A business-driven mindset aiming to solve real business issues
Ability to interact meaningfully with brand clients
Ability to work independently and manage multiple priorities and tight deadlines
Experience working in an international environment within multiple time zones
Excellent written and verbal communication skills
Advanced degree in Math, Economics, Statistics, or a similar field
Benefits
Competitive salary
Remote work, hybrid options, and home office set-up stipend
Coworking office subscription
Comprehensive health, dental, and life insurance for you and your dependents*
Open vacation policy and flexible holiday schedule
Paid parental leave to let you spend valuable time with your loved ones
MacBooks are our standard
Career development, internal and external training opportunities
Employee-led DEIB task force and annual Traackr Olympics
*Please note: Benefit programs vary by country/state of residency, are subject to eligibility requirements, and may be modified occasionally. Ask for more details about benefits in your specific region.
Traackr is an equal
Employment Opportunity Employer
. We do not discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. All your information will be kept confidential according to EEO guidelines.
Traackr Privacy Notice
all questions, comments, and requests regarding data processing at Traackr Inc. should be addressed to
datarequest@traackr.com
Show more
Show less","Datadriven influencer marketing platform, R, Python, SQL, Tableau, Data visualization, Data analytics, Text analysis, Project management, Leadership, Digital marketing, Social media marketing, Statistics, Regression analysis, Segmentation, ANOVA, Cluster analysis, Pivot tables, Machine learning, Natural language processing","datadriven influencer marketing platform, r, python, sql, tableau, data visualization, data analytics, text analysis, project management, leadership, digital marketing, social media marketing, statistics, regression analysis, segmentation, anova, cluster analysis, pivot tables, machine learning, natural language processing","anova, cluster analysis, dataanalytics, datadriven influencer marketing platform, digital marketing, leadership, machine learning, natural language processing, pivot tables, project management, python, r, regression analysis, segmentation, social media marketing, sql, statistics, tableau, text analysis, visualization"
Senior Data Analyst,Proven Recruiting,"Houston, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-proven-recruiting-3771218654,2023-12-17,Baytown,United States,Associate,Onsite,"Senior Data Analyst – Contract to Hire
Must be local to Houston and able to work a hybrid work schedule downtown.
What you will do:
Work closely with Stakeholders to understand and gather requirements, create visualization in Tableau, write complex SQL queries.
Requirements:
5 years' experience in Tableau –
5 years experience in SQL and writing complex queries
GCP BigQuery Experience
Bachelor's Degree
Eligible to work in the US without sponsorship
8-5 M-F
What does this position pay?
Compensation is determined by several factors which may include skillset, experience level, and geographic location.
The expected range for this role is $45.00 to $53.00 per hour. Please note this range is an estimate and actual pay may vary based on qualifications and experience.
Show more
Show less","Tableau, SQL, GCP BigQuery","tableau, sql, gcp bigquery","gcp bigquery, sql, tableau"
Senior Data Analyst,Proven Recruiting,"Houston, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-proven-recruiting-3775130294,2023-12-17,Baytown,United States,Associate,Hybrid,"What you will be doing:
Writing complex queries, joins, and stored procedures in SQL server
Creating Power BI reports to present to the business
Azure Cloud Migration
Requirements:
3 years experience in Data Analytics
3 years of experience in SQL Server and Power BI
Stored Procedure experience in SQL
Bachelor's Degree
Location:
Houston, TX
Schedule:
Hybrid 4 days in Office, 1 day WFH
#IND3
What does this position pay?
Compensation is determined by several factors which may include skillset, experience level, and geographic location.
The expected range for this role is $80,000 to $130,000 per year. Please note this range is an estimate and actual pay may vary based on qualifications and experience.
Show more
Show less","SQL, Power BI, Azure Cloud, Stored Procedures, Data Analytics","sql, power bi, azure cloud, stored procedures, data analytics","azure cloud, dataanalytics, powerbi, sql, stored procedures"
Power & Gas Portfolio Data Analyst,Selby Jennings,"Houston, TX",https://www.linkedin.com/jobs/view/power-gas-portfolio-data-analyst-at-selby-jennings-3785738308,2023-12-17,Baytown,United States,Associate,Hybrid,"A Global Energy Marketer and Trading House in Houston is looking to bring on a Power & Gas Portfolio Data Analyst to their Data Team.
This individual will be supporting the greater Trading function by monitoring North American and European Power Markets and assessing impacts as it relates to Data Management, as well as report and explain P&L impact to the Middle Office and greater business.
The ideal candidate will have at least 2 years of experience in a Data Analyst role for another Commodities Trading firm, with an understanding of North American Power Markets. Strong candidates will possess technical skills in SQL, Tableau, Python, etc.
Responsibilities:
Provide analytical support to the North American Power trading team by implementing new power contracts, market rules, and ISO protocols
Report and explain P&L to Middle Office and the larger business
Work with IT and Trading Teams to develop and test new systems and tools
Monitor North American and European Power Markets and assessing impacts as it relates to Data Management
Be the first point of contact for traders for new technical developments and reports
Qualifications:
2+ YOE in a Data Analyst/Data Focused role
Understanding of North American Power Markets (ERCOT, PJM, CAISO, etc.)
Working ability in Python, SQL, Tableau, Alteryx, etc.
Strong communication skills and ability to work across different business lines
Show more
Show less","SQL, Tableau, Python, Power BI, Alteryx, ERCOT, PJM, CAISO, Energy Trading, Data Management, P&L Analysis, Market Rules, ISO Protocols, Data Analysis, Reporting","sql, tableau, python, power bi, alteryx, ercot, pjm, caiso, energy trading, data management, pl analysis, market rules, iso protocols, data analysis, reporting","alteryx, caiso, data management, dataanalytics, energy trading, ercot, iso protocols, market rules, pjm, pl analysis, powerbi, python, reporting, sql, tableau"
Junior Data Visualization Developer,SynergisticIT,"Houston, TX",https://www.linkedin.com/jobs/view/junior-data-visualization-developer-at-synergisticit-3767588947,2023-12-17,Baytown,United States,Mid senior,Onsite,"Job Description
SYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out you need to have exceptional skills and technologies that's where we come in to make sure you get the attention that you need
We are looking for Jobseekers wanting to file H1b visa
Position open to all visas and US citizens
We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, Walmart labs etc to name a few.
We have an excellent reputation with the clients. Currently, We are looking for entry-level Python developers, and Data analysts/ Data Scientists.
We welcome candidates with all visas and citizens to apply.
We assist in filing for STEM extension and also for H1b and Green card filing to Candidates looking to upskill/enhance their IT skills.
Candidates having difficulty in finding jobs or cracking interviews or who want to improve their skill portfolio. We also offer Skill enhancement programs if the candidates are missing skills or experience that our clients need with great outcomes
Candidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement
We are looking for Jobseekers wanting to file H1b visa
Position open to all visas and US citizens
Candidates Who Lack Experience
Have had a break in careers
Lack Technical Competency
Different visa candidates who want to get employed and settle down in the USA
please also check the below links
https://www.synergisticit.com/candidate-outcomes/
https://www.synergisticit.com/java-track/
https://www.synergisticit.com/data-science-track/
https://www.synergisticit.com/blog/
Required Skills
Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Knowledge of Statistics, Python, and data visualization tools
Excellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, Time series analysis
Please understand skills are required by clients for selection even if it's a Junior or entry-level position the additional skills are the only way a candidate can be picked by clients.
No third-party candidates or c2c candidates
To apply for this position, please apply to the posting
No phone calls please . Shortlisted candidates would be reached out
Show more
Show less","Python, Statistics, Data visualization tools, NLP, Text mining, Tableau, Time series analysis, Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT","python, statistics, data visualization tools, nlp, text mining, tableau, time series analysis, computer science, computer engineering, electrical engineering, information systems, it","computer engineering, computer science, data visualization tools, electrical engineering, information systems, it, nlp, python, statistics, tableau, text mining, time series analysis"
Data Analytics Engineer,NR Consulting,"Houston, TX",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-nr-consulting-3768017759,2023-12-17,Baytown,United States,Mid senior,Onsite,"Summary
Members of the Analytics Engineering team become experts in the use of our software products and work with customers in helping them solve their industrial process analytics challenges and foster user engagement. The mission of the Analytics Engineering team is to design and implement technical solutions to extract maximum customer value from Client's solution adoption. This means partnering with customers to deliver sustainable, scalable, and profitable business outcomes.
Key Responsibilities
As a Key Customer-facing Team Member, You Will
Showcase Client solutions.
Introduce the art of the possible to both active and prospective customers with Client through promotional content and targeted demos to land and expand Client use.
Implement Client solutions to foster user engagement.
Leverage customer relationships to Client and implement high-value use cases that expand Client usage.
When applicable, collaborate with Client partners to deliver value for the customer.
Train and coach customer team on analytics methodology to implement and operationalize Client solution.
Serve as customers Trusted Technical Advisor
Identify and build relationships with customer champions and end-users to understand business objectives and process pain points.
Collaborate with customer champions and end-users to complete use cases that deliver a high return on investment for the customer.
Identify gaps in Client capabilities required to meet customer needs and give relevant technical product feedback to the product
Show more
Show less","Technical Consulting, Software Products, Analytics, Technical Solutions, Client Solutions, Customer Engagement, HighValue Use Cases, Analytics Methodology, Product Feedback, Training and Coaching, Problem Solving, Demos, Customer Relationships, User Engagement, Communication, Collaboration, Product Knowledge, Implementation, Use Cases, Gap Analysis","technical consulting, software products, analytics, technical solutions, client solutions, customer engagement, highvalue use cases, analytics methodology, product feedback, training and coaching, problem solving, demos, customer relationships, user engagement, communication, collaboration, product knowledge, implementation, use cases, gap analysis","analytics, analytics methodology, client solutions, collaboration, communication, customer engagement, customer relationships, demos, gap analysis, highvalue use cases, implementation, problem solving, product feedback, product knowledge, software products, technical consulting, technical solutions, training and coaching, use cases, user engagement"
Associate Data Engineer,48forty Solutions,"Houston, TX",https://www.linkedin.com/jobs/view/associate-data-engineer-at-48forty-solutions-3737143441,2023-12-17,Baytown,United States,Mid senior,Onsite,"48forty Solutions is the largest pallet management services company in North America. We provide end-to-end pallet solutions, from supply to retrieval, on-site services, reverse logistics, and packaging materials. 48forty Solutions is truly Pallet Management Made Simple. Our operations workforce is the heart and soul of our business. We are currently looking for an
Associate Data Engineer.
Summary
As an Associate Data Engineer, you will be responsible for supporting the design, development, and maintenance of data pipelines and data processing systems using various technologies such as Azure, Spark SQL, SQL Server, Azure Data Factory, Azure Databricks, Azure DevOps, Git, Power BI, Azure Synapse Analytics, and Azure Databricks. You will work under the guidance of more senior data engineers and play a vital role in ensuring the availability, reliability, and scalability of data infrastructure and supporting data-driven initiatives within the organization.
Essential Duties And Responsibilities
Data pipeline development: Collaborate with the data engineering team to develop and maintain data pipelines, ETL processes, and data integration workflows using Azure Data Factory, Azure Synapse Analytics, Spark SQL, and other relevant technologies. Ensure the timely and accurate movement of data between systems.
Data processing and transformation: Assist in implementing data processing and transformation logic using Azure Synapse Analytics, Spark SQL or Databricks. Extract insights from raw data and transform it into meaningful and structured formats in our data products.
Azure service utilization: Work with Azure services such as SQL Server, Azure Data Factory, Azure Synapse and Azure Databricks to build and manage data infrastructure components. Leverage the capabilities of these services to ensure efficient and scalable data processing.
Version control and collaboration: Utilize Azure DevOps and Git for version control and collaborate effectively with the team to manage code repositories and ensure proper documentation and knowledge sharing.
Azure DevOps integration: Assist in integrating data engineering workflows with Azure DevOps for continuous integration, continuous deployment, and automated testing. Contribute to the implementation of CI/CD pipelines for data engineering projects.
Data visualization and reporting: Gain exposure to Power BI and support the team in developing insightful and visually appealing reports and dashboards to communicate data insights effectively to stakeholders.
Troubleshooting and support: Assist in identifying and resolving data pipeline issues, bottlenecks, and data quality problems. Provide support in investigating and troubleshooting data-related incidents.
Continuous learning and growth: Stay updated with the latest advancements in data engineering technologies and tools. Continuously enhance your knowledge of Azure services and data engineering best practices.
Qualifications And Skills
2 years of experience in data engineering, data integration, or related roles.
Bachelor's Degree in Computer Science, Information Technology, or a related field preferred.
Hands-on experience with Azure services, including Azure Data Factory, Azure Synapse, and Azure Databricks.
Proficiency in SQL programming and experience working with SQL Server.
Familiarity with Spark SQL or Databricks for data processing and transformation.
Exposure to Git for version control and collaborative development.
Knowledge of Azure DevOps practices for CI/CD and automated testing.
Experience with data visualization tools, such as Power BI, for developing reports and dashboards.
Understanding of Azure Analysis Services and Azure Synapse Analytics is a plus.
Strong problem-solving skills and attention to detail.
Good communication and teamwork skills, with the ability to work effectively in a collaborative environment.
Self-motivated and eager to learn and grow in the field of data engineering.
Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential job functions.
The employee will be working at the corporate office and will be required to sit for long periods of time at a desk working on a laptop.
The noise level in the work environment is usually moderate.
Benefits
Competitive Pay
Holiday Pay
Referral Bonuses
Long-Term Career Advancement
Great Team Environment
PTO
Full-time employees eligible for Medical, Dental, Vision, Basic Life, AD&D and Short-Term & Long-Term Disability insurance on the 1st of the month following 60 days of employment
48forty Solutions
is an equal opportunity employer.
Privacy Policy
CA Applicant
CA Workforce
Show more
Show less","Azure, Spark SQL, SQL Server, Azure Data Factory, Azure Databricks, Azure DevOps, Git, Power BI, Azure Synapse Analytics, ETL, Data pipelines, Data integration, Data processing, Data transformation, Data visualization, Data reporting, CI/CD, Automated testing, Version control, Collaborative development, SQL programming, Data modeling, Data warehousing, Cloud computing, Big data","azure, spark sql, sql server, azure data factory, azure databricks, azure devops, git, power bi, azure synapse analytics, etl, data pipelines, data integration, data processing, data transformation, data visualization, data reporting, cicd, automated testing, version control, collaborative development, sql programming, data modeling, data warehousing, cloud computing, big data","automated testing, azure, azure data factory, azure databricks, azure devops, azure synapse analytics, big data, cicd, cloud computing, collaborative development, data integration, data processing, data reporting, data transformation, datamodeling, datapipeline, datawarehouse, etl, git, powerbi, spark sql, sql, sql server, version control, visualization"
Staff Data Engineer,Fig - Food Is Good,"Houston, TX",https://www.linkedin.com/jobs/view/staff-data-engineer-at-fig-food-is-good-3734114597,2023-12-17,Baytown,United States,Mid senior,Onsite,"Fig is building a digital food-as-medicine platform to help anyone with complex dietary needs live an easier, healthier life around food. We're a fast-growing, early-stage startup supporting over 800,000 users and backed by top VCs including Sequoia, Artis Ventures, Goodwater Capital, and Correlation Ventures. Our users love us (4.8 stars on app stores) as Fig changes their lives: ""THANK YOU!! It is sometimes such a struggle to find PCOS-compliant snacks, ingredients, etc. This app has truly changed my life in the 2 weeks I’ve had it!”
Fig’s mission is to help millions of people with dietary restrictions more easily find food. Our core product, the Fig phone app, already helps hundreds of thousands of people navigate food at grocery stores and restaurants each month.
Our company is seeking a highly skilled Data Engineer with extensive experience in building data ingestion, processing, and instrumentation systems at scale.
What You Will Do
As a Data Engineer at Fig, you will:
Design, develop, and maintain our data architecture, data models, ETL pipelines, and data warehouse.
Collaborate with cross-functional teams to identify business needs and translate them into data solutions.
Implement and optimize data ingestion processes from multiple data sources for real-time analytics and business intelligence.
Set up orchestration engines to run data ingestion processes in the cloud.
Monitor, troubleshoot, and optimize the performance of data pipelines and database systems.
Assist in developing data governance and data quality processes and ensure compliance with data privacy and security policies.
Stay up-to-date with the latest technologies and trends in the data engineering field.
Qualifications
Who you are:
At least 3-5 years of professional experience in data engineering or related roles.
Proficiency in SQL and Python and experience with ETL tools.
Experience with cloud platforms (AWS, GCP, Azure)
Solid understanding of database design, data warehousing concepts, and data modeling.
Proven ability to build and maintain data pipelines and deliver high-quality data solutions.
Strong problem-solving skills, analytical capabilities, and attention to detail.
Excellent communication skills to collaborate with diverse teams.
EXTREME attention-to-detail, efficiency, competence and a desire to get things right the first time (our users are counting on you!)
Stellar references
Nice To Haves
Experience with web scraping at scale
Personal experience with dietary restrictions (either yourself or a loved one); navigating tricky dietary needs is a daunting task, and prior understanding of these challenges will be helpful.
How We Work
Fig is a fully remote team. We generally work US hours, with most of our meetings happening between 12-5 PM EST. We speak and write in English and generally expect employees to be proficient in communicating in English. That said, we don’t have a strict geographic requirement if you are willing to work within those constraints. We expect this role to integrate into our software development lifecycle and join team meetings for sprint planning, retrospectives, and review.
What We Offer
Competitive salary
Be part of building company culture
For full-time employees:
Meaningful equity in the form of stock options
401(k) plan
Medical, Dental, and Vision Insurance
Flexible time off and paid holidays
How To Apply
Please reach out to
recruiting@foodisgood.com
with a resume or any questions. If we are interested, we will schedule an introductory call to get to know you more!
Show more
Show less","Data engineering, ETL pipelines, Data architecture, Data models, SQL, Python, Data ingestion, Realtime analytics, Business intelligence, Cloud platforms, Database design, Data warehousing, Data quality, Data privacy, Data security, Data governance, Problemsolving, Analytical capabilities, Attention to detail, Communication, Teamwork, Web scraping","data engineering, etl pipelines, data architecture, data models, sql, python, data ingestion, realtime analytics, business intelligence, cloud platforms, database design, data warehousing, data quality, data privacy, data security, data governance, problemsolving, analytical capabilities, attention to detail, communication, teamwork, web scraping","analytical capabilities, attention to detail, business intelligence, cloud platforms, communication, data architecture, data engineering, data governance, data ingestion, data models, data privacy, data quality, data security, database design, datawarehouse, etl pipelines, problemsolving, python, realtime analytics, sql, teamwork, web scraping"
Sr. Data Engineer,Inceed,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-inceed-3786531568,2023-12-17,Baytown,United States,Mid senior,Onsite,"Compensation:
$140000
Location:
Houston, TX
Sr. Data Engineer
Inceed has partnered with a great company to help find a skilled
Sr. Data Engineer
to join their team!
We are looking for a talented and motivated Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining the data architecture, infrastructure, and tools necessary for optimal extraction, transformation, and loading (ETL) of data from various sources to support our organization's data-driven initiatives.
Responsibilities:
Data Pipeline Development:
Design, implement, and maintain scalable and efficient data pipelines for the extraction, transformation, and loading of data from diverse sources.
Collaborate with cross-functional teams to understand data requirements and ensure the smooth flow of data between systems.
Data Modeling and Schema Design:
Develop and implement data models and schemas to support efficient storage and retrieval of structured and unstructured data.
Optimize database performance and ensure data integrity.
ETL Process Optimization:
Continuously improve and optimize ETL processes for better efficiency and reliability.
Implement monitoring and logging mechanisms to identify and address issues in a timely manner.
Data Integration:
Integrate data from various sources to create a unified and comprehensive view of enterprise data.
Work with data architects to ensure data integration solutions align with overall data architecture goals.
Database Management:
Administer and maintain databases, ensuring high availability, security, and performance.
Collaborate with database administrators to implement best practices for data storage and retrieval.
Data Quality Assurance:
Implement data quality checks and validation processes to ensure accuracy and consistency of data.
Collaborate with data stewards and business users to resolve data quality issues.
Collaboration:
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and provide timely and accurate data solutions.
Participate in cross-functional teams to contribute technical expertise to overall data strategy.
Documentation:
Create and maintain documentation for ETL processes, data models, and database architecture.
Develop and share best practices for data engineering within the organization.
Required Qualifications & Experience:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Engineer or in a similar role.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with ETL tools and frameworks (e.g., Apache NiFi, Apache Beam, Talend).
Strong SQL skills and expertise in working with relational and NoSQL databases.
Familiarity with cloud platforms and services (e.g., AWS, Azure, Google Cloud).
Knowledge of data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Perks & Benefits:
Health, Dental, Vision and Life Insurance
401k matching
Bonus
Other Information:
Hybrid work schedule
If you are interested in learning more about the Sr. Data Engineer opportunity, please submit your resume for consideration. Our client is unable to provide sponsorship at this time.
We are Inceed, a staffing and direct placement firm who believes in the possibility of something better. Our mission is simple: We’re here to help every person, whether client, candidate, or employee, find and secure what’s better for them.
Inceed is an equal opportunity employer. Inceed prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Show more
Show less","Python, Java, Scala, Apache NiFi, Apache Beam, Talend, SQL, Relational databases, NoSQL databases, AWS, Azure, Google Cloud, Data warehousing, ETL, Data modeling, Data integration, Database management, Data quality assurance, Collaboration, Documentation","python, java, scala, apache nifi, apache beam, talend, sql, relational databases, nosql databases, aws, azure, google cloud, data warehousing, etl, data modeling, data integration, database management, data quality assurance, collaboration, documentation","apache beam, apache nifi, aws, azure, collaboration, data integration, data quality assurance, database management, datamodeling, datawarehouse, documentation, etl, google cloud, java, nosql databases, python, relational databases, scala, sql, talend"
Hybrid Work - Need Data Engineer III in Houston TX,Steneral Consulting,"Houston, TX",https://www.linkedin.com/jobs/view/hybrid-work-need-data-engineer-iii-in-houston-tx-at-steneral-consulting-3722357058,2023-12-17,Baytown,United States,Mid senior,Onsite,"100% onsite in Houston, TX 77002, Must be local to Houston
Schedule: 5 days a week onsite | 7:30am-5pm | **Fridays are half days** (totaling 40 hrs./week)
Top Skills
Experience ddesigning and implementing reliable data pipelines to integrate disparate data sources into a single Data Lakehouse
5+ years of experience as a Data Engineer designing and maintaining data pipeline architectures
5+ years of programming experience in Python, ANSI SQL, PLSQL, and TSQL
Experience with Dremio and Airbyte
Additional Notes
Experience with Data Lakehouse technologies is REQUIRED, the tools Dremio or Airbyte. Please ensure this is listed on resume.
Responsibilities Include
Design and implement reliable data pipelines to integrate disparate data sources into a single Data Lakehouse
Design and implement data quality pipelines to ensure data correctness and building trusted datasets
Design and implement a Data Lakehouse solution which accurately reflects business operations
Assist with data platform performance tuning and physical data model support including partitioning and compaction
Provide guidance in data visualizations and reporting efforts to ensure solutions are aligned to business objectives
The Successful Candidate Will Meet The Following Qualifications
5+ years of experience as a Data Engineer designing and maintaining data pipeline architectures
5+ years of programming experience in Python, ANSI SQL, PLSQL, and TSQL
Experience in various data integration patterns including ETL, ELT, Pub/Sub, and Change Data Capture
Experience with common Python Data Engineering packages including pandas, Numpy, Pyarrow Pytest, Scikit-Learn, and Boto3
Experience in software development practices such as Design Principles and Patterns, Testing, Refactoring, CI/CD, and version control
Experience in implementing a Data Lakehouse using Apache Iceberg or Delta Lake
Knowledgeable of modern data platform technologies including Apache Airflow, Kubernetes, and S3 Object Storage
Experience with Dremio and Airbyte is preferred
Show more
Show less","Data Engineering, Data Lakehouse technologies, Apache Iceberg, Delta Lake, Apache Airflow, Kubernetes, S3 Object Storage, Dremio, Airbyte, Python, ANSI SQL, PLSQL, TSQL, pandas, Numpy, Pyarrow, Pytest, ScikitLearn, Boto3, Design Principles and Patterns, Testing, Refactoring, CI/CD, Version control","data engineering, data lakehouse technologies, apache iceberg, delta lake, apache airflow, kubernetes, s3 object storage, dremio, airbyte, python, ansi sql, plsql, tsql, pandas, numpy, pyarrow, pytest, scikitlearn, boto3, design principles and patterns, testing, refactoring, cicd, version control","airbyte, ansi sql, apache airflow, apache iceberg, boto3, cicd, data engineering, data lakehouse technologies, delta lake, design principles and patterns, dremio, kubernetes, numpy, pandas, plsql, pyarrow, pytest, python, refactoring, s3 object storage, scikitlearn, testing, tsql, version control"
"Urgent need Senior Data Engineer/Houston, TX,",TekIntegral,"Houston, TX",https://www.linkedin.com/jobs/view/urgent-need-senior-data-engineer-houston-tx-at-tekintegral-3667473687,2023-12-17,Baytown,United States,Mid senior,Onsite,"Hi ,
Hope you are doing great
I have very urgent need Senior Data Engineer role let me know if you are interested.
Job Title: Senior Data Engineer
Location: Houston, TX,
Remote work possible. May need to come in for a couple of days induction at start.
Duration : 6+ Month
Interview Process : 1.
Online test through Codility
2.
client Tech interview
3
.End Client Interview
Core Skills
Advanced working SQL knowledge Python, Pyspark, Spark-SQL Cloud computing platforms AWS, Azure (preferably AWS) Messaging technologies (Kafka, Kinesis) Orchestration tools (Airflow) Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Responsibilities
Expanding and optimizing our data and data pipeline architecture Optimizing data flow and collection Experienced data pipeline builder and data wrangler Assemble large, complex data sets Analytics working with unstructured datasets
Mohit Rajput
Technical Recruiter
TekIntegral Inc.
500 N Central Expwy #500G
Plano, TX USA 75074
www.tekintegral.com
Show more
Show less","SQL, Python, Pyspark, SparkSQL, AWS, Azure, Kafka, Kinesis, Airflow, Data Pipeline Architecture, Data Flow, Data Collection, Data Wrangler, Data sets, Analytics","sql, python, pyspark, sparksql, aws, azure, kafka, kinesis, airflow, data pipeline architecture, data flow, data collection, data wrangler, data sets, analytics","airflow, analytics, aws, azure, data collection, data flow, data pipeline architecture, data sets, data wrangler, kafka, kinesis, python, spark, sparksql, sql"
DATA - Sr. Data Engineer (Azure Synapse/PowerBI) - Hybrid in Houston,Addison Group,"Houston, TX",https://www.linkedin.com/jobs/view/data-sr-data-engineer-azure-synapse-powerbi-hybrid-in-houston-at-addison-group-3595563534,2023-12-17,Baytown,United States,Mid senior,Onsite,"Senior Data Engineer III
Hybrid in Houston
Direct Hire
$130-150K depending on experience
We are working with a client that is looking for a Senior Data Engineer. They recently started their analytics division; there is a lot of excitement within the organization around this position due to the huge impact it will make with the business.
Requirements
7-10 years of Data Engineering experience
Azure Synapse
Power BI
Ability to write complex SQL queries
Data modeling and engineering tools such as Kafka, Spark, and Hadoop
Building data pipelines in our brand-new analytics infrastructure that they are working to build out.
Bachelor’s degree preferred, not required.
The client is eager to get this position hired. It will be a one step interview before a decision is made.
Apply now.
No C2C or third parties
IND 005-009
Show more
Show less","Data Engineering, Azure Synapse, Power BI, SQL, Kafka, Spark, Hadoop, Data Modeling, Data Pipelines","data engineering, azure synapse, power bi, sql, kafka, spark, hadoop, data modeling, data pipelines","azure synapse, data engineering, datamodeling, datapipeline, hadoop, kafka, powerbi, spark, sql"
Sr Data Engineer,MHK TECH INC,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-mhk-tech-inc-3769086257,2023-12-17,Baytown,United States,Mid senior,Onsite,"Sr. Data Engineer
Location: REMOTE
Position Type: Contract( Only W2)
Responsibilities
Designing and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink.
Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data.
Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions.
Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms.
Designing and implementing data models and schemas for efficient data storage and retrieval.
Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning.
Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing.
Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle.
Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data.
Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.
Requirements
Bachelor's or higher degree in Computer Science, Engineering, or a related field.
Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices.
Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions.
Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar.
Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles.
Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP).
Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes).
Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar.
Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability.
Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders.
Show more
Show less","Apache Hadoop, Apache Spark, Apache Flink, Data pipelines, Data infrastructure, Data warehouses, Data security, Data privacy, Java, Scala, Python, SQL, NoSQL, Cloud platforms, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), HDFS, YARN, Docker, Kubernetes, Apache Kafka, Apache Pulsar, Realtime streaming, Distributed computing, Problemsolving skills, Communication skills, Teamwork skills","apache hadoop, apache spark, apache flink, data pipelines, data infrastructure, data warehouses, data security, data privacy, java, scala, python, sql, nosql, cloud platforms, amazon web services aws, microsoft azure, google cloud platform gcp, hdfs, yarn, docker, kubernetes, apache kafka, apache pulsar, realtime streaming, distributed computing, problemsolving skills, communication skills, teamwork skills","amazon web services aws, apache flink, apache hadoop, apache kafka, apache pulsar, apache spark, cloud platforms, communication skills, data infrastructure, data privacy, data security, data warehouses, datapipeline, distributed computing, docker, google cloud platform gcp, hdfs, java, kubernetes, microsoft azure, nosql, problemsolving skills, python, realtime streaming, scala, sql, teamwork skills, yarn"
Staff data engineer,bp,Greater Houston,https://www.linkedin.com/jobs/view/staff-data-engineer-at-bp-3666050621,2023-12-17,Baytown,United States,Mid senior,Onsite,"Entity:
Innovation & Engineering
Job Family Group:
IT&S Group
Job Summary:
Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.
Independently identifies problems to be solved in their broader organization as a result of in-depth technical and business understanding.
Architects, designs, implements and maintains reliable and scalable data infrastructure to move, process and serve data.
Normally serves as a tech lead role for one large scale project or multiple medium size projects.
Effectively manages project delivery with mature agile management skills.
Writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.
Adheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation).
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline.
Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity and engineering excellence.
Participates in industry working group for standard evolution.
Actively mentors others, contributes to or leads data engineering learning and development paths
Why join us
At bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly.
There are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others!
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Job Description:
BS degree in computer science or related field
A record of successes in deep and hands-on experience (typically 8+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
Development experience in one or more object-oriented programming languages (e.g. Python, Scala, Java, C#)
Master of database and SQL knowledge
Experience leading design and implementation of large-scale distributed data systems
Deep knowledge and hands-on experience in a wide range of technologies across all data lifecycle stages, staying on top of the latest developments in the field
Travel Requirement
Up to 10% travel should be expected with this role
Relocation Assistance:
This role is eligible for relocation within country
Remote Type:
This position is a hybrid of office/remote working
Skills:
Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Show more
Show less","Python, Scala, Java, C#, SQL, Data infrastructure, Data products, Objectoriented programming, Distributed data systems, Machine learning, Data science, Agile, Software engineering, CI/CD, Serverless, Cloud computing, Microservices, DevOps, DataOps","python, scala, java, c, sql, data infrastructure, data products, objectoriented programming, distributed data systems, machine learning, data science, agile, software engineering, cicd, serverless, cloud computing, microservices, devops, dataops","agile, c, cicd, cloud computing, data infrastructure, data products, data science, dataops, devops, distributed data systems, java, machine learning, microservices, objectoriented programming, python, scala, serverless, software engineering, sql"
Sr. Data Engineer (1000983),The Judge Group,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-1000983-at-the-judge-group-3717698301,2023-12-17,Baytown,United States,Mid senior,Onsite,"Location:
Dallas, TX
Salary:
$140,000.00 USD Annually - $160,000.00 USD Annually
Description:
Our client is currently seeking local Sr. Data Engineers in Plano, TX. Please apply!
Java/Python developer on Hadoop/Spark/Big Data platform with AWS experience preferably on EMR, EKS, Glue, Lake Formation. (6+ years of experience)
W2 ONLY-No sponsorship or 3rd parties
HYBRID- 3 Days on-site in Houston, TX or in Plano, TX
Contact:
jfahs@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Java, Python, Hadoop, Spark, Big Data, AWS, EMR, EKS, Glue, Lake Formation","java, python, hadoop, spark, big data, aws, emr, eks, glue, lake formation","aws, big data, eks, emr, glue, hadoop, java, lake formation, python, spark"
Data Analyst II-Mid-level,ReviveRX,"Houston, TX",https://www.linkedin.com/jobs/view/data-analyst-ii-mid-level-at-reviverx-3787930410,2023-12-17,Baytown,United States,Mid senior,Onsite,"Summary/Objective
This position supports the product and strategy team by collecting, processing, and analyzing datasets to extract meaningful insights. Statistical methods and data visualization tools are utilized to present findings.
Essential Functions
Involved in projects related to implementing data-driven solutions, particularly focused on AI and LLM based projects, machine learning, and chatbot technologies.
Enhances efficiency, user experience, and overall success through advanced analytics and innovative technologies.
Plays a crucial role in creating operational dashboards and KPIs for different departments.
Indicates a comprehensive approach to data analytics and visualization, aligning with organizational objectives and contributing to enhanced operational insights.
Works on assigned daily and weekly tasks to maintain automations and business intelligence (BI) processes.
Uses a ticketing system to ensure a systematic approach in addressing new requests and monitors the progress of tasks, providing a clear and organized way to manage workflows and product orders.
Collaborates with team leads and team members in setting priorities for the tickets, ensuring efficient task management and timely completion.
Conducts trend analysis to identify patterns, anomalies, and potential areas for improvement. Stays informed about industry trends and best practices in data analysis.
Develops and implements predictive models to forecast trends and outcomes.
Evaluates model performance and iterates as needed for continuous improvement.
Creates visually appealing and understandable dashboards and reports.
Communicates findings effectively to both technical and non-technical audiences.
Performs ETL by extracting data from various sources, transforming it into a consistent format, and loading it into a destination for analysis.
Writes scripts or codes to perform statistical analysis on datasets.
Performs other related duties as required and assigned.
Skills And Attributes
Capability to generate innovative ideas and think outside conventional boundaries to foster creativity.
Adaptability to adjust and thrive in changing environments.
Strong analytical and problem-solving skills.
Ability to quickly grasp and comprehend new information or skills.
Attention to detail and ability to work independently or in a team.
Supervisory Responsibility
This position has no supervisory responsibly.
Work Environment
This is a remote position which would ideally be conducted in a home office setting. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines.
Physical Demands
This position requires frequent sitting, walking, standing, reading, seeing, speaking, hearing, listening, organizing, interpreting data and information, operating office equipment, typing using a computer keyboard and mouse, viewing a computer screen monitor, and use of a telephone. Some positions may require lifting, occasional bending or kneeling. The working environment consists of an indoor and climate-controlled setting the majority of the time. The employee may regularly lift and/or move objects up to 25 pounds.
Travel
Travel is primarily local during the business day, although some out-of-area and overnight travel may be expected.
Required Education And Experience
BA
2-4 years’ experience in healthcare or Tech environment.
Strong knowledge of Receiving, Warehouse, and Material Handling operations, methods, and practices.
Additional Eligibility Qualifications
Power BI or Data Visualization Certification preferred.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice
Powered by JazzHR
fp3q8HxeJT
Show more
Show less","Data Analysis, Data Visualization, AI, Machine Learning, Chatbot Technologies, Advanced Analytics, Business Intelligence, Data Warehousing, ETL, Statistical Analysis, Predictive Modeling, Dashboard Creation, Report Creation, Data Communication, Power BI, Data Visualization Certification","data analysis, data visualization, ai, machine learning, chatbot technologies, advanced analytics, business intelligence, data warehousing, etl, statistical analysis, predictive modeling, dashboard creation, report creation, data communication, power bi, data visualization certification","advanced analytics, ai, business intelligence, chatbot technologies, dashboard creation, data communication, data visualization certification, dataanalytics, datawarehouse, etl, machine learning, powerbi, predictive modeling, report creation, statistical analysis, visualization"
DATA - BI Developer - Hybrid Houston,Addison Group,"Houston, TX",https://www.linkedin.com/jobs/view/data-bi-developer-hybrid-houston-at-addison-group-3556667398,2023-12-17,Baytown,United States,Mid senior,Onsite,"Job Title:
Business Intelligence Developer
Location:
Hybrid Onsite Houston, TX
Type:
Direct Hire
Salary:
$110k-$145k
No Sponsorship or Visa Transfer Available
The Business Intelligence Developer is responsible for designing and implementing business intelligence solutions that align with modern architectures and technologies so that business reporting is scalable, accurate and useful. The ideal candidate will have a solid understanding of the underlying processes of an upstream and/or midstream oil and gas company coupled with the technical wherewithal to deliver solutions to the business.
Essential Job Functions
Design, develop, test, & deploy ETL solutions to move data from various source systems into data warehouse and/or analytics platforms
Develop dashboards, reports, key performance indicators (KPIs) and metrics utilizing SSRS, Power BI and other front-end reporting tools
Develop business intelligence roadmaps to improve organizational maturity around analysis solutions & systems
Develop strategic enterprise-level BI solutions that drive improved decision-making, efficiency and performance
Proficiency in T-SQL, with the ability to understand, formulate and create efficient queries, views and stored procedures to prepare data for ETL, ad-hoc requests and presentation within the reporting tools
Maintain data warehousing processes including load management, scheduling, performance monitoring and tuning
Support, maintain and enhance documentation for new applications/solutions as well as legacy solutions
Minimum Qualifications:
Four-year degree in Computer Science or Computer Information Systems, or equivalent work experience
Minimum ten years’ in business intelligence solutions implementation and design roles
Minimum three years in a leadership or advisor role related to business intelligence
Minimum three years Oil and gas industry experience with a general understanding of oil and gas operations, terminology, and how information flows through the business
Strong T-SQL experience
Industry knowledge of various business intelligence solutions, languages, & providers, including cloud service providers such as Microsoft Azure Data Factory & Power BI, Snowflake & Sigma, Amazon AWS/Glue & Redshift, etc.
Understanding of OLAP and Tabular model design for data warehouses
Preferred Qualifications:
???????
Experience managing projects around the implementation of technology solutions/applications through to effective user adoption/change management procedures
Experience managing and/or working within certain oil & gas applications
Enertia
P2 ProCount
EMK3
Maestro
Enersight
Traditional development stack experience
Show more
Show less","Business Intelligence, Data Warehousing, ETL, Dashboards, Reporting, SSRS, Power BI, TSQL, Data Modeling, OLAP, Tabular Modeling, Microsoft Azure Data Factory, Snowflake, Sigma, Amazon AWS, Glue, Redshift, Enertia, P2 ProCount, EMK3, Maestro, Enersight","business intelligence, data warehousing, etl, dashboards, reporting, ssrs, power bi, tsql, data modeling, olap, tabular modeling, microsoft azure data factory, snowflake, sigma, amazon aws, glue, redshift, enertia, p2 procount, emk3, maestro, enersight","amazon aws, business intelligence, dashboard, datamodeling, datawarehouse, emk3, enersight, enertia, etl, glue, maestro, microsoft azure data factory, olap, p2 procount, powerbi, redshift, reporting, sigma, snowflake, ssrs, tabular modeling, tsql"
Sr Data Engineer,Diverse Lynx,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-diverse-lynx-3682304958,2023-12-17,Baytown,United States,Mid senior,Onsite,"Role - Sr Data Engineer
Location – Houston, USA (Onsite, Hybrid)
Duration – Contract
Job Description
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications For Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","SQL, Relational Databases, Data Pipelines, Big Data Technologies, AWS, Analytics Tools, Data Systems, Data Structures, Metadata, Dependency Management, Workload Management, Data Transformation, Data Extraction, Message Queuing, Stream Processing, Big Data Data Stores","sql, relational databases, data pipelines, big data technologies, aws, analytics tools, data systems, data structures, metadata, dependency management, workload management, data transformation, data extraction, message queuing, stream processing, big data data stores","analytics tools, aws, big data data stores, big data technologies, data extraction, data structures, data systems, data transformation, datapipeline, dependency management, message queuing, metadata, relational databases, sql, stream processing, workload management"
Sr. Data Analyst,Spruce Power,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-analyst-at-spruce-power-3787722493,2023-12-17,Baytown,United States,Mid senior,Onsite,"THE POSITION: Sr. Data Analyst
Responsible for performing analysis and generating actionable recommendations for improving performance in a leading residential solar energy services company.
Reports to the Business Intelligence Manager and conducts advanced analysis of data to arrive at insights that drive tactical and strategic efforts. The analyst is expected to work closely with the solar asset operations team and the external data service provider to proactively monitor and detect/diagnosis underperforming solar systems. He/she is also expected to create/automate weekly, monthly and ad hoc reports as well as perform analysis to guide data-driven business decisions.
The ability to dive deep into data to arrive at insights that inform tactical and strategic efforts is required. The role also requires an attention to detail and the aptitude to tell a complete, compelling and actionable story through data and analysis.
Key responsibilities include:
Generate and maintain reports to track solar system performance on a weekly, monthly, and quarterly basis
Extract, parse and translate large production data sets into meaningful reports
Work closely with various data service providers, maintain data quality and maintain/improve automated dispatch system
Escalate technical support issues to internal and external resources
Utilize advanced analytics to drive new insights from various data sets and help generate actionable steps for improving solar system performance
Define, document and calculate Key Performance Indicators (KPIs) and methods for tracking these KPIs for internal and external stakeholders
Generate and publish reports and visualizations with Power BI and/or other applications to be used by management for informed decision making and tracking performance
Create and present analysis to upper management and other key internal and external stakeholders
Assist in identifying and advancing Business Intelligence and Analytical capabilities and functionality thought out the organization
Performs other duties as assigned
Qualifications
Deep experience with Power BI software
Proven focus on and experience with generating actionable recommendations through data-driven analysis
Ability to understand how data maps to business processes and to make recommendations on how to adjust procedures to gain efficiencies, increase quality or improve service
Ability to communicate clearly and succinctly
Proven experience with a combination of SQL, SSRS, SSAS, Power BI, Excel
Ability to write complex SQL queries and work with Sql Server relational databases
Strong Power Point presentation skills
Experience with programming languages including Python and R are preferred, but not required
Experience in data management and data quality concepts, practices and services
Experience with various file formats (XML, CSV, etc.)
Outstanding organizational, analytical and facilitation skills.
Experience with Microsoft Office Suite…Excel, Word and PowerPoint
Familiarity with Azure Cloud is preferred, but not required
Experienced with Data Warehouse concepts and practices
Familiar with ETL and data transfer concepts and capabilities
Authorized to work in the United States
Experience in solar industries a plus
EDUCATION
Minimum of a Bachelor's degree or equivalent in computer science or STEM field
EQUAL OPPORTUNITY EMPLOYER
We value a diverse work environment. Spruce Power is an equal opportunity employer and hires without consideration to race, religion, national origin, age, gender, sexual orientation, marital status, veteran status or disability.
Powered by JazzHR
FVGbTPWjIT
Show more
Show less","Power BI, SQL, SSRS, SSAS, Python, R, Azure Cloud, Data Warehouse, ETL, XML, CSV, Data management, Data quality, Microsoft Office Suite, Data visualization","power bi, sql, ssrs, ssas, python, r, azure cloud, data warehouse, etl, xml, csv, data management, data quality, microsoft office suite, data visualization","azure cloud, csv, data management, data quality, datawarehouse, etl, microsoft office suite, powerbi, python, r, sql, ssas, ssrs, visualization, xml"
Principal Data Engineer,Inceed,"Houston, TX",https://www.linkedin.com/jobs/view/principal-data-engineer-at-inceed-3770814508,2023-12-17,Baytown,United States,Mid senior,Onsite,"Compensation:
$140,000-$160,000
Location:
Houston, TX Hybrid
Principal Data Engineer
Inceed has partnered with a growing supply chain and logistics company to help find an eager and experienced Principal Data Engineer to add to their team!
The successful candidate will be responsible for leading a team of data engineers who are responsible for designing, building, and maintaining robust data pipelines and processing systems.
Responsibilities:
Lead and inspire a team of data engineers, providing day-to-day guidance and support to ensure the successful design, development, and maintenance of data pipelines and processing systems.
Utilize your deep expertise in technologies such as Azure, Spark SQL, SQL Server, Azure Data Factory, Azure Synapse, Azure Databricks, Dimensional Modeling, Agile, Azure DevOps, and DataOps to drive innovative and efficient solutions.
Provide technical guidance and promote best practices in data engineering, ensuring the team follows industry standards and delivers high-quality solutions.
Take ownership of the successful delivery of data engineering projects, collaborating with cross-functional teams and resolving technical challenges.
Assist in shaping and evolving our data infrastructure to support current and future data needs.
Demonstrate strong verbal and written communication skills, effectively communicating technical concepts to both technical and non-technical stakeholders.
Required Qualifications & Experience:
10+ years of overall working experience.
5+ years of experience in a data engineer, data integration, or data warehousing role.
5+ years of hands-on experience with Azure Data Factory, Azure Synapse, and Azure Databricks.
Proficiency in SQL programming and experience working with SQL Server.
Strong communication skills, both verbally and in writing.
Perks & Benefits:
Competitive salary
Health, dental, and vision insurance
Retirement plans
Flexible working hours
Bonus opportunity
Opportunities for professional development and growth
Other Information:
This position requires a hybrid schedule of 4 days onsite, 1 day remote.
If you are interested in learning more about the Principal Data Engineer opportunity, please submit your resume for consideration. Our client is unable to provide sponsorship at this time.
We are Inceed, a staffing and direct placement firm who believes in the possibility of something better. Our mission is simple: We’re here to help every person, whether client, candidate, or employee, find and secure what’s better for them.
Inceed is an equal opportunity employer. Inceed prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Show more
Show less","Data Engineering, Azure, Spark SQL, SQL Server, Azure Data Factory, Azure Synapse, Azure Databricks, Dimensional Modeling, Agile, Azure DevOps, DataOps, SQL Programming","data engineering, azure, spark sql, sql server, azure data factory, azure synapse, azure databricks, dimensional modeling, agile, azure devops, dataops, sql programming","agile, azure, azure data factory, azure databricks, azure devops, azure synapse, data engineering, dataops, dimensional modeling, spark sql, sql, sql server"
Staff data engineer,bp,Greater Houston,https://www.linkedin.com/jobs/view/staff-data-engineer-at-bp-3703467987,2023-12-17,Baytown,United States,Mid senior,Onsite,"Entity:
Innovation & Engineering
Job Family Group:
IT&S Group
Job Summary:
Are you passionate about bringing together the world of data and business to motivate change? Data and Analytics are a cross-collaborative team of experts who build, integrate, manage, and maintain intelligent data products at rapid pace.
We’re looking for curious minds who are excited by the opportunity to create and deliver an exciting suite of digital products and services to advance the global energy transition. You will need to solve complex business challenges whilst collaborating and co-innovating globally, backed by a culture that encourages creativity and curiosity, and a team that brings out the best in each other.
Job Description:
Key Responsibilities
Leads the team to write, deploy, and maintain software to build, integrate, manage, maintain, and quality-assure data
Architects, designs, implements, and maintains reliable and scalable data solutions in the Azure cloud environment using Scrum/Agile methodology
Establishes metadata management, data catalogs, data lineage, data standards, data stewardship, and standard practices across the organization
Leads the team to establish Data Governance model including people, process, and technology to sustain data quality
Researches and promotes new tools and techniques to shape the future of the data engineering environment
Works closely with data scientists, micro-service developers, and security authorities to build out data platform incrementally and securely
Develops data modelling standard processes and acts as a domain expert for data models across all areas of the business
Mentors and shares knowledge with the team to provide design reviews, discussions, and prototypes
Leads customer discussions from a technical standpoint to deploy, run, and audit standard methodologies for cloud products
Guides team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI / CD pipeline
Works with following site-reliability engineering standard methodologies: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity
Job Requirements
Education
Bachelor or higher degree in computer science, Engineering, Information Systems, or other quantitative fields
Required Experience
Demonstrable coding expertise in one or more object-oriented programming languages (e.g., Python, Scala, Java, etc)
Deep and hands-on experience (typically 7+ years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments
Hands on experience with:
Databricks and using Spark for data processing (batch and/or real-time)
Configuring Delta Lake on Azure Databricks
Languages: Python, Scala, SQL
Cloud platforms: Azure (ideally) or AWS
Azure Data Factory
Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
Data modeling with relational or data-warehouse systems
Sophisticated hand-on experience with different query languages
Azure DevOps (or similar tools) for source control & building CI/CD pipelines
Understanding Data Structures & Algorithms & their performance
Experience designing and implementing large-scale distributed systems
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Internal client management and ability to lead large organizations via influence
Why join us
At bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly. There are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others! We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Travel Requirement
Negligible travel should be expected with this role
Relocation Assistance:
This role is not eligible for relocation
Remote Type:
This position is a hybrid of office/remote working
Skills:
Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Show more
Show less","Python, Java, Scala, Azure, Azure DevOps, SQL, Spark, Databricks, Delta Lake, Azure Data Factory, Azure Data Lake, Azure SQL DB, Synapse, Cosmos DB, Azure Storage, Stream Analytics, Event Hubs, Data modeling, Data warehouses, Data structures, Algorithms, Distributed systems, Data governance, Data quality, Data stewardship, Data lineage, Metadata management, Data catalogs, Data standards, Data management, CI/CD, Microservices, Agile methodology, Scrum","python, java, scala, azure, azure devops, sql, spark, databricks, delta lake, azure data factory, azure data lake, azure sql db, synapse, cosmos db, azure storage, stream analytics, event hubs, data modeling, data warehouses, data structures, algorithms, distributed systems, data governance, data quality, data stewardship, data lineage, metadata management, data catalogs, data standards, data management, cicd, microservices, agile methodology, scrum","agile methodology, algorithms, azure, azure data factory, azure data lake, azure devops, azure sql db, azure storage, cicd, cosmos db, data catalogs, data governance, data lineage, data management, data quality, data standards, data stewardship, data structures, data warehouses, databricks, datamodeling, delta lake, distributed systems, event hubs, java, metadata management, microservices, python, scala, scrum, spark, sql, stream analytics, synapse"
Principal Data Engineer,MD Anderson Cancer Center,"Houston, TX",https://www.linkedin.com/jobs/view/principal-data-engineer-at-md-anderson-cancer-center-3766419805,2023-12-17,Baytown,United States,Mid senior,Remote,"The Principal Data Engineer in the area of Data Analytics & Delivery is a pivotal role in the Enterprise Data Engineering & Analytics Department in operationalizing critical data and analytics for MD Anderson's digital business initiatives. The Principal Data Engineer manages business requirements gathering, end-to-end solution planning and optimizes data analytics delivery within the Context Engine. The Principal Data Engineer partners with other Enterprise Data Engineering & Analytics teams to manage & build analytics deliverables for production use by our key data and analytics consumers.
The Principal Data Engineer also manages and coordinates data analytics delivery activities in compliance with data governance processes and data security requirements. This results in enabling faster data delivery, integrated data reuse and vastly improved time-to-solution for MD Anderson data and analytics initiatives.
The Principal Data Engineer role requires working creatively and collaboratively with IS and Institutional leaders across the enterprise. It involves evangelizing effective data accessibility practices and promoting better understanding of data and analytics. The Principal Data Engineer partners closely with teams across MD Anderson, including Enterprise Development & Integration and Enterprise Data Science departments in the build out and delivery of end-to-end analytic solutions through the Context Engine Framework.
Data Engineering - End-to-End Solution Delivery
Lead/Communicate/Participate End-to-end solution delivery that increases information capabilities and realizes data value across the institution. End-to-End solutions include build out of data sources and tools across the Context Engine framework by integrating data governance processes through data ingestion, ingress, egress, curation, pipeline build, data transformation and modeling steps. Incorporating highly integrated data governance processes that consistently tracking data provenance, security, data quality and ontology as well as through to data visualization and insights.
Lead/Communicate/Participate in existing end-to-end data pipelines consisting of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases).
Lead/Communicate/Participate and incorporate data governance and metadata management processes into the data ingestion, curation and pipeline building efforts.
Lead/Promote Data Analytics & Delivery efforts and manage relationships with stakeholders across the organization. This includes proactively communicating with stakeholders and prioritizing work for the team.
Drive and lead data requirements for various end-to-end analytics deliverables to ensure we are delivering what is needed, not only what is requested.
Lead/Communicate/Participate and implement complex data analytics deliverables, including data analysis, report requests, metrics, extracts, visualizations, projects or dashboards in a timely manner by leveraging tools and methodologies in line with the Context Engine Strategy.
Lead/Communicate/Perform complex problem solving and formulation and testing and analysis of data. Designs queries using structure query language and NoSQL.
Collaborate with other data engineers on integration efforts. Promote and ensure institutional data management strategies.
Standards, Testing and Maintenance
Manage, coordinate and adhere to standard operating procedures set by IS division as well as all MDA policies and maintain build standards (data steward / governance oversight sign off) for support of MDA Institutional data strategy including Context Engine.
Manage Documentation preparation as needed for the implementation of enhancements or new technology
Manage & follow documented change control processes and may perform change control audits
Manage & perform quality control and testing and review the build of other analysts to ensure that solutions are technically sound
Oversee analytics system updates/new releases for assigned modules
Manage and execute the adherence to regulatory requirements, quality standards and best practices for systems and processes, and collaborate with internal and external stakeholders
Lead and/or participate in after-hours application support and downtime procedures
Educate and Train
Lead, promote & train counterparts, such as data scientists, data analysts, LOB users or any data consumers, in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
Lead, plan & establish training plans for various systems in the Context Engine Tools suite and develop curricula in partnership with the MDA Training team and EDEA system experts.
Provide institutional, department and one-on-one training on EDEA deliverables.
Coach and provide advice, guidance, encouragement, constructive feedback and transfer knowledge to less experienced team members across OneIS and the institution.
Manage liaison relationships with customers and OneIS to provide effective technical solutions and customer service.
OneIS
To provide innovative, quality, and sustainable IT solutions and services. Our success is driven by our people through Integrity and Trust, Partnership, and Quality.
Promotes trust, respect, support, and honestly with customers and each other.
Commits to being a good partner focused on building productive, collaborative, and trusting relationships with our customers and each other.
Models a commitment to excellence and strives to continually improve. Achieves desired outcomes, usability, and value that exceed expectations of others and our own.
Other duties as assigned
Education Required: Bachelor's degree.
Preferred Education: Master's Level Degree
Certification Required: Must obtain at least one Epic Data Model certification (Clinical, Access, or Revenue) issued by Epic within 180 days of date of entry into job.
Preferred Certification: the Access Data Model or the Clinical Data Model.
Experience Required: Seven years of relevant information technology experience. May substitute required education with years of related experience on a one to one basis. With preferred degree, five years of experience required.
Preferred Experience: Epic Cogito Analytics Experience
Prior data warehouse and business intelligence solutions experience.
Healthcare industry experience.
Web intelligence experience
Prior experience in building Foundry data pipelines
It is the policy of The University of Texas MD Anderson Cancer Center to provide equal employment opportunity without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, disability, protected veteran status, genetic information, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.html
Additional Information
Requisition ID: 163593
Employment Status: Full-Time
Employee Status: Regular
Work Week: Days
Minimum Salary: US Dollar (USD) 119,500
Midpoint Salary: US Dollar (USD) 149,500
Maximum Salary : US Dollar (USD) 179,500
FLSA: exempt and not eligible for overtime pay
Fund Type: Hard
Work Location: Remote
Pivotal Position: No
Referral Bonus Available?: Yes
Relocation Assistance Available?: Yes
Science Jobs: No
Show more
Show less","Data Engineering, Data Analytics, Data Management, Big Data, Data Governance, Data Security, Data Visualization, Data Warehousing, Business Intelligence, ETL, Data Integration, Data Quality, Data Modeling, Data Pipelining, Data Analysis, Reporting, Metrics, Dashboards, SQL, NoSQL, Hadoop, Hive, Spark, Informatica, Tableau, Power BI, GIS, Cloud Computing, AWS, Azure, GCP, Machine Learning, Artificial Intelligence, Data Science, Python, R, Java, C++, Unix, Linux, Windows","data engineering, data analytics, data management, big data, data governance, data security, data visualization, data warehousing, business intelligence, etl, data integration, data quality, data modeling, data pipelining, data analysis, reporting, metrics, dashboards, sql, nosql, hadoop, hive, spark, informatica, tableau, power bi, gis, cloud computing, aws, azure, gcp, machine learning, artificial intelligence, data science, python, r, java, c, unix, linux, windows","artificial intelligence, aws, azure, big data, business intelligence, c, cloud computing, dashboard, data engineering, data governance, data integration, data management, data quality, data science, data security, dataanalytics, datamodeling, datapipeline, datawarehouse, etl, gcp, gis, hadoop, hive, informatica, java, linux, machine learning, metrics, nosql, powerbi, python, r, reporting, spark, sql, tableau, unix, visualization, windows"
"Data Conversion Developer, Senior Associate",PwC,"Houston, TX",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749939440,2023-12-17,Baytown,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Maximo, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), Azure ADF, AWS Glue, SSIS, DataBricks, Python, PySpark, Scala, SQL","maximo, ibm db2, oracle, microsoft sql server, maximos integration framework mif, azure adf, aws glue, ssis, databricks, python, pyspark, scala, sql","aws glue, azure adf, databricks, ibm db2, maximo, maximos integration framework mif, microsoft sql server, oracle, python, scala, spark, sql, ssis"
Expression of Interest: Data Scientist,Fingerprint for Success (F4S),"Houston, TX",https://www.linkedin.com/jobs/view/expression-of-interest-data-scientist-at-fingerprint-for-success-f4s-3787782245,2023-12-17,Baytown,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
nJk3fcIHtA
Show more
Show less",F4S work style assessment,f4s work style assessment,f4s work style assessment
Senior Backend Engineer / Data Engineer,Fluence,"Houston, TX",https://www.linkedin.com/jobs/view/senior-backend-engineer-data-engineer-at-fluence-3774813683,2023-12-17,Baytown,United States,Mid senior,Hybrid,"About Fluence:
Fluence Energy, Inc. (Nasdaq: FLNC) is a global market leader in energy storage products and services, and optimization software for renewables and storage. With a presence in over 47 markets globally, Fluence provides an ecosystem of offerings to drive the clean energy transition, including modular, scalable energy storage products, comprehensive service offerings, and the Fluence IQ Platform, which delivers AI-enabled SaaS products for managing and optimizing renewables and storage from any provider. Fluence is transforming the way we power our world by helping customers create more resilient and sustainable electric grids.
For more information, visit our website , or follow us on LinkedIn or Twitter . To stay up to date on the latest industry insights, sign up for Fluence's Full Potential Blog .
OUR CULTURE AND VALUES
We are guided by our passion to transform the way we power our world. Achieving our goals requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed.
We are Leading
Fluence currently has thousands of MW of energy storage projects operated or awarded worldwide in addition to the thousands of MW of projects managed by our trading platform—and we are growing every day.
We are Responsible
Fluence is defined by its unwavering commitment to safety, quality, and integrity.
We are Agile
We achieve our goals and meet our customer’s needs by cultivating curiosity, adaptability, and self-reflection in our teams.
We are Fun
We value the diversity in thought and experience of our coworkers and customers. Through honest, forthcoming, and respectful communications we work to ensure that Fluence is an inclusive and welcoming environment for all.
As a Senior Backend Software Engineer / Data Engineer, you will be responsible for developing and maintaining the backend systems and data infrastructure for a large grid-scale storage energy company. Your role will involve working closely with our software engineering teams to optimize access to operational data and design and implement data models and APIs that work well for our user-facing application. You will also work heavily with our data science teams to build and optimize data pipelines, integrate data sets, and ensure data quality and governance.
What does a Senior Backend Engineer/Data Engineer do at Fluence?
Build and optimize data pipelines to ensure efficient data integration and flow across various sources and destinations.
Implement and maintain automated testing and deployment processes to ensure the reliability and scalability of the backend systems.
Optimize the performance and scalability of the backend systems to handle large-scale data processing and storage requirements, and design load testing to test and prove systems before field deployments.
Collaborate with cross-functional teams to design and implement data models that support product features and data-driven decision-making.
Develop and maintain backend systems for data cataloging, schema management, and data discovery to facilitate effective data governance and access.
What does the ideal candidate look like?
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
5+ years of experience working on high-traffic systems, optimizing performance, and implementing scalable solutions to meet business needs effectively.
Strong knowledge of backend programming languages such as Python, Go, C++, and Rust.
Experience with database systems, including PostgreSQL, TimescaleDB as well as interacting with larger, more static stores such as data lakes.
Strong understanding of professional software development practices, including version control, code reviews, and continuous integration and deployment.
Familiarity with testing frameworks and automation tools for backend software development, such as Jenkins and TestRail.
Excellent problem-solving and communication skills, with the ability to work effectively in a collaborative team environment.
Experience in the energy industry is a plus, but not a must.
$0 - $0 a year
Fluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.
Show more
Show less","Python, Go, C++, Rust, PostgreSQL, TimescaleDB, Data Lakes, Version control, Code reviews, Continuous integration, Deployment, Testing frameworks, Automation tools, Jenkins, TestRail, SQL, NoSQL, Data pipelines, Data modeling, Data governance, Data cataloging, Schema management, Data discovery, Data quality, Data integration, Data science, Machine learning, Artificial intelligence, Energy storage, Renewables, Optimization, Software development, Backend programming, Software engineering, Load testing","python, go, c, rust, postgresql, timescaledb, data lakes, version control, code reviews, continuous integration, deployment, testing frameworks, automation tools, jenkins, testrail, sql, nosql, data pipelines, data modeling, data governance, data cataloging, schema management, data discovery, data quality, data integration, data science, machine learning, artificial intelligence, energy storage, renewables, optimization, software development, backend programming, software engineering, load testing","artificial intelligence, automation tools, backend programming, c, code reviews, continuous integration, data cataloging, data discovery, data governance, data integration, data lakes, data quality, data science, datamodeling, datapipeline, deployment, energy storage, go, jenkins, load testing, machine learning, nosql, optimization, postgresql, python, renewables, rust, schema management, software development, software engineering, sql, testing frameworks, testrail, timescaledb, version control"
Global Market Leading Energy Firm - Staff Data Engineer,Xcede,"Houston, TX",https://www.linkedin.com/jobs/view/global-market-leading-energy-firm-staff-data-engineer-at-xcede-3606685239,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Global Market Leading Energy Firm - Staff Data Engineer
My client is a Global Market Leading Energy Firm leading the global energy transition. They offer the opportunity to work with cutting-edge technologies with excellent professional growth. They have a great reputation across industry & offer an excellent work environment.
The role is paying $180,000 - $210,000 base plus a bonus & stock.
NOTE: The role is flexible with 3 days in the Chicago office and 2 days working from home - this is non-negotiable.
·
MUST
have a Masters's degree or a relevant bachelor's degree - with a
high GPA from college or university.
·
Hands-on experience
designing, planning, productionizing & maintaining reliable and scalable data infrastructure and data products in complex environments.
·
Experience
designing and implementing large-scale distributed systems.
·
Experience
with different query languages
·
Demonstrable
coding expertise in one or more object-oriented programming languages.
Show more
Show less","Data Engineering, Data Infrastructure, Data Products, Distributed Systems, Query Languages, ObjectOriented Programming","data engineering, data infrastructure, data products, distributed systems, query languages, objectoriented programming","data engineering, data infrastructure, data products, distributed systems, objectoriented programming, query languages"
Senior Cloud Data Engineer,BDO USA,"Houston, TX",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469453,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, .Net, Qlik, Tableau, Synapse, IoT, Data Lake, Stream, Cube, Microsoft, SQL Server, RedShift, UiPath, Cloud, RPA, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Athena, Data Pipeline, Glue, Star Schema, SSIS, SSAS, SSRS, PySpark, Delta, Pandas, Spark SQL, dbt, Terraform, Bicep, Data Ops, Purview, Hadoop","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, net, qlik, tableau, synapse, iot, data lake, stream, cube, microsoft, sql server, redshift, uipath, cloud, rpa, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, athena, data pipeline, glue, star schema, ssis, ssas, ssrs, pyspark, delta, pandas, spark sql, dbt, terraform, bicep, data ops, purview, hadoop","ai algorithms, alteryx, application development, artificial intelligence, athena, automation tools, aws, aws lake formation, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud, cloud data analytics, computer vision, cube, data lake, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, glue, hadoop, iot, java, kinesis, linux, machine learning, microsoft, microsoft fabric, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, sql server, ssas, ssis, ssrs, star schema, star schema construction, stream, streaming data ingestion, synapse, tableau, tabular modeling, terraform, uipath"
Power BI Data Analyst,ClearpointCo,"Houston, TX",https://www.linkedin.com/jobs/view/power-bi-data-analyst-at-clearpointco-3781732472,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Title: Power BI Data Analyst
Client: Energy Oil & Gas Company
Location: Houston, Texas
Pay: $55/hour
Type: Contract role (Houston, Texas)
Summary:
As a Data Analyst specializing in Power BI, and/or Python you will play a pivotal role in transforming raw data into actionable insights that drive informed decision-making within our supply chain operations. The ideal candidate will possess a strong analytical mindset, attention to detail, and the ability to work independently. This is a project-based role with the potential for long-term engagement based on performance.
***Must have portfolio with case studies to showcase Power BI projects.***
Duties:
Data Analysis focused on analyzing large datasets to extract meaningful trends, patterns, and insights.
Utilize Power BI to create interactive and visually appealing dashboards for reporting purposes.
Disseminate data developing Excel spreadsheet templates maintaining reports that provide key performance indicators (KPIs) to support supply chain decision-making.
Collaborate with cross-functional teams to understand reporting requirements and deliver timely, accurate reports.
Leverage Power BI expertise demonstrating proficiency in Power BI, including data modeling, DAX calculations, and dashboard design.
Extract, transform, and load (ETL) data from various sources into Power BI for analysis and reporting purposes.
Design and implement robust data models and relationships to ensure accurate and efficient data analysis.
Perform data cleansing, validation, and manipulation to maintain data accuracy and consistency.
Stay current with industry best practices and advancements in Power BI functionality.
Perform Data Visualization designing visually compelling and user-friendly dashboards that effectively communicate complex data insights to stakeholders.
Ensure data visualizations align with business objectives and enhance overall decision-making processes.
Manage Power BI assets, including reports, dashboards, workspaces, semantic models, and other components.
Oversee the sharing and distribution of Power BI items, ensuring accessibility and collaboration among team members.
Implement and maintain security measures to safeguard Power BI assets and sensitive data.
Requirements:
Must have portfolio with case studies to showcase Power BI projects.
Minimum of 2-4 years of relevant experience in data analysis with a focus on Power BI.
Proven experience as a Data Analyst with a focus on Power BI, including Power Query and Power Pivot.
Proficiency in Python is highly preferred.
Proficiency in SQL for data manipulation, extraction, and querying.
Proficiency in a range of other sources of data ingestion.
Ability to translate complex business requirements into technical specifications for data visualization.
Proficiency in data visualization techniques and best practices.
Education:
Bachelor's degree in Data Science, Business Analytics, or related field.
Show more
Show less","Power BI, Python, Data Analysis, Data Visualization, DAX, ETL, Data Modeling, Excel, SQL, Business Analytics, Power Query, Power Pivot","power bi, python, data analysis, data visualization, dax, etl, data modeling, excel, sql, business analytics, power query, power pivot","business analytics, dataanalytics, datamodeling, dax, etl, excel, power pivot, power query, powerbi, python, sql, visualization"
"Data Scientist - Up to $150,000 + Huge Bonus + Package",Hunter Bond,"Houston, TX",https://www.linkedin.com/jobs/view/data-scientist-up-to-%24150-000-%2B-huge-bonus-%2B-package-at-hunter-bond-3784570993,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Job title: Data Scientist- Up to $140,000 + Huge Bonus + Package
Client: Scaling fintech
Experience Level: 1+ years'
Salary: Up to $140,000 + Huge Bonus + Package
Location: Houston (Hybrid)
Sells: Cutting-edge tech, ownership of multiple greenfield projects, no red tape, gold medal Olympiads, highest regarded technologists around, a welcoming/ collaborative environment, fantastic office spaces
An elite scaling fintech are searching for Data Scientists to join a group of the highest-regarded talent around!
This team has an unlimited tech budget, promotes a great culture, and is made up of incredible like-minded individuals.
Role:
Develop a deep understanding of North American Energy Markets and build cutting edge analytics tools for ML models.
Working as part of a high performing team solving complex data issues and making deductions which you'll see directly impact the business.
Skills/Experience:
3+ years Experience as a Data Scientist
Degree in Computer Science/ Engineering or Mathematics (or related field)
Good skills in Python
Strong SQL proficiency and understanding of database technologies, such as AWS RDS and Snowflake
Strong proficiency in Python, with knowledge of common ML and statistical packages such as Scikit-Learn, SciPy, Prophet, etc
Excellent exposure to Generalized Linear and Non-Linear Models, Time Series Analysis, Random Forest.
Show more
Show less","Data Science, Python, SQL, AWS RDS, Snowflake, ScikitLearn, SciPy, Prophet, Generalized Linear Models, NonLinear Models, Time Series Analysis, Random Forest, Statistics, Machine Learning, ETL, Data Visualization, Business Intelligence, Data Mining, Data Analysis, Advanced Analytics, Hypothesis Testing, Causal Inference, Experimental Design, Ensemble Methods, Natural Language Processing, Cloud Computing, Big Data, Hadoop, Spark, Hive, Pig, Greenplum, Redshift, Teradata","data science, python, sql, aws rds, snowflake, scikitlearn, scipy, prophet, generalized linear models, nonlinear models, time series analysis, random forest, statistics, machine learning, etl, data visualization, business intelligence, data mining, data analysis, advanced analytics, hypothesis testing, causal inference, experimental design, ensemble methods, natural language processing, cloud computing, big data, hadoop, spark, hive, pig, greenplum, redshift, teradata","advanced analytics, aws rds, big data, business intelligence, causal inference, cloud computing, data mining, data science, dataanalytics, ensemble methods, etl, experimental design, generalized linear models, greenplum, hadoop, hive, hypothesis testing, machine learning, natural language processing, nonlinear models, pig, prophet, python, random forest, redshift, scikitlearn, scipy, snowflake, spark, sql, statistics, teradata, time series analysis, visualization"
Power BI Data Analyst,Korn Ferry,"Houston, TX",https://www.linkedin.com/jobs/view/power-bi-data-analyst-at-korn-ferry-3785871646,2023-12-17,Baytown,United States,Mid senior,Hybrid,"We have partnered with our global technology company client in search for a Power Business Intelligence Analyst for their corporate office for a long-term contract. Our client is open to hybrid or remote consultant and seeking an someone available to start by next week.
Responsibilities
Power BI expert that can help us build reports with data sets that already exist
Automation and clean up of what exists
Ability to manually load budgets and excel based datasets
Ability to create ad hoc reporting in BI by linking excel files/data sets
Ability to build in calculations in a BI reports
Ability to put together data sets out of ERP systems and put them in a data warehouse. This person will build formulas and reporting.
Skills Required
BI Power User
Finance background
Education & Work
Experience
Bachelors degree in related discipline
5+ years experience
Title Power BI Data Analyst
Location Open
Client Industry Technology
About Korn Ferry
Korn Ferry unleashes potential in people, teams, and organizations. We work with our clients to design optimal organization structures, roles, and responsibilities. We help them hire the right people and advise them on how to reward and motivate their workforce while developing professionals as they navigate and advance their careers. To learn more, please visit Korn Ferry at
www.Kornferry.com
Show more
Show less","Power BI, Data Analysis, Data Visualization, Reporting, Automation, Data Cleaning, Data Warehousing, ERP Systems, Finance, Excel","power bi, data analysis, data visualization, reporting, automation, data cleaning, data warehousing, erp systems, finance, excel","automation, data cleaning, dataanalytics, datawarehouse, erp systems, excel, finance, powerbi, reporting, visualization"
Senior Data Analyst,"V-Soft Consulting Group, Inc.","Houston, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-v-soft-consulting-group-inc-3782727862,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Hiring-Data Analyst- Houston, TX (Hybrid) - Must go onsite in Downtown Houston 3x a week.
Job Title: Data Analyst
Houston, TX (Hybrid) - Must go onsite in Downtown Houston 3x a week.
6-12 Month Contract
Opening with Chevron for a Data Analyst. I would like to show profiles to the manager today for a quick turn around. Ive attached the resume of someone that has just got the offer on the same team for a data analyst position as well.
The priorities for this role are: (Need local candidates)
• PowerBI
• SQL
• Oil & Gas / Utility domain experience
This is Data analyst IV so we can target 5+ years of experience .
The official Job description is below but again, we only need 5 years of experience despite what the job description is
Seeking data analysts. Responsibilities are to collect, clean, and interpret datasets from various sources to support building of the Surface Data Hub, an ecosystem of defined interrelated data products to service the Surface domain.
Key job responsibilities
• Receive direction from Product owner and Data Architect for data analysis objectives and tasks (story level)
• Act as a mentor to junior Data Analysts on the team
• Collaborating with data architects, data analysts and data engineers to develop the desired solutions
o Collaborate and receive rules and requirements from business, functional, or application SMEs to inform analysis tasks.
o Analyze datasets, entities, application and system of record data sources, define relationships and attributes, and determine source to target mapping requirements
o Clean and validate data to ensure accuracy and quality prior to maturing the feature to build
o Define data engineering requirements for pipeline and data products build
o Spike with other IT professionals to define interface requirements between applications or systems within the Chevron portfolio or externally with vendors apps
o Occasional collaboration with the Enterprise Data Architects and the Surface Platform Architects to develop and refine the Data Models as part of the Chevron Enterprise Data Model
o Initial QC and validation of produced data products before delivering to stakeholders
Required skills
• Strong knowledge of the Facilities business domain and processes
• Strong data-level analytical skills
• Strong interpersonal skills working within a team
• Proficient interpersonal skills working external to the immediate team (i.e. system of record SMEs or admins, functional SMEs, vendor SMEs)
• Proficient SQL skills to build queries
Preferred Requirements
• Basic or awareness of data modeling and approaches
• Experience with cloud services such as ADLS gen 2, Databricks, Synapse, Azure SQL
• Basic or awareness of knowledge of the Chevron Enterprise Data Model
• ERP system experience (e.g. SAP S/4HANA, JDE)
• Product allocation experience (e.g. Energy Components)
• Awareness of industry standards such as ISO-14224, CFIHOS
Location preference:
1. Houston
2. Aberdeen, UK
3. Any US onshore or nearshore option like Buenos Aires that has timezone overlap with Houston and Aberdeen,
Warm Regards
Priyanka
pneelam@vsoftconsulting.com
502-242-1668
#LI-PN1
Show more
Show less","PowerBI, SQL, Oil & Gas / Utility, Data Analysis, Data Modeling, Data Engineering, Azure Data Lake Storage gen 2, Databricks, Synapse, Azure SQL, SAP S/4HANA, JDE, ISO14224, CFIHOS","powerbi, sql, oil gas utility, data analysis, data modeling, data engineering, azure data lake storage gen 2, databricks, synapse, azure sql, sap s4hana, jde, iso14224, cfihos","azure data lake storage gen 2, azure sql, cfihos, data engineering, dataanalytics, databricks, datamodeling, iso14224, jde, oil gas utility, powerbi, sap s4hana, sql, synapse"
Spatiotemporal Data Scientist - 4669,"Delphi-US, LLC","Houston, TX",https://www.linkedin.com/jobs/view/spatiotemporal-data-scientist-4669-at-delphi-us-llc-3766684694,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Title: Senior Data Scientist (Contract) - Job#4669
Location:
Remote or Hybrid (Houston TX)
Job Description:
Our Machine Learning team tackles major opportunities for operational efficiency and revenue generation and deploys these models across environments where they re-shape the way we work. We are seeking a curious, proactive, and innovative Data Scientist to join our team. The ideal candidate has a strong scientific background, experience blending various datasets, building statistical/machine learning models, and communicating the results back to business partners.
Candidates Must Have the following background to be considered:
Domain expertise with methane, air quality measurement, or atmospheric modeling would be very valuable since gas concentrations and movements are a lot of the modeling work. Modeling and studying other related scientific phenomena could be a good backup too.
Spatiotemporal statistical modeling expertise. Time series expertise alone would also be a good backup even if we don’t have the spatial statistics component. Bonus points for someone with Bayesian time series/spatial experience.
Python / R programming to model and analyze the data.
SQL – we’re working with a variety of disparate datasets as we build out sensors at our environmental sites, so it’s important that candidates can navigate uncharted datasets and quickly figure out how to link these accurately. SQL Expertise is a Must: to collect, aggregate new data sources & stage for modelling & analysis
methane measurement and atmospheric sciences
dispersion models
statistical modeling
data management (SQL) experience
Responsibilities:
Apply machine learning, advanced analytical techniques, and critical thinking to solve complex business problems
Seek, build, and consolidate data inputs proactively to create and improve models
Translate business partners’ needs into data science projects and make technical decisions based on the tradeoff between complexity and value
Drive projects, individually and collaboratively, through the data science lifecycle including data collection, exploration/analysis, model development, validation, and deployment
Develop presentations to communicate key messages to senior sponsors, non-technical partners, fellow data scientists, and other stakeholders
Work with technical partners to deploy models and streamline the process to scale our work
Required Qualifications:
Building statistical or machine learning models for prediction and inference
Ability to code within Python (R also considered)
Advanced SQL knowledge to pull and transform data
Education and Experience
Quantitative Foundation: B.S. degree in a quantitative field (e.g., Statistics, Engineering, Computer Science, Meteorology, Physics, Mathematics, Economics, Operations Research).
Experience: 3+ years’ experience in data science building and deploying models into production
Technical Skills
Programming: compose clean, efficient, and reusable code in Python and/or R.
Machine Learning / Modeling: understand what types of algorithms to use, their benefits and drawbacks, and how to implement them.
Data Wrangling: write efficient SQL queries from scratch that blend data from a variety of sources in the right form for modeling, analysis, and scoring.
Statistical Analysis: investigate trends, patterns, and relationships using statistical methods and tests to reach actionable conclusions and understand causal relationships.
Data Visualization: grasp data-ink ratios and bring data-driven stories to life through visuals.
Scientific Research: experience using adjacent scientific literature to advance our thinking and modeling in novel areas where existing research is thin
Soft Skills
Curious: bring intellectual curiosity, an inquisitive nature, and a desire to deepen your knowledge and continue learning.
Ownership: take responsibility to proactively advance projects, contribute to the team/organization, and improve existing processes.
Business acumen: understand the bigger picture for customers and the business while connecting your work to key objectives for both.
Storytelling: write and present messages and insights clearly to persuade non-technical partners that they can confidently use our models and solutions.
Preferred Qualifications
A graduate degree (Masters or PhD) in a quantitative field
Experience with spatial modeling, greenhouse gas emissions, atmospheric modeling, or remote sensing data
Proficiency using git for version control, collaboration, and releases
Expertise with cloud services like AWS (SageMaker, S3, Redshift, etc.) and Snowflake
Built dashboards for visualizing model outputs and performance monitoring using Microsoft PowerBI, Python dash, or similar tools
About Delphi-US
Delphi-US is a national recruiting firm based in Newport, Rhode Island. We specialize in IT, Engineering and Professional Staffing services for organizations from Main Street to Wall Street. Our mission is simple: To connect great people to great companies. We accomplish this with a proprietary skill-based and cultural matching process that results in higher qualified submissions along with increased interviews and offer rates. You’ll find our team is friendly, professional and ready to advocate on your behalf, armed with industry trends, and an understanding of employer expectations.
Show more
Show less","Python, R, SQL, Machine Learning, Modeling, Statistical Analysis, Data Wrangling, Data Visualization, Scientific Research, Spatial Modeling, Greenhouse Gas Emissions, Atmospheric Modeling, Remote Sensing Data, Git, AWS, Snowflake, Microsoft PowerBI, Python Dash, Data Science, Data Management, Data Analysis, Model Deployment, Model Validation, Presenting, Statistics, Meteorology, Physics, Mathematics, Economics, Operations Research, Causal Relationships","python, r, sql, machine learning, modeling, statistical analysis, data wrangling, data visualization, scientific research, spatial modeling, greenhouse gas emissions, atmospheric modeling, remote sensing data, git, aws, snowflake, microsoft powerbi, python dash, data science, data management, data analysis, model deployment, model validation, presenting, statistics, meteorology, physics, mathematics, economics, operations research, causal relationships","atmospheric modeling, aws, causal relationships, data management, data science, data wrangling, dataanalytics, economics, git, greenhouse gas emissions, machine learning, mathematics, meteorology, microsoft powerbi, model deployment, model validation, modeling, operations research, physics, presenting, python, python dash, r, remote sensing data, scientific research, snowflake, spatial modeling, sql, statistical analysis, statistics, visualization"
Azure Data Architect,P97 Networks,Greater Houston,https://www.linkedin.com/jobs/view/azure-data-architect-at-p97-networks-3768650630,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Please note this a Hybrid role with 3 days onsite in Houston.
P97 Networks is the leading provider of mobile commerce and digital marketing solutions which power new and emerging daily journeys. When you’re looking for fuel, food, parking, EV charging, or convenience retail—our innovative cloud-based mobility services platform enables mobile payments, customer loyalty programs, and connectivity with the largest universe of gas stations, charging networks, and convenience stores across the globe. We focus on creating connected commerce experiences whether through your connected car, smart phone, or voice enabled devices like Alexa. We are experts at cloud-based payment and loyalty aggregation, identity and security, digital offers and settlement, omni-channel marketing and data insights, and connectivity with retail sites and point of sale systems—which is required to make it all happen.
Every day, we are re-imagining connected consumer experiences and trying to make life’s daily journeys a little bit easier for everyone. Convenience, speed, and safety is our mission—and these are the expectations for the new generation of buyers, sellers and anyone looking to conduct commerce during their daily drive. P97 is recognized as a thought leader for enabling digital transformation in the convenience retail, fuels marketing, and energy transition, and we take hiring very seriously. We further believe the combination of an experienced team, our passion for technology, and our mutual desire to win—will make us successful together. We trust you will find our company and this opportunity equally compelling.
Description
Please note this a Hybrid role with 3 days onsite at our Houston office.
We are seeking an experienced Azure Data Architect to join our organization and play a crucial role in designing and implementing data architecture solutions using Microsoft Azure. As an Azure Data Architect, you will be responsible for overseeing the end-to-end data architecture, ensuring data integrity, scalability, and security in Azure-based environments. You will collaborate with cross-functional teams and stakeholders to understand business requirements, define data strategies, and deliver robust data solutions aligned with industry best practices.
Responsibilities:
Design and develop end-to-end data architecture solutions on Microsoft Azure, including data lakes, data warehouses, data marts, and data pipelines.
Collaborate with business stakeholders and technical teams to understand data requirements, identify data sources, and define data models and structures for optimal data integration and processing.
Define and enforce data governance policies, data quality standards, and data lifecycle management procedures to ensure data integrity, privacy, and compliance.
Architect and implement scalable and performant data storage and retrieval mechanisms, leveraging Azure data services such as Azure SQL Database, Azure Data Lake Storage, Azure Synapse Analytics, and Azure Cosmos DB.
Design and implement data integration and ETL/ELT processes using Azure Data Factory, Azure Databricks, Azure Logic Apps, and other relevant technologies.
Collaborate with solution architects and developers to implement data security measures, access controls, and encryption mechanisms to protect sensitive data in Azure.
Stay up-to-date with the latest Azure data services and technologies, evaluate their applicability to the organization's data landscape, and provide recommendations for their adoption.
Provide technical leadership and guidance to development teams, ensuring adherence to data architecture principles and best practices.
Perform performance tuning, optimization, and troubleshooting of data solutions to ensure efficient data processing and analytics.
Collaborate with infrastructure teams to design and optimize Azure infrastructure components required for data processing, storage, and analytics.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field. A master's degree is a plus.
Proven experience as a Data Architect, specifically working with Azure data services and technologies. 10+ years.
Strong knowledge and hands-on experience with Microsoft Azure cloud platform and its various data-related services such as Azure SQL Database, Azure Data Lake Storage, Azure Synapse Analytics, Azure Cosmos DB, Azure Data Factory, etc.
Experience in designing and implementing data integration, ETL/ELT processes, and data pipelines using Azure data integration tools.
Solid understanding of data modeling, data warehousing, and dimensional modeling concepts.
Familiarity with data governance, data quality, and data security practices in Azure.
Strong analytical and problem-solving skills with the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders.
Azure certifications such as Azure Data Engineer Associate or Azure Solutions Architect Expert are highly desirable.
Knowledge of other cloud platforms (AWS, Google Cloud) and hybrid cloud environments is a plus.
This job description provides many examples of the types of responsibilities required but is not intended to be a comprehensive list. The incumbent will complete other job responsibilities as needed which are not specifically described in this document.
P97’s employees are recognized as a thought leader by peers, clients, and others in the industry with a respected reputation both internally and externally. If you want to be the newest member of a vibrant group of technology leaders in an innovative and entrepreneurial environment, then apply online today to start building solutions that customers will love to use and join a company that truly rewards your hard work and values your talented contributions. For a complete list of our open positions, please visit our website at www.P97.com and career center at http://p97.com/careers-at-p97/.
P97 rewards your hard work with a competitive compensation plan, comprehensive benefits plan, stock options, 401k with a match, PTO package, Company-subsidized lunches twice a week with a stocked kitchen the rest of the time, access to an on-site fitness facility, and amazing opportunities for career growth and personal development.
Equal Opportunity Employer
P97 is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability, or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.
No calls or agencies please. We thank all respondents for their interest in P97. However, only those selected for an interview will be contacted.
Show more
Show less","Microsoft Azure, Data Lakes, Data Warehouses, Data Marts, Data Pipelines, Data Governance, Data Quality, Data Lifecycle Management, Azure SQL Database, Azure Data Lake Storage, Azure Synapse Analytics, Azure Cosmos DB, Azure Data Factory, Azure Databricks, Azure Logic Apps, ETL/ELT Processes, Azure Data Engineer Associate, Azure Solutions Architect Expert, AWS, Google Cloud, Hybrid Cloud Environments, Data Integration, Data Modeling, Data Warehousing, Dimensional Modeling, Data Security","microsoft azure, data lakes, data warehouses, data marts, data pipelines, data governance, data quality, data lifecycle management, azure sql database, azure data lake storage, azure synapse analytics, azure cosmos db, azure data factory, azure databricks, azure logic apps, etlelt processes, azure data engineer associate, azure solutions architect expert, aws, google cloud, hybrid cloud environments, data integration, data modeling, data warehousing, dimensional modeling, data security","aws, azure cosmos db, azure data engineer associate, azure data factory, azure data lake storage, azure databricks, azure logic apps, azure solutions architect expert, azure sql database, azure synapse analytics, data governance, data integration, data lakes, data lifecycle management, data marts, data quality, data security, data warehouses, datamodeling, datapipeline, datawarehouse, dimensional modeling, etlelt processes, google cloud, hybrid cloud environments, microsoft azure"
Lead Data Analyst,David Weekley Homes,"Houston, TX",https://www.linkedin.com/jobs/view/lead-data-analyst-at-david-weekley-homes-3780062363,2023-12-17,Baytown,United States,Mid senior,Hybrid,"Job Description
David Weekley Homes is seeking a Lead Data Analyst for a full-time, in office role. This position will play a key role in implementing our data strategy, serving as our company’s technical expert in Reporting & Analytics, while understanding our data's business and IT implications, usage, and significance. You will be instrumental in bridging the gap between data, business, and IT, turning data into actionable business insights.
Responsibilities
Data Expertise and Standards Implementation:
Develop a comprehensive understanding of how data is utilized across the organization, understanding its impact in various business and IT contexts.
Establish and enforce standards for data usage, presentation, and Power BI report formatting, ensuring consistency and quality across all data-driven initiatives.
Serve as a leading authority on the data's meaning, origins, and applications, providing guidance on best practices in data management and reporting.
Collaborate with the Director of Data Analytics to align the reporting and analytics strategy with the overall business objectives.
Ad-hoc Analysis And Insights
Conduct targeted ad-hoc data analyses to address specific business challenges.
Translate complex datasets into clear, actionable insights for various departments, adhering to established standards for data presentation.
Effectively communicate analytical findings to a wide range of stakeholders, maintaining high standards in data clarity and accuracy.
Power BI Report Development
Lead the development and maintenance of Power BI reports and dashboards, following organizational guidelines for report design and data visualization.
Ensure reporting tools are accurate, timely, and aligned with established data standards and best practices.
Participation In Data Solutions Testing
Collaborate in testing data solutions, including data integrations and data warehouses, ensuring alignment with business, IT requirements, and data standards.
Assist in identifying and resolving data-related issues, maintaining a focus on data quality and adherence to standards.
Stakeholder Engagement
Work closely with both business and IT units, leveraging your extensive data knowledge to meet their analytical needs.
Provide guidance and support on Power BI tools, data interpretation, and best practices in data reporting.
Continuous Improvement
Continually enhance data analysis methodologies, reporting processes, and standards.
Stay updated on the latest trends in data analysis, Power BI, and business intelligence.
Qualifications
Minimum of 7 years of experience performing data analysis.
Minimum of 2 years of experience in Power BI report development, including data modeling, DAX, and Power Query.
Strong proficiency in SQL, Microsoft Excel, and other data analysis tools.
Experience in participating in the testing of data solutions and integrations.
Exceptional analytical and problem-solving skills.
Excellent communication skills, able to effectively articulate complex data insights to both technical and non-technical audiences.
Self-motivated, able to work independently in a dynamic business and IT environment.
Work Location:
This is a full-time position that is required to be on-site. Our office is located in West Houston, near the intersection I-10 Katy Freeway and 610 W Loop N Frwy.
About Us
David Weekley Homes is an award-winning home builder that has earned spots on FORTUNE® magazine's list of “100 Best Companies to Work For” 17 times! We build homes in 19 markets coast to coast and are the largest privately held builder in America.
We have an excellent benefits package, with some unique features including:
Great opportunities to participate in outreach and community involvement programs
Medical, Dental and Vision insurance
Paid Vacation, Holidays and PTO
401(k) with discretionary 8% match
Profit Sharing
Team Member & Family New Home Discount
Team Member Product Discounts
College Scholarship Program
And More!!
Show more
Show less","Data Analysis, Data Management, Data Reporting, Data Standards, Data Strategy, Power BI, Power Query, Microsoft Excel, SQL, DAX, Business Intelligence, Data Visualization, Data Integration, Data Warehousing, Analytical Thinking, Problem Solving, Stakeholder Engagement, Communication Skills","data analysis, data management, data reporting, data standards, data strategy, power bi, power query, microsoft excel, sql, dax, business intelligence, data visualization, data integration, data warehousing, analytical thinking, problem solving, stakeholder engagement, communication skills","analytical thinking, business intelligence, communication skills, data integration, data management, data reporting, data standards, data strategy, dataanalytics, datawarehouse, dax, microsoft excel, power query, powerbi, problem solving, sql, stakeholder engagement, visualization"
Data Analyst,PRI Global,"Hershey, PA",https://www.linkedin.com/jobs/view/data-analyst-at-pri-global-3774198033,2023-12-17,Lebanon,United States,Associate,Onsite,"Data Analyst
Hershey, PA
3+ Months (Extension Possible)
The main function of a Data analyst is to conduct full lifecycle analysis to include requirements, activities, and design. Data analysts will develop analysis and reporting capabilities. They will also monitor performance and quality control plans to identify improvements.
Responsibilities
:
Interpret data, analyse results using statistical techniques and provide ongoing reports.
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.
Acquire data from primary or secondary data sources and maintain databases/data systems.
Identify, analyse, and interpret trends or patterns in complex data sets.
Filter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.
Locate and define new process improvement opportunities.
Skills
:
Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.
Basic ability to work independently and manage one's time.
Basic knowledge of database management software.
Education/Experience:
Associate degree in computer programming or equivalent training required.
2-4 years’ experience required.
Show more
Show less","Statistical techniques, Data analysis, Data reporting, Database management, Data collection systems, Data analytics, Statistical efficiency, Data quality, Data interpretation, Data visualization, Data mining, Data cleaning, Data integration, Data warehousing, Business intelligence, Data governance, Machine learning, Artificial intelligence","statistical techniques, data analysis, data reporting, database management, data collection systems, data analytics, statistical efficiency, data quality, data interpretation, data visualization, data mining, data cleaning, data integration, data warehousing, business intelligence, data governance, machine learning, artificial intelligence","artificial intelligence, business intelligence, data cleaning, data collection systems, data governance, data integration, data interpretation, data mining, data quality, data reporting, dataanalytics, database management, datawarehouse, machine learning, statistical efficiency, statistical techniques, visualization"
Data Architect,Fulton Bank,"East Petersburg, PA",https://www.linkedin.com/jobs/view/data-architect-at-fulton-bank-3746739845,2023-12-17,Lebanon,United States,Associate,Remote,"Our values define us and our culture inspires us to
change lives for the better.
Our employees are the heart and soul of our company and every success we experience begins with them. We hire team members who raise the bar, who are
collaborative and inclusive
, who build relationships, and who are looking for a long-term career with Fulton. We strive to promote a culture of
work-life balance
, to unplug and recharge to be your best self, at work and at home.
Overview
Responsible for data modeling and source to target mapping for database development projects, design and user support for business intelligence implementation, design and ownership of the corporate data architecture and related standards, design and maintenance of reference and master data management initiatives, support of a comprehensive data quality program, establishment of meta data management policies and procedures, and development and enforcement of data management standards and controls.
Responsibilities
Data Architects work on a broad range of data governance projects. Providing detailed data modeling and design for all data oriented development projects. Design logical and physical data models and maintain documentation. Conduct source to target analysis and mapping and maintain documentation. Create and update data dictionary. Participate in acceptance testing as required.
Support implementation of reference data and master data programs. Define and maintain Match rules, Data cleansing rules, Hierarchies and affiliations, Golden records. Manage changes to reference and master data by identifying sources and contributors and establishing processes for managed change.
Support the data quality program. Profile, analyze and assess data quality problems. Determine root cause of data quality issues. Define data quality metrics and business rules.
Support metadata management policies and procedures. Develop, maintain and enforce standards. Manage metadata repositories. Create and manage widely accessible data dictionaries. Enforce policies and processes needed to improve and maintain the quality and ownership of data including data owners, stewards and administrators.
Develop and monitor conformance with data standards, policies and procedures. Manage and resolve data related issues and conflicts. Monitor and ensure regulatory compliance. Establish and monitor documentation standards and repositories.
Assist with defining the corporate data architecture and related standards. Data integration architecture. Data warehouse and business intelligence architecture. Taxonomies and namespaces. Data modeling tools and standards. Metadata standards and requirements.
Required Education
Bachelor Degree or the equivalent experience. Specialty: Computer Science, Computer Information Science. (Required)
Required Experience
3 or more years Data Engineer, Integration ETL architect or data modeler, big data architect experience. (Required)
1 or more years of Azure experience (Preferred)
Fulton Bank (“Fulton”) is an equal opportunity employer and is committed to providing equal employment opportunity for all qualified persons. Fulton will recruit, hire, train and promote persons in all job titles, and ensure that all other personnel actions are administered, without regard to race, color, religion, creed, sexual orientation, national origin, citizenship, gender, gender identity, age, genetic information, marital status, disability, covered veteran status, or any other legally protected status.
This role may perform other job duties as assigned by the manager. Each employee of the Organization, regardless of position, is accountable for reading, understanding and acting on the contents of all Company-assigned and/or job related Compliance Programs, regulations and policies and procedures, as well as ensure that all Compliance Training assignments are completed by established due dates. This includes but is not limited to, understanding and identifying compliance risks impacting their department(s), ensuring compliance with applicable laws or regulations, and escalating compliance risks to the appropriate level of management.
As a condition of employment, individuals must be authorized to work in the United States without sponsorship for a work visa by Fulton Financial Corporation currently or in the future.
Show more
Show less","Data modeling, Data mapping, Business intelligence, Data architecture, Data quality, Metadata management, Data governance, Data engineering, ETL architecture, Big data architecture, Azure, Data dictionary, Data integration, Data warehouse, Taxonomies, Namespaces, Data modeling tools, Metadata standards, Computer Science, Computer Information Science, Machine learning","data modeling, data mapping, business intelligence, data architecture, data quality, metadata management, data governance, data engineering, etl architecture, big data architecture, azure, data dictionary, data integration, data warehouse, taxonomies, namespaces, data modeling tools, metadata standards, computer science, computer information science, machine learning","azure, big data architecture, business intelligence, computer information science, computer science, data architecture, data dictionary, data engineering, data governance, data integration, data mapping, data modeling tools, data quality, datamodeling, datawarehouse, etl architecture, machine learning, metadata management, metadata standards, namespaces, taxonomies"
Sr Engineer Data Visualization,The Hershey Company,"Hershey, PA",https://www.linkedin.com/jobs/view/sr-engineer-data-visualization-at-the-hershey-company-3737692335,2023-12-17,Lebanon,United States,Mid senior,Remote,"Job Title:
Sr Engineer Data Visualization
Job Location: Hershey, PA
This position is open to a 100% remote as well as hybrid.
Summary:
The Enterprise Data organization drives value for Hershey by providing high-quality, well governed data to the Enterprise for analytics and decision-making.
The Sr. Data Visualization Engineer will be part of an agile execution team, working with Hershey business partners, data scientists, technical engineers, data architects, and project managers to design, develop and grow the data visualization, reporting and analytical solutions. This role will also ensure visualization standards adhere to company best practice and help to deliver rapid impactful benefits.
The Sr. Data Visualization Engineer will act as a trusted advisor for Hershey business partners by ensuring that data solutions meet expectations and requirements. You will work with a diverse team of business analysts, data scientists, technical engineers, data architects, and project managers to deliver outcomes aligned with our business partner’s strategy. In addition, you will report directly to the Sr. Manager Data Visualization to ensure consistency and compliance of deliverables to frameworks and governance processes.
Major Duties/Responsibilities:
Data Visualization Engineering Solution Delivery:
Develop and deliver high-quality data visualization, reporting and analytical solutions - adhering to best practice, privacy, and governance principles.
Data Visualization Maintenance/Optimization:
Work existing solutions to enhance its performance, quality and/or functionality. Resolve incidents escalated by support teams or business users.
Data Visualization Engineering Domain:
Collaborate with IT and business partners to define, manage and deliver innovative Data Visualization solutions to drive growth and adoption of capabilities at Hershey.
Data Visualization Engineering Advocacy:
Evangelize future Data Visualization solutions identified by Enterprise Data leadership, including innovations such as: cloud-based intelligent systems to collect, distribute, model, analyze, and visualize disparate/diverse data assets of all sizes.
Specific Job Responsibilities:
Collaborates with the agile execution team throughout the design process—from creating user flows and wireframes to building user interface mockups and prototypes for Enterprise initiatives.
Consults the business and project teams on the visualization and impacts on data requirements.
Develops complex analytical solutions, while maintaining the output performance
Ensures the operations of the reporting and analytical platforms meet Service Level Agreements.
Maintains the performance and optimization of certified reports and analytical solutions.
Ensures the visualization answers the questions of the business and tells the story of the data following guiding principles.
Models the standards and best practices established by Enterprise Data leadership, including change management, prioritization, and audit.
Works with data engineers to understand data solutions and data availability.
Leads the operational and infrastructure activities post go-live.
Able to articulate the holistic benefits of data visualization from a business perspective, while.
Uses best practice frameworks and governance, evaluate, design, and analyze solution engineering for agile execution delivery.
Acts upon guidance from Enterprise Data leadership, oversee the health and evolution of agile execution team visualization technologies.
Minimum knowledge, skills and abilities required to successfully perform major duties/responsibilities:
Experience designing, and developing data visualization, reporting and analytical solutions.
Working knowledge of agile frameworks
Ability to manage multiple priorities, meet deadlines and produce quality results under pressure.
Demonstrated leadership and managerial skills.
Strong problem solving and analytical skills.
Strong team player, change agent, and advocate.
Excellent customer service skills.
High energy self-starter.
Excellent verbal and written communication skills.
Minimum Education and Experience Requirements:
Education: Bachelor’s in a STEM degree
Master’s degree and/or related equivalent experience preferred.
Experience:
5+ years of experience with reporting and visualization tools such as Power BI, Tableau, much of which has been focused on working with cross-functional teams and enterprise-wide data management programs
3+ Experience working with Alteryx or similar data wrangling tool for data modeling, data mining, forecasting, simulation.
3+ years of developing and maintaining the reporting ecosystem, ensuring service level agreements and data quality.
3+ years building analytical models.
3+ years of experience working with business owners to maintain and optimize key performance indicators within data and analytical solutions.
Ability to speak to the visualization tools and discuss tradeoff between their functionalities.
Knowledge and experience of working with relational/non-relational databases e.g., SQL, Teradata, Snowflake, Databricks, Azure Data solutions or Hadoop.
Experience working in a high performing agile delivery model, aligning with Scrum Masters, Product Owners, and other execution team members to deliver rapid and impactful solutions that align to business partner strategy.
Excellent communication and presentation skills, with the ability to articulate new ideas and concepts to technical and non-technical partners.
Leadership Competencies:
Strategic Thinking
Thinks strategically by clearly anticipating future trends, challenges, and consequences, creating breakthrough business strategies and plans to achieve a competitive advantage. Creates clear and compelling vision and strategy, translates strategy into business plan, and communicates strategic vision to employees.
Business Acumen
Leverages business judgment to shape strategy, based on understanding of operational, commercial, financial, and organizational requirements and capabilities. Understands the Hershey business units, regions, and functions, and the manufacturing, commercialization and market access of its products and services.
Drive for Results
Pushes self and others to exceed goals and achieve breakthrough results. Recognizes the key actions necessary to achieve results, establishes and communicates the priorities to others, and maintains own and others focus on achieving the important goals. Demonstrates persistence in removing barriers to achieving results and encourages others to do the same.
Prioritization and Judgment
Prioritizes and focuses on the right ideas, opportunities, issues, and projects. Develops decision criteria and considers benefits, costs, and risks of each decision and its immediate and long-range implications. Makes timely, sound judgments in uncertain and changing situations.
Influential Leadership
Leads and persuades others, within and outside of Hershey and without direct authority or formalized structure. Understands other’s needs motivations, concerns, and positions. Establishes credibility with stakeholders and confidently influences their opinions and actions. Inspires and leads others to adopt common vision, achieve organizational change, and accomplish business strategies.
Partnership
Develops and maintains quality, long-term relationships and partnerships based on trust, transparency, communication, and credibility with key internal and external stakeholders to accomplish strategic objectives. Works to find common ground and mutually beneficial solutions to conflicts. Uses diplomacy and tact to diffuse high-tension situations.
Global / M&A Mindset
Thinks from a global / M&A perspective and understands market, regulatory, political, economic, and cultural differences across countries and regions and their interdependencies. Understands how stakeholders and teams work and communicate in other countries and regions and how to adapt behavior and strategy to ensure alignment with market and cultural differences.
Talent Management and Development
Defines and communicates performance standards and continuously raises the bar. Motivates employees to perform at their highest potential and achieve breakthrough results, holding individuals accountable for own actions, and recognizing individual contributions and achievements. Leads development of internal talent to meet the future needs of the organization.
Customer Focus
Builds a culture that strives to exceed customer needs and is creatively challenged by and responsive to customer experiences. Creates clarity within the organization of what successfully meeting customer needs looks like. Gives priority to customer needs and makes organizational adjustments based on continuously reevaluating how well customer needs are being met.
Show more
Show less","Data visualization, Reporting, Analytical solutions, Agile frameworks, Power BI, Tableau, Alteryx, SQL, Teradata, Snowflake, Databricks, Azure Data solutions, Hadoop, Scrum, Product Owners, Business Acumen, Prioritization, Influential Leadership, Partnership, Global / M&A Mindset, Talent Management and Development, Customer Focus","data visualization, reporting, analytical solutions, agile frameworks, power bi, tableau, alteryx, sql, teradata, snowflake, databricks, azure data solutions, hadoop, scrum, product owners, business acumen, prioritization, influential leadership, partnership, global ma mindset, talent management and development, customer focus","agile frameworks, alteryx, analytical solutions, azure data solutions, business acumen, customer focus, databricks, global ma mindset, hadoop, influential leadership, partnership, powerbi, prioritization, product owners, reporting, scrum, snowflake, sql, tableau, talent management and development, teradata, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"Clearwater, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783325345,2023-12-17,Seminole,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, S3, EMR, Restful APIs, Relational databases, Nonrelational databases","scala 212, spark 24, aws, s3, emr, restful apis, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783325318,2023-12-17,Seminole,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR clusters, S3, Relational databases, Nonrelational databases, RESTful APIs, Software engineering, Software architecture, Code testing, Agile development, Communication, Problem solving, Analytical thinking, Leadership","scala 212, spark 24, aws, emr clusters, s3, relational databases, nonrelational databases, restful apis, software engineering, software architecture, code testing, agile development, communication, problem solving, analytical thinking, leadership","agile development, analytical thinking, aws, code testing, communication, emr clusters, leadership, nonrelational databases, problem solving, relational databases, restful apis, s3, scala 212, software architecture, software engineering, spark 24"
Sr. Data Scientist/Deep Learning Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/sr-data-scientist-deep-learning-engineer-at-steampunk-inc-3770811519,2023-12-17,Providence,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around.
We are looking for seasoned Data Scientist/Deep Learning Engineer to work with our existing team of Data Architects and Developers to develop strategies, create ML/AI solutions, and deliver results. We are looking for a more than just a ""Deep Learning Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Strong knowledge of Deep Learning architectures – model layers, operation, hyperparameters, and I/O
Experience building deep learning architectures and running training experiments
Experience fine-tuning state-of-the-art architectures in various data domains – tabular data, NLP, image, multi-modal
Experience implementing models with TensorFlow, PyTorch or similar frameworks
Knowledge of task-based hardware optimizations for CPU and GPU efficiency
Knowledge of statistical concepts for data collection and model evaluation
Ability to leverage and integrate various data management tools at scale – cloud experience preferred
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
2-4 years industry experience developing ML/AI solutions and a passion for solving complex problems.
2-4 years direct experience in Data Modeling and Data Solution Development
1+ year performing queue modeling and survival analysis
Bachelor's degree in computer science, data science/statistics, information systems, engineering, business, or a scientific or technical discipline
Demonstrated experience manipulating structured and unstructured data for analysis
Strong programming skills in SQL, Python, R, Scala, Java, or similar language
Experience using Python to manipulate large data sets
Experience with DevSecOps, as it applies to data science
Demonstrated experience with TensorFlow, Keras, Torch, or other Deep Learning tools and frameworks
Demonstrated experience in big data systems including Hadoop, Spark, Scala
Data visualization skills in Tableau, Power BI, D3, ArcGIS, or similar
Experience with Git, Bash, Unix commands
Experience in Cloud analytics (AWS, Azure, or GCP) with tools such as AWS SageMaker, Azure HDInsight, etc.
Familiarity with Flask is a plus
Knowledge of DBOps and MLOps framework is a plus
Experience with Solr, Elasticsearch, or similar tool is a plus
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Deep Learning, ML/AI, Data Analytics, Data Visualization, Tableau, Power BI, D3, ArcGIS, Python, SQL, Scala, Java, TensorFlow, PyTorch, Keras, Torch, Hadoop, Spark, Git, Bash, Unix, AWS, Azure, GCP, AWS SageMaker, Azure HDInsight, Elasticsearch, Solr, Flask, DBOps, MLOps","deep learning, mlai, data analytics, data visualization, tableau, power bi, d3, arcgis, python, sql, scala, java, tensorflow, pytorch, keras, torch, hadoop, spark, git, bash, unix, aws, azure, gcp, aws sagemaker, azure hdinsight, elasticsearch, solr, flask, dbops, mlops","arcgis, aws, aws sagemaker, azure, azure hdinsight, bash, d3, dataanalytics, dbops, deep learning, elasticsearch, flask, gcp, git, hadoop, java, keras, mlai, mlops, powerbi, python, pytorch, scala, solr, spark, sql, tableau, tensorflow, torch, unix, visualization"
Data Center Engineer,World Wide Technology,"Roanoke, TX",https://www.linkedin.com/jobs/view/data-center-engineer-at-world-wide-technology-3782269155,2023-12-17,Corinth,United States,Associate,Onsite,"Data Center Engineer
Company Overview
World Wide Technology (WWT), a global technology solutions provider with $17 billion in annual revenue, combines the power of strategy, execution and partnership to accelerate transformational outcomes for large public and private organizations around the world. Through its Advanced Technology Center, a collaborative ecosystem of the world's most advanced hardware and software solutions, WWT helps customers and partners conceptualize, test and validate innovative technology solutions for the best business outcomes and then deploys them at scale through its 4 million square feet of global warehousing, distribution and integration space. With over 10,000 employees and more than 55 locations around the world, WWT's culture, built on a set of core values and established leadership philosophies, has been recognized 11 years in a row by Fortune and Great Place to Work® for its unique blend of determination, innovation and leadership for diversity and inclusion. With this culture at its foundation, WWT bridges the gap between business and technology to make a new world happen for its customers, partners and communities.
World Wide Technology Holding Co, LLC. (WWT) has an opportunity available for a
Data Center Engineer
to support our client in an ongoing Data Center refresh project.
Location:
Roanoke, TX
Available Shifts:
(2 Openings) 7 PM – 7 AM CST Thursday, Friday, Saturday, and Alternating Wednesdays.
MUST BE OKAY WITH 12 HOUR NIGHT SHIFTS - Expectation is to have 80 hours of work in a 2 week period.
Duration:
12 Months (Expected to renew for up to 3 years, on an ongoing 12-month renewal)
Contract Designation:
Full Time Contingent – Contractors will be eligible for WWT’s Full Time Employee Benefits Package including Medical, Vision, Dental, PTO, Paid Holidays, and more.
Responsibilities:
Installing/de-installing/relocating all distributed systems and network hardware (CSUs, DSUs, routers, switches, encryptors, firewalls, etc.) in the Americas Data Centers within the internal service level mandates
Installing/de-installing /extending/relocating/testing all carrier circuits to the network hardware
Installing/de-installing/relocating all patch cabling for systems and network hardware
Installing/de-installing/relocating all Data Center hardware
Assist with the coordination of cabinet power, circuit, and patch infrastructure installations w/various facilities, electrical and communications vendors
Assist with the coordination of network component configurations
Coordinate and Install SAN cabling infrastructure
Managing network ports and assist with the management of all consumable items (cables, labels, tie wraps, rail kits, etc.)
Maintaining the integrity of the data center facilities, systems and communications environments through general housekeeping and best operations practices
Qualifications:
Required skills include 3+ years of experience in the implementation, maintenance and analysis of data center facilities, hardware, communications infrastructure, strategies, tools and effective troubleshooting techniques.
Basic background on enterprise data center facilities and infrastructure environments such as PDUs, RPPs, network and SAN infrastructures. In depth knowledge on complex, Enterprise class inter-networked environments involving a combination of switched/routed/shared Ethernet, TwinAx (100GigE, 25GigE,10GigE, GigE, 100M, and 10M), token ring, SAN, and wide area connectivity.
Strong knowledge of WAN technologies (OC-x, DS-x), subnetting and TCP/IP protocol a must.
Excellent communication and writing skills a must.
Knowledge of trouble ticketing systems, change control, Project processes and associated tools.
Logical problem- solving techniques and associated experience in system, data center facilities, and telecommunications.
Must be Able to Lift up to 50lbs.
Equal Opportunity Employer Minorities/Women/Veterans/Disabled
Show more
Show less","Data Center Facilities, Hardware, Communications Infrastructure, Troubleshooting, PDUs, RPPs, Network Infrastructures, SAN Infrastructures, Switched/Routed/Shared Ethernet, TwinAx, Token Ring, Wide Area Connectivity, WAN Technologies, OCx, DSx, Subnetting, TCP/IP Protocol, Trouble Ticketing Systems, Change Control, Project Processes, Logical ProblemSolving Techniques","data center facilities, hardware, communications infrastructure, troubleshooting, pdus, rpps, network infrastructures, san infrastructures, switchedroutedshared ethernet, twinax, token ring, wide area connectivity, wan technologies, ocx, dsx, subnetting, tcpip protocol, trouble ticketing systems, change control, project processes, logical problemsolving techniques","change control, communications infrastructure, data center facilities, dsx, hardware, logical problemsolving techniques, network infrastructures, ocx, pdus, project processes, rpps, san infrastructures, subnetting, switchedroutedshared ethernet, tcpip protocol, token ring, trouble ticketing systems, troubleshooting, twinax, wan technologies, wide area connectivity"
"Analyst, Data Accuracy Response Team",Palo Alto Networks,"Plano, TX",https://www.linkedin.com/jobs/view/analyst-data-accuracy-response-team-at-palo-alto-networks-3783916444,2023-12-17,Corinth,United States,Associate,Onsite,"Company Description
Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Our Approach to Work
We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your wellbeing support to your growth and development, and beyond!
At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together!
Job Description
Your Career
Palo Alto Networks® is the fastest-growing security company in history. If you are a motivated, intelligent, creative, and hardworking individual, then this is the place for you! One of the keys to our success is our relentless focus on leveraging data to drive insights and decision making. Our Data Accuracy Response Team is tasked with ensuring our data is regulated, accurate, and reliable on a day to day basis. This includes managing urgent escalations, executing and monitoring changes in data, and observing trends to guide system improvements. The Data Governance Analyst is the front line in ensuring that these goals come to fruition. This position reports to the Director, Sales Operations for Data Integrity and Policy and will have a high degree of interaction with the Analytics, Business Operations, and IT functions across Palo Alto Networks.
Your Impact
Collaborate with internal stakeholders to solve service requests/issue resolution related to Data Governance
Efficiently manage and execute against overall ticket queue to adhere to SLA requirements
Research and analyze public data to validate changes to account hierarchies, deduplication, and firmographic, etc.
Enforce data standards, policies, and processes
Perform mass data analysis and updates of account information to increase account accuracy
Execute regular and periodic data maintenance and validation activities
Handle issues and escalations to closure
Monitor service request trends
Assist in SLA design, development, and execution
Collaborate with the Data Integrity and Policy Team on documentation of data standard and policies
Qualifications
Your Experience
Interest in the concerns and topics related to data maintenance
Can execute tasks with high attention to detail and a commitment to accuracy
Organized, proactive individual that can operate independently in solving complex problems with little guidance
Strong interpersonal skills for internal collaboration
Skillfully manage requestor escalations through appropriate channels
Proficient in public data research and analysis
Experience with SFDC is required
Experience using data management tools such as SOQL/ETL a big plus
Experience working with Google Suite or similar platforms
Comfortable working in a fast-paced, dynamic work environment
Ability to speak fluently in business language
Spanish fluency considered a significant advantage
Additional Information
The Team
Our Strategy & Operations team at Palo Alto Networks works on a set of strategic initiatives to drive process improvement projects to deliver improved efficiency and scale through simplification, automation and innovation across Sales Operations tools and processes.
You will be part of a growing, passionate, and dynamic team with an opportunity to work on challenging and exciting projects – centered on what we believe is one of the most significant mission statements in the world.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $81,900/yr to $132,450/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.
Show more
Show less","Data Governance, Data Accuracy, Data Integrity, Data Analysis, Data Management, Data Maintenance, Research, SFDC, SOQL, ETL, Google Suite","data governance, data accuracy, data integrity, data analysis, data management, data maintenance, research, sfdc, soql, etl, google suite","data accuracy, data governance, data integrity, data maintenance, data management, dataanalytics, etl, google suite, research, sfdc, soql"
"Data Analyst, Loan Servicing Analytics",DriveTime,"Dallas, TX",https://www.linkedin.com/jobs/view/data-analyst-loan-servicing-analytics-at-drivetime-3774970292,2023-12-17,Corinth,United States,Associate,Onsite,"What’s Under The Hood
DriveTime Family of Brands includes in-house financing and servicing through Bridgecrest, which is one of the country’s leading financial servicing providers. Bridgecrest services roughly $17 billion in finance receivables for DriveTime and other third parties. We service auto loans across a wide credit spectrum with the intent of creating a strong path to vehicle ownership for our customers.
That’s Nice, But What’s the Job?
This is not a position for which sponsorship will be provided. Individuals with temporary visas or who need sponsorship now or in the future are not eligible for hire at this time.
That’s Nice, But What’s the Job?
In short, as an analyst you will apply your analytical skills as you initiate, build, and develop reports and useful insights into company initiatives.
In long, our analysts are responsible for:
Collect, clean, and validate data from various sources to ensure accuracy, completeness, and reliability.
Analyze large datasets using statistical techniques to identify trends, patterns, and insights that support business decision-making.
Develop and maintain reports, dashboards, and visualizations to present data in a clear and meaningful manner.
Partnering with product, tech, data science, and operations to understand data requirements and translate them into effective visualizations.
Effectively communicate with concise insights and/or visualizations utilizing software such as Tableau while partnering collaboratively with colleagues and senior leaders in the organization.
Write and optimize SQL/Snowflake queries to extract, manipulate, and analyze data from relational databases.
Perform data validation and quality checks to ensure accuracy and completeness of SQL query results.
Leveraging emerging technologies and critical thinking skills to continually innovate and automate existing methodologies, processes, and reporting. Design, develop, test, and implement new products and solutions.
So What Kind of Folks Are We Looking for?
Intellectual curiosity. Why? What? How? Do you find yourself always wanting to learn more and broaden your knowledge base? If so, this could be the role for you.
Passionate and goal-oriented. We are looking for someone that is enthusiastic about their work and is passionate about not only meeting their goals but exceeding them.
Killer analytical and reporting abilities. You’ll need the capability to analyze data and in return, prepare timely reports on your findings.
Master multi-tasker. We are looking for someone that is not only good at multi-tasking but thrives in it.
Excellent verbal and written communication skills. The ability to talk and write with confidence, charisma and competence for a wide variety of audiences including management.
A mind for the details. Okay we know “detail-oriented” is on about every job description – but we really mean it!
The Specifics.
Bachelor's or Master's degree in a quantitative field such as economics, engineering, finance, statistics, mathematics, or hard science.
0-2 years’ experience in analytics, engineering, consulting, or marketing
SQL, Tableau, advanced Excel skills
Nice to Haves.
Python, Snowflake
#DICE
So What About the Perks? Perks matter
Medical, dental, and vision, oh my! DriveTime Family of Brands covers a sizable amount of insurance premiums to ensure our employees receive top-tier healthcare coverage.
But Wait, There’s More. 401(K), Company paid life insurance policy, short and long-term disability coverage to name a few.
Growth Opportunities. You grow, I grow, we all grow! But seriously, DriveTime Family of Brands is committed to providing its employees with every opportunity to grow professionally with roughly over 1,000 employees promoted year over year.
Tuition Reimbursement. We’re as passionate about your professional development as you are. With that, we’ll put our money where our mouth is.
Wellness Program. Health is wealth! This program includes self-guided coaching and journeys, cash incentives and discounts on your medical premiums through engaging in fun activities!
Gratitude is Green. We offer competitive pay across the organization, because, well… money matters!
In-House Gym. We want our employees to be the best versions of themselves. So come early, take a break in your day or finish strong with a workout!
Give Us a Reason (or not), and We’ll Celebrate. Regardless of whether there is a holiday or not, we are finding ways to kick back and enjoy each other’s company outside of day-to-day work.
Smart-Casual Dress. Come dressed in jeans (you’ll fit right in with the rest of us).
Paid Time Off & Paid Holidays. Not just lip service: we work hard, to play hard.
Anything Else? Absolutely.
DriveTime Family of Brands is Great Place to Work Certified! And get this: 90% of our rockstar employees say they feel right at home here. We could spend a lot of time having you read about ALL our awards, but we’ll save time (and practice some humility) just naming a few others; Comparably Awards: Best Company for Diversity, Best Company Culture and Best Company Leadership, oh and don’t forget Phoenix Business Journal Healthiest Employers (okay, we’ll stop there)!
Hiring is contingent upon successful completion of our background and drug screening process. DriveTime is a drug-free, tobacco-free workplace and an Equal Opportunity Employer.
And when it comes to hiring, we don't just look for the right person for the job, we seek out the right person for DriveTime. Buckle up for plenty of opportunities to grow in a professional, fun, and high-energy environment!
Show more
Show less","SQL, Snowflake, Python, Tableau, Excel, Statistics, Data Analysis, Data Visualization, Data Mining, Data Validation, Data Quality, Business Intelligence, Reporting, Communications, Problem Solving, Critical Thinking, DetailOriented, MultiTasking, Teamwork","sql, snowflake, python, tableau, excel, statistics, data analysis, data visualization, data mining, data validation, data quality, business intelligence, reporting, communications, problem solving, critical thinking, detailoriented, multitasking, teamwork","business intelligence, communications, critical thinking, data mining, data quality, data validation, dataanalytics, detailoriented, excel, multitasking, problem solving, python, reporting, snowflake, sql, statistics, tableau, teamwork, visualization"
Business Intelligence Analyst – Data Engineering Support,The University of Texas at Dallas,"Richardson, TX",https://www.linkedin.com/jobs/view/business-intelligence-analyst-%E2%80%93-data-engineering-support-at-the-university-of-texas-at-dallas-3785764107,2023-12-17,Corinth,United States,Associate,Onsite,"The Business Intelligence Analyst position in OISDS will work collaboratively as a member of the team and will interact with internal and external stakeholders to support a variety of data management, training, analysis and benchmarking initiatives at The University of Texas at Dallas. The person in this role will assist with the ongoing development and implementation of tools to promote the development and use of analytic dashboards designed for wide distribution to UT Dallas campus stakeholders.
The primary functions of this role will be to
1)
assist OISDS with building relationships with campus constituents to develop analytic data resources and institutional research studies,
2)
assist with training and knowledge transfer activities related to OISDS tools, and
3)
conduct internal institutional research studies, assist with external data requests, perform data analysis, generate analytics, and communicate results involving internal data across numerous domains.
In this role, the Business Intelligence Analyst will provide direct support to internal and external stakeholders, manage relationships with data providers and end users. This role will also be tasked with enhancing existing and developing new analytics and data resources. The person in this role will report to the Senior Director of Institutional Reporting and Analytics.
Minimum Education And Experience
Bachelors degree in related field.
Four (4) years related professional experience.
Equivalent combination of education and experience may be considered.
Preferred Education And Experience
Master's Degree in a related field and two years of data analysis work experience.
Possess a strong working knowledge of SQL and SAS programming.
Experience in predictive analytics and machine learning.
Experience working in an institutional research setting, a basic working knowledge of financial aid, admissions, and student data sources in a higher education setting (e.g., institutional, Federal (Integrated Post-Secondary Educational Data System), and the State via the Texas Higher Education Coordinating Board.
Basic working knowledge of economic and accounting principles and practices, legal statutes, and analysis and reporting of financial data.
Experience in designing business intelligence and/or qualitative/quantitative research studies to assist organizations in developing data-informed strategies.
Essential Duties And Responsibilities
Reporting to the Senior Director, this Business Intelligence Analyst will focus on the data preparation processes for business intelligence solutions. The person in this role will work closely with the Data Warehouse Team to support the vision and delivery of optimized and accessible institutional data to deliver metrics capable of making data informed decisions across the University through reporting and analytics.
Writes complex SQL and/or DAX statements and scripts to support the development of business intelligence deliverables.
Develops and validates data models and schemas.
Develops and documents processes and procedures in collaboration with OISDS team members and other stakeholders for accessing data and for ensuring reliability of information retrieved.
Automates processes for ingesting, merging, and working with data from disparate sources using tools and applications.
Maintains knowledge of best practices and emerging trends for infrastructure and processes (e.g., ETL, API) needed to collect, store, and analyze institutional data.
Additional Information
Remote Work:
This role is eligible for a hybrid (partly remote/partly in office) work schedule, subject to business need and manager approval. A UT Dallas Remote Work Agreement will be required within 14 days after approval. The Business Intelligence Analyst – Data Visualizations must be located within the DFW Area and have the ability to be on campus within 24 hours of notice.
What We Can Offer
UT Dallas is an Equal Opportunity Employer. We offer an employee-friendly work environment with a comprehensive benefit package including:
Competitive Salary
Tuition Benefits
Internal Training
Medical insurance – including
100% paid
employee medical coverage for full-time employees
Dental Insurance
Vision Insurance
Long and short-term disability
Retirement Plan Options
Paid time off
Paid Holidays All UT Dallas employees have access to various
professional development
opportunities
, including a membership to Academic Impressions, LinkedIn Learning, and UT Dallas Bright Leaders Program.
Visit
https://hr.utdallas.edu/employees/benefits/
for more information.
About Us:
UT Dallas is a top public research university located in one of the nation's fastest-growing metropolitan regions. Our seven schools offer more than 140 undergraduate and graduate programs, plus professional certificates and fast-track programs. Our student body is 31,000 strong, reflecting students from over 100 countries and a multiplicity of identities and experiences. UT Dallas is committed to graduating well-rounded members of the global community whose education has prepared them for rewarding lives and productive careers in a constantly changing world. The University has a variety of programs and initiatives to support engagement and success for all members of the campus community. Employee benefits include a range of physical and mental wellness resources. “LilyPad” lactation facilities are located throughout the campus. There are several Employee Resource Groups (ERGs) comprised of individuals who share common interests to help build community among UT Dallas faculty and staff (e.g., Universal Access ERG, Military and Veteran ERG, UT Dallas Young Professionals). Rich with visual and performing arts venues, museum districts, professional and semi-professional athletics teams, botanical gardens, accessible trails and so much more, the Dallas-Fort Worth (DFW) metroplex has something for everyone to explore. UT Dallas partners with regional higher education institutions and school districts and with the
Richardson Innovation Quarter
(Richardson IQ), a major hub for innovation, entrepreneurship, and educational activities.
Important Message
All employees serve as a representative of the University and are expected to display respect, civility, professional courtesy, consideration of others and discretion in all interactions with members of the UT Dallas community and the general public.
The University of Texas at Dallas is committed to providing an educational, living, and working environment that is welcoming, respectful, and inclusive of all members of the university community. UT Dallas does not discriminate on the basis of race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, national origin, disability, genetic information, or veteran status in its services, programs, activities, employment, and education, including in admission and enrollment. EOE, including disability/veterans. The Universityis committed to providing access, equal opportunity, and reasonable accommodationfor individuals with disabilities.To request reasonable accommodation in the employment application and interview process, contact theADA Coordinator.For inquiries regarding nondiscrimination policies, contact theTitle IX Coordinator.
Show more
Show less","SQL, DAX, ETL, API, Data mining, Data preparation, Predictive analytics, Machine learning, Data modeling, Data analysis, Data visualization, Data management, Data warehousing, Institutional research, Business intelligence, Data governance, Data architecture, Data infrastructure, SAS","sql, dax, etl, api, data mining, data preparation, predictive analytics, machine learning, data modeling, data analysis, data visualization, data management, data warehousing, institutional research, business intelligence, data governance, data architecture, data infrastructure, sas","api, business intelligence, data architecture, data governance, data infrastructure, data management, data mining, data preparation, dataanalytics, datamodeling, datawarehouse, dax, etl, institutional research, machine learning, predictive analytics, sas, sql, visualization"
Data Center Lead Engineer,Kforce Inc,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-lead-engineer-at-kforce-inc-3777072402,2023-12-17,Corinth,United States,Associate,Onsite,"Responsibilities
Kforce is immediately seeking an experienced Lead Data Center Engineer in support of our enterprise networking and cloud solutions client based in Richardson, TX. Summary: In this role, the Data Center Lead Engineer will work as part of the L&C Data Center Operations Team, which plans, operates, and maintains the L&C Data Center footprint globally, catering to tens of thousands of lab requests per year. They will also work with the wider L&C team to support IT professionals' learning and training needs in outstanding ways. We provide world class training, community outreach, educational events, technical documentation, and hands-on learning across all of the company's portfolio of solutions. We are passionate about education, learning and improving careers. The Data Center Lead Engineer will also work closely with Cisco product teams across the entire Cisco portfolio - including Networking, Security, Data Center, Collaboration, and Applications. Safe to say, you won't always be in a Data Center, though you'll feel right at home in one. Responsibilities:
Dedicated to maintaining a 'Source of truth' for the network through our NetOps Tooling
Generate first rate technical documentation of new, existing, evolving data center and network implementation
Own high and low visibility issues and be accountable for resolving them
Troubleshoot & resolve incidents, providing excellent levels of clear communication
Innovate - Identify and lead continuous improvement of systems, tools, and processes
The Engineer will plan, build and manage the existing footprint and expansion of all aspects of a data center including:
Power, cooling, and equipment air flow management
Rack, Infrastructure, Cable Design & Management
Device Network & IP Management
Issue Resolution Management of Infrastructure & Network
Asset Management
Capacity Management
Vendor Management
Compliance Management
Data Center Policy Compliance
Requirements
CCNP/CCIE certification (R&S and/or DC)
5+ years of experience in Data Center Operations & Production Network Operation roles
3+ years of experience with various Operating Systems: Windows, Linux, and Cisco network operating systems
Strong Data Center knowledge including Nexus Routing & Switching
Experience operating and troubleshooting complex networks with segmentation and virtualization
Experience with implementing dynamic routing with OSPF, EIGRP and BGP
Experience with implementing client and site-to-site VPN solutions
Experience with virtual machine networking with VMWare
VMWare, ESXi, vCenter, and VM configuration, and operation experience
Experience with Cisco ACI, DNA and automation with Ansible and Python
Able to manage core routers, switches, firewalls, etc.
Self-starter who requires little guidance to achieve the goals of complex data center projects
Deep and multifaceted technical expertise and enjoy working on a team and mentoring others
See the big picture even when analyzing multiple sophisticated factors under pressure, having an eye and a passion for the details
Excellent verbal and written skills and hold yourself to high standards in your results and communication
Experience in the following products/solutions:
Cisco Nexus 2000, 3000, 5000, 7000, 9000
Catalyst 9000 switches
ISR 4000 and 1000 routers
ASA and Firepower Firewalls
UCS server management
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $50 - $68 per hour
Show more
Show less","Cisco Nexus, Catalyst, Cisco ISR, ASA, Firepower Firewalls, UCS, NetOps, VMWare, ESXi, vCenter, VM, Cisco ACI, DNA, Ansible, Python, OSPF, EIGRP, BGP, Linux, Windows, Cisco network operating systems, Nexus Routing & Switching, Segmentation, Virtualization, Dynamic Routing, VPN solutions, Virtual Machine Networking, CCNP/CCIE (R&S and/or DC)","cisco nexus, catalyst, cisco isr, asa, firepower firewalls, ucs, netops, vmware, esxi, vcenter, vm, cisco aci, dna, ansible, python, ospf, eigrp, bgp, linux, windows, cisco network operating systems, nexus routing switching, segmentation, virtualization, dynamic routing, vpn solutions, virtual machine networking, ccnpccie rs andor dc","ansible, asa, bgp, catalyst, ccnpccie rs andor dc, cisco aci, cisco isr, cisco network operating systems, cisco nexus, dna, dynamic routing, eigrp, esxi, firepower firewalls, linux, netops, nexus routing switching, ospf, python, segmentation, ucs, vcenter, virtual machine networking, virtualization, vm, vmware, vpn solutions, windows"
Data Analytics Intern (Summer 2024),DriveTime,"Dallas, TX",https://www.linkedin.com/jobs/view/data-analytics-intern-summer-2024-at-drivetime-3706625453,2023-12-17,Corinth,United States,Associate,Onsite,"What’s Under The Hood
DriveTime Family of Brands is the largest privately owned used car sales finance & servicing company in the nation. Headquartered in Tempe, Arizona and Dallas, Texas, we create opportunities and improve the lives of our customers and our employees by placing a focus on putting the right customer, in the right vehicle, on the right terms and on their path to ownership.
The DriveTime Family of Brands spans across DriveTime, Bridgecrest and SilverRock. You can find us at the intersection of technology and innovation as we use our proprietary tools and over two decades of industry knowledge to redefine the process of purchasing, financing, and protecting your vehicle.
That’s Nice, But What’s the Job?
We are looking for skilled Analytics Interns for our 2024 summer internship program. Asan Analytics Intern, you will have the ability to work within one of our analytics groups that are dedicated to supporting a vertical within our complex business. These verticals range from operational analytics to risk analytics to servicing analytics (just to name a few).
The Anlaytics Intern will be responsible for things like:
Developing and maintaining forecasting models
Providing ad hoc analysis and special projects as required.
Working with department leaders to understand changes needed and collaborate on solutions for change.
Manipulating, analyzing, and presenting data using programs like SQL, Snowflake, Excel, Tableau, and Python
Innovating on existing products and processes while using emerging technologies and data analysis to identify new opportunities to improve customer experience and profitability.
Creating in-depth analyses, collaborating with stakeholders, and leveraging resources skillfully to capitalize on those new opportunities.
Demonstrating strong communication skills in making recommendations to senior executives across DriveTime’s family of companies.
So what are we looking for?
Killer analytical and reporting abilities. You’ll need the capability to analyze data and in return, prepare timely reports on your findings.
A mind for the details. Okay we know “detail-oriented” is on about every job description – but we really mean it!
Fantastic problem solver. Your job is not only to find the problem, but more importantly, find the solution.
Agile in a fast-past environment. We move, and we move quickly. Thriving in an environment that never stops, is a must.
Excellent verbal and written communication skills. The ability to talk and write with confidence, charisma and competence for a wide variety of audiences including management.
Intellectual curiosity. Why? What? How? Do you find yourself always wanting to learn more and broaden your knowledge base? If so, this could be the role for you.
The Specifics.
You are currently pursuing a bachelor's or master’s with a graduation date of December 2024 – May 2025
A quantitative degree in either; economics, finance, analytics, math, engineering, computer science or other STEM field.
Data analysis experience
Knowledge of Excel, SQL, Tableau, Python or R
Excel skills
Our internship program will be 11 weeks starting May 2024 to August 2024
Internships will be based out of our Home Office in Tempe, AZ and Dallas, TX
You can expect a regular on-site schedule Monday-Friday, 40 hours per week
So What About the Perks? Perks matter.
Not Just Coffee Runs. We’re talking real world experience. You’ll walk away from our internship program with hands-on experience, completed projects, full portfolios, and newly developed skills.
Who Says You Have to Walk Away? Across both our Tempe, AZ and Dallas, TX offices, over 60% of our interns stay through the fall semester, return the following summer, or are brought on full-time after graduation.
Growth & Development. You will be mentored by industry professionals, be given guidance along the way, and the tools to be successful.
Philanthropy: Give for Good. We are proud to be difference makers in our communities. We dedicate time for our Interns to give back with us.
Gratitude is Green. Out Internship Program is paid, because, well… money matters!
In-House Gym. We want our employees to be the best versions of themselves. So come early, take a break in your day or finish strong with a workout!
Give Us a Reason (or not), and We’ll Celebrate. Regardless of whether there is a holiday or not, we are finding ways to kick back and enjoy each other’s company outside of day-to-day work.
Smart-Casual Dress. Come dressed in jeans (you’ll fit right in with the rest of us).
Anything Else? Absolutely.
DriveTime Family of Brands is Great Place to Work Certified! And get this: 90% of our rockstar employees say they feel right at home here. We could spend a lot of time having you read about ALL our awards, but we’ll save time (and practice some humility) just naming a few others; Comparably Awards: Best Company for Diversity, Best Company Culture and Best Company Leadership, oh and don’t forget Phoenix Business Journal Healthiest Employers (okay, we’ll stop there)!
Hiring is contingent upon successful completion of our background and drug screening process. DriveTime is a drug-free, tobacco-free workplace and an Equal Opportunity Employer.
And when it comes to hiring, we don't just look for the right person for the job, we seek out the right person for DriveTime. Buckle up for plenty of opportunities to grow in a professional, fun, and high-energy environment!
Show more
Show less","Analytics, SQL, Snowflake, Excel, Tableau, Python, R, Data Analysis, Data Manipulation, Data Presentation, Forecasting, Reporting, Problem Solving, Communication, DetailOriented","analytics, sql, snowflake, excel, tableau, python, r, data analysis, data manipulation, data presentation, forecasting, reporting, problem solving, communication, detailoriented","analytics, communication, data manipulation, data presentation, dataanalytics, detailoriented, excel, forecasting, problem solving, python, r, reporting, snowflake, sql, tableau"
Vendor Data Analyst,The University of Texas at Dallas,"Richardson, TX",https://www.linkedin.com/jobs/view/vendor-data-analyst-at-the-university-of-texas-at-dallas-3775031928,2023-12-17,Corinth,United States,Associate,Onsite,"This position is responsible for on-boarding new vendors and maintaining data for current vendors. The Vendor Data Analyst will interact with staff in campus departments, back-office staff in OBF (Office of Budget and Finance), as well as external vendors.
Minimum Education And Experience
Bachelor's degree; two (2) years accounts payable experience; or equivalent combination of education and experience.
Preferred Education And Experience
Bachelor's degree; two (2) years accounts payable experience; two (2) years vendor management experience; or equivalent combination of education and experience.
Essential Duties And Responsibilities
Work with vendors and campus departments for vendor setups and process support
Verify and evaluate vendor information and legal documents
Run campus trainings and generate campus/supplier tools on vendor management process
Review, evaluate, and clear vendor holds/sanctions
Assist with and manage enhancements to the vendor management area
Provide team support of systems, processes, and upgrades
Additional Information
Looking for a rewarding career opportunity with great benefits?
Look no further!
About Us
Located in one of the nation's fastest growing metropolitan regions, The University of Texas at Dallas is a top value public university with more than 31,000 students across seven schools. The University of Texas at Dallas is committed to graduating well-rounded members of the global community whose education has prepared them for rewarding lives and productive careers in a constantly changing world. A collection of people, ideas and perspectives is crucial to our vision and mission. A charter member of
SEA (STEMM Equity Achievement) Change
, UT Dallas is a place where members of the community from all backgrounds are welcomed, treated fairly, and encouraged in their pursuit of excellence.
What We Can Offer
UT Dallas aims to attract and retain talented faculty and staff to support the university's mission. We offer a comprehensive compensation and benefits package, including medical, dental, vision, paid time off, retirement, life insurance and AD&D coverage. UT Dallas also offers employee wellness programs, tuition assistance, and professional development through their BRIGHT Leaders program and a variety of virtual learning platforms. BRIGHT Leaders aims to support, encourage, and serve as a resource to cultivate and nurture effective leadership at all levels by providing trainings and resources to help all UTD employees grow their leadership skills. Visit our benefits webpage,
https://hr.utdallas.edu/employees/benefits/
for more information.
FOR TEXAS RESIDENTS
– 100% remote work environment available and a UT Dallas Remote Work Agreement is required upon employment. Use of personal computer and other standard office equipment may be required. Must be located within the State of Texas and have the ability to be on campus with 24 hours notice. )
Work in a place where your team values you. Join our team today!
Important Message
All employees serve as a representative of the University and are expected to display respect, civility, professional courtesy, consideration of others and discretion in all interactions with members of the UT Dallas community and the general public.
The University of Texas at Dallas is committed to providing an educational, living, and working environment that is welcoming, respectful, and inclusive of all members of the university community. UT Dallas does not discriminate on the basis of race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, national origin, disability, genetic information, or veteran status in its services, programs, activities, employment, and education, including in admission and enrollment. EOE, including disability/veterans. The Universityis committed to providing access, equal opportunity, and reasonable accommodationfor individuals with disabilities.To request reasonable accommodation in the employment application and interview process, contact theADA Coordinator.For inquiries regarding nondiscrimination policies, contact theTitle IX Coordinator.
Show more
Show less","Vendor Management, Accounts Payable, Data Analysis, Legal Documents, Process Enhancement, Systems Support, Employee Benefits, Trainings, Reasonable Accommodation","vendor management, accounts payable, data analysis, legal documents, process enhancement, systems support, employee benefits, trainings, reasonable accommodation","accounts payable, dataanalytics, employee benefits, legal documents, process enhancement, reasonable accommodation, systems support, trainings, vendor management"
Data Center Building Operating Engineer (3rd Shift),JLL,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-3rd-shift-at-jll-3748264637,2023-12-17,Corinth,United States,Associate,Onsite,"General Description:
The Data Center Operations Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
Show more
Show less","Data Center Operations, UPS, Electrical Systems, Generators, Cooling Systems, HVAC, Chillers, CRAC, CRAH, Plumbing, Controls, ATS, STS, PDU, Primary Switchgear, Power Distribution, Transformers, CMMS, Vendor Management, Customer Facing Tickets, Corrigo, MCIM, Salesforce, Zendesk, Service Now, EPA 608, NFPA 70E, Word, Excel","data center operations, ups, electrical systems, generators, cooling systems, hvac, chillers, crac, crah, plumbing, controls, ats, sts, pdu, primary switchgear, power distribution, transformers, cmms, vendor management, customer facing tickets, corrigo, mcim, salesforce, zendesk, service now, epa 608, nfpa 70e, word, excel","ats, chillers, cmms, controls, cooling systems, corrigo, crac, crah, customer facing tickets, data center operations, electrical systems, epa 608, excel, generators, hvac, mcim, nfpa 70e, pdu, plumbing, power distribution, primary switchgear, salesforce, service now, sts, transformers, ups, vendor management, word, zendesk"
Data Center Building Operating Engineer (3rd Shift),JLL,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-3rd-shift-at-jll-3748261897,2023-12-17,Corinth,United States,Associate,Onsite,"General Description:
The Data Center Operations Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
Show more
Show less","Data Center Operations, Electrical Systems, Mechanical Systems, HVAC, Chillers, CRAC, CRAH, Plumbing, Controls, Emergency Backup Systems, Lighting, UPS, ATS, STS, PDU, Generators, Primary Switchgear, Power Distribution, Transformers, Hot Water Systems, Refrigeration, Chilled Water, Air Conditioning Equipment, Boilers, Ventilating, Water Heaters, Pumps, Valves, Piping, Filters, CMMS, Vendor Management, Customer Facing Tickets, Emergency Escalation Procedures, Corrigo, MCIM, Salesforce, Zendesk, ServiceNow, EPA 608, NFPA70E","data center operations, electrical systems, mechanical systems, hvac, chillers, crac, crah, plumbing, controls, emergency backup systems, lighting, ups, ats, sts, pdu, generators, primary switchgear, power distribution, transformers, hot water systems, refrigeration, chilled water, air conditioning equipment, boilers, ventilating, water heaters, pumps, valves, piping, filters, cmms, vendor management, customer facing tickets, emergency escalation procedures, corrigo, mcim, salesforce, zendesk, servicenow, epa 608, nfpa70e","air conditioning equipment, ats, boilers, chilled water, chillers, cmms, controls, corrigo, crac, crah, customer facing tickets, data center operations, electrical systems, emergency backup systems, emergency escalation procedures, epa 608, filters, generators, hot water systems, hvac, lighting, mcim, mechanical systems, nfpa70e, pdu, piping, plumbing, power distribution, primary switchgear, pumps, refrigeration, salesforce, servicenow, sts, transformers, ups, valves, vendor management, ventilating, water heaters, zendesk"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Allen, TX",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783183647,2023-12-17,Corinth,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Allen-DataScientist.012
Show more
Show less","Generative AI, Python, JavaScript, JSON, Natural Language Processing, Data Science, Product Engineering, Research, Machine Learning, AI Training, Stakeholder Management, Communication Skills, Project Management, R, OOP Languages, Educational Technology, Data Analytics, Learning Science, Remote Work, Independent Contractor","generative ai, python, javascript, json, natural language processing, data science, product engineering, research, machine learning, ai training, stakeholder management, communication skills, project management, r, oop languages, educational technology, data analytics, learning science, remote work, independent contractor","ai training, communication skills, data science, dataanalytics, educational technology, generative ai, independent contractor, javascript, json, learning science, machine learning, natural language processing, oop languages, product engineering, project management, python, r, remote work, research, stakeholder management"
Research Data Specialist I,The University of Texas at Dallas,"Richardson, TX",https://www.linkedin.com/jobs/view/research-data-specialist-i-at-the-university-of-texas-at-dallas-3776128940,2023-12-17,Corinth,United States,Associate,Hybrid,"The Decision Support Specialist assists the Office of Institutional Success and Decision Support with a variety of tasks related to accreditation, assessment, policy development and evaluation, institutional research, and office management. This entry-level role will support data management, data collection, administrative functions of the office and documentation of processes and procedures.
The specialist reports to the Associate Vice President and works closely with the Assistant Director on the day-to-day tasks of the unit, ensuring that project milestones are communicated to unit stakeholders, and assists with the coordination of unit events (webinars, workshops, trainings, and development programming). The specialist also supports the assessment staff in maintaining program assessment documentation, including assessment reports, using the University's online assessment platform.
The specialist will also assist with the collection and analysis of assessment data, organizing materials, and editing and proofreading documents and reports. The specialist may also be asked to conduct reviews of other institutions' policies and best practices within higher education and review documentation related to accreditation reports.
Minimum Education And Experience
Bachelor's degree with six months of experience in a related field. An equivalent combination of relevant education and/or experience may be considered.
Six (6) months of process documentation
Six (6) months of technical communication
Preferred Education And Experience
Preferred Education and Experience
Education:
A Bachelor's degree in business administration, social sciences, education, or a related field.
Experience:
Experience working in higher education, some experience with qualitative and/or quantitative data collection and analysis.
Skills
: Basic project management; excellent customer service skills; strong verbal, written, and interpersonal communication skills; collaboration, ability to manage multiple tasks and projects, and ability to respond to requests in a timely manner.
Essential Duties And Responsibilities
General unit support includes assisting colleagues with:
reviewing annual assessment plan and report submissions in the University's assessment platform for completion and communicating with assessment staff, as well as academic and administrative programs about assessment reports.
proofreading reports and documents, including identifying discrepancies within text and visuals.
making business processes transparent and sustainable
tracking project milestones and communicating progress to unit leaders
assisting with entry-level data collection, coding, and analysis tasks
collection and preparation of business expense and travel expense reimbursements
office purchases and maintaining office supply inventory
processing and reviewing eCATs for building access, keys, and computer access
work order submissions to facilities management
scheduling of Alpine Conference Room
collecting and distributing mail
maintaining and documenting computer inventory
procurement activities: utilize procurement software to prepare, review, submit, and receive purchase orders
other duties as assigned
Additional Information
Remote Work:
After the probationary period, this role may be eligible for a hybrid (partly remote/partly in office) work schedule, subject to business need and manager approval. A UT Dallas Remote Work Agreement will be required within 14 days after approval. The Research Data Specialist I must be located within the DFW Area and have the ability to be on campus within 24 hours of notice.
What We Can Offer:
UT Dallas aims to attract and retain talented faculty and staff to support the university's mission. We offer a comprehensive compensation and benefits package.
Medical
Dental
Vision
Paid time off
Retirement
Life insurance
AD&D coverage
UT Dallas also offers employee wellness programs, tuition assistance, and professional development through their BRIGHT Leaders program and a variety of virtual learning platforms. BRIGHT Leaders aims to support, encourage, and serve as a resource to cultivate and nurture effective leadership at all levels by providing trainings and resources to help all UTD employees grow their leadership skills. Visit our benefits webpage,
https://hr.utdallas.edu/employees/benefits/
for more information.
If you are looking for a rewarding career opportunity with great benefits? Look no further! Join our team!
Important Message
All employees serve as a representative of the University and are expected to display respect, civility, professional courtesy, consideration of others and discretion in all interactions with members of the UT Dallas community and the general public.
The University of Texas at Dallas is committed to providing an educational, living, and working environment that is welcoming, respectful, and inclusive of all members of the university community. UT Dallas does not discriminate on the basis of race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, age, national origin, disability, genetic information, or veteran status in its services, programs, activities, employment, and education, including in admission and enrollment. EOE, including disability/veterans. The Universityis committed to providing access, equal opportunity, and reasonable accommodationfor individuals with disabilities.To request reasonable accommodation in the employment application and interview process, contact theADA Coordinator.For inquiries regarding nondiscrimination policies, contact theTitle IX Coordinator.
Show more
Show less","Data Management, Data Collection, Administrative Functions, Documentation, Processes, Project Management, Customer Service, Communication, Collaboration, Multiple Tasks Management, Timely Response, Annual Assessment, Proofreading, Business Processes, Project Milestones, Data Collection, Coding, Analysis, Business Expenses, Travel Expenses, Office Purchases, Office Supplies, Computer Access, Work Order Submissions, Conference Room Scheduling, Mail Collection, Computer Inventory, Procurement Activities, Purchase Orders","data management, data collection, administrative functions, documentation, processes, project management, customer service, communication, collaboration, multiple tasks management, timely response, annual assessment, proofreading, business processes, project milestones, data collection, coding, analysis, business expenses, travel expenses, office purchases, office supplies, computer access, work order submissions, conference room scheduling, mail collection, computer inventory, procurement activities, purchase orders","administrative functions, analysis, annual assessment, business expenses, business processes, coding, collaboration, communication, computer access, computer inventory, conference room scheduling, customer service, data collection, data management, documentation, mail collection, multiple tasks management, office purchases, office supplies, processes, procurement activities, project management, project milestones, proofreading, purchase orders, timely response, travel expenses, work order submissions"
Data engineer - Python,MANDO TECHNOLOGIES INC,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-python-at-mando-technologies-inc-3767594146,2023-12-17,Corinth,United States,Mid senior,Onsite,"Mando Technologies is specializes in helping organizations make the most of their information assets. From acquiring, organizing, analyzing, and delivering data to closing the loop by integrating intelligence into the operations of the enterprise, Mando Technologies covers the full spectrum of Business Intelligence.
Our data engineers are powering the capability to make decisions using data to improve operations and our customer and employee experience.
This role is a part of the Data Engineering and Analytics team with our Client Technology group. You'll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective.
What you'll do:
Be a part of the data governance team. Work closely with data application teams and product owners to design, implement and support data governance and analytics solutions that provide insights to make better decisions.
Implement data engineering solutions using Cloud services: (Snowflake, GitLab, CI/CD, Airflow, etc.) and traditional data warehouse tools.
Implement batch and streaming data pipelines using cloud technologies.
Leads development of coding standards, best practices and privacy and security guidelines.
Mentors' others on technical and domain skills to create multi-functional teams.
Develop and support data privacy and governance related frameworks and help other team implement them for compliance
Experience working in an Agile environment and with Agile teams
Preferred Qualifications:
Minimum Qualifications- Bachelor's degree in Computer Science, Computer Engineering, Technology or related technical discipline
2-3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions
7 years data analytics experience using SQL
Tools/platforms required:
Programming/Scripting: Python, Spark, Unix, SQL, APIs, Java
Data Platforms: Cassandra, Netezza, SQL Server, Snowflake
Cloud Technologies: AWS
CI/CD: GitLab, Jenkins, Kubernetis, Terraform
BI Analytics Tool Stack - Tableau, and Grafana
Tools: DataStage, Control-M
Show more
Show less","Data Engineering, Data Analytics, Business Intelligence, Cloud Services, Data Governance, Data Warehousing, Batch and Streaming Data Pipelines, Coding Standards, Best Practices, Privacy and Security Guidelines, Agile Development, DevOps, Software Solution Development, SQL, Python, Spark, Unix, APIs, Java, Cassandra, Netezza, SQL Server, Snowflake, AWS, GitLab, Jenkins, Kubernetes, Terraform, Tableau, Grafana, DataStage, ControlM","data engineering, data analytics, business intelligence, cloud services, data governance, data warehousing, batch and streaming data pipelines, coding standards, best practices, privacy and security guidelines, agile development, devops, software solution development, sql, python, spark, unix, apis, java, cassandra, netezza, sql server, snowflake, aws, gitlab, jenkins, kubernetes, terraform, tableau, grafana, datastage, controlm","agile development, apis, aws, batch and streaming data pipelines, best practices, business intelligence, cassandra, cloud services, coding standards, controlm, data engineering, data governance, dataanalytics, datastage, datawarehouse, devops, gitlab, grafana, java, jenkins, kubernetes, netezza, privacy and security guidelines, python, snowflake, software solution development, spark, sql, sql server, tableau, terraform, unix"
Database Engineer,Advanced Knowledge Tech LLC,"Plano, TX",https://www.linkedin.com/jobs/view/database-engineer-at-advanced-knowledge-tech-llc-3673946387,2023-12-17,Corinth,United States,Mid senior,Onsite,"For database engineer we have 2 roles.
For 1 we need SQL server and Sybase
for 2nd we need oracle and DB2.
For primary vs secondary out of both, we want 50-50 or even 60-40 is okay.
Our largest DB platform is DB2 and SQL Server.
One should be familiar with installation of software, configuration and security - hardening the newly built host as per enterprise standards.
Allocating SAN storage and mounts with DB restores and point-in-time recovery.
Job Description
Manage SQL Server databases
Configure and maintain database servers and processes
Monitor system's health and performance
Ensure high levels of performance, availability, sustainability and security
Analyze, solve, and correct issues in real time
Provide suggestions for solutions
Refine and automate regular processes, track issues, and document changes
Assist developers with query tuning and schema refinement
Perform scheduled maintenance and support release deployment activities after hours
Qualifications
5 years of experience as a SQL Server DBA/Sybase/DB2 or similar role
4 years of experience with SQL Server DBA/Sybase/DB2 Administration experience required
Critical thinker and problem-solving skills
Show more
Show less","SQL Server, Sybase, Oracle, DB2, SAN, Enterprise standards, Database restores, Pointintime recovery, Monitoring, Performance tuning, Availability, Security, Troubleshooting, Problemsolving, Query tuning, Schema refinement, Maintenance, Deployment, DBA, Administration","sql server, sybase, oracle, db2, san, enterprise standards, database restores, pointintime recovery, monitoring, performance tuning, availability, security, troubleshooting, problemsolving, query tuning, schema refinement, maintenance, deployment, dba, administration","administration, availability, database restores, db2, dba, deployment, enterprise standards, maintenance, monitoring, oracle, performance tuning, pointintime recovery, problemsolving, query tuning, san, schema refinement, security, sql server, sybase, troubleshooting"
Data Engineer with AWS,IQuest Solutions Corporation,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-with-aws-at-iquest-solutions-corporation-3700281134,2023-12-17,Corinth,United States,Mid senior,Onsite,"Data Engineer
Plano TX
CTS/JPMC
3 Positions: Developer/Sr Developer / Lead Developer.
Responsibilities
Design, develop, and maintain Python-based applications, tools, and scripts to support database migration processes.
Collaborate with cross-functional teams to analyze existing systems, databases, and applications to identify migration needs and opportunities.
Plan, execute, and oversee database migration projects, ensuring minimal downtime and data integrity throughout the process.
Implement effective data mapping, transformation, and validation techniques during migration to ensure accurate and complete transfer of data.
Develop and maintain scripts and automation workflows for seamless integration of migration tasks into the deployment pipeline.
Optimize application performance and scalability using appropriate AWS services, considering factors such as compute, storage, security, and networking.
Work closely with DevOps teams to deploy and manage applications within AWS cloud infrastructure.
Monitor, troubleshoot, and resolve database migration issues, performance bottlenecks, and system failures.
Implement security best practices in accordance with AWS standards to safeguard sensitive data and ensure compliance.
Collaborate with QA teams to conduct thorough testing of migrated databases and applications to validate their functionality and performance.
Stay up to date with the latest advancements in AWS services, Python development, and database migration techniques.
Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent work experience).
Proven experience as a Python Developer, with a strong portfolio showcasing Python-based projects and tools.
In-depth understanding of database migration strategies, data mapping, and transformation techniques.
Solid experience with AWS services such as EC2, RDS, S3, Lambda, DynamoDB, and others for application deployment, data storage, and scaling.
Familiarity with AWS migration services like Database Migration Service (DMS) and Server Migration Service (SMS).
AWS certification (e.g., AWS Certified Developer, AWS Certified Solutions Architect) is a significant plus.
Proficiency in SQL and experience with both relational and NoSQL databases.
Strong problem-solving skills and the ability to troubleshoot complex migration and application issues.
Excellent teamwork and communication skills, with the ability to collaborate effectively across different teams and stakeholders.
Knowledge of DevOps practices and tools for continuous integration and deployment.
Experience with version control systems like Git and agile development methodologies.
Strong attention to detail and a commitment to delivering high-quality solutions.
Thanks & Regards
Sonali Singh
Sr. Technical Recruiter
Desk:
+1(940)-536-0395
Email:
s.singh@iquestsols.com
Visit:
www.iquestsols.com
Show more
Show less","Python, SQL, AWS, EC2, RDS, S3, Lambda, DynamoDB, Database Migration Service (DMS), Server Migration Service (SMS), DevOps, Git, NoSQL, Relational databases, Version control systems, Agile development methodologies","python, sql, aws, ec2, rds, s3, lambda, dynamodb, database migration service dms, server migration service sms, devops, git, nosql, relational databases, version control systems, agile development methodologies","agile development methodologies, aws, database migration service dms, devops, dynamodb, ec2, git, lambda, nosql, python, rds, relational databases, s3, server migration service sms, sql, version control systems"
Senior Data Engineer,Warrior Tech Solutions,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-warrior-tech-solutions-3766018260,2023-12-17,Corinth,United States,Mid senior,Onsite,"Job Title:
Data Engineer
Location:
San Antonio, TX/Plano, TX - onsite
Visa:
No GC
Job Description
We are in search of a highly experienced Senior Data Engineer who possesses strong expertise in DBT and Snowflake.
Requriments For Candidates With Below Experiences
DBT (data build tool)
SQL Performance tunings in DBT
Synchronization process in DBT
Complex SQL queries
Real-time CDC process
SCD type process
Kindly share the resumes to *pavithran@warriortechsolutions.com* or contact@ +1 (813) 398-2457
Deadline for Submission : 11/21/2023
Show more
Show less","Data Engineering, DBT (Data Build Tool), Snowflake, SQL Performance Tuning, Synchronization Processes, Complex SQL Queries, Realtime CDC Process, SCD Type Process","data engineering, dbt data build tool, snowflake, sql performance tuning, synchronization processes, complex sql queries, realtime cdc process, scd type process","complex sql queries, data engineering, dbt data build tool, realtime cdc process, scd type process, snowflake, sql performance tuning, synchronization processes"
"Senior Data Engineer, Data Engineering",PepsiCo,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-data-engineering-at-pepsico-3701097237,2023-12-17,Corinth,United States,Mid senior,Onsite,"Overview
PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Data Management and Operations does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries
Qualifications
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Salesforce Cloud Technologies is must.
4+ years of experience with Sales force Customer data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
BA/BS in Computer Science, Math, Physics, or other technical fields.
Skills, Abilities, Knowledge:
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Salesforce Data Cloud Accreditation
Relevant Salesforce certifications and consulting experience are strongly recommended
Familiarity with Data Regulation
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
Please view our Pay Transparency Statement
Show more
Show less","Data engineering, Salesforce Cloud Technologies, Data modeling, Data warehousing, ETL/ELT pipelines, Data Lake Infrastructure, Data Analytics, SQL optimization, Python, PySpark, Scala, Azure cloud services, Azure Certification, Kubernetes, Github, Azure Data Factory, Azure Databricks, Azure Machine learning tools, Statistical/ML techniques, Retail, Supply chain, Metadata management, Data lineage, Data glossaries, DevOps, DataOps, PowerBI, Communication skills, Leadership, Teamwork, Problemsolving, Analytical skills, Attention to detail, Multitasking, Time management, Stress management, Salesforce Data Cloud Accreditation, Data Regulation","data engineering, salesforce cloud technologies, data modeling, data warehousing, etlelt pipelines, data lake infrastructure, data analytics, sql optimization, python, pyspark, scala, azure cloud services, azure certification, kubernetes, github, azure data factory, azure databricks, azure machine learning tools, statisticalml techniques, retail, supply chain, metadata management, data lineage, data glossaries, devops, dataops, powerbi, communication skills, leadership, teamwork, problemsolving, analytical skills, attention to detail, multitasking, time management, stress management, salesforce data cloud accreditation, data regulation","analytical skills, attention to detail, azure certification, azure cloud services, azure data factory, azure databricks, azure machine learning tools, communication skills, data engineering, data glossaries, data lake infrastructure, data lineage, data regulation, dataanalytics, datamodeling, dataops, datawarehouse, devops, etlelt pipelines, github, kubernetes, leadership, metadata management, multitasking, powerbi, problemsolving, python, retail, salesforce cloud technologies, salesforce data cloud accreditation, scala, spark, sql optimization, statisticalml techniques, stress management, supply chain, teamwork, time management"
Lead Data Engineer,InfoVision Inc.,"Richardson, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-infovision-inc-3774848104,2023-12-17,Corinth,United States,Mid senior,Onsite,"We are looking for
Lead Data Engineer with
Azure Synapse,
Azure ADF, Power BI, ETL (SSIS) AND ITIL V4 Concepts
Experience. It’s an
onsite
position in
Dallas, TX
.
Qualifications:
15+ years of experience in ETL & Data Warehousing
Should have excellent leadership & communication skills
Should have in depth knowledge on SSIS ETL Tool and good working knowledge on Power BI
Should have worked on data sources such as SAP and Salesforce
Should have very good knowledge of SSIS (ETL Tool), StreamSets (ETL Tool), Azure Cloud, ADF, Azure Synapse Analytics & Azure Hub Events
Should have executed atleast 2 Azure Cloud Data Warehousing projects
Should have worked atleast 4 projects using Agile/SAFe methodology
Should have demonstrated working knowledge on ITIL V4 concepts such as Incident Management, Problem Management, Change Management & Knowledge Management
Should have working experience on any DevOps tools like GitHub, Jenkins, etc & on semi-structured data formats like JSON, Parquet and/or XML files & written complex SQL queries for data analysis and extraction.
Should have in depth understanding on Data Warehousing, Data Analysis, Data Profiling, Data Quality & Data Mapping
Should have cross global location experience and led a team of atleast 15+ members in a global delivery model
Should have experience in working with product managers, project managers, business users, applications development team members, DBA teams and Data Governance team on a daily basis to analyze requirements, design, development and deployment technical solutions
Responsibilities:
Lead nearshore and offshore team to do production support of existing EDW, design, build & deploy enhancements and bug fixes to existing EDW
Support the existing EDW system built using Azure SQL, Microsoft SSIS, StreamSets & Power BI
Be on call to support job incidents on rotational basis
Work with business and technology stakeholders to communicate EDW incidents/problems and manage their expectations
Leverage ITIL concepts to circumvent incidents, manage problems and document knowledge
Analyze the different source systems, profile data, understand, document & fix Data Quality issues
Gather requirements and business process knowledge in order to transform the data in a way that is geared towards the needs of end users
Write complex SQLs to extract & format source data for ETL/data pipeline
Create design documents, Source to Target Mapping documents and any supporting documents needed for deployment/migration
Design, Develop and Test ETL/Data pipelines
Write Unit Test cases, execute Unit Testing and document Unit Test results
Deploy ETL/Data pipelines
Use DevOps tools to version, push/pull code and deploy across environments
Support team during troubleshooting & debugging defects & bug fixes, business requests, environment migrations & other adhoc requests
Maintain and improve already existing processes
Ensure that the data pipelines are stable, secure, maintainable and highly available
--
Anjanna (ARJUN) Pallapu
Manager- Recruitment
anjanna.pallapu@infovision.com
www.infovision.com
Show more
Show less","Azure Synapse, Azure ADF, Power BI, ETL (SSIS), ITIL V4 Concepts, SSIS ETL tool, StreamSets ETL tool, Azure Cloud, Azure Hub Events, Agile/SAFe methodology, GitHub, Jenkins, JSON, Parquet, XML files, Data Warehousing, Data Analysis, Data Profiling, Data Quality, Data Mapping, DevOps, Product Management, Project Management, Business User, Application Dev Team, DBA, Data Governance, EDW, SQL, Source to Target Mapping, ETL/Data pipeline development, Unit Testing, Troubleshooting, Debugging","azure synapse, azure adf, power bi, etl ssis, itil v4 concepts, ssis etl tool, streamsets etl tool, azure cloud, azure hub events, agilesafe methodology, github, jenkins, json, parquet, xml files, data warehousing, data analysis, data profiling, data quality, data mapping, devops, product management, project management, business user, application dev team, dba, data governance, edw, sql, source to target mapping, etldata pipeline development, unit testing, troubleshooting, debugging","agilesafe methodology, application dev team, azure adf, azure cloud, azure hub events, azure synapse, business user, data governance, data mapping, data profiling, data quality, dataanalytics, datawarehouse, dba, debugging, devops, edw, etl ssis, etldata pipeline development, github, itil v4 concepts, jenkins, json, parquet, powerbi, product management, project management, source to target mapping, sql, ssis etl tool, streamsets etl tool, troubleshooting, unit testing, xml files"
Data Engineer Manager,PepsiCo,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-manager-at-pepsico-3755657823,2023-12-17,Corinth,United States,Mid senior,Onsite,"Overview
PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s
Enterprise Data Operations (EDO)
team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Enterprise Data Operations (EDO) does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
Responsibilities
As a data engineering manager, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Accountabilities
Provide leadership and management to a team of data engineers, managing processes and their flow of work, vetting their designs, and mentoring them to realize their full potential.
Act as a subject matter expert across different digital projects.
Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
8+ years of overall technology experience that includes at least 6+ years of hands-on software development, data engineering, and systems architecture.
6+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
6+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
4+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience scaling and managing a team of engineers.
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
BA/BS in Computer Science, Math, Physics, or other technical fields.
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring, hiring and scaling data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Understands both the engineering and business side of the Data Products released.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
Please view our Pay Transparency Statement.
Show more
Show less","Data Engineering, Data Warehousing, Data Analytics, SQL, Python, PySpark, Scala, Azure, Kubernetes, Data Modeling, Data Quality, MPP, Data Lineage, DevOps, DataOps, PowerBI, Big Data, Data Lake, Data Pipeline, Data Science, Machine Learning, Statistics, Retail, Supply Chain, Metadata, ETL, ELT, Data Profiling, Data Visualization, Data Integration, Business Intelligence, Data Governance","data engineering, data warehousing, data analytics, sql, python, pyspark, scala, azure, kubernetes, data modeling, data quality, mpp, data lineage, devops, dataops, powerbi, big data, data lake, data pipeline, data science, machine learning, statistics, retail, supply chain, metadata, etl, elt, data profiling, data visualization, data integration, business intelligence, data governance","azure, big data, business intelligence, data engineering, data governance, data integration, data lake, data lineage, data pipeline, data profiling, data quality, data science, dataanalytics, datamodeling, dataops, datawarehouse, devops, elt, etl, kubernetes, machine learning, metadata, mpp, powerbi, python, retail, scala, spark, sql, statistics, supply chain, visualization"
Sr Data Engineer,MANDO TECHNOLOGIES INC,"Plano, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-mando-technologies-inc-3767598096,2023-12-17,Corinth,United States,Mid senior,Onsite,"Mando Technologies is specializes in helping organizations make the most of their information assets. From acquiring, organizing, analyzing, and delivering data to closing the loop by integrating intelligence into the operations of the enterprise, Mando Technologies covers the full spectrum of Business Intelligence.
Job Description
Only W2 Candidates*********
Looking for experienced Data Engineer with strong Pyspark and SQL skills with exposure to AWS services for one of our Direct client in Dallas, TX.
At least 3-5 years’ experience with Big Data / Hadoop architecture and related technologies
Hands-on experience with Spark RDDs, Datasets, Dataframes, etc with Python or Java
Hands-on experience using SQL, Spark SQL, HiveQL and performance tuning for big data operations
Hands-on experience using technologies such as Hive, Pig, Sqoop, UNIX environment etc
Experience using CI/CD processes for application software integration and deployment using Git, Jenkins
Experience with AWS (S3. EMR, EC2 & Lambda) or similar technologies
Experience using SDLC and Agile software development practices
Exposure to Snowflake or relational databases experience with SQL optimization skills
Soft-skills
Good written, verbal, presentation, and interpersonal communication skills, given an opportunity willing to work in a challenging and cross platform environment.
Strong Analytical and problem-solving skills. Ability to quickly master new concepts and applications
Preferable knowledge in Business Intelligence and Visualization
Show more
Show less","Pyspark, SQL, AWS, Big Data, Hadoop, Spark RDDs, Datasets, Dataframes, Python, Java, Spark SQL, HiveQL, Hive, Pig, Sqoop, UNIX, CI/CD, Git, Jenkins, SDLC, Agile, Snowflake, Relational databases, Business Intelligence, Visualization","pyspark, sql, aws, big data, hadoop, spark rdds, datasets, dataframes, python, java, spark sql, hiveql, hive, pig, sqoop, unix, cicd, git, jenkins, sdlc, agile, snowflake, relational databases, business intelligence, visualization","agile, aws, big data, business intelligence, cicd, dataframes, datasets, git, hadoop, hive, hiveql, java, jenkins, pig, python, relational databases, sdlc, snowflake, spark, spark rdds, spark sql, sql, sqoop, unix, visualization"
NETWORK/DATA ENGINEER,ASK Consulting,"Irving, TX",https://www.linkedin.com/jobs/view/network-data-engineer-at-ask-consulting-3778725589,2023-12-17,Corinth,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: NETWORK/DATA ENGINEER
Location: Irving, TX, 75039
Duration: 32 months
Pay Rate : $50 on W2
In office 3 days per week
Job Description:
Required Skills:
TOP 5 Skills Needed:
Project based work in a team environment
Cisco CCNA certification
Experience in new build configuration on IOS, IOS-XR, Arista EOS, Ciena
Cloud computing / Whitebox
Ethernet/L2 & L3 Troubleshooting
Responsibilities:
Overall Purpose: Responsible for providing custom data network operational support, design, engineering, and planning for complex global data network and communications projects involving TCP/IP and related protocol connectivity for networks supporting AT&T customers in a 7x24 environment. Roles & Responsibilities: 1) Responsible for the management of the interoperability between Cisco and Juniper Layer 3 Platform, through the use of alarm and ticket systems, individual designed customized scripts, customer notification, and Business Partner escalations. 2) Responsible for deploying new routers, switches, and trunk capacity as well as configuring them. 3) Responsibilities include providing real time in-depth analysis and real time trouble resolution of incidents associated with the CISCO, Juniper, and associated Operations Support Systems, and Data Communications Network Technology platforms.
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Cisco CCNA, IOS, IOSXR, Arista EOS, Ciena, Cloud Computing, Whitebox, Ethernet, L2, L3, Troubleshooting, TCP/IP, Juniper Layer 3 Platform, Scripts, Network Technology","cisco ccna, ios, iosxr, arista eos, ciena, cloud computing, whitebox, ethernet, l2, l3, troubleshooting, tcpip, juniper layer 3 platform, scripts, network technology","arista eos, ciena, cisco ccna, cloud computing, ethernet, ios, iosxr, juniper layer 3 platform, l2, l3, network technology, scripts, tcpip, troubleshooting, whitebox"
Senior Data Engineer - Remote,Crash Champions,"Richardson, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-crash-champions-3761294463,2023-12-17,Corinth,United States,Mid senior,Onsite,"Overview
Crash Champions is home to a collection of team members driven by a deeply rooted purpose and guided by a powerful principle: Here, We Do More.
It is the Crash Champions DNA and it is the mark of who we are. Above everything, we believe in what we do; and we hold fast to the conviction of why we do it. As the largest founder-led collision repair service provider in the U.S., we serve our customers and business partners with an unexpected level of personal and professional service to build trust at every touch point.
Responsibilities
Crash Champions is seeking a Senior Database Engineer to join our IT Data Analytics team which supports Data andReportinginitiatives across the organization. The primary responsibility of this role is to design, implement, and support reporting and our data warehousesolutions.The candidatemusthave strong experience using the Microsoft SQL Server Platform with an emphasis in T-SQL and SSIS development. As a member of our Data Analytics team, the role will work closely with ourData Architect and IT Directorand business stakeholders across the organization.
Responsible for working with business leaderships and IT Data Analytics team to gather business requirements, use cases, technical design, and implement solutions for data analysis, reporting, and data interfaces
Works in cross-functional team environment and has a strong track record to communicating insights and technical solutions to business functional leaders
SQL performance tuning and optimization
Build technical/data architecture for ODS/DW using Kimball methodology
Develop and test SSIS ETL packages using SSDT
Managing Timelines and task priorities utilizing Agile Scrum methodology
Create complex queries, data transformation, aggregation, stored procedures, and triggers.
Implement and administer SQL Server Master Data Management Services.
Perform unit, system, and integration testing of all developed code and DB objects.
Fully support the documentation of the data integration solutions, provide data dictionaries, Data Flow/Process Flow diagrams and update runbooks.
Automate DB deployments using DevOps Pipelines.
Qualifications
Experience with SQL Server 2012 and higher (SQL Server 2016 and higher is a plus), including SQL Server Master Data Management Services
Experience with Snowflake, Azure SQL Server, Data Warehouse, Power App and Data Factory a plus
Knowledge and experience implementing ETL design patterns for data warehousing including Kimball methodology
Experience working with different source data such as SQL Server, Excel, Access, Flat Files, On-Prem, Cloud, APIs, RESTful API calls within SSIS/ETL , and many more
Demonstrate understanding of error handling techniques in MS SQL code and Microsoft SSIS solutions.
Strong analytical ability, problem-solving, and critical thinking skills
Strong attention to detail and data validationand in documentation of methods, procedures, standards
Excellent communication and presentation skills to effectively collaborate with multiple business functions across our organization; and works effectively in a team environment
Full lifecycle development (SDLC) skillsusing Azure DevOps, GIT, and SmartSheetto work and roadmap in a fast paced, dynamic environment with changing deadlines and requirements
Bachelor’s Degree in Management Systems, Engineering, Mathematics, Economics, Computer Science or related field with 7+ years of daily experience with Microsoft SQL Server administration and development
Must be U.S. Citizen or Permanent Resident
Technologies
Microsoft SQL Server 2012+ JSON/XML
T-SQL
Master Data Services
MS BI Stack (SSRS, SSIS, SSAS)
Power BI preferred but not required
Snowflake, Microsoft Azure SQL Server, Data Warehouse and Data Factory
TFS/VSTS/GIT Source Control
Submit a Referral (https://careers-cchampions.icims.com/jobs/3852/senior-data-engineer----remote/job?mode=apply&apply=yes&in\_iframe=1&hashed=-1834384512)
Location
US-TX-Richardson
ID
2023-3852
Category
Information Technology
Position Type
Regular Full-Time
Remote
Yes
Location : Address
808 S. Central Expressway
Show more
Show less","Microsoft SQL Server, TSQL, SSIS, SSRS, SSAS, Kimball methodology, DevOps, Agile Scrum, JSON/XML, Master Data Services, Power BI, Snowflake, Azure SQL Server, Data Warehouse, Data Factory, TFS/VSTS/GIT, ETL, ODS/DW, SQL Server Master Data Management Services, Data dictionaries, Data Flow/Process Flow diagrams, Runbooks, Git, SmartSheet, SDLC","microsoft sql server, tsql, ssis, ssrs, ssas, kimball methodology, devops, agile scrum, jsonxml, master data services, power bi, snowflake, azure sql server, data warehouse, data factory, tfsvstsgit, etl, odsdw, sql server master data management services, data dictionaries, data flowprocess flow diagrams, runbooks, git, smartsheet, sdlc","agile scrum, azure sql server, data dictionaries, data factory, data flowprocess flow diagrams, datawarehouse, devops, etl, git, jsonxml, kimball methodology, master data services, microsoft sql server, odsdw, powerbi, runbooks, sdlc, smartsheet, snowflake, sql server master data management services, ssas, ssis, ssrs, tfsvstsgit, tsql"
Data Engineer Assoc Manager,PepsiCo,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-assoc-manager-at-pepsico-3767172909,2023-12-17,Corinth,United States,Mid senior,Onsite,"Overview
PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development. PepsiCo’s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Data Management and Operations does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
BA/BS in Computer Science, Math, Physics, or other technical fields.
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
Skills, Abilities And Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Competencies
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Understands both the engineering and business side of the Data Products released.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
Please view our Pay Transparency Statement.
Show more
Show less","Data engineering, Data pipelines, Data quality, Cloud data engineering, Azure cloud services, Data modeling, Data warehousing, ETL/ELT pipelines, Data profiling, Data quality tools, MPP database technology, Redshift, Synapse, SnowFlake, Cloud infrastructure, Kubernetes, DevOps, DataOps, Business intelligence tools, PowerBI, Python, PySpark, Scala, SQL, Azure Data Factory, Azure Databricks, Azure Machine learning tools, Statistical/ML techniques, Metadata management, Data lineage, Data glossaries, Agile development","data engineering, data pipelines, data quality, cloud data engineering, azure cloud services, data modeling, data warehousing, etlelt pipelines, data profiling, data quality tools, mpp database technology, redshift, synapse, snowflake, cloud infrastructure, kubernetes, devops, dataops, business intelligence tools, powerbi, python, pyspark, scala, sql, azure data factory, azure databricks, azure machine learning tools, statisticalml techniques, metadata management, data lineage, data glossaries, agile development","agile development, azure cloud services, azure data factory, azure databricks, azure machine learning tools, business intelligence tools, cloud data engineering, cloud infrastructure, data engineering, data glossaries, data lineage, data profiling, data quality, data quality tools, datamodeling, dataops, datapipeline, datawarehouse, devops, etlelt pipelines, kubernetes, metadata management, mpp database technology, powerbi, python, redshift, scala, snowflake, spark, sql, statisticalml techniques, synapse"
Data Engineer Sr Manager - Enterprise Data Foundation,PepsiCo,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-sr-manager-enterprise-data-foundation-at-pepsico-3754930775,2023-12-17,Corinth,United States,Mid senior,Onsite,"Overview
Location:
Must be located within 90 miles of Plano TX, Purchase NY, or Chicago IL. This is a
Hybrid Position:
Targeting 3 days in office per week
Data & Analytics at Pepsico:
PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo’s global business scale to enable business insights, advanced analytics and new product development.
PepsiCo’s
Enterprise Data Operations (EDO)
team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Enterprise Data Operations (EDO) does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
Responsibilities
As a Senior data engineering manager, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems. This role will lead a group of Senior Engineers working on Customer domain data.
Assess and recommend architecture frameworks, design and implement high-performance solutions to support data and analytical products.
Act as a subject matter expert across different digital projects.
Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Execute on a technical solution with minimal guidance, creating solution prototypes, providing top quality design, and create documentation for support and understanding of code created for each initiative
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Strong technical background in data science, business intelligence or data engineering and ETL best practices.
Define and manage SLA’s for data products and processes running in production.
Continue to improve code quality by tracking, reducing, and avoiding technical debt
Work collaboratively with product managers, senior scientists, engineers, and other team members in an agile and scrum environment to fulfill modeling needs.
Work closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Implement Azure products and services, including Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc., as well as conventional data warehouse tools, to create data migration and data engineering solutions.
Qualifications
10+ years of overall technology experience that includes at least 8+ years of hands-on software development, data engineering, and systems architecture.
8+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
8+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
5+ years in cloud data engineering experience in at least one cloud (Azure, AWS, GCP).
Fluent with Azure cloud services. Azure Certification is a plus.
Experience scaling and managing a team of 5+ engineers.
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data quality and data profiling tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Databricks and Mlflow is a plus.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
BA/BS in Computer Science, Math, Physics, or other technical fields.
Preferably some experience in the CPG Industry
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring, hiring and scaling data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Understands both the engineering and business side of the Data Products released.
Teams up and collaborates for speed, agility, and innovation.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.
Compensation and Benefits:
The expected compensation range for this position is between $125,000 $190,600 based on a full-time schedule
Location, confirmed job-related skills and experience will be considered in setting actual starting salary
Bonus based on performance and eligibility; target payout is 15% of annual salary paid out Annually.
Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement
In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan.
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the availableEEO is the Law&EEO is the Law Supplementdocuments. ViewPepsiCo EEO Policy.
Please view ourPay Transparency Statement
Show more
Show less","DataOps, Data Engineering, ETL/ELT Pipelines, Data Warehousing, SQL, Data Quality and Profiling, Data Analytics, Data Modeling, Cloud Data Engineering, Azure Data Services, Cloud Infrastructure, Containerized Services, Kubernetes, Version Control Systems, DevOps and DataOps Concepts, Agile Development, Business Intelligence Tools, Hadoop, Data Lake Storage, Data Factory, Machine Learning, Statistical Techniques, Supply Chain Management, Metadata Management, Data Lineage, Data Glossaries, PowerBI, Tableau, Virtualization","dataops, data engineering, etlelt pipelines, data warehousing, sql, data quality and profiling, data analytics, data modeling, cloud data engineering, azure data services, cloud infrastructure, containerized services, kubernetes, version control systems, devops and dataops concepts, agile development, business intelligence tools, hadoop, data lake storage, data factory, machine learning, statistical techniques, supply chain management, metadata management, data lineage, data glossaries, powerbi, tableau, virtualization","agile development, azure data services, business intelligence tools, cloud data engineering, cloud infrastructure, containerized services, data engineering, data factory, data glossaries, data lake storage, data lineage, data quality and profiling, dataanalytics, datamodeling, dataops, datawarehouse, devops and dataops concepts, etlelt pipelines, hadoop, kubernetes, machine learning, metadata management, powerbi, sql, statistical techniques, supply chain management, tableau, version control systems, virtualization"
Senior Data Engineer,Tata Consultancy Services,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tata-consultancy-services-3778777265,2023-12-17,Corinth,United States,Mid senior,Onsite,"Job Title
Senior Data Engineer
Technical/Functional Skills
Primary – Cloud Based ETL Tool, DBT, Snowflake (Cloud DB), Strong SQL, Unix/Python, Control-M, Service Now, RPA
Experience Required
8-10 yrs
Roles & Responsibilities
Role Description
Analyze requirements and existing resources to Propose, create ETL designs and database objects
Work with project and business analyst leads in order to develop and clarify in-dep th technical requirements including logical and physical data modeling activities
Design and implement ETL processes for data transactions related to Enterprise Data Warehouse, Operational Data Store (ODS), and other data structures to support our Business Intelligence operations
Develops, enhances, debugs, supports, maintains and tests software applications that support business units or supporting functions using IBM Infosphere Data Stage ETL or any other cloud based ETL tool both ETL and ELT approaches. These application program solutions may involve diverse development platforms, software, hardware, technologies and tools.
Must have hands-on on Snowflake development environment with all SQL operations. Must be aware of ELT approach as well.
Participates in the design, development and implementation of complex applications, often using IBM Infosphere Information Server (IIS) products like Data Stage, Quality Stage on a Linux Grid environment. Control-M/Scheduling tools.
Required Skills:10+ Yrs Relevant IT software experience (Technical) in ETL Datastage or any other cloud based ETL Tool development Experience with databases like Snowflake (Cloud DB), Oracle, Netezza, MS SQL Server 2012+, DB2 and MS Access
Experience with job automation & scheduling software (Control-M) Strong ability to write SQL queries
Desired Skills:
Familiar with Snowflake (Cloud DB), DBT,
Python, UNIX, Windows, File transfer utilities, process flow creation, ETL technologies, Hadoop, ServiceNow, RPA
Good to have Skills: Snow-Pro Certified. Service Now Certified,.Strong SQL, Strong conceptual understanding of core DW Concepts including different approaches/methodologies. dbt (data build tool) experience is an added advantage
Show more
Show less","Cloud Based ETL Tool, DBT, Snowflake, SQL, Unix, Python, ControlM, Service Now, RPA, Data Warehousing, Data Modeling, Data Migration, Data Integration, Data Quality, Hadoop, UNIX, Windows, File transfer utilities, ETL technologies, dbt","cloud based etl tool, dbt, snowflake, sql, unix, python, controlm, service now, rpa, data warehousing, data modeling, data migration, data integration, data quality, hadoop, unix, windows, file transfer utilities, etl technologies, dbt","cloud based etl tool, controlm, data integration, data migration, data quality, datamodeling, datawarehouse, dbt, etl technologies, file transfer utilities, hadoop, python, rpa, service now, snowflake, sql, unix, windows"
Sr Data Engineer,Professional Diversity Network,"Plano, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-professional-diversity-network-3777672007,2023-12-17,Corinth,United States,Mid senior,Onsite,"PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo's global business scale to enable business insights, advanced analytics and new product development. PepsiCo's Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Data Management And Operations Does
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to ""productionalize"" data science models.
Define and manage SLA's for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
BA/BS in Computer Science, Math, Physics, or other technical fields.
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
Skills, Abilities And Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Competencies
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Understands both the engineering and business side of the Data Products released.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
Please view our Pay Transparency Statement
Show more
Show less","Data engineering, Data architecture, Cloud data engineering, Data management, Data warehousing, Data lakes, Data analytics, Data science, Data quality, Data integration, DevOps, DataOps, Agile development, Machine learning, Statistical techniques, Python, PySpark, Scala, SQL, MPP databases, Redshift, Synapse, Snowflake, Kubernetes, GitHub, Azure Data Factory, Azure Databricks, Azure Machine Learning, PowerBI, Apache Griffin, Deequ, Great Expectations","data engineering, data architecture, cloud data engineering, data management, data warehousing, data lakes, data analytics, data science, data quality, data integration, devops, dataops, agile development, machine learning, statistical techniques, python, pyspark, scala, sql, mpp databases, redshift, synapse, snowflake, kubernetes, github, azure data factory, azure databricks, azure machine learning, powerbi, apache griffin, deequ, great expectations","agile development, apache griffin, azure data factory, azure databricks, azure machine learning, cloud data engineering, data architecture, data engineering, data integration, data lakes, data management, data quality, data science, dataanalytics, dataops, datawarehouse, deequ, devops, github, great expectations, kubernetes, machine learning, mpp databases, powerbi, python, redshift, scala, snowflake, spark, sql, statistical techniques, synapse"
Sr Data Engineer - Clinical Data Repository,CVS Health,"Irving, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-clinical-data-repository-at-cvs-health-3777364967,2023-12-17,Corinth,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
A Brief Overview:
As a member of the Data and Analytics organization, you will be responsible fordesigning, building, and maintaining best-in-class data pipelines aimed at driving best-in-class solutions. You will collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to ensure the efficient and reliable processing, storage, and retrieval of data.
What you will do:
Build high-performing clinical data processing frameworks leveraging Google Cloud Platform and utilizing GCP Services like Dataflow, Pub-Sub, Composer, Healthcare API and Big Query.
Design and develop clinical data pipelines for ingestion, enrichment, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.).
Perform Health care Data Analysis, Data profiling of source data to derive meaningful insights, and document data requirements to support the new data source onboarding.
Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels. .
Develop proof of concepts for batch/real-time data engineering solutions that leverage emerging technologies.
Required Qualifications
6+years of hands-on experience in design, and development of enterprise data processing applications (ETL/ELT)
5+years of hands-on and robust experience in Python, Unix Shell scripting, SQL and handling of JSON, XML data.
3+years of experience in building batch and streaming data pipelines using cloud data engineering technologies ( in GCP/AWS/Azure etc)
2+years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)
Must have great articulation and communication skills.
Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information.
Preferred Qualifications
GCP data engineering technologies such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)
Knowledge and Experience in Big Query is strongly preferable.
Experience working with health care data and understanding of analytics and how it is leveraged within the healthcare industries.
Experience in working cross functional initiatives communicating effectively and confidently with business partners, project team members and senior management.
Knowledge and experience HL7 FHIR, HL7V2 and C-CDA standards is a strong plus.
Education
Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline. Master's degree preferred.
Pay Range
The typical pay range for this role is:
$90,000.00 - $180,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Data Analytics, Data Pipelines, ETL/ELT, Python, Unix Shell Scripting, SQL, JSON, XML, Cloud Data Engineering Technologies, Cloud Dataflow, Cloud Storage, Pub/Sub, Cloud Composer, Big Query, Health Care API (FHIR Store), Big Query, Healthcare Data, Analytics, HL7 FHIR, HL7 V2, CCDA Standards, Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering","data analytics, data pipelines, etlelt, python, unix shell scripting, sql, json, xml, cloud data engineering technologies, cloud dataflow, cloud storage, pubsub, cloud composer, big query, health care api fhir store, big query, healthcare data, analytics, hl7 fhir, hl7 v2, ccda standards, mathematics, statistics, computer science, business analytics, economics, physics, engineering","analytics, big query, business analytics, ccda standards, cloud composer, cloud data engineering technologies, cloud dataflow, cloud storage, computer science, dataanalytics, datapipeline, economics, engineering, etlelt, health care api fhir store, healthcare data, hl7 fhir, hl7 v2, json, mathematics, physics, pubsub, python, sql, statistics, unix shell scripting, xml"
Senior Data Analyst,Glocomms,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-glocomms-3745374607,2023-12-17,Corinth,United States,Mid senior,Onsite,"This client is a fast growing subscription based tech brand that partners directly with law enforcement and is present in every metro area of the country. They are looking for an adaptive problem solver who is flexible working in an agile, fast-paced environment.
Responsibilities:
Architect and standardize advanced metrics, along with meticulous metric definitions and calculations, fostering a cohesive data interpretation throughout the organization. Thoroughly document metadata where applicable.
Devise and implement sophisticated systems to consistently scrutinize and validate data completeness and configuration accuracy across multifaceted data sources, such as business transactions, case data, sales activity, and pipeline data.
Champion the development and curation of optimal practices for the storage, condensation, transmission, and utilization of intricate technical and operational data. This encompasses formulating strategic approaches like generating concise summary tables, constructing robust data feeds and pipelines, crafting efficient stored procedures, and streamlining workflows.
Build foundational data structures in support of functional teams across the business to enable continuous awareness into system and operational performance.
Develop dashboards, tools, and automated reports at scale oriented to a) provide insight into performance and b) provide user-friendly access of the data and insights to non-expert audiences for analysis and manipulation.
Establish and maintain effective performance tracking; identify improvement opportunities, form hypotheses, and propose, design and implement tests to drive strategy enhancement and optimization.
Partner with functional teams to proactively analyze performance; use analytical techniques to assess data and provide recommendations to improve teams’ operational and strategic approach.
Analyze data to determine goals and performance thresholds to drive growth; create processes for automating performance insights via dashboard alerts, notifications, etc.
Provide analytical support for weekly, monthly, and quarterly business insight reviews.
Serve internal and external customers with diligence, persistence, and energy.
Initiate and drive special projects such as: critically evaluating user behavior to uncover what drives usage and adoption and help us uncover what our critical success factors should be along the way (e.g., Facebook’s “7 friends in 10 days”).
Qualifications:
• 4-5 years of Data Analytics experience; preferably working with big data and are able to make complex analytical concepts look and sound simple to others.
Have deep understanding of and the ability to execute statistical analysis techniques.
Proficiently using various analytical tools such as SQL, Python, Power BI, Tableau, Excel.
Have a Bachelor’s of Science/Bachelors of Arts, preferably in a field relevant to analytics (statistics, information systems, industrial engineering, finance, business, math, etc.).
Masters/Advanced degree in data/analytical field preferred.
Strong individual contributor that can work cross-functionally with various department stakeholders.
What We Offer:
• Medical insurance.
Vision insurance.
Dental insurance.
401(k) Safe Harbor match of 4% after 1 year of employment.
Paid maternity leave.
Paid paternity leave.
Disability insurance.
Flex schedule.
Robust PTO package.
$95,000 - $120,000 a year
Company-wide Bonus Target 10%
Show more
Show less","SQL, Python, Power BI, Tableau, Excel, Data analytics, Statistical analysis, Data visualization, Data interpretation, Business intelligence, Data warehousing, Data mining, Data modeling, Data governance, Data quality management, Performance management, Process improvement, Strategic planning, Problemsolving, Communication, Teamwork, Collaboration, Leadership, Datadriven decisionmaking, Critical thinking, Analytical thinking, Attention to detail, Problemsolving, Communication, Collaboration, Teamwork","sql, python, power bi, tableau, excel, data analytics, statistical analysis, data visualization, data interpretation, business intelligence, data warehousing, data mining, data modeling, data governance, data quality management, performance management, process improvement, strategic planning, problemsolving, communication, teamwork, collaboration, leadership, datadriven decisionmaking, critical thinking, analytical thinking, attention to detail, problemsolving, communication, collaboration, teamwork","analytical thinking, attention to detail, business intelligence, collaboration, communication, critical thinking, data governance, data interpretation, data mining, data quality management, dataanalytics, datadriven decisionmaking, datamodeling, datawarehouse, excel, leadership, performance management, powerbi, problemsolving, process improvement, python, sql, statistical analysis, strategic planning, tableau, teamwork, visualization"
Principal Software Engineer – Data Platforms,CVS Health,"Irving, TX",https://www.linkedin.com/jobs/view/principal-software-engineer-%E2%80%93-data-platforms-at-cvs-health-3770984470,2023-12-17,Corinth,United States,Mid senior,Onsite,"Bring your heart to CVS Health.
Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Principal Software Engineer – Data Platforms (Data, PubTech Data, Insights & Analytics)
In this role, you will lead the architecture and development of data infrastructure and services. Our data products and services empower teams across CVS Health with near real time data to support big data analytics, insights, and machine learning at scale. Our structured datasets enable Scientist to train and deploy Machine learning models, provide Publishers with near real time reporting and insights of their data products, and enable Engineers, Product Managers with access to structured and unstructured data. You will build scalable data-intensive infrastructure that processes petabyte size data, catalogs, transactional data, and telemetry signals.
What The Candidate Will Do:
Technical Leadership: Provide technical leadership and guidance in designing and implementing data solutions that meet business requirements and align with data governance and architecture principles.
Solution Development: Collaborate with stakeholders to understand business needs and translate them into scalable and reliable data systems and tools, while ensuring data quality, privacy, and compliance.
Data Governance: Champion and enforce data governance practices, including data lineage, metadata management, data quality controls, and privacy regulations.
Data Architecture: Design and develop large-scale data systems, including databases, data warehouses, and big data platforms, with a strong focus on data governance and compliance requirements.
Software Engineering: Apply software engineering best practices to build robust and maintainable data solutions, ensuring code quality, performance, and scalability in line with data governance guidelines.
Performance Optimization: Optimize data processing and query performance using technologies such as data proc, gcs, and Big Query, while ensuring adherence to data governance SLAs.
Automation and Efficiency: Drive automation initiatives by developing scripts, utilities, and frameworks to streamline data processes, improve efficiency, and enforce data governance practices.
Collaboration and Mentoring: Collaborate with cross-functional teams, mentor junior engineers, and foster a culture of data governance and compliance awareness within the team.
Innovation and Continuous Improvement: Stay updated with the latest industry trends and technologies in data engineering and data governance, evaluate new tools and techniques, and propose innovative solutions to enhance data systems, processes, and governance practices.
Documentation and Knowledge Sharing: Maintain comprehensive documentation of data solutions, processes, best practices, and data governance frameworks, and actively share knowledge with the team.
Required Qualifications
Experience: 7 – 10+ years of experience as a software engineer, with a strong focus on data engineering and large-scale data systems.
Technical Expertise: Proficiency in technologies such as Hive, Spark, and Python. Experience with additional data technologies and tools is a plus.
Data Governance: Strong understanding of data governance principles, regulations, and industry best practices, with practical experience in implementing and enforcing data governance frameworks.
Data Architecture: Solid understanding of data architecture principles and proven experience in designing and developing scalable data systems, while considering data governance requirements.
Preferred Qualifications
Software Engineering Skills: Excellent programming skills, with expertise in building robust and scalable software solutions using modern software engineering practices in alignment with data governance guidelines.
Problem-Solving Abilities: Analytical mindset with the ability to understand complex business or technical problems and propose effective data solutions, considering data governance and compliance aspects.
Leadership Skills: Demonstrated ability to provide technical leadership, mentor junior engineers, and drive successful outcomes in a collaborative team environment, with a focus on data governance excellence.
Communication Skills: Strong verbal and written communication skills to effectively convey complex technical concepts
Education
Bachelor's or master's degree in Computer Science, Engineering, or a related field.
Pay Range
The typical pay range for this role is:
$140,000.00 – $280,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. This position also includes an award target in the company’s equity award program.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.
For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Software Engineering, Data Platforms, Databases, Data Warehouses, Big Data Platforms, Data Governance, Data Architecture, Data Quality, Privacy, Compliance, Python, Data Proc, GCS, Big Query, Hive, Spark, Machine Learning, AI, Automation, Collaboration, Mentoring, Documentation, Knowledge Sharing, Software Engineering Skills, ProblemSolving Abilities, Leadership Skills, Communication Skills, Computer Science, Engineering","software engineering, data platforms, databases, data warehouses, big data platforms, data governance, data architecture, data quality, privacy, compliance, python, data proc, gcs, big query, hive, spark, machine learning, ai, automation, collaboration, mentoring, documentation, knowledge sharing, software engineering skills, problemsolving abilities, leadership skills, communication skills, computer science, engineering","ai, automation, big data platforms, big query, collaboration, communication skills, compliance, computer science, data architecture, data governance, data platforms, data proc, data quality, data warehouses, databases, documentation, engineering, gcs, hive, knowledge sharing, leadership skills, machine learning, mentoring, privacy, problemsolving abilities, python, software engineering, software engineering skills, spark"
Senior Data Engineer,Wells Fargo,"Irving, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-wells-fargo-3779720157,2023-12-17,Corinth,United States,Mid senior,Onsite,"At Wells Fargo, we want to satisfy our customers' financial needs and help them succeed financially. We're looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you'll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Wells Fargo Technology
sets IT strategy; enhances the design, development, and operations of our systems; optimizes the Wells Fargo infrastructure footprint; provides information security; and enables continuous banking access through in-store, online, ATM, and other channels to Wells Fargo's more than 70 million global customers.
Wells Fargo Bank N.A. seeks a
Senior Data Engineer
in Irving, TX.
Job Role And Responsibility
As a Senior Data Engineer, you will support the Marketing line of business in the Hemisphere Teradata platform as well as two key data domains within the Enterprise Data Environment (EDE) on the Data Lake which utilizes the Big Data Hadoop platforms. This position will fill a key role on the team to provide development support oversight of various data ingestion activities and ETL conformance processes. The ingestion activities and data product outputs are keys to support a broad user community and drive analytic outcomes. In this capacity, the applicant will work with other MDT team members and upstream data providers to ensure efficient and accurate data ingestion solutions and support a variety of data curation and consumption patterns by various partners. The applicant will also be tasked with partnering across cross functional teams to harden platform tools used across the environment by performing and supporting various ETL activities. Through the creation of these data assets and supporting platform capabilities, it will allow for a more streamlined data ingestion and data consumer experience on the Data Lake. This position has the ability to telecommute. Position must appear in person to the location listed as the work address.
Travel required:
NONE
Required Qualifications
Bachelor's degree
in Computer information Systems, Information Technology, or related technical field. Foreign degree equivalent accepted. Employer will accept a 3 year or 4 year Bachelor degree.
Five (5) years
of experience in the job offered or in a related position involving application development and implementation experience.
Specific Skills Required
5 years of ETL (Extract, Transform, Load) Programming experience
3 years of experience delivering ETL, data warehouse and data analytics capabilities on big data architecture such as Hadoop
3 years of Agile experience
3 years of SQL experience
2 years of Hadoop experience
1 years of Apache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs)
1 years of experience in Hadoop ecosystem tools for real-time batch data ingestion, processing and provisioning such as Apache Flume, Apache Kafka, Apache Sqoop, Apache Flink, Apache Spark or Apache Storm
Knowledge of Hadoop, Aster, and R
Qualified applicants send resume to: recruiter_inbox@wellsfargo.com and reference Requisition #001601 in the subject line.
We Value Diversity
At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.
Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.
Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.
Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.
Drug and Alcohol Policy
Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.
Reference Number
R-325331
Show more
Show less","Data Engineering, Teradata, Data Lake, Hadoop, ETL, Apache Spark, Scala, Java, Python, Data Frames, Resilient Distributed Datasets, Hadoop ecosystem tools, Apache Flume, Apache Kafka, Apache Sqoop, Apache Flink, Apache Storm, Hadoop, Aster, R, SQL, Agile","data engineering, teradata, data lake, hadoop, etl, apache spark, scala, java, python, data frames, resilient distributed datasets, hadoop ecosystem tools, apache flume, apache kafka, apache sqoop, apache flink, apache storm, hadoop, aster, r, sql, agile","agile, apache flink, apache flume, apache kafka, apache spark, apache sqoop, apache storm, aster, data engineering, data frames, data lake, etl, hadoop, hadoop ecosystem tools, java, python, r, resilient distributed datasets, scala, sql, teradata"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Frisco, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748829418,2023-12-17,Corinth,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Continuous Delivery, Python, Snowflake, Kafka, Spark, MySQL, Agile Engineering, StreamProcessing Systems, Dimensional Data Modeling, ETL, Data Warehouses, SQL, Legal Compliance, Data Management Tools, Data Classification, Retention, R&D, Prototyping","data engineering, tdd, continuous delivery, python, snowflake, kafka, spark, mysql, agile engineering, streamprocessing systems, dimensional data modeling, etl, data warehouses, sql, legal compliance, data management tools, data classification, retention, rd, prototyping","agile engineering, continuous delivery, data classification, data engineering, data management tools, data warehouses, dimensional data modeling, etl, kafka, legal compliance, mysql, prototyping, python, rd, retention, snowflake, spark, sql, streamprocessing systems, tdd"
Data Analyst,MANDO TECHNOLOGIES INC,"Plano, TX",https://www.linkedin.com/jobs/view/data-analyst-at-mando-technologies-inc-3768770013,2023-12-17,Corinth,United States,Mid senior,Onsite,"Mando Technologies is specializes in helping organizations make the most of their information assets. From acquiring, organizing, analyzing, and delivering data to closing the loop by integrating intelligence into the operations of the enterprise, Mando Technologies covers the full spectrum of Business Intelligence.
Need talented Data Analyst for a long-term contract based in Plano, TX (Hybrid) On W2
Note - No H1b
Key Skills And Experience
Develop and maintain SQL queries and scripts for data extraction and reporting.
Handle and customize APIs to integrate external data sources into our analysis.
Work with PySpark for big data processing and analysis.
Experience in Python and SQL for data manipulation and analysis.
Experience in data patterns, data filling, and data extension techniques.
Experience with PySpark, is a plus.
Preferred
SQL (strong)
AWS S3, CI/CD, etc. (intermediate)
Nice To Have
Python programming
Familiarity with BI tools like Tableau or Power BI
Show more
Show less","SQL, Python, PySpark, API Integration, Data Extraction, Data Reporting, Data Manipulation, Data Analysis, Data Patterns, Data Filling, Data Extension, AWS S3, CI/CD, Tableau, Power BI","sql, python, pyspark, api integration, data extraction, data reporting, data manipulation, data analysis, data patterns, data filling, data extension, aws s3, cicd, tableau, power bi","api integration, aws s3, cicd, data extension, data extraction, data filling, data manipulation, data patterns, data reporting, dataanalytics, powerbi, python, spark, sql, tableau"
Spark (data engineer) Lead,Diverse Lynx,"Irving, TX",https://www.linkedin.com/jobs/view/spark-data-engineer-lead-at-diverse-lynx-3747323850,2023-12-17,Corinth,United States,Mid senior,Onsite,"Hello
Hope you are doing well today !
We are currently hiring for
Spark(Data engineer) Lead
for my client . JD are mention below please go through the JD once & if you find the JD relevant to your prior experience then please reverts back with “
Updated resume , Visa type & current location”
Role: Spark Lead
Location: Irving Tx(Day one onsite role)
Type:- Fulltime
Required skill ::-
Jenkins , RLM, Ansible , Sensu , Linux/Unix.
Job Description
Strong understanding of
SRE concept
and practices with automation implementation.
Spark Architecture and different critical components and services
Strong hands-on experience in shell/python scripting language and good understanding of Linux/Unix.
Good to have
knowledge on Sensu
Hands on experience in Jenkins , RLM, Ansible
Understanding of
Docker containers
and ability to author
Docker images / Understanding of Kubernetes/ OpenShift/Service
Understanding of CI/CD workflow and best practices.
Good to have knowledge and
understanding of K8s.
Good communication skill.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Spark, Jenkins, RLM, Ansible, Sensu, Linux, Unix, Shell scripting, Python scripting, Docker, Kubernetes, OpenShift, CI/CD","spark, jenkins, rlm, ansible, sensu, linux, unix, shell scripting, python scripting, docker, kubernetes, openshift, cicd","ansible, cicd, docker, jenkins, kubernetes, linux, openshift, python scripting, rlm, sensu, shell scripting, spark, unix"
Lead Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-capital-one-3774775828,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasLead Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, i inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
On this team, we are building a suite of products to help our dealers connect with potential car buyers! This team is focusing on building the data infrastructure (right from ingestion to consumption) for all of our products from ground up. We build intelligence for scaling to more dealers and build personalized customer experience.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 6 years of experience in application development (Internship experience does not apply)
At least 2 years of experience in big data technologies
At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
Preferred Qualifications:
7+ years of experience in application development including Python, SQL, Scala, or Java
4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
4+ year experience working on real-time data and streaming applications
4+ years of experience with NoSQL implementation (Mongo, Cassandra)
4+ years of data warehousing experience (Redshift or Snowflake)
4+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Agile engineering practices, Hadoop, Hive, Java, Scala, Python, Unix/Linux, SQL, NoSQL, MapReduce, EMR, Kafka, Spark, Gurobi, Redshift, Snowflake, Machine learning, Distributed microservices, Open Source RDBMS, MySQL, MongoDB, Cassandra, Realtime data, Streaming applications, Data warehousing, Cloud computing, AWS, Microsoft Azure, Google Cloud","agile engineering practices, hadoop, hive, java, scala, python, unixlinux, sql, nosql, mapreduce, emr, kafka, spark, gurobi, redshift, snowflake, machine learning, distributed microservices, open source rdbms, mysql, mongodb, cassandra, realtime data, streaming applications, data warehousing, cloud computing, aws, microsoft azure, google cloud","agile engineering practices, aws, cassandra, cloud computing, datawarehouse, distributed microservices, emr, google cloud, gurobi, hadoop, hive, java, kafka, machine learning, mapreduce, microsoft azure, mongodb, mysql, nosql, open source rdbms, python, realtime data, redshift, scala, snowflake, spark, sql, streaming applications, unixlinux"
SENIOR DATA ENGINEER @Irving - TX,Diverse Lynx,"Irving, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-%40irving-tx-at-diverse-lynx-3764420980,2023-12-17,Corinth,United States,Mid senior,Onsite,"Job title SENIOR DATA ENGINEER 2.Job summary Perform the Data Engineer activities 3.Experience 8to12Yrs 4.Required Skills ,BigQuery,Cloud Dataflow,Airflow 5.Nice to have skills ,PySpark,GCP Services,Python 6.Technology -Not Applicable 7.Shift Day 8.Roles & Responsibilities Analyzes complex data structures from disparate data sources and design large scale data engineering pipeline Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs Collaborates with product business and data science team to collect user stories and translate into technical specifications Uses knowledge in Cloud & Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines Uses strong programming skills in PySpark, Python, Java or any of the major languages to build robust data pipelines and dynamic systems Builds highly scalable and extensible data marts and data models to support Data Science and other internal customers on Cloud. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions Experiments with available tools and advice on new tools to determine optimal solution given the requirements dictated by the model/use case Required Qualifications 3+ years of progressively complex related experience in cloud data engineering and data analysis Knowledge in programing languages such as PySpark, Java, Python, Hive, SQL Knowledge in Cloud Technology, Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment Strong knowledge of large-scale search applications and building high volume data pipelines, preferably using Dataproc, composer services on GCP or other Cloud Platforms 2+ years of development experience in GCP and native tools & services such as DataProc, Composer, BigQuery, Airflow 2 to 3+ years of PySpark or Scala or Python programing proficient in SQL 3&plus; years
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","BigQuery, Cloud Dataflow, Airflow, PySpark, GCP Services, Python, Hadoop, HDFS, Apache Hive, SQL, Dataproc, Composer, Composer, Scala","bigquery, cloud dataflow, airflow, pyspark, gcp services, python, hadoop, hdfs, apache hive, sql, dataproc, composer, composer, scala","airflow, apache hive, bigquery, cloud dataflow, composer, dataproc, gcp services, hadoop, hdfs, python, scala, spark, sql"
Principal DATA Engineer   Data Analytics Platform,Verdant Infotech Solutions,"Copper Canyon, TX",https://www.linkedin.com/jobs/view/principal-data-engineer-data-analytics-platform-at-verdant-infotech-solutions-3785514142,2023-12-17,Corinth,United States,Mid senior,Onsite,"Title: Principal DATA Engineer – Data Analytics Platform
Duration: Direct Hire
Visa: USC and GC only ( No Fake , Need Genuine Only)
Location: Hybrid available in the following cities: Phoenix, AZ / Chicago, IL / New York, NY / San Francisco, CA ( Need local of these cities only)
Interview: 2 Videos
Need: Updated LinkedIn with profile pic.
Job Description
This position is a hands-on, individual contributor and technical leader involved in setting the standards and ensuring excellence in quality of outputs across multiple teams to create an Analytics Data Platform. This initiative consists of creating an appropriate replica of various bespoke operational data stores and customer contribution data into a data platform that is fit for analytics and data science workloads. Scale and velocity should excite you, not scare you. We are looking for someone who has successfully built and delivered an Analytics Data Platform from the ground up.
This is a principal level, individual contributor role responsible for architecting a Data Analytics Platform based on AWS technologies. They will also be responsible for acting as a player/coach with our existing analytics technical platform team consisting of senior and junior data engineers. They will have the support of our Sr. Director, Analytics Technical Platform. It is critical to have actual, hands-on experience architecting and building a data analytics platform for medium to large enterprise based on AWS technologies. Theoretical knowledge is not enough.
Key Responsibilities
Minimum 15 or more years of experience in designing and developing complex software projects.
Experience in influencing cross-functional teams to create high throughput, high velocity data pipelines using both streaming and batch processing paradigms.
Effective communicator with exceptional public speaking skills. Comfortable presenting to all levels within the company.
Experience designing, implementing and operationalizing an analytics data platform in AWS.
Networking experience relevant to moving data between on-prem and cloud environments.
Experience with data handling processes such as retention policies, masking and encryption for compliance and visibility purposes.
Current development skills using python, java or Scala.
Experience with multiple data stores such as Oracle, MS-SQL Server, Postgres and noSql variety.
Experience running a vendor selection process for relevant analytics platform needs.
Expertise in terraform, k8s and/or other cloud orchestration technologies.
Experience creating AWS observability and monitoring systems.
Experience with several of the following AWS technologies: Glue/Glue Catalog/Glue Crawler, Kinesis/Firehose, RedShift, DynamoDB, Athena.
Experience with ETL tools.
Experience with data processing orchestration tools.
Experience with data cataloging tools.
Essential Functions
Partners with product management to craft product strategy, create product descriptions and ensure alignment to technology roadmaps.
Be a thought leader: a senior point of expertise on Data Engineering, Data Science, Business Intelligence, software engineering issues, industry trends and developing technologies. Be a role model to others on the team; coach and mentor team members.
Takes ownership for creating technical product design and architecture – evaluating buy vs. build decisions and bought product maturity and fit for purpose.
Works closely with customers to understand their needs and create a partnership for making products better.
Documents SDLC artifacts so that other team members can understand and ""follow the leader” – such as Confluence documentation, jira artifacts and associated MS-office documents (Excel, Word, PPT).
Designs, implements and operationalizes a data analytics platform using AWS tools such as:
S3 storage
Glue and related tools
Redshift, DynamoDB
Athena
Crawlers
Observability matters using CloudWatch and CloudTrail
Coach and develop others to use the above tools by creating small technical demonstrators or POCs.
Creates and conducts presentations for small-to-medium size groups.
Support the company's commitment to risk management and protecting the integrity and confidentiality of systems and data.
The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow instructions and perform other related duties as assigned by their supervisor.
Minimum Qualifications
Education and/or experience typically obtained through a bachelor's degree in computer science, or related technical field.
Minimum 15 or more years of experience in designing and developing complex software projects.
Experience in influencing cross-functional teams to create high throughput, high velocity data pipelines using both streaming and batch processing paradigms.
Effective communicator with exceptional public speaking skills. Comfortable presenting to all levels within the company.
Knowledge of Software Development Lifecycle (SDLC) best practices, software development methodologies (Agile, Scrum, LEAN etc) and DevOps practices.
Experience designing, implementing and operationalizing an analytics data platform in AWS.
Networking experience relevant to moving data between on-prem and cloud environments.
Experience with data handling processes such as retention policies, masking and encryption for compliance and visibility purposes.
Current development skills using python, java or Scala.
Experience with multiple data stores such as Oracle, MS-SQL Server, Postgres and noSql variety.
Experience running a vendor selection process for relevant analytics platform needs.
Expertise in terraform, k8s and/or other cloud orchestration technologies.
Experience creating AWS observability and monitoring systems.
Experience with several of the following AWS technologies: Glue/Glue Catalog/Glue Crawler, Kinesis/Firehose, RedShift, DynamoDB, Athena.
Experience with ETL tools.
Experience with data processing orchestration tools.
Experience with data cataloging tools.
Background and drug screen.
Preferred Qualifications
Master's degree in computer science or related field.
|
,
5208 Windsor Ln, Copper Canyon, Texas, 75077
Show more
Show less","AWS, Data Analytics Platform, Data Engineering, Data Science, Business Intelligence, Software Engineering, Cloud Orchestration, Terraform, K8s, Python, Java, Scala, Oracle, MSSQL Server, Postgres, NoSQL, ETL Tools, Data Processing Orchestration Tools, Data Cataloging Tools, Confluence, Jira, CloudWatch, CloudTrail, Glue, Glue Catalog, Glue Crawler, Kinesis, Firehose, RedShift, DynamoDB, Athena, Crawlers","aws, data analytics platform, data engineering, data science, business intelligence, software engineering, cloud orchestration, terraform, k8s, python, java, scala, oracle, mssql server, postgres, nosql, etl tools, data processing orchestration tools, data cataloging tools, confluence, jira, cloudwatch, cloudtrail, glue, glue catalog, glue crawler, kinesis, firehose, redshift, dynamodb, athena, crawlers","athena, aws, business intelligence, cloud orchestration, cloudtrail, cloudwatch, confluence, crawlers, data analytics platform, data cataloging tools, data engineering, data processing orchestration tools, data science, dynamodb, etl tools, firehose, glue, glue catalog, glue crawler, java, jira, k8s, kinesis, mssql server, nosql, oracle, postgres, python, redshift, scala, software engineering, terraform"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Irving, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830440,2023-12-17,Corinth,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Spark, pySpark, Kafka, Storm, TDD, Pair Programming, Continuous Integration, ETL, Data Warehouses, Kafka, Data classification, Data retention","python, sql, snowflake, airflow, kubernetes, docker, spark, pyspark, kafka, storm, tdd, pair programming, continuous integration, etl, data warehouses, kafka, data classification, data retention","airflow, continuous integration, data classification, data retention, data warehouses, docker, etl, kafka, kubernetes, pair programming, python, snowflake, spark, sql, storm, tdd"
Senior / Principal Backend Engineer – Data Platform (HYBRID),Dematic,"Plano, TX",https://www.linkedin.com/jobs/view/senior-principal-backend-engineer-%E2%80%93-data-platform-hybrid-at-dematic-3771477049,2023-12-17,Corinth,United States,Mid senior,Onsite,"About Us
At Dematic, we are gearing up to revolutionize our data landscape by building a cutting-edge Enterprise Data Lakehouse Platform. We are forming two pivotal platform teams that will spearhead the creation of the platform's foundational components. These teams go beyond traditional data ingestion; they are architects of a microservices-driven platform, providing abstractions that empower other teams to seamlessly extend the platform.
Role Overview
We are seeking a dynamic and highly skilled Principal Data Engineer to lead these foundational efforts. This role demands someone who not only possesses a profound understanding of the data engineering landscape but is also at the forefront of their game. The ideal candidate will contribute significantly to platform development, leading a team of contractors while actively shaping the future of our data ecosystem.
What we offer:
Career Development
Competitive Compensation and Benefits
Pay Transparency
Global Opportunities
Learn More Here: https://www.dematic.com/en-us/about/careers/what-we-offer
Dematic provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
The base pay range for this role is estimated to be $110,500-138,500 at the time of posting. Final compensation will be determined by various factors such as work location, education, experience, knowledge, and skills.
What We Are Looking For:
Architect, design and develop core data platform components with a microservices architecture, abstracting platform, and infrastructure intricacies.
Create and maintain essential data platform SDKs and libraries, adhering to industry best practices.
Design and develop connector frameworks and modern connectors to source data from disparate application both on-prem and cloud.
Design and optimize data storage, processing, and querying performance for large-scale datasets using industry best practices.
Design and develop data quality frameworks and processes to ensure the accuracy and reliability of data.
Collaborate with data scientists, analysts, and cross functional teams to design data models, database schemas and data storage solutions.
Design and develop advanced analytics and machine learning capabilities on the data platform.
Design and develop observability and data governance frameworks and practices.
Stay up to date with the latest data engineering trends, technologies, and best practices.
Drive the deployment and release cycles, ensuring a robust and scalable platform.
Tasks and Qualifications:
Requirements:
10+ (for senior) 15+ (for principal) of proven experience in modern cloud data engineering, data architectures, data warehousing, and software engineering.
Expertise in architecting, designing, and building end to end data platforms in the GCP environment using BigQuery and other services while adhering to best practices guidelines such as open standards, cost, performance, time to market and minimize vendor lock.
Solid experience building data platforms in GCP environment.
Solid experience designing and developing modular, distributed data platform components with a microservices architecture. Strong experience with Docker, Kubernetes, APIs is needed.
Proficiency in data engineering tools and technologies - SQL, Python, Spark, DBT, Airflow, Kafka.
Solid experience implementing data lineage, data quality and data observability for big data workflows.
Strong experience with modern data modeling, data architecture, and data governance principles.
Excellent experience with DataOps principles and test automation.
Excellent experience with observability tools - Grafana and Datadog
Applicants must be authorized to work for any employer in the US without visa requirements. This position is not eligible for an employment visa sponsorship.
Nice to have:
Experience with Data Mesh architecture.
Experience building Semantic layers for data platforms.
Experience building scalable IoT architectures
Show more
Show less","Microservices, GCP, BigQuery, Docker, Kubernetes, APIs, SQL, Python, Spark, DBT, Airflow, Kafka, Data lineage, Data quality, Data observability, Data modeling, Data architecture, Data governance, DataOps, Grafana, Datadog","microservices, gcp, bigquery, docker, kubernetes, apis, sql, python, spark, dbt, airflow, kafka, data lineage, data quality, data observability, data modeling, data architecture, data governance, dataops, grafana, datadog","airflow, apis, bigquery, data architecture, data governance, data lineage, data observability, data quality, datadog, datamodeling, dataops, dbt, docker, gcp, grafana, kafka, kubernetes, microservices, python, spark, sql"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3781379615,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data Engineering, Scala, Python, Java, Linux, Agile, RDBMS, NoSQL, Cloud Computing, AWS, Azure, Google Cloud, Hadoop, Hive, Kafka, Spark, Mongo, Cassandra, Redshift, Snowflake, Unit Testing, SQL, MapReduce, EMR, Gurobi, MySQL","data engineering, scala, python, java, linux, agile, rdbms, nosql, cloud computing, aws, azure, google cloud, hadoop, hive, kafka, spark, mongo, cassandra, redshift, snowflake, unit testing, sql, mapreduce, emr, gurobi, mysql","agile, aws, azure, cassandra, cloud computing, data engineering, emr, google cloud, gurobi, hadoop, hive, java, kafka, linux, mapreduce, mongo, mysql, nosql, python, rdbms, redshift, scala, snowflake, spark, sql, unit testing"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774778159,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 5 (31065), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking a
Senior Data Engineer
who is passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do
Proactively seeks out opportunities to address customer needs and influences stakeholders so that we are building the best solutions for the most important problems
Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data
Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity
Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling
Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs
Use cutting edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka or Spark)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Python, SQL, Scala, Java, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Redshift, Snowflake, UNIX/Linux, Agile","python, sql, scala, java, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, redshift, snowflake, unixlinux, agile","agile, aws, emr, google cloud, hadoop, hive, java, kafka, mapreduce, microsoft azure, python, redshift, scala, snowflake, spark, sql, unixlinux"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3781022656,2023-12-17,Corinth,United States,Mid senior,Onsite,"Locations: TX - Plano, United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
On this team, we are building a suite of products to help our dealers connect with potential car buyers! This team is focusing on building the data infrastructure (right from ingestion to consumption) for all of our products from ground up. We build intelligence for scaling to more dealers and build personalized customer experience.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, SQL, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Agile","java, scala, python, rdbms, nosql, redshift, snowflake, sql, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongo, cassandra, unixlinux, agile","agile, cassandra, emr, gurobi, hadoop, hive, java, kafka, mongo, mysql, nosql, python, rdbms, redshift, scala, snowflake, spark, sql, unixlinux"
Senior Data Engineer (Python),Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-python-at-capital-one-3783102110,2023-12-17,Corinth,United States,Mid senior,Onsite,"Center 1 (19052), United States of America, McLean, VirginiaSenior Data Engineer (Python)
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
San Francisco, California (Hybrid On-Site): $171,500 - $195,800 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data Engineering, Python, Java, Scala, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Agile development, Unit testing, Cloud Computing, AWS, Microsoft Azure, Google Cloud, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX/Linux, Realtime data, Streaming applications, Data warehousing, MongoDB, Cassandra","data engineering, python, java, scala, open source rdbms, nosql databases, redshift, snowflake, agile development, unit testing, cloud computing, aws, microsoft azure, google cloud, hadoop, hive, emr, kafka, spark, gurobi, mysql, unixlinux, realtime data, streaming applications, data warehousing, mongodb, cassandra","agile development, aws, cassandra, cloud computing, data engineering, datawarehouse, emr, google cloud, gurobi, hadoop, hive, java, kafka, microsoft azure, mongodb, mysql, nosql databases, open source rdbms, python, realtime data, redshift, scala, snowflake, spark, streaming applications, unit testing, unixlinux"
Principal Data Engineer North Dallas or Detroit Metro,Comerica Bank,"Plano, TX",https://www.linkedin.com/jobs/view/principal-data-engineer-north-dallas-or-detroit-metro-at-comerica-bank-3787966287,2023-12-17,Corinth,United States,Mid senior,Onsite,"Principal Data Engineer
The Principal Data Engineer will provide data engineering and architecture support for complex Data management activities related to system software, hardware, performance, problem determination or resource management requiring communication and coordination with vendors, technicians, clients, and management. Provide technical input to management decision making in the areas of software, hardware, and performance. Responsibilities include performing detailed analysis of various projects and requests; developing designs for projects that have medium to high complexity; formulating procedures; monitoring key performance indicators; and implementing advanced business solutions. This role may also integrate Data solutions or build via DevOps.
The Principal Data Engineer will develop strong collaborative relationships with key technology and data teams and will be responsible for data modeling, applying data quality rules & standards, leveraging the latest technologies, and performing proof-of-concept tasks to evaluate new data systems, products, and data source performance. This role will also assist development teams, troubleshoot with data ingestion, and manage ELT/ETL extraction needs and issues.
Position Responsibilities
Design and Methodology
Work closely with business units, application teams, infrastructure areas and vendors to identity, review and evaluate the solution requirements.
Investigate and propose strategic fits for virtualization, consolidation and rationalization solution opportunities within the infrastructure or business. Propose changes to the technical architecture and design solutions as applicable.
Evaluate and align strategic fit solutions across infrastructure platforms and solutions specific to system hardware and software technologies.
Understand, participate, review and influence long term capacity planning and technology investments.
Technical Consulting
Provide Client consulting and planning guidance as applicable for moderate to large highly complex projects/programs.
Provide consultation and works closely with other functional infrastructure areas/departments on multiple initiatives to meet common organizational / business goals and objectives.
Participate in and provides consulting to project teams on architectural, design development, integration opportunities, planning of highly complex systems and assures it is aligned to our established strategies, guiding principles, rationales and practices.
Planning and Organizing
Identify and evaluate projects/programs/initiatives and design processes that enhance and rationalize existing and upcoming solutions.
Map requirements into standard services solution, identity opportunities for integrating to existing or reuse technology and provide cost effective solutions for moderate to large highly complex project/programs/initiatives.
Review, identify and manage requirements for moderate to complex solutions and do a cost value, feasibility and risk analysis.
Risk Management
Review, participate, develop and update architectural standards, guiding principles, rationales and strategies.
Evaluate, review and approve highly complex design solutions for business and Infrastructure project or programs or initiatives.
Position Qualifications
Bachelor's degree in computer science, engineering or in a technology related field, OR equivalent through a combination of education and/or technology experience, OR 12 years of technology experience
6 years of Technology experience
7 years in identifying technical solutions for complex business problems, identifying the benefits and risks of the solutions and providing recommendations
5 years of experience mentoring another technologist
4 years of experience working in enterprise data warehouse solutions and platforms, and working knowledge of different databases (e.g., SQL & NoSQL), S3 Datalake, CICD, Jenkins and AWS cloud technologies
4 years of experience independently administering, managing compute & storage, applying best practices for schema, table, DDL creations, querying, evaluating load operations & performance, monitoring, cost effective solutions for RDS and Snowflake database systems & tools
4 years of data governance experience, including column-level data security using secure views and dynamic data masking features, define & implement RBAC secure access to objects
4 years of experience supporting production database systems, CDC, scheduling backup & recovery, archiving, replication, and Disaster Recovery management
This position is not eligible for sponsorship. Must have indefinite employment authorization
Plano8:00am - 5:00pm Monday - Friday
Show more
Show less","Data Engineering, Data Architecture, Data Management, System Software, Hardware, Performance, Problem Determination, Resource Management, DevOps, Data Modeling, Data Quality, ProofofConcept, Data Systems, Data Products, Data Source Performance, ELT/ETL Extraction, Virtualization, Consolidation, Rationalization, Technical Consulting, Project Planning, Risk Management, AWS Cloud Technologies, RDS, Snowflake, Data Governance, Data Security, RBAC, CDC, Backup & Recovery, Archiving, Replication, Disaster Recovery","data engineering, data architecture, data management, system software, hardware, performance, problem determination, resource management, devops, data modeling, data quality, proofofconcept, data systems, data products, data source performance, eltetl extraction, virtualization, consolidation, rationalization, technical consulting, project planning, risk management, aws cloud technologies, rds, snowflake, data governance, data security, rbac, cdc, backup recovery, archiving, replication, disaster recovery","archiving, aws cloud technologies, backup recovery, cdc, consolidation, data architecture, data engineering, data governance, data management, data products, data quality, data security, data source performance, data systems, datamodeling, devops, disaster recovery, eltetl extraction, hardware, performance, problem determination, project planning, proofofconcept, rationalization, rbac, rds, replication, resource management, risk management, snowflake, system software, technical consulting, virtualization"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3780755514,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
New York City (Hybrid On-Site): $161,900 - $184,800 for Senior Data EngineerSan Francisco, California (Hybrid On-Site): $171,500 - $195,800 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Cloud based data warehousing, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Agile engineering, MapReduce, AWS, Microsoft Azure, Google Cloud","java, scala, python, open source rdbms, nosql databases, redshift, snowflake, cloud based data warehousing, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongo, cassandra, unixlinux, agile engineering, mapreduce, aws, microsoft azure, google cloud","agile engineering, aws, cassandra, cloud based data warehousing, emr, google cloud, gurobi, hadoop, hive, java, kafka, mapreduce, microsoft azure, mongo, mysql, nosql databases, open source rdbms, python, redshift, scala, snowflake, spark, unixlinux"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774777399,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 6 (31066), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking a
Senior Data Engineer
who is passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do
Proactively seeks out opportunities to address customer needs and influences stakeholders so that we are building the best solutions for the most important problems
Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data
Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity
Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling
Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs
Use cutting edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka or Spark)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data engineering, Agile development, Python, SQL, Scala, Java, TensorFlow, Linux, Hadoop, Hive, EMR, Kafka, Spark, Redshift, Snowflake, AWS, Azure, Google Cloud","data engineering, agile development, python, sql, scala, java, tensorflow, linux, hadoop, hive, emr, kafka, spark, redshift, snowflake, aws, azure, google cloud","agile development, aws, azure, data engineering, emr, google cloud, hadoop, hive, java, kafka, linux, python, redshift, scala, snowflake, spark, sql, tensorflow"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3786336016,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 5 (31065), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Machine learning, Distributed microservices, Full stack systems, Cloudbased data warehousing, Unit testing, Agile engineering, UNIX/Linux, Shell scripting, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra","java, scala, python, rdbms, nosql, redshift, snowflake, machine learning, distributed microservices, full stack systems, cloudbased data warehousing, unit testing, agile engineering, unixlinux, shell scripting, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongo, cassandra","agile engineering, cassandra, cloudbased data warehousing, distributed microservices, emr, full stack systems, gurobi, hadoop, hive, java, kafka, machine learning, mapreduce, mongo, mysql, nosql, python, rdbms, redshift, scala, shell scripting, snowflake, spark, unit testing, unixlinux"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3779612006,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 6 (31066), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Agile, AWS, Cassandra, Cloud, EMR, Gurobi, Hadoop, Hive, Java, Kafka, MapReduce, Microsoft Azure, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux","agile, aws, cassandra, cloud, emr, gurobi, hadoop, hive, java, kafka, mapreduce, microsoft azure, mongo, mysql, nosql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unixlinux","agile, aws, cassandra, cloud, emr, gurobi, hadoop, hive, java, kafka, mapreduce, microsoft azure, mongo, mysql, nosql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unixlinux"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774775763,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 3 (31063), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Agile, AWS, Cassandra, Cloud Computing, EMR, Gurobi, Hadoop, Hive, Java, Kafka, Linux, MapReduce, Mongo, MySQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX","agile, aws, cassandra, cloud computing, emr, gurobi, hadoop, hive, java, kafka, linux, mapreduce, mongo, mysql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unix","agile, aws, cassandra, cloud computing, emr, gurobi, hadoop, hive, java, kafka, linux, mapreduce, mongo, mysql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unix"
Senior Data Engineer,Capital One,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774772993,2023-12-17,Corinth,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Machine Learning, Microservices, Agile, Unit Testing, Git, UNIX/Linux, Shell Scripting, AWS, Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL","java, scala, python, rdbms, nosql, redshift, snowflake, machine learning, microservices, agile, unit testing, git, unixlinux, shell scripting, aws, azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql","agile, aws, azure, emr, git, google cloud, gurobi, hadoop, hive, java, kafka, machine learning, mapreduce, microservices, mysql, nosql, python, rdbms, redshift, scala, shell scripting, snowflake, spark, unit testing, unixlinux"
"Python Data Engineer (W2 Employees Only, No Corp-to-Corp)",Ccube,"Plano, TX",https://www.linkedin.com/jobs/view/python-data-engineer-w2-employees-only-no-corp-to-corp-at-ccube-3787928953,2023-12-17,Corinth,United States,Mid senior,Onsite,"Location: Plano, TX (Onsite - Hybrid, absolutely not remote)
Requirements:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using Databricks or similar cloud database.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark and Experience in Spark.
Knowledge or experience in architectural best practices in building data lakes
Qualifications:
Bachelor’s or master's degree in Computer Science or a related field (a must)
5+ years of experience building scalable enterprise data pipelines in the cloud
W2 Employees Only, No Corp-to-Corp
Powered by JazzHR
IZ0PrT3m57
Show more
Show less","Data Pipelines, AWS Cloud, Databricks, SQL, Python, Spark, Data Lakes, Cloud Computing, ObjectOriented Programming","data pipelines, aws cloud, databricks, sql, python, spark, data lakes, cloud computing, objectoriented programming","aws cloud, cloud computing, data lakes, databricks, datapipeline, objectoriented programming, python, spark, sql"
Data Analyst (Hybrid),BGSF,"Irving, TX",https://www.linkedin.com/jobs/view/data-analyst-hybrid-at-bgsf-3781102803,2023-12-17,Corinth,United States,Mid senior,Onsite,"BGSF is representing a $1.5B private equity backed Manufacturing client in Irving, Texas seeking to hire Data Analyst with experience in Finance, Procurement, Strategic Sourcing, or Data Analysis.
Major deliverable is to create reports and conduct analytics for procurement to drive decision making. This will be a task heavy role for a while as the company is working within 13 ERP systems currently as they work towards converting all prior acquisitions to MS Dynamics. The ideal individual will need to be able to roll data up, have eye for detail, and be creative in how to bring data together to make sense. Additionally, candidate should be comfortable getting data from market (company subscribes to several market reports).
Hybrid work model: 4 days in-office / 1 day remote option (Friday)
Great Culture ~ laid back environment and no micro-management (everyone is accountable)
Ideal experience is to be data savvy and have strong analytics in Excel, strong math or procurement background, spend management analytics, and cost savings analytics
QUALIFICATIONS:
Bachelors degree in Business, Finance, Engineering, or Supply Chain
3+ years in Finance, Procurement, Strategic Sourcing, or Data Analysis
Experience with financial data analytics, multiple ERP systems, data mining, validations, data manipulation, interpretation & visualization of data
Solid Excel skills (pivot tables, VBA, H/VLOOKUP, COUNTIFs, index match
Power BI required
Base + Bonus + Benefits
Show more
Show less","Data Analysis, Finance, Procurement, Strategic Sourcing, Data Mining, Data Manipulation, Data Visualization, Power BI, Excel, Pivot Tables, VBA, H/VLOOKUP, COUNTIFs, Index Match, MySQL Dynamics","data analysis, finance, procurement, strategic sourcing, data mining, data manipulation, data visualization, power bi, excel, pivot tables, vba, hvlookup, countifs, index match, mysql dynamics","countifs, data manipulation, data mining, dataanalytics, excel, finance, hvlookup, index match, mysql dynamics, pivot tables, powerbi, procurement, strategic sourcing, vba, visualization"
Lead Data Engineer,"Dynatron Software, Inc.","Richardson, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-dynatron-software-inc-3768749551,2023-12-17,Corinth,United States,Mid senior,Remote,"Lead Data Engineer
100% Remote | Full Time
We are seeking a dynamic and experienced Lead Data Engineer to join our newly formed Data Platform Team. As a Lead Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing big data pipelines in Python and DBT transformations in SQL. The ideal candidate will have a strong technical background, a passion for data processing, and the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and DBT data models on Snowflake.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test, and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and adherence to standards.
Assist in database design and the development of our Snowflake data warehouse.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or a related field.
5+ years of experience in designing, implementing, and maintaining relational/data warehousing environments.
Strong proficiency in programming languages commonly used in data engineering, such as Python, SQL, and big data technologies like Snowflake, Spark, Kafka, and distributed computing frameworks.
Experience working with data warehouses and relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, Beam, Flink, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience working on a project that involved multi-classification
Experience with hybrid cloud workflows
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, work from home and flexible scheduling depending on job requirements, professional development opportunities, 9 paid holidays, and 15 days PTO.
Home office setup support for remote employees.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation:
$130,000 - $160,000
Show more
Show less","Python, DBT, SQL, Snowflake, Data pipelines, AWS, Apache Spark, Data cleaning, ETL, Hadoop, Kafka, Beam, Flink, Distributed computing, Data warehouses, Relational databases, Multiclassification, Hybrid cloud, Software design, Software implementation, Software maintenance, Data processing, Debugging, Troubleshooting, Code review, Requirements gathering","python, dbt, sql, snowflake, data pipelines, aws, apache spark, data cleaning, etl, hadoop, kafka, beam, flink, distributed computing, data warehouses, relational databases, multiclassification, hybrid cloud, software design, software implementation, software maintenance, data processing, debugging, troubleshooting, code review, requirements gathering","apache spark, aws, beam, code review, data cleaning, data processing, data warehouses, datapipeline, dbt, debugging, distributed computing, etl, flink, hadoop, hybrid cloud, kafka, multiclassification, python, relational databases, requirements gathering, snowflake, software design, software implementation, software maintenance, sql, troubleshooting"
Senior Civil Engineer - Data Center (Remote),RemoteWorker US,"Plano, TX",https://www.linkedin.com/jobs/view/senior-civil-engineer-data-center-remote-at-remoteworker-us-3788704110,2023-12-17,Corinth,United States,Mid senior,Remote,"Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Show more
Show less","Civil engineering, Data center design, Project management, Design engineering, Planning and design documents, Design calculations, Team and client standards, Quality assurance, Quality control, Complex projects, Coordination with Olsson teams, Professional staff, Technical staff, Clients, Consultants, Observation, Client meetings","civil engineering, data center design, project management, design engineering, planning and design documents, design calculations, team and client standards, quality assurance, quality control, complex projects, coordination with olsson teams, professional staff, technical staff, clients, consultants, observation, client meetings","civil engineering, client meetings, clients, complex projects, consultants, coordination with olsson teams, data center design, design calculations, design engineering, observation, planning and design documents, professional staff, project management, quality assurance, quality control, team and client standards, technical staff"
Senior Civil Engineer - Data Center (Remote),Olsson,"Plano, TX",https://www.linkedin.com/jobs/view/senior-civil-engineer-data-center-remote-at-olsson-3784205418,2023-12-17,Corinth,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil Engineering, Project Management, Design Engineering, Planning Documents, Design Calculations, Team Standards, Client Standards, Quality Assurance, Quality Control, Client Coordination, Technical Staff, Consultants, Civil 3D, Proficient Software","civil engineering, project management, design engineering, planning documents, design calculations, team standards, client standards, quality assurance, quality control, client coordination, technical staff, consultants, civil 3d, proficient software","civil 3d, civil engineering, client coordination, client standards, consultants, design calculations, design engineering, planning documents, proficient software, project management, quality assurance, quality control, team standards, technical staff"
Experienced Civil Engineer - Data Center (Remote),Olsson,"Plano, TX",https://www.linkedin.com/jobs/view/experienced-civil-engineer-data-center-remote-at-olsson-3784206328,2023-12-17,Corinth,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil 3D, Civil Engineering, Engineering, AutoCAD","civil 3d, civil engineering, engineering, autocad","autocad, civil 3d, civil engineering, engineering"
Discipline Director Design Management - Data Centers (DTC),Exyte Management GmbH-Recruiter,"Plano, TX",https://www.linkedin.com/jobs/view/discipline-director-design-management-data-centers-dtc-at-exyte-management-gmbh-recruiter-3762679585,2023-12-17,Corinth,United States,Mid senior,Remote,"Discover your exciting role
The Discipline Director - Design Management - Data Centers (DTC) is responsible for managing staff within the Design Management and Coordination department. Manages career development for discipline staff while aligning capabilities to meet current and future market needs. Ensures the team meets task schedules, produces quality deliverables, and respects project budgets and earned value projections. Cooperates with and supports Project Management in achieving contracted deliverables and target profitability margins on projects. Manages the discipline budget.
Explore your tasks and responsibilities
Adopts and promotes the Exyte commitment to always provide a safe workplace that strives for an Incident and Injury Free environment where everyone goes home safe every day
Ensures company safety policies and the Site Specific Plan are proactively communicated and consistently enforced on all projects
Attracts and retains premier talent through recruiting, mentoring, training and other employee development activities
Encourages and promotes an atmosphere of collaboration, creativity, ingenuity, progress, efficiency, and enthusiasm within the respective Discipline, including contributing to professional association workgroups, industry forums, and technical seminars and conferences
Functions as a senior level client liaison, soliciting feedback from clients regarding team performance, potential projects, and other support needed. Through this process, establish strong client relationships in order to position Exyte as the preferred partner for key clients
Collaborates with Discipline Directors, Project Managers and Construction Managers to develop and integrate processes and procedures (Project Execution Systems) including Quality Management and Continuous Improvement Programs
Supports development and effective use of Project Controls including budget and cost analysis, project scheduling, use of the Fee Development Guide (FDG), and Earned Value Analysis
Accountable to the Director Design Operations for the Discipline
Responsible for managing a multi office organization
Responsible for providing a balance of support, guidance, empowerment and accountability of the department’s staff
Manages annual Forecast projections, financial performance assessment, and ongoing staffing and utilization forecasts
Establishes and maintains a succession strategy for key positions including the Director in the discipline
If a registered Professional Engineer, the Discipline Director is expected to serve as the Company’s Engineer in Responsible Charge or qualifier (for purposes of acquiring and maintaining corporate licenses) in all jurisdictions in which he/she maintains a professional registration. The Discipline Director may also be required, as business needs dictate, to seek licensure in additional jurisdiction
Other duties as assigned
Show your expertise
Bachelor’s Degree in Engineering or related field preferred
Professional Engineering (PE) License required
Project Management Professional (PMP) certification preferred
7+ years of experience in Data Centers (DTC)
4+ years of Management experience
Prior experience in Engineering Project Delivery Leadership role, including management
Experience in the role of Project Discipline Lead
Prior experience in roles with primary function as a Project Engineer/Architect as well as a project lead for design or design-build project(s) for high-tech manufacturing or “clean” facilities, scope including all typical facility design disciplines (CSA, MEP, etc.)
Experience of minimum of two projects with at least $100M in COW for Engineering Design services
Experience working directly with clients and internal team leads for active change management through all phases of project development
Experience in leading proposal development
Proven examples of at least two direct reports growing into new roles
Experience should include examples of promoting a strong Safety Culture, including incorporation of Safety in Design
Ability to inspire, direct and manage others
Ability to manage a multi office organization
Able to manage discipline specific consultants as required by projects; developing master service agreements and scopes of work for project contracts
Demonstrated ability managing at least five direct reports
Strong Communication skills and inclusion of all Stakeholders in decision process
Strong problem-solving and organizational skills
Excellent general understanding of related engineering disciplines such as mechanical, electrical, process systems, automation, structural and architectural
Show more
Show less","Safety policies, Project execution systems, Quality management, Continuous improvement programs, Project controls, Budget analysis, Cost analysis, Project scheduling, Earned value analysis, Succession planning, Engineering design, Change management, Proposal development, Safety culture, Project management, Stakeholder management, Problemsolving, Organizational skills, Communication skills, Engineering disciplines","safety policies, project execution systems, quality management, continuous improvement programs, project controls, budget analysis, cost analysis, project scheduling, earned value analysis, succession planning, engineering design, change management, proposal development, safety culture, project management, stakeholder management, problemsolving, organizational skills, communication skills, engineering disciplines","budget analysis, change management, communication skills, continuous improvement programs, cost analysis, earned value analysis, engineering design, engineering disciplines, organizational skills, problemsolving, project controls, project execution systems, project management, project scheduling, proposal development, quality management, safety culture, safety policies, stakeholder management, succession planning"
Looking for Database Engineer - Plano TX/ Mclean VA - C2C,Extend Information Systems Inc.,"Plano, TX",https://www.linkedin.com/jobs/view/looking-for-database-engineer-plano-tx-mclean-va-c2c-at-extend-information-systems-inc-3673991291,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Hi,
I hope you are doing well!
We have an opportunity for
Database Engineer
with one of our clients for
Plano TX/ Mclean VA
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Database Engineer
Location:
Plano TX/ Mclean VA
Terms:
C2C
Job Details
Manage SQL Server databases
Configure and maintain database servers and processes
Monitor system's health and performance
Ensure high levels of performance, availability, sustainability and security
Analyze, solve, and correct issues in real time
Provide suggestions for solutions
Refine and automate regular processes, track issues, and document changes
Assist developers with query tuning and schema refinement
Perform scheduled maintenance and support release deployment activities after hours
Qualifications
5 years of experience as a SQL Server DBA/Sybase/DB2 or similar role
4 years of experience with SQL Server DBA/Sybase/DB2 Administration experience required
Critical thinker and problem-solving skills
Thanks & Regards
Monika Singh
Extend Information System Inc
Phone:571-622-3980
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","SQL Server, Database Administration, System Health Monitoring, Performance Analysis, Problem Solving, Query Tuning, Schema Refinement, Release Deployment, Critical Thinking","sql server, database administration, system health monitoring, performance analysis, problem solving, query tuning, schema refinement, release deployment, critical thinking","critical thinking, database administration, performance analysis, problem solving, query tuning, release deployment, schema refinement, sql server, system health monitoring"
Sr. Big Data/ Hadoop Engineer (Plano),Experis,"Plano, TX",https://www.linkedin.com/jobs/view/sr-big-data-hadoop-engineer-plano-at-experis-3779917482,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Experis is a part of the Manpower Group family of brands. ManpowerGroup has partnered with a leading Financial Services organization in the
Plano, TX
area for a
Sr. Big Data/ Hadoop Engineer
contract position to assist their team.
Job
Title
:
Sr. Big Data/ Hadoop Engineer
Location
:
Plano, TX
Contract
: 12 to 18 Months
Pay Rate Range
: $63 to $65
Client
: Financial Client
Position Summary
The data solution delivery team is looking for an experienced Senior Hadoop Developer. Candidate should have in-depth knowledge of Hadoop technologies, SQL, and ETL processes. Candidate should possess excellent analytical capabilities, get a deep understanding of the data, and build co-relation between various data elements/objects. The data solution delivery team is also responsible for ingesting the data from multiple sources, The candidate should be able to support the ingestion process using big data tools and frameworks.
Primary Skills
Hadoop
Secondary Skills
Python
Tertiary Skills
UNIX/SHELL SCRIPTS
Required Skills
Bachelor's degree in a technical or business-related field or equivalent working experience.
4+ years of experience in data warehousing architectural approaches.
Minimum of 4 years in big data.
(Cloudera) Sound understanding and experience with the Hadoop ecosystem (Cloudera). Able to understand and explore the constantly evolving tools within the Hadoop ecosystem and apply them appropriately to the relevant problems at hand.
Experience in working with Big Data implementation in a production environment.
Must have experience with Big Data technologies like Hadoop, Hive, Spark, Python, Scala, etc. Experience in Python and Unix shell scripting. Experience in scheduling tools like Autosys Understanding of Agile methodologies and technologies Sound knowledge of relational databases (SQL) and experience with large SQL-based systems. Exposure to and strong working knowledge of distributed systems.
Excellent understanding of client-service models and customer orientation in service delivery. Ability to grasp the 'big picture' for a solution by considering all potential options in the impacted area. Aptitude to understand and adapt to newer technologies. The ability to work with teammates in a collaborative manner to achieve a mission. Experience in query optimization, and performance tuning of complex SQL queries. Benchmark and debug critical issues with algorithms and software as they arise.
Frequently Asked Questions
Benefits? Yes, you can enroll in our corporate discount benefits package.
Remote Workers? Hybrid with 3 days onsite and 2 days remote in a week.
Visa Sponsorship? Not offered
Corp-to-Corp? Not eligible
How To Apply
For immediate attention please apply online and send an email
About Experis
https://www.experis.com/
Experis is part of the ManpowerGroup family of brands. ManpowerGroup is the world leader in innovative workforce solutions, connecting human potential to the power of business. ManpowerGroup serves both large and small organizations across all industry sectors through our brands and offerings: Manpower, Experis, Talent Solutions, and Jefferson Wells.
Show more
Show less","Hadoop, SQL, ETL, Python, UNIX/SHELL SCRIPTS, Cloudera, Spark, Scala, Autosys, Agile, Distributed systems, SQL, Performance tuning, Data warehousing, Data processing, Data ingestion, Big data tools and frameworks, Data analytics, Data solution delivery","hadoop, sql, etl, python, unixshell scripts, cloudera, spark, scala, autosys, agile, distributed systems, sql, performance tuning, data warehousing, data processing, data ingestion, big data tools and frameworks, data analytics, data solution delivery","agile, autosys, big data tools and frameworks, cloudera, data ingestion, data processing, data solution delivery, dataanalytics, datawarehouse, distributed systems, etl, hadoop, performance tuning, python, scala, spark, sql, unixshell scripts"
"Senior Data Engineer // Dallas, TX",Motion Recruitment,"Irving, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-dallas-tx-at-motion-recruitment-3767070916,2023-12-17,Corinth,United States,Mid senior,Hybrid,"We are searching for a Senior Data Engineer for our client. This opportunity is local to Irving, TX. Ideally, they prefer this candidate go in 5-days per-week; however, for the right candidate hybrid could be considered. This is a full-time direct-hire opportunity.
As for what the client does, they are an innovative industry leader within automotive restoration. They collaborate with well-known partners within both land and marine industries (sports, leisure, etc.). This company cares about their employees and offers stellar growth opportunities within.
Required Skills & Experience
Lengthy experience with ETL
Strong abilities related to Python
Usage of Scala in recent positions
Confident in SQL abilities
Desired Skills & Experience
Data Warehousing
Data Modeling and SQL Querying
Database architecture
Azure- Databricks
PowerBI
What You Will Be Doing
Tech Breakdown
100% Data Engineering
Daily Responsibilities
80% Hands-On
10% Mentorship
10% Team Collaboration
The Offer
Bonus eligible
You Will Receive The Following Benefits
Medical, Dental, and Vision Insurance
PTO
Stock Options
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Katherine Spalding
Show more
Show less","ETL, Python, Scala, SQL, Data Warehousing, Data Modeling, SQL Querying, Database architecture, Azure, Databricks, PowerBI","etl, python, scala, sql, data warehousing, data modeling, sql querying, database architecture, azure, databricks, powerbi","azure, database architecture, databricks, datamodeling, datawarehouse, etl, powerbi, python, scala, sql, sql querying"
Sr. Data Engineer,Diverse Lynx,"Plano, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-diverse-lynx-3764419990,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Technical/Functional Skills
Primary – Cloud Based ETL Tool, Snowflake (Cloud DB), Snaplogic , DBT, Strong SQL, Unix/Python, Control-M
Experience Required
10&plus; Years
Roles & Responsibilities
Role Description
Analyze requirements and existing resources to Propose, create ETL designs and database objects
Work with project and business analyst leads in order to develop and clarify in-depth technical requirements including logical and physical data modeling activities
Design and implement ETL processes for data transactions related to Enterprise Data Warehouse, Operational Data Store (ODS), and other data structures to support our Business Intelligence operations
Develops, enhances, debugs, supports, maintains and tests software applications that support business units or supporting functions using IBM Infosphere Data Stage ETL or any other cloud based ETL tool both ETL and ELT approaches. These application program solutions may involve diverse development platforms, software, hardware, technologies and tools.
Must have hands-on on Snowflake development environment with all SQL operations. Must be aware of ELT approach as well.
Participates in the design, development and implementation of complex applications, often using IBM Infosphere Information Server (IIS) products like Data Stage, Quality Stage on a Linux Grid environment. Control-M/Scheduling tools.
Required Skills:10&plus; Yrs Relevant IT software experience (Technical) in ETL Datastage or any other cloud based ETL Tool development Experience with databases like Snowflake (Cloud DB), Oracle, Netezza, MS SQL Server 2012&plus;, DB2 and MS Access
Experience with job automation & scheduling software (Control-M) Strong ability to write SQL queries
Desired Skills:
Familiar with Java API’s, Snowflake (Cloud DB), Snaplogic,
Python, UNIX, Windows, File transfer utilities, process flow creation, ETL technologies, Hadoop
Good to have Skills: Java Springboot, Snow-Pro Certified, Snaplogic, Strong SQL, Strong conceptual understanding of core DW Concepts including different approaches/methodologies. dbt (data build tool) experience is an added advantage
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","CloudBased ETL Tool, Snowflake, Snaplogic, DBT, SQL, Unix, Python, ControlM, Data Stage, Linux Grid, Java API's, Hadoop, Java Springboot, SnowPro Certified, dbt","cloudbased etl tool, snowflake, snaplogic, dbt, sql, unix, python, controlm, data stage, linux grid, java apis, hadoop, java springboot, snowpro certified, dbt","cloudbased etl tool, controlm, data stage, dbt, hadoop, java apis, java springboot, linux grid, python, snaplogic, snowflake, snowpro certified, sql, unix"
Principal Data Engineer,hackajob,"Plano, TX",https://www.linkedin.com/jobs/view/principal-data-engineer-at-hackajob-3776974501,2023-12-17,Corinth,United States,Mid senior,Hybrid,"hackajob has partnered with a disruptive automotive business that puts innovation at the heart of its new digital products and works with an unrivaled amount of automotive consumer data.
Role
: Principal Data Engineer, MDM
Location
: Plano, TX
Work model:
hybrid
Qualifications and Requirements
At least eight years of hands on ETL development experience
At least four years of hands-on experience developing in Azure Data Factory, Databricks Python and Spark
At least 2 years leading a technical team, providing leadership, design documentation and diagrams
At least 2 years of hands-on experience working with configuration of Azure subscriptions, resource groups, resources and provisioning
At least 2 years leading the end-to-end design and development of ETL integrations to be consumed by the enterprise, including monitoring and production support
Experience with data driven initiatives, with background in MDM or other data management and governance platforms
Proven ability to learn and fully understand new technology, becoming the SME for the platforms and services your team interacts with
Solid understanding of DevOps capabilities such as automated testing, continuous integration, and continuous delivery
Experience working with a Master Data Management platform tool (pref. Reltio)
Proven track record of exhibiting strong critical thinking by analyzing facts in order to understand a business request or requirement thoroughly.
Proven ability to mentor and develop others. Experience positively influencing team norms, culture, and technical vision.
Experience with agile methodologies
Very strong communication both written and verbal
If you're interested in finding out more about this fantastic opportunity please get your application in and we can arrange a call.
hackajob is a recruitment platform that will match you with relevant roles based on your preferences and in order to be matched with the roles you need to create an account with us.
*This role requires you to be based in the US*
Show more
Show less","Data Engineering, ETL Development, Azure Data Factory, Databricks Python, Spark, Azure Subscriptions, Azure Resource Groups, Azure Resources, DevOps, Automated Testing, Continuous Integration, Continuous Delivery, Master Data Management, Reltio, Agile Methodologies, Communication Skills","data engineering, etl development, azure data factory, databricks python, spark, azure subscriptions, azure resource groups, azure resources, devops, automated testing, continuous integration, continuous delivery, master data management, reltio, agile methodologies, communication skills","agile methodologies, automated testing, azure data factory, azure resource groups, azure resources, azure subscriptions, communication skills, continuous delivery, continuous integration, data engineering, databricks python, devops, etl development, master data management, reltio, spark"
Data Architect / BI Developer,Stellent IT,"Coppell, TX",https://www.linkedin.com/jobs/view/data-architect-bi-developer-at-stellent-it-3692571731,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Job Title -Data Architect/ PowerBI Developer
Location - Coppell, TX (4 days a week onsite, 1 day remote)
Phone + Skype
Visa - Green Card Holder or US Citizen
Job Description
Manager Notes:
Architect for data enterprise operations
Need experience with SQL Server database (SSAS, SSIS) and Power BI
Must understand how system consumes data and put in cubes and schedule it to run and then reporting on the data environment
Very hands on technically with data architecture and analytics
Job Summary
The Director of Enterprise Systems
will be responsible for leading the
Enterprise Applications
team overseeing Smoothie King's enterprise solutions and data management environment across the organization. The Director of Enterprise Systems will assist in planning, implementing, and managing the data warehouse, analytics and reporting platforms as well as related support activities.
Essential Functions / Major Responsibilities
Provide leadership and expertise in managing Information systems across the enterprise, including but not limited to: Data Warehouse, Corporate Systems, and custom applications.
Set The Strategy, Roadmap, And Key Milestones To Operationalize a Vision Which Delivers Business Value By Innovative Use Of Data, Including
Data architecture.
Data integration and analytics solutions and services.
Analytics platforms, tools, architecture, and engineering.
Management of data and analytics products.
Data stewardship, data analytics and enablement, data science, and data/AI operations.
Own strategic decisions for our data analytics infrastructure such as analytics tool selection and design.
Manage the overall management and upkeep of a Company's Enterprise systems, including installing and configuring software, applying security patches, and monitoring system performance.
Oversee the end-to-end integration of software components and systems to support the effective and efficient delivery of data services across the organization.
Establish and maintain standards for analysis, design, coding, testing, and documentation of the Data Warehouse and Enterprise Applications.
Participate in projects to integrate new technologies and upgrades of existing technologies including installing, configuring, and testing new hardware and software. Monitor and report on the status of projects.
Recommend processes for requesting and prioritizing requests for data requests and application development projects.
Participates in cross-functional, cross-discipline project teams to better leverage the existing systems.
Coordinates information and data changes that occur as part of system lifecycle to ensure consistency.
Recognizes and identifies potential areas where existing policies and procedures that require change, or where new ones need to be developed.
Supervise, lead, hire, coach, and evaluate performance of staff has one direct report
Required Skills / Abilities / Competencies
Ability to work independently on multiple assignments and to work collaboratively within internal and cross-functional teams.
Ability to adapt to new technology platforms and services.
Demonstrates experience in delivering successful outcomes.
Demonstrates experience in setting priorities and performance management measures.
Demonstrates Experience With The Following
Infrastructure as a Service (Azure/AWS).
Data as a Service.
Storage as a Service.
Containerization.
Strong experience with Microsoft SQL and database architecture, database administration principles, best practices, and the uses of information technology to support organizational requirements.
Managing Enterprise Infrastructure Systems in a Hybrid Cloud environment between on-premises data centers, AWS, and Azure.
Experience developing custom, Python, R, and Microsoft PowerShell scripts.
Knowledge of IT operation processes including asset management, project management, configuration management, incident management and change management.
A working knowledge of TCP/IP, Network layers and routing, DNS, DHCP, Active Directory, LDAP, information security; including system hardening, log analysis, intrusion detection, and vulnerability scanning.
Promote the culture, values, and mission of Smoothie King.
Education And Experience
Bachelor's degree in Computer Science, or related field.
Ten (10) + years IT Experience; or equivalent combination of education and experience.
Seven (7) + years of demonstrated hands-on experience with the planning, delivery and management of enterprise information systems including Data Warehouse.
Five (5)+ years of supervisor experience leading internal and external teams.
Five (5)+ years of experience planning and managing systems in a high availability, fast-paced environment.
Working knowledge of the design, installation, and support of Data Warehouse models, tools, and analytics platforms.
Cloud experience with AWS, Azure, O365 and other Microsoft stack tools.
Show more
Show less","Data Architecture, Power BI, SQL Server, SSAS, SSIS, Data Management, Analytics, Data Warehousing, Data Integration, Data Science, Data Analytics, Data Stewardship, Data Enablement, Data/AI Operations, Python, R, Microsoft PowerShell, Azure, AWS, Microsoft SQL, Database Administration, Information Technology, Hybrid Cloud, Active Directory, LDAP, Information Security, System Hardening, Log Analysis, Intrusion Detection, Vulnerability Scanning, TCP/IP, Network Layers, Routing, DNS, DHCP","data architecture, power bi, sql server, ssas, ssis, data management, analytics, data warehousing, data integration, data science, data analytics, data stewardship, data enablement, dataai operations, python, r, microsoft powershell, azure, aws, microsoft sql, database administration, information technology, hybrid cloud, active directory, ldap, information security, system hardening, log analysis, intrusion detection, vulnerability scanning, tcpip, network layers, routing, dns, dhcp","active directory, analytics, aws, azure, data architecture, data enablement, data integration, data management, data science, data stewardship, dataai operations, dataanalytics, database administration, datawarehouse, dhcp, dns, hybrid cloud, information security, information technology, intrusion detection, ldap, log analysis, microsoft powershell, microsoft sql, network layers, powerbi, python, r, routing, sql server, ssas, ssis, system hardening, tcpip, vulnerability scanning"
Cloud Data Engineer/Developer,Talener,"Plano, TX",https://www.linkedin.com/jobs/view/cloud-data-engineer-developer-at-talener-3757621969,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Our client is a health-tec firm that provides a platform for claims management and an online pharmacy. They are looking to hire a Cloud Data Engineer/Developer that is strong with Azure, Synapse and data lakes.
Title:
Cloud Data Engineer/Developer
Location: Plano. TX
Required Skills And Responsibilities
Manage all aspects of database design, including implementation, maintenance, security, audits, performance, and the ability to generate reports
Ability to consolidate date to Synapse
Design, architect, and build databases from business requirements
Manage and develop processes for Disaster recovery and HA with DevOps
Generate traces, execution plans, performance issues, deadlocks, and remediate
Define and implement storage, replication, partitioning, and archiving procedures
Monitoring DB system performance and delivering reports to management
Set up and database infrastructure supporting Analytics / BI processes
4+ years of product or project development experience
Nice To Have Skills
Knowledge of HIPAA and SOC2
DB tools- Azure SQL DB,Azure SQL Managed Instance, ADF)
Experience with ElasticSearch, Redshift, Snowflake, PostgreSQL
Azure cloud technology stack (ARM, Bicep)
Terraform Cloud
Compensation
$120,000-150,000
Annual bonus
For additional information, please reach out to Jed Pillion at jpillion@talener.com
Show more
Show less","Azure, Synapse, Data lakes, Database design, Data consolidation, Business requirements, Disaster recovery, DevOps, Performance tuning, Storage procedures, Replication, Partitioning, Archiving, Database infrastructure, Analytics, BI processes, HIPAA, SOC2, Azure SQL DB, Azure SQL Managed Instance, Azure Data Factory (ADF), ElasticSearch, Redshift, Snowflake, PostgreSQL, Azure cloud technology stack, ARM, Bicep, Terraform Cloud","azure, synapse, data lakes, database design, data consolidation, business requirements, disaster recovery, devops, performance tuning, storage procedures, replication, partitioning, archiving, database infrastructure, analytics, bi processes, hipaa, soc2, azure sql db, azure sql managed instance, azure data factory adf, elasticsearch, redshift, snowflake, postgresql, azure cloud technology stack, arm, bicep, terraform cloud","analytics, archiving, arm, azure, azure cloud technology stack, azure data factory adf, azure sql db, azure sql managed instance, bi processes, bicep, business requirements, data consolidation, data lakes, database design, database infrastructure, devops, disaster recovery, elasticsearch, hipaa, partitioning, performance tuning, postgresql, redshift, replication, snowflake, soc2, storage procedures, synapse, terraform cloud"
Data Engineer,Brooksource,"Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-at-brooksource-3757803281,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Data Engineer
Plano, TX
Hybrid (onsite Monday - Wednesday)
As the Data Engineer for a Fortune 50 company, you will be responsible for day-to-day data collection, transportation, maintenance and access to enterprise data. You will be a member of a team of data engineers who build data pipelines into various source systems, rest data on the data lake and enable exploration and access for analytics, visualization, machine learning and product development efforts across the company.
MINIMUM QUALIFICATIONS
QUALIFICATIONS:
BA/BS in Computer Science, Math, Physics, or other technical fields.
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus. (AWS will work)
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
RESPONSIBILITES:
Active contributor to code development in projects and services
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products
Build and own the automation and monitoring frameworks that capture metrics and operational KPIs for data pipeline quality and performance
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Responsible for implementing best practices around systems integration, security, performance and data management
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape
WHAT’S IN IT FOR YOU…?
Health benefits, 401K Plan, Weekly Pay Checks, Onsite cafeteria
Opportunity to work with a Fortune 400, international retail brand
Opportunity to diversify skillset and learn new technologies support several different retail applications
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","Data Engineering, Data Collection, Data Transportation, Data Maintenance, Data Access, Data Warehousing, Data Analytics, Data Modeling, Data Quality, ETL/ELT Pipelines, Data Profiling, MPP Databases, Redshift, Synapse, SnowFlake, Cloud Infrastructure, Kubernetes, Version Control Systems, Azure Data Factory, Azure Databricks, Azure Machine Learning, Statistical/ML Techniques, Metadata Management, Data Lineage, Data Glossaries, Agile Development, DevOps, DataOps, Business Intelligence Tools, PowerBI, Python, PySpark, Scala, SQL, Azure, AWS, Apache Griffin, Deequ, Great Expectations","data engineering, data collection, data transportation, data maintenance, data access, data warehousing, data analytics, data modeling, data quality, etlelt pipelines, data profiling, mpp databases, redshift, synapse, snowflake, cloud infrastructure, kubernetes, version control systems, azure data factory, azure databricks, azure machine learning, statisticalml techniques, metadata management, data lineage, data glossaries, agile development, devops, dataops, business intelligence tools, powerbi, python, pyspark, scala, sql, azure, aws, apache griffin, deequ, great expectations","agile development, apache griffin, aws, azure, azure data factory, azure databricks, azure machine learning, business intelligence tools, cloud infrastructure, data access, data collection, data engineering, data glossaries, data lineage, data maintenance, data profiling, data quality, data transportation, dataanalytics, datamodeling, dataops, datawarehouse, deequ, devops, etlelt pipelines, great expectations, kubernetes, metadata management, mpp databases, powerbi, python, redshift, scala, snowflake, spark, sql, statisticalml techniques, synapse, version control systems"
Sr. Data Engineer II,Pizza Hut,"Plano, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-ii-at-pizza-hut-3760695074,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Job Description
We are seeking an experienced Analytics Data Engineer to join our Pizza Hut US team. This person will be responsible for taking ownership of existing data and analytics builds, automating new data and analytics jobs, and documenting processes in collaboration with, and in support of, teams that build data driven insights that contribute to Pizza Hut’s growth. While this role will operate in the background, this person will play a critical part in shaping our business strategy on key initiatives, ensuring we deliver the best pizza experience to our customers. The work environment is highly collaborative in a semi-agile setting with cross-functional teammates. The selected candidate will be assigned to the analytics team, but will work closely with data engineering to strictly follow best practices that team defines, especially for changes made to production tables and systems, and for transitioning analytics jobs over to the data engineering team's ownership.
Responsibilities
Support Data Analysis and Reporting:
Support internal clients that analyze large datasets to extract meaningful insights and trends.
Develop and maintain data and analytics processes that feed dashboards, reports, and visualizations that relay trends, results, and outcomes to stakeholders and senior leadership.
Perform exploratory and investigative analysis to identify and quantify opportunities across various aspects of our business that can be measured with data.
Be an expert on our data used for analytical purposes.
Take ownership of existing automated processes and data products.
Set up data feeds to 3rd party vendors and partners.
QA And Data Evolution
Design and test processes prior to promoting to production.
Troubleshoot and fix automation errors.
Provide recommendations for column, or table design enhancements, or calculations, or attributes that would optimize analytics efficiency or improve data quality.
Be creative and innovative in building solutions.
Cross-Functional Collaboration
Work closely with cross-functional teams to understand business goals and objectives.
Effectively influence, champion, and drive for results through prioritizing data cleansing and standardizing of production tables and views, assisting in the hand off of automated processes/data builds to the data engineering team, and setting up new automations to support the analytics team and dashboard development.
Provide analytics support to various departments and assist in solving complex business challenges through set up/automation, and maintenance of, analytics processes.
Identify processes that are good candidates to transition ownership to the data engineering team. Refactor or port over anything necessary to another type of platform or language to promote to the production environment owned by the data engineering team.
Data Management
Ensure data quality and accuracy by collaborating with the data engineering team.
Stay up-to-date with industry best practices and technologies related to analytics and data management.
Factors of Success
Required
6+ years of experience in data engineering.
2+ years of experience in data analytics.
Proficiency in data engineering and governance languages and tools such as Airflow, SQL, and Alation.
Proficiency in data analysis languages and tools such as SQL, Python, R, Alteryx, and dbt.
Proficiency with code repositories and file version systems such as Git.
Data visualization skills using tools like Tableau, Power BI, Domo, or similar.
Written and verbal communication skills to convey complex findings to non-technical stakeholders.
Strong problem-solving abilities and a results-oriented mindset.
Stakeholder management and partnership capabilities.
Visionary thinker, with the ability to see the bigger picture and ability to design data and systems for where the company should go.
Proficiency in documentation, including:
Diagramming data processes and table relationships
Documenting within code
Creating and updating project documentation
Creating data dictionaries
And creating documentation for data and processes for the broader enterprise to leverage
Ability to translate intricate details into understandable explanations.
Technically astute, with a strong grounding in SQL and the conversion of business requirements to practical data and analytics design.
Ability to think and work in terms of customer-level analytics.
Preferred
QSR industry experience in data engineering and analytics
Experience with data modeling
Salary Range: $­­125,000 to $135,000 annually + bonus eligibility.
This is the expected salary range for this position. Ultimately, in determining pay, we'll consider the successful candidate’s location, experience, and other job-related factors.
About Us
Who We Are
Founded in 1958, Pizza Hut - a subsidiary of Yum! Brands, Inc. - now operates more than 18,000 restaurants in more than 100 countries. Pizza Hut is leading the way in providing customers with great experiences, innovating with technology and new products, as well as delivering exceptional service.
Our People & Culture
We're looking for people who LOVE pizza and thrive in a fun, past paced, and customer-centric environment. At our corporate campuses, Pizza Hut has created the perfect place for you to grow your career. Every day, you’ll work to support our franchisees and teams across the U.S., continuously challenging yourself to feed more possibilities. In return, we’ll provide professional development and career growth opportunities so that you can become your best and achieve your goals. And we’ll sweeten the deal by immersing you in our world-class recognition culture and providing a robust array of benefits, some highlights include:
4 weeks PTO, plus standard holidays and time off to volunteer
Generous parental leave (16 weeks for moms, 6 weeks for dads)
401(k) with 6% match, vested immediately
On-site daycare
24/7 fitness center with laundry services
Half-day Fridays, year round
Giving Back
As a global company, Pizza Hut aims to make the world better by acting responsibly with respect to food, planet and people. Whether it’s donating food through the Harvest Program or supporting literacy with the Pizza Hut BOOK IT! Program – the company, our franchisees and our team members are committed to improving the communities we serve.
Pizza Hut is an equal opportunity workplace and committed to fostering an inclusive, diverse culture . All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, sexual orientation, or gender identity), national origin, age, disability and genetic information (including family medical history).
Show more
Show less","Airflow, SQL, Alation, Python, R, Alteryx, dbt, Git, Tableau, Power BI, Domo, Data visualization, Data engineering, Data analytics, Data management, Data modeling, Business intelligence, Data governance, Data quality assurance, Data integration, Data mining, Machine learning, Natural language processing","airflow, sql, alation, python, r, alteryx, dbt, git, tableau, power bi, domo, data visualization, data engineering, data analytics, data management, data modeling, business intelligence, data governance, data quality assurance, data integration, data mining, machine learning, natural language processing","airflow, alation, alteryx, business intelligence, data engineering, data governance, data integration, data management, data mining, data quality assurance, dataanalytics, datamodeling, dbt, domo, git, machine learning, natural language processing, powerbi, python, r, sql, tableau, visualization"
Data Center Engineer,Mitchell Martin Inc.,"Grapevine, TX",https://www.linkedin.com/jobs/view/data-center-engineer-at-mitchell-martin-inc-3762681379,2023-12-17,Corinth,United States,Mid senior,Hybrid,"Our direct hiring manager is looking for a DataCenter Engineer who has Hvac and racking /Stacking experience. Please review the details below and let me know if you are qualified and interested.
Primary point of contact for Data Center technologies and related mechanical devices. Participates in projects to design and implement mechanical and electrical improvements to data centers. Develops procedures to outline data center processes, and related tasks. Participates and leads in the design and engineering data center technologies and future datacenter related initiatives. Mentors Data Center Operations team members and helps increase their effectiveness and skillset.
HVAC and Racking/Stacking. Have to have both (Electrical and Network Knowledge)
Hands on data center
Must be flexible with work schedule and hours
Bachelors degree and 4 years related experience
Knowledge of:
Datacenter HVAC, CRAC, CRAH cooling technologies
Datacenter power, UPS, generator operations, etc
Datacenter fire suppression systems (FM200, HFC125, etc)
General knowledge of datacenter design and construction
LAN/WAN
Emerging Technology Trends
Datacenter Best Practice and Industry Standard Methodology
Skills/Abilities:
Evaluate critical systems, prioritize workflow and determine solutions
Excellent written and verbal communication skills
Interpret and apply laws, regulations and policies
Read and understand technical manuals
Work for extended time at keyboard/terminal
Maintain effective working relationships with supervisor and coworkers
Work flexible hours, including weekends and evenings
Value open, honest communication
Eager to be transparent and truthful as a primary matter of course
Show more
Show less","HVAC, Racking, Stacking, Electrical, Network, Datacenter, CRAC, CRAH, UPS, Generator, Fire suppression systems, LAN, WAN, Industry Standard Methodology, Communication, Interpretation","hvac, racking, stacking, electrical, network, datacenter, crac, crah, ups, generator, fire suppression systems, lan, wan, industry standard methodology, communication, interpretation","communication, crac, crah, datacenter, electrical, fire suppression systems, generator, hvac, industry standard methodology, interpretation, lan, network, racking, stacking, ups, wan"
Distinguished Data Engineer,Verizon,"Irving, TX",https://www.linkedin.com/jobs/view/distinguished-data-engineer-at-verizon-3786568411,2023-12-17,Corinth,United States,Mid senior,Hybrid,"When you join Verizon
Verizon is one of the world's leading providers of technology and communications services, transforming the way we connect around the world. We're a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together-lifting up our communities and striving to make an impact to move the world forward. If you're fueled by purpose, and powered by persistence, explore a career with us. Here, you'll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
As a Distinguished Engineer- Cloud within the Artificial Intelligence and Data Organization (AI&D), you will be strategizing, identifying and driving state-of-the-art technologies for Platform Rationalization and Work with leadership and help align on strategies across the enterprise. You will be defining a blueprint for GCP's adoption and GCP's services and come up with high-level design to implement solutions and Partner with stakeholders to identify real business opportunities and build a roadmap. You will be a good thought leader with innovation in every thought. You are a good team player and have a go-getter kind of attitude with good interpersonal skills. Understanding the Latest happening in Cloud technologies, Generative AI, Datascience, ML and Model Operations area and building good work culture with stakeholders including, Business, Enterprise architects, Data Engineers, Data Scientists. Adhering to the organization priorities and policies. Building external stakeholder communication with Business user and Data ScientistsThe work you'll be doing will be Architecting, Build & support at AI&D platforms, which is going to enhance the experience for Data scientists and ML engineers to build world class solutions at scale.
What you'll be doing...
Performing the Proof of Concepts (POCs) & Proof of Technologies (POTs) with new services/technologies in cloud, on-prem & open source technologies/
Developing hybrid solutions to effectively utilize the GPUs across on-prem & cloud by lowering the operational costs.
Working closely with the Enterprise architects, AI&D Leaders, network & security teams and application teams to build the scalable platforms that meets both business and technical requirements within the budget.
Influencing the stakeholders to get the desired outcome.
Performance GCP Bigquery administration, workload management & BQ tuning.
Providing support for the engineering team on Kubernetes Clusters, Dataproc, Kafka, HDP and CDP cluster related issues.
Identifying the opportunities to automate the platform engineering activities including but not limited to Security compliance, platform remediation & governance.
Creating tools and assets for managing data products, Creating and maintaining documentation for platform configurations, changes, and issues.
Designing and recommending solution frameworks for teams to develop self-serve / auto-heal solutions in GCP & On Prem Hadoop Platform.
Developing and deploying Python based custom solutions using Cloud Functions, Pubsub, BQ etc services in GCP.
Creating GCP infrastructure using Terraform and optimize the custom terraform modules.
Developing ansible scripts and integrate with Terraform as part of application environment provisioning in GCP / On Pem.
Collecting, integrating and Developing platform observability dashboards .
Analyzing and designing SRE solutions on top of existing GCP networks spanning across different GCP projects and on Prem Hadoop.
Resolving user issues for data services in Hadoop, Spark and GCP like dataproc, dataflow, composer, GKE, storage, Compute, BQ, cloud functions to name a few.
Handling production support activities and following the incident management process.
Where you'll be working:
In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
What we're looking for...
You'll need to have:
Bachelor's degree or four or more years of work experience.
Six or more years of relevant work experience.
Even better if you have one or more of the following:
4 or more years of experience as Platform/solution/Enterprise architect.
Deep hands-on experience with Spark, Hive, big data & open source technologies .
DevOps / SRE / Hadoop experience.
2 or more years experience in GCP cloud technologies
Experience in a modern scripting language (preferably Python / shell scripting ) for automation of build tasks. Two or more years Experience with Big Data on GCP - Dataproc, dataflow, Cloud Function, BigQuery, Composer, GKE, IA, etc.
Experience in working with REST APIs and event based Architecture
Experience in distributed processing (Hadoop / Spark ) Framework , resource management and optimization techniques.
Strong analytical and troubleshooting skills.
Generative AI certification & implementation experience.
Experience in optimizing the complex SQLs .
Experience in Git , Jenkins CI/CD, Python and GCP client libraries.
GCP Cloud Architect certification(s) or equivalent public cloud certifications.
If Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every ""even better"" qualification listed above.
Where you'll be working
In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
Scheduled Weekly Hours
40
Equal Employment Opportunity
We're proud to be an equal opportunity employer - and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.
Show more
Show less","Cloud Technologies, Big Data, Data Science, Machine Learning, Data Engineering, Kubernetes, Dataproc, Apache Kafka, Hortonworks Data Platform, Cloudera Data Platform, Data Observability, Site Reliability Engineering, DevOps, Apache Spark, Apache Hive, SQL, Git, Jenkins, Python, GCP Client Libraries, Cloud Function, Compute Engine, Terraform","cloud technologies, big data, data science, machine learning, data engineering, kubernetes, dataproc, apache kafka, hortonworks data platform, cloudera data platform, data observability, site reliability engineering, devops, apache spark, apache hive, sql, git, jenkins, python, gcp client libraries, cloud function, compute engine, terraform","apache hive, apache kafka, apache spark, big data, cloud function, cloud technologies, cloudera data platform, compute engine, data engineering, data observability, data science, dataproc, devops, gcp client libraries, git, hortonworks data platform, jenkins, kubernetes, machine learning, python, site reliability engineering, sql, terraform"
Data Engineer - Senior,HTC Global Services,"Addison, TX",https://www.linkedin.com/jobs/view/data-engineer-senior-at-htc-global-services-3779397961,2023-12-17,Corinth,United States,Mid senior,Hybrid,"HTC – A brief profile
Established in 1990, HTC Inc., a CMM Level 5 company with headquarters in Troy, Michigan, is a leading global Information Technology solution and BPO provider. HTC assists clients across multiple industry verticals, offering turnkey project lifecycle in, e-business, data warehousing, embedded systems, ECM, SCM, CRM, and ERP solutions. HTC Inc offers Connect IT, our Global Delivery Methodology that enables seamlessly delivery of outsourced IT services. HTC has global delivery centers across the globe.
Responsibilities:
Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and micro-services.
Develop new services in AWS using server-less and container-based services. Work with Spark clusters and Bigdata ecosystem tools on-prem and in the cloud.
Minimum Qualifications:
Proficient in Python and Spark
Hands-on experience with Azure/AWS/GCP
Hands-on experience with Data Lake or Data Warehouse
Intermediate to advanced SQL skills
Experience in using Serverless Development
Should have the ability to work and contribute beyond defined responsibilities
Excellent communication/inter-personal skills a must
Attitude and aptitude to learn new technologies in a fast-paced environment
Effective problem-solving skills
Ability to work in a fast-paced environment with a ""can do"" attitude
Preferred Qualifications:
8+ Years of working experience in relevant technologies.
Besides Minimum Qualification below will be considered added advantages
Working experience on Python, Airflow, Apache Spark , Apache Beam, Apache Flink, Kubernetes etc.
Experience with CI/CD and DevOps is added advantage
Working Knowledge of OpenShift
Familiarity in using AIOPS platforms like mlFlow, AutoML
Knowledge on Kafka is added advantage
Bachelors in Computer Engineering and/or Computer Science and/or Information Technology.
Benefits:
At HTC Global Services our associates have access to a comprehensive benefits package that includes Health, Dental, Vision, Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short- & Long-Term Disability Insurance, and a variety of other offerings.
Diversity & Inclusion
Our success as a company is built on practicing inclusion and embracing diversity. HTC Global Services is committed to providing a work environment free from discrimination and harassment, where all employees are treated with respect and dignity. Together we work to create and maintain an environment where everyone feels valued, included, and respected. At HTC Global Services, our differences are embraced and celebrated. HTC is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce. HTC is proud to be recognized as a National Minority Supplier
EEO/M/F/V/H
Show more
Show less","Python, Spark, Azure, AWS, GCP, Data Lake, Data Warehouse, SQL, Serverless Development, Airflow, Apache Spark, Apache Beam, Apache Flink, Kubernetes, CI/CD, DevOps, OpenShift, AIOPS platforms, mlFlow, AutoML, Kafka","python, spark, azure, aws, gcp, data lake, data warehouse, sql, serverless development, airflow, apache spark, apache beam, apache flink, kubernetes, cicd, devops, openshift, aiops platforms, mlflow, automl, kafka","aiops platforms, airflow, apache beam, apache flink, apache spark, automl, aws, azure, cicd, data lake, datawarehouse, devops, gcp, kafka, kubernetes, mlflow, openshift, python, serverless development, spark, sql"
Data Engineer,Barry-Wehmiller Design Group,"St Louis, MO",https://www.linkedin.com/jobs/view/data-engineer-at-barry-wehmiller-design-group-3786849642,2023-12-17,Collinsville,United States,Associate,Onsite,"Who You’ll Work With
You will join one of our 45 offices in the US, be part of a committed team of over 1500 professionals, and work in teams and directly with our clients doing work that is shaping the world around us. You will be welcomed into a rapidly growing business and team and empowered to make an impact. You will be valued, cared for, and challenged on your path to becoming a world-class professional consultant and surrounded by leaders who are committed to creating an environment that enables you to realize your own success and fulfillment.
When you join Design Group as a Data Engineer, you are joining a team that will challenge you and position you for growth. In this role, you will work with a team of industry experts to help the world’s leading companies solve their most difficult problems. You will partner with seasoned leaders, technical specialists, and subject matter experts to deliver the highest quality solutions to our clients with consistency and accuracy.
What You’ll Do
Design, develop, and maintain cloud data infrastructure leveraging technologies like Azure SQL Database, Azure Data Lakes, and Azure Synapse Analytics to enable advanced analytics (AWS experience is acceptable)
Build scalable cloud data solutions and pipelines for big data sources and large datasets using services like Azure Synapse Spark Pools, Azure Data Pipelines, Azure Data Lakes, and Spark Notebooks.
Create optimized data models, efficient ETL/ELT logic, transformations, and metadata to structure and relate data for business insights.
Automate and schedule regular data integrations from transactional systems, on-prem data warehouses, enterprise databases and SaaS applications into high performance cloud data platforms.
Implement data quality checks, validation processes, error handling, partitioning, security, and resilience capabilities into data solutions based on requirements.
Support migration initiatives to move on-premises data and BI systems into cloud data lakehouses, data lakes and other cloud analytics services.
What You’ll Bring
0-2 years’ experience as a data engineer, ETL developer, or similar role (internships experience included).
Experience with cloud data technologies (Azure preferred) such as Azure Synapse Analytics and Spark Pools.
Strong expertise with SQL and Python. Exposure to DAX and M preferred.
Familiarity with PowerBI or Tableau is a plus.
Experience utilizing structured and unstructured data.
Knowledge of data modeling, warehousing principles, ETL/ELT, and metadata standards.
Understanding of CI/CD pipelines and DevOps processes is a plus.
Ability to relate data opportunities to business needs.
Strong communication, collaboration, curiosity, and perseverance skills.
Analytical mindset with problem solving and troubleshooting skills.
Bachelor’s degree in Computer Science, Analytics, Information Systems or similar field required.
Our culture and commitment to our people is what sets us apart. We foster an environment of mutual respect, integrity, and unconditional interest in the individual and collective success of our professionals. Our model and entrepreneurial mindset offer a rewarding, challenging, and highly flexible path. As an Application Support Analyst, you will build a meaningful and fulfilling career with the support of professional development resources and mentorships including our First Year Experience program, Individual Development Plans, and Career Path resources and tools. You will be surrounded by exceptional talent who will support your development as both a world-class professional and a highly effective leader.
Show more
Show less","Azure SQL Database, Azure Data Lakes, Azure Synapse Analytics, AWS, Azure Synapse Spark Pools, Azure Data Pipelines, Spark Notebooks, ETL, ELT, DAX, M, PowerBI, Tableau, CI/CD, DevOps, Data Modeling, Warehousing, Metadata","azure sql database, azure data lakes, azure synapse analytics, aws, azure synapse spark pools, azure data pipelines, spark notebooks, etl, elt, dax, m, powerbi, tableau, cicd, devops, data modeling, warehousing, metadata","aws, azure data lakes, azure data pipelines, azure sql database, azure synapse analytics, azure synapse spark pools, cicd, datamodeling, datawarehouse, dax, devops, elt, etl, m, metadata, powerbi, spark notebooks, tableau"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Swansea, IL",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783189240,2023-12-17,Collinsville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Swansea-DataResearchAn.014
Show more
Show less","Python, JavaScript, JSON, OOP, R, Generative AI, Data Science, Research, Product Development, Communication, Project Management, Stakeholder Management, SQL, MongoDB, Agile, Scrum, Cloud Computing, Machine Learning, Natural Language Processing, Computer Vision","python, javascript, json, oop, r, generative ai, data science, research, product development, communication, project management, stakeholder management, sql, mongodb, agile, scrum, cloud computing, machine learning, natural language processing, computer vision","agile, cloud computing, communication, computer vision, data science, generative ai, javascript, json, machine learning, mongodb, natural language processing, oop, product development, project management, python, r, research, scrum, sql, stakeholder management"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"St Louis, MO",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783184848,2023-12-17,Collinsville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-StLouis-DataResearchAn.015
Show more
Show less","Python, JavaScript, JSON, OOP, Generative AI, Machine Learning, Data Science, Algorithms, Research, Product Development, Data Analysis, Communication, Collaboration, Time Management, Team Management, EdTech, Teaching, Learning","python, javascript, json, oop, generative ai, machine learning, data science, algorithms, research, product development, data analysis, communication, collaboration, time management, team management, edtech, teaching, learning","algorithms, collaboration, communication, data science, dataanalytics, edtech, generative ai, javascript, json, learning, machine learning, oop, product development, python, research, teaching, team management, time management"
Data Analyst,Brooksource,"St Louis, MO",https://www.linkedin.com/jobs/view/data-analyst-at-brooksource-3763821614,2023-12-17,Collinsville,United States,Associate,Hybrid,"Associate Data Analyst
St. Louis, MO
(Required to go onsite 3X/week - no remote candidates will be considered)
Contract-to-hire
$60-65k
As the Associate Data Analyst, you will analyze data to promote client growth alongside a product analytics team of a Fortune 15 company. You will utilize SQL to support in the reporting of client specific analyses, as well as support in the buildout of client dashboard templates. You will have the ability to present findings to various stakeholders and collaborate cross functionally with business teams. This is a great opportunity to access mentorship and training in various technologies.
Minimum Qualifications:
· Bachelor’s Degree OR relevant equivalent experience (Military, technical bootcamp, etc.)
· Solid understanding and hands on experience with SQL
· Familiarity or ability to tell a data story through visualization tools, Tableau is a plus
· Data Presentation experience through classroom or internship projects
· Passion for working with clients and collaborating with high-level stakeholders
Responsibilities:
· Write SQL to create new queries and dashboards that will become standard for client visits
· Analyze client data and visualize this analysis within Tableau dashboards
· Present data and conclusions to stakeholders on a daily basis
· Aid in general project coordination tasks including coordinating client events to ensure they are properly supported
What’s In It For You?
· Opportunity to start your career at a Fortune 15 Healthcare Company
· You will be trained and mentored by senior technologists
· You will attend networking events with senior leadership
· Weekly paychecks
Show more
Show less","SQL, Tableau, Data Visualization, Data Analysis, Data Presentation, Client Collaboration, Dashboard Creation, Project Coordination","sql, tableau, data visualization, data analysis, data presentation, client collaboration, dashboard creation, project coordination","client collaboration, dashboard creation, data presentation, dataanalytics, project coordination, sql, tableau, visualization"
Senior Data Engineer,Advanced Hires,"Creve Coeur, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-advanced-hires-3784368665,2023-12-17,Collinsville,United States,Mid senior,Remote,"Title:
Senior Data Engineer
Location:
100% Remote
Duration:
Long term contract
What you will do is why you should join us:
Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets
Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate
Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact
Explore relevant technology stacks to find the best fit for each dataset
Pursue opportunities to present our work at relevant technical conferences
Project your talent into relevant projects. Strength of ideas trumps position on an org chart
If you share our values, you should have:
At least 7 years’ experience in software engineering
At least 2 years’ experience with Go
Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach
Experience with stream processing using Apache Kafka
A level of comfort with Unit Testing and Test-Driven Development methodologies
Familiarity with creating and maintaining containerized application deployments with a platform like Docker
A proven ability to build and maintain cloud-based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform
Experience data modeling for large scale databases, either relational or NoSQL
Bonus points for:
Experience with protocol buffers and gRPC
Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes
Experience working with scientific datasets, or a background in the application of quantitative science to business problems
Bioinformatics experience, especially large-scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation
This is a remote position.
Show more
Show less","Data engineering, Software engineering, Go, RESTful API development, Apache Kafka, Unit testing, Testdriven development, Docker, Cloud infrastructure, AWS, Azure, Google Cloud Platform, Data modeling, Relational databases, NoSQL databases, Protocol buffers, gRPC, Google Cloud Platform, Apache Beam, Google Cloud Dataflow, Google Kubernetes Engine, Kubernetes, Scientific datasets, Quantitative science, Bioinformatics, Variant data, Variant annotation, Genotype to phenotype correlation","data engineering, software engineering, go, restful api development, apache kafka, unit testing, testdriven development, docker, cloud infrastructure, aws, azure, google cloud platform, data modeling, relational databases, nosql databases, protocol buffers, grpc, google cloud platform, apache beam, google cloud dataflow, google kubernetes engine, kubernetes, scientific datasets, quantitative science, bioinformatics, variant data, variant annotation, genotype to phenotype correlation","apache beam, apache kafka, aws, azure, bioinformatics, cloud infrastructure, data engineering, datamodeling, docker, genotype to phenotype correlation, go, google cloud dataflow, google cloud platform, google kubernetes engine, grpc, kubernetes, nosql databases, protocol buffers, quantitative science, relational databases, restful api development, scientific datasets, software engineering, testdriven development, unit testing, variant annotation, variant data"
Staff Data Engineer,Recruiting from Scratch,"Lakeview, IL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744396248,2023-12-17,Herrin,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Docker, Spark, pySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Kafka, Storm, Sparkstreaming, Dimensional data modeling, Schema design, ETL, Data Warehouses, Legal compliance, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, docker, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, legal compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, kafka, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Lakeview, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832228,2023-12-17,Herrin,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Lakeview, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395237,2023-12-17,Herrin,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Pipeline, Data Warehouse, Big Data, Agile Engineering, TDD, Continuous Integration, Kafka, Storm, Spark, Hadoop, Hive, SparkStreaming, PySpark, SQL, Python, Snowflake, Airflow, Kubernetes, Docker, Helm","data engineering, data pipeline, data warehouse, big data, agile engineering, tdd, continuous integration, kafka, storm, spark, hadoop, hive, sparkstreaming, pyspark, sql, python, snowflake, airflow, kubernetes, docker, helm","agile engineering, airflow, big data, continuous integration, data engineering, data pipeline, datawarehouse, docker, hadoop, helm, hive, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Lakeview, IL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090532,2023-12-17,Herrin,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Modeling, Data Analysis, Data Visualization, Data Preprocessing, Data Postprocessing, Data Pipelines, Data Platforms, Data Frameworks, Data Governance, Data Security, Data Compliance, ETL, Python, Java, Bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Applied Machine Learning, NLP, Large Language Models, Microservices, Conversational AI APIs, Recommender Systems","data engineering, machine learning, data mining, data modeling, data analysis, data visualization, data preprocessing, data postprocessing, data pipelines, data platforms, data frameworks, data governance, data security, data compliance, etl, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, applied machine learning, nlp, large language models, microservices, conversational ai apis, recommender systems","airflow, applied machine learning, aws, azure, bash, conversational ai apis, data compliance, data engineering, data frameworks, data governance, data mining, data platforms, data postprocessing, data preprocessing, data security, dataanalytics, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, microservices, nlp, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm, visualization"
Field Data Engineer,Ascend.io,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/field-data-engineer-at-ascend-io-3783648367,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Ascend's mission is to enable every organization to innovate faster by harnessing the power of automation. We help customers spend more time bringing data innovations to life by simplifying and accelerating data orchestration.
As a Field Data Engineer at Ascend you will help our customers accelerate their data projects, focused primarily on the initial technical implementation of Ascend. While you will have direct responsibility for building parts of the overall solution, you will also focus intently on up-leveling the customer in their ability to design and build data systems themselves. When you are not working directly with a customer, you will be part of the broader field engineering team working on customer POC's, staying current on leading edge technologies, helping other customers, and creating content. If you love a challenge, working with insanely bright and motivated people, and learning more faster than you thought possible, join Ascend, recently named a Gartner Cool Vendor!
What you'll do:
Support our customers in region
With the help of a Solution Architect, you will design cutting-edge data solutions specific to customer requirements.
Implement aspects of the solutions while providing support and guidance to customer developers on the team.
Help our customer developers become raving fans of Ascend.
Provide feedback to the product team on new features that will drive adoption
Assist/drive POC's with new potential customers.
Research and implement technologies that are adjacent to and complementary to Ascend.
Generate content that will accelerate customer learnings and promote self-sufficiency.
Build additional features / extensions for the Ascend platform.
What we look for:
Deep desire to learn/improve and be part of a winning team
3+ years of Software or Data Engineering experience.
Experienced with at least one cloud (AWS, GCP, Azure)
Hands-on with at least 1 scripting language (such as Python, Ruby, Javascript)
Education
: Bachelor degree in Computer Science or Math.
Skills: Must have a software engineering/developer background.
What we offer:
Medical/Dental/Vision
Long-term & Short-term disability
Life Insurance
Stock options
401(k)
Flexible PTO
Remote Friendly
Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Ascend we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Ascend.io is an equal opportunity employer. We are committed to treating all applicants fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law.
Show more
Show less","Data Engineering, Software Engineering, AWS, Azure, GCP, Python, Ruby, Javascript, Computer Science, Math","data engineering, software engineering, aws, azure, gcp, python, ruby, javascript, computer science, math","aws, azure, computer science, data engineering, gcp, javascript, math, python, ruby, software engineering"
Senior Data Engineer - GCP,CUBE,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-gcp-at-cube-3754984442,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Role:
Senior Data Engineer
Location:
United Kingdom
Recently listed as a ""RegTech Top Performer"" in Market Fintech's RegTech Supplier Performance Report, CUBE is pioneering the development of machine automated compliance.
We are a global RegTech business defining and implementing the gold standard of regulatory intelligence and change for the financial services industry. We deliver our services through a SaaS platform, powered by an innovative combination of AI and proprietary data ontology, to simplify the complex and everchanging world of compliance for our clients.
CUBE is creating the future and are a company rooted in strong values, team spirit and commitment to our customers and wider communities. We serve some of the largest financial institutions globally and are expanding our footprint very fast. As we do so, we are keen for new talent to join us and realize their full potential to grow into leadership positions within the business.
Our Products:
RegPlatform
is an Enterprise grade technology platform that streamlines regulatory change management. It provides firms with a one-stop, continuously maintained inventory of global regulations, with effortless horizon scanning, integration capabilities and workflow management. RegPlatform combines industry leading AI technology with expert validated insights to simplify the complexities of multi-jurisdictional regulatory content.
RegAssure
caters for nimble, lean financial organizations that want assured, seamless compliance without the burden of set-up costs and implementation processes. RegAssure provides fast, automated regulatory intelligence that intuitively knows your business needs and can grow with you.
Role mission:
Establish a strong data architecture in GCP for the backend of AI/ML/DS Services supporting a SaaS product.
Build cost-efficient, low latency data pipelines into and within GCP.
Represent the interests of the RegBrain AI team in the establishment of a global data strategy at CUBE.
Assist ML engineering team to build SOTA applications on structured and unstructured data, at scale, by providing fitted data solutions.
Ensure seamless integration of RegBrain’s product features to analytics via CUBE’s data lake.
Continuously educate the ML team regarding best data practices.
Ensure reliability, completeness and quality of datasets replicated from internal sources.
Own the data governance needs of RegBrain, educate the ML Engineers on the subject.
Improve and manage ETL/ELT pipelines to the graph database(s) supporting CUBE's Knowledge Graph.
Responsibilities:
Create, orchestrate and monitor strong ETL pipelines for core datasets in the business from on-prem to GCP.
Build monitoring solutions for data quality within GCP, with reporting to management stakeholders.
Support creation, orchestration and monitoring of data pipelines to graph database.
Support the establishment of a data catalogue.
Coordinate with the Infrastructure team to ensure seamless creation of needed solutions.
Manage data lifecycle and operational costs.
What we’re looking for:
Cloud champion. You must have extensive experience in
GCP
specifically. None of
the secrets of BigQuery, DataFlow, Pub/Sub, Composer etc... elude you. You can quickly decide which solution is suited to a given use case.
You are at ease with all the tools in GCP: CloudSQLBigQuery,DataFlow/Fusion/Proc, Cloud Composer.
A specialist in their domain. You must be able to identify CUBE/RegBrain's needs, communicate planned solutions efficiently and carry them out.
You must be able to deal with:
-> Data transport
-> NoSql
-> Caching
-> Message Buses
->Data cleansing, esp. for textual data.
You not only merely think about data as bytes to be moved around, but as valuable information about the real world.
You are proactive. We will be relying on you to set the standards and raise them. Your work should enable others in the team.
An appetite for leadership. When the team grows from its success, you must be eager to assist its composition and lead more junior data engineers.
A can-do attitude. As a mid-size company, pragmatism is key. We need to enable short iteration cycles within the ML team while slowly establishing strong data foundations for its future growth.
You like working remotely as much as in-person.
Why Us?
🌍
Globally, we are one of a kind!
CUBE is a well-established player in Regtech (we were around before Regtech was even a thing!), and our category-defining product is used by leading financial institutions around the world (including Revolut, Citi, and HSBC). We cover over 5,000 issuing bodies in 712 jurisdictions across 180 countries. Substantially more than our closest competitors.
🗽
Freedom, flexibility & progression.
We are the market leader within Regtech, and yet we are constantly evolving and you will have a significant influence over how we develop moving forwards. We will help you realise your full potential and grow into leadership positions across CUBE.
🗣️
Internationally collaborative culture
CUBE has more than 300 CUBERs across 11 locations in Europe, the Americas and APAC and we are united by our strong team-oriented culture. You will have the opportunity to work with likeminded colleagues from all over the world, gaining exposure to different cultures and business practices.
🌱
Innovative & meaningful work
We are always exploring new technologies and innovations that can help clients solve complex problems. If you’re someone seeking the opportunity to work with cutting edge technology and work on challenging projects that have a real impact then we want to hear from you!
💻
Work life balance
CUBE is a remote first business, you'll be able to design your home office and choose your own work equipment. Unable to work from home one week, or desperate for in-person interaction with colleagues? No problem—book a room in a coworking space or join one of our department monthly team meets (they’re happening all the time!).
🤝
Diversity, Equity and Inclusion:
We believe in equal opportunities and encourage applicants to apply to our open roles regardless of gender, marital status, race, nationality, ethnicity, religion, neurodiversity, sexual orientation and/or age and are committed to providing a working environment where everyone is supported to be their authentic best selves.
At CUBE we do our best to implement inclusive hiring processes to build a culture where we value diversity. We are a values driven company who care about growth and learning, therefore recognise we still have long way to go from where we want to be in regards to DE&I.
As part of the application, we ask for personal data for our internal diversity and inclusion bench-marking, so that we can tailor our DE&I strategies and identify opportunities to improve as a business.
Show more
Show less","BigQuery, DataFlow, Pub/Sub, Composer, Data Lake, Hadoop, NoSQL, CloudSQLBigQuery, DataFusion, Cloud Composer, Data Catalog, Caching, Message Buses, Data Governance, Data Integration, Data Quality, Data pipelines, Machine Learning, Data Analytics, Data Architecture, Data Engineering, ETL, ELT, Artificial Intelligence, Cloud Computing, Data Science, Programming, Software Development, GCP, SaaS, RegTech","bigquery, dataflow, pubsub, composer, data lake, hadoop, nosql, cloudsqlbigquery, datafusion, cloud composer, data catalog, caching, message buses, data governance, data integration, data quality, data pipelines, machine learning, data analytics, data architecture, data engineering, etl, elt, artificial intelligence, cloud computing, data science, programming, software development, gcp, saas, regtech","artificial intelligence, bigquery, caching, cloud composer, cloud computing, cloudsqlbigquery, composer, data architecture, data catalog, data engineering, data governance, data integration, data lake, data quality, data science, dataanalytics, dataflow, datafusion, datapipeline, elt, etl, gcp, hadoop, machine learning, message buses, nosql, programming, pubsub, regtech, saas, software development"
Lead Data Engineer // £75k plus package // Remote working // Financial Services,Mentmore,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-%C2%A375k-plus-package-remote-working-financial-services-at-mentmore-3765284265,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Job title
– Lead Data Engineer
Salary
- £75k plus package
Working Model
– Fully remote with minimal requirements to attend the office.
Industry
– Financial Services
A mid-size financial services company looking to onboard a highly skilled Lead Data Engineer to help mature the current data engineering function. My client is looking for someone who is highly proficient in Redshift & can help develop junior members of the team.
Key responsibilities:
Technical leadership of the data engineering department.
Work closely with the CTO and head of Software Product to define and implement the data strategies.
Strong communication skills.
Make recommendations for the MI reports, processes, and standards.
Mentor junior members of the team.
Proficient in AWS Redshift.
Experience working with Python scripting, Glue & QuickSIght.
Proficient in data modeling.
Experience developing ETL & ETL processes from multiple data sources.
Exposure to working in Agile environments.
If you require visa sponsorship please could I ask you not to apply as my client is currently unable to provide sponsorship, thank you.
Show more
Show less","Data Engineering, Data Leadership, Redshift, Python Scripting, Glue, Quicksight, Data Modeling, ETL Processes, Agile","data engineering, data leadership, redshift, python scripting, glue, quicksight, data modeling, etl processes, agile","agile, data engineering, data leadership, datamodeling, etl, glue, python scripting, quicksight, redshift"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Newyorkuniversity,"Parksville, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-newyorkuniversity-3757207271,2023-12-17,Nanaimo, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, DataDriven DecisionMaking, Data Modeling, Algorithms, A/B Testing, Data Quality, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, data modeling, algorithms, ab testing, data quality, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, algorithms, data manipulation, data quality, dataanalytics, datadriven decisionmaking, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Analyst,Infinity-Outsourcing,"McLean, VA",https://www.linkedin.com/jobs/view/data-analyst-at-infinity-outsourcing-3741171745,2023-12-17,Sterling,United States,Associate,Onsite,"Role: Data Analyst
Location: McLean, VA
Pay rate /salary to candidate : $47-$50/Hr or $95,000-$100,000 /Annum
Client: Freddie Mac
experience: +10
Data Analyst
Analyze the current source of data, for key data elements, understand the current usage and identify the new/alternative source for these data elements
Experience in Sybase and DB2 is needed and knowledge of ECDA is desired.
Identify customer needs and intended use of requested data in the development of database requirements and support the planning and engineering of databases.
Maintain comprehensive knowledge of database technologies, complex coding languages, and computer system skills.
Collaborate with developers and project managers to understand integration requirements.
Assist in designing and implementing API Integrations between different software systems in Modeling applications.
Write and maintain API Documentation for internal and external stakeholders.
Troubleshoot and debug integration issues, identifying and resolving technical obstacles.
Stay up to date with industry trends and best practices in API Integration.
Work with people with different functional expertise respectfully and cooperatively to work toward a common goal
Skilled in documentation and database reporting for the purposes of analysis, data discovery, and decision-making with the use of relevant software such as Crystal Reports, Excel, or SSRS
Experience in the process of analyzing data to identify trends or relationships to inform conclusions about the data
Show more
Show less","Data Analysis, Sybase, DB2, ECDA, API Integration, Software Modeling, API Documentation, Troubleshooting, Industry Trends, Crystal Reports, Excel, SSRS, Data Mining, Data Trends","data analysis, sybase, db2, ecda, api integration, software modeling, api documentation, troubleshooting, industry trends, crystal reports, excel, ssrs, data mining, data trends","api documentation, api integration, crystal reports, data mining, data trends, dataanalytics, db2, ecda, excel, industry trends, software modeling, ssrs, sybase, troubleshooting"
"Data Engineer, Sr.",Atlas Research,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-sr-at-atlas-research-3111073000,2023-12-17,Sterling,United States,Associate,Onsite,"<span style=""font-family:""Arial"",sans-serif"">Data Engineer, Senior
<span style=""font-family:""Arial"",sans-serif"">
Atlas Research is seeking qualified candidates for a Data Analyst position to join an emerging technology and innovation practice.  Primary objectives involve participating with a team responsible for data analysis and analytics utilizing database management software/tools against various database platforms. In this role you will assist in the development of automated queries and data extraction across multiple computing environments. You will develop technical expertise in data mining techniques and tools to ensure deliverables meet the needs of both internal and external business customers.  This position serves as an excellent starting point for becoming a Data Scientist.
<span style=""font-family:""Arial"",sans-serif"">
Essential Duties and Responsibilities
<span style=""font-family:""Arial"",sans-serif"">
Joins and combines various data sources, systems, databases, reports, to come up with a single data output.
<span style=""font-family:""Arial"",sans-serif"">
Convert business requirements into specifications, jobs, process, scripts, programs, and queries;  Writes basic to intermediate queries (like CRUD statements, joins, stored procedure, functions) and views to extract required data
<span style=""font-family:""Arial"",sans-serif"">
Enhances and optimizes advanced query, views and procedures to improve efficiency and reliability of data manipulation (i.e. SSIS, VBA, Excel Macro, VB, .Net, SQL).
<span style=""font-family:""Arial"",sans-serif"">
Design and develop, build and modify, and maintain complex reports using data visualization tools (i.e. MSSQL, SSRS, Power BI, Tableu, Excel)
<span style=""font-family:""Arial"",sans-serif"">
Provide management with status reports of current ETL (extract, transform, load) processes
<span style=""font-family:""Arial"",sans-serif"">
Create visually rich presentations using various forms, charts, and other graphical elements into dashboards
<span style=""font-family:""Arial"",sans-serif"">
Produce clear, concise, relevant, and accurate documentation as needed
<span style=""font-family:""Arial"",sans-serif"">
Import and export data in different formats and various sources for data manipulation, wrangling, modeling, migration, and reporting
<span style=""font-family:""Arial"",sans-serif"">
Key Requirements:
<span style=""font-family:""Arial"",sans-serif"">
Bachelor's degree in Information Technology, Computer Science, or related Field
<span style=""font-family:""Arial"",sans-serif"">
Must have 5+ years of relevant data analysis experience
<span style=""font-family:""Arial"",sans-serif"">
Must be proficient MSSQL stored procedure creation, job creation, and general report automation
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with advanced MSSQL features (linked servers, dB mail, bulk copy program (bcp) utility) to use in automating data delivery, file creation, notifications
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with Open Database Connectivity (ODBC) creation/usage
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with Microsoft Access table creation, queries, macros, forms and reports, Visual Basic for Applications (VBA)
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with MSSQL (2000/2005/2008) and Oracle (10G/11G) databases
<span style=""font-family:""Arial"",sans-serif"">
Must possess strong organizational skills with Must be able to work under tight deadlines efficiently demonstrated attention to detail
<span style=""font-family:""Arial"",sans-serif"">
Must flexible and able to adapt in a changing business environment
<span style=""font-family:""Arial"",sans-serif"">
Must possess strong written and verbal communication
<span style=""font-family:""Arial"",sans-serif"">
Must be resourceful, industrious, and willing to take on new tasks and proactively learn new technologies to keep up with business needs for example developing a proficiency in R or Python
<span style=""font-family:""Arial"",sans-serif"">
Knowledge or familiarity to VA data management systems to include but not limited to Corporate Data Warehouse (CDW), Veterans Health Administration Support Services (VSSC), Computerized Patient Record System (CPRS) is a plus
<span style=""font-family:""Arial"",sans-serif"">
Special skills/must be proficient in the following or willing to improve proficiency in these areas:
<span style=""font-family:""Arial"",sans-serif"">
Structured Query Language (SQL)
<span style=""font-family:""Arial"",sans-serif"">
Cursory knowledge of programming and scripting languages (.Net C#, C++, VBS, etc.)
<span style=""font-family:""Arial"",sans-serif"">
Microsoft Office 2007/2010 (Excel, Access, Word, PowerPoint)
<span style=""font-family:""Arial"",sans-serif"">
Database structure and design
<span style=""font-family:""Arial"",sans-serif"">
Excellent written and oral communication skills
Show more
Show less","Data analysis, Data mining, Data visualization, SQL, MSSQL, SSRS, Power BI, Tableu, Data wrangling, Data modeling, Data migration, Data reporting, Data management, Data Scientist, Data extraction, Data integration, Data transformation, Data cleansing, Data loading, Data warehousing, Data mining techniques, Data manipulation, Data delivery, Data automation, Data migration, Data modeling, Data reporting, Data visualization, Relational databases, NoSQL databases, Big data technologies, Cloud computing, Machine learning, Artificial intelligence, Data governance, Data security, Data privacy","data analysis, data mining, data visualization, sql, mssql, ssrs, power bi, tableu, data wrangling, data modeling, data migration, data reporting, data management, data scientist, data extraction, data integration, data transformation, data cleansing, data loading, data warehousing, data mining techniques, data manipulation, data delivery, data automation, data migration, data modeling, data reporting, data visualization, relational databases, nosql databases, big data technologies, cloud computing, machine learning, artificial intelligence, data governance, data security, data privacy","artificial intelligence, big data technologies, cloud computing, data automation, data delivery, data extraction, data governance, data integration, data loading, data management, data manipulation, data migration, data mining, data mining techniques, data privacy, data reporting, data scientist, data security, data transformation, data wrangling, dataanalytics, datacleaning, datamodeling, datawarehouse, machine learning, mssql, nosql databases, powerbi, relational databases, sql, ssrs, tableu, visualization"
Senior Data Analyst,Accroid Inc,"Reston, VA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-accroid-inc-3768300248,2023-12-17,Sterling,United States,Associate,Onsite,"12+ Months
VA
Technical skills: SQL (required), Tableau (required), Python (nice to have), some AWS exposure desired (Redshift, s3 Bucket)
Personality: ambitious, self-starter, proactive, great communication skills, unafraid to ask questions/curious
Show more
Show less","SQL, Tableau, Python, AWS, Redshift, S3 Bucket","sql, tableau, python, aws, redshift, s3 bucket","aws, python, redshift, s3 bucket, sql, tableau"
Sr. Data Analyst,eStaffing Inc.,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-analyst-at-estaffing-inc-3740219360,2023-12-17,Sterling,United States,Associate,Onsite,"Job Title -
Sr. Data Analyst:
Washington, DC
Location-
Washington, DC
Job Type- Full Time
Shift- Day
Job Requirements-
An Active DoD Issued Secret Clearance at the time of application.
Bachelor’s Degree in an applied mathematical discipline such as physics, engineering, computer science, or similar, and 5 to 7 years of relevant experience is required.
Experience supporting senior leaders and collaborating across large enterprises.
Excellent communication (i.e., oral, writing, presentation) skills for briefing Flag/SES leaders; specifically, presentation skills.
Experience using at least one statistical analytic package (Minitab, SPSS, etc.) and SQL
Experience with data visualization using Tableau.
NAVSEA or Navy experience, either active duty officer or as a contractor Project Manager and/or Ship Building Specialist.
Apply for full job position and will share full job details in your email id.
#DataAnalyst #SecurityClearance #DoDIssuedSecretClearance #BachelorDegree #Mathematics #Engineering #ComputerScience #Collaboration #CommunicationSkills #PresentationSkills #StatisticalAnalysis #DataVisualization #SQL #NAVSEA #NavyExperience #MachineLearning #DataScience #DataAnalytics #DataManagement #MetricsAnalysis #DashboardDevelopment #Teleworking #DataVisualization #Tableau #DataAnalysisTools #BusinessValue #DataMining #PredictiveAnalysis #DataInterpretation #MachineLearningMethods #StatisticalMethods #BusinessSolutions #AnalyticsJobs #SeniorDataAnalyst #DataAnalyticsCareer #GovernmentJobs #WashingtonDCJobs #JobOpportunity #DataScienceJobs #BusinessAnalysis #MaintenanceMetrics #AdvancedAnalytics #LeadershipBriefs #DataManagementSchemas #ProductivityImprovement #StatisticalAnalysis #PredictiveModeling #DataQuality #DecisionMaking #StakeholderEngagement #BusinessProcesses
Show more
Show less","Secret Clearance, Bachelor's Degree, Mathematics, Engineering, Computer Science, Collaboration, Communication Skills, Presentation Skills, Statistical Analysis, Data Visualization, SQL, NAVSEA, Navy Experience, Data Analytics, Data Management, Metrics Analysis, Dashboard Development, Teleworking, Tableau, Data Analysis Tools, Business Value, Data Mining, Predictive Analysis, Data Interpretation, Machine Learning Methods, Statistical Methods, Business Solutions, Analytics Jobs, Data Analytics Career, Government Jobs, Data Science Jobs, Business Analysis, Maintenance Metrics, Advanced Analytics, Leadership Briefs, Data Management Schemas, Productivity Improvement, Predictive Modeling, Data Quality, Decision Making, Stakeholder Engagement, Business Processes","secret clearance, bachelors degree, mathematics, engineering, computer science, collaboration, communication skills, presentation skills, statistical analysis, data visualization, sql, navsea, navy experience, data analytics, data management, metrics analysis, dashboard development, teleworking, tableau, data analysis tools, business value, data mining, predictive analysis, data interpretation, machine learning methods, statistical methods, business solutions, analytics jobs, data analytics career, government jobs, data science jobs, business analysis, maintenance metrics, advanced analytics, leadership briefs, data management schemas, productivity improvement, predictive modeling, data quality, decision making, stakeholder engagement, business processes","advanced analytics, analytics jobs, bachelors degree, business analysis, business processes, business solutions, business value, collaboration, communication skills, computer science, dashboard development, data analysis tools, data analytics career, data interpretation, data management, data management schemas, data mining, data quality, data science jobs, dataanalytics, decision making, engineering, government jobs, leadership briefs, machine learning methods, maintenance metrics, mathematics, metrics analysis, navsea, navy experience, predictive analysis, predictive modeling, presentation skills, productivity improvement, secret clearance, sql, stakeholder engagement, statistical analysis, statistical methods, tableau, teleworking, visualization"
IDB Invest Data Engineer Consultant,IDB Invest,"Washington, DC",https://www.linkedin.com/jobs/view/idb-invest-data-engineer-consultant-at-idb-invest-3780117228,2023-12-17,Sterling,United States,Associate,Onsite,"Analytics, Knowledge, and Information Division
Would you like to use your skills and experience to contribute to the economic and social development of Latin America and the Caribbean?
Location:
The IDB INVEST, a member of the Inter-American Development Bank (IDB) Group, is a multilateral organization based in Washington, D.C., that is committed to the development of Latin America and the Caribbean through the private sector.
IDB Invest is looking for a data engineer with SQL proficiency and experience structuring information for flexible querying and fast updates to provide business insight., preferably with experience in observational analyses, trend analyses, modeling, and new measurement strategies.
About This Position
Data Management and Analytics is the team under AKI Division that is responsible to define and implement the data strategy for IDB Invest.
What You’ll Do
Designs and develops data pipelines that extract data from various sources, transform it into the desired format, and load it into the appropriate data storage systems
Collaborates with data scientists and analysts to optimize models and algorithms for data quality, security, and governance.
Integrates data from different sources, including databases, data warehouses, APIs, and external systems.
Ensures data consistency and integrity during the integration process, performing data validation and cleaning as needed.
Transforms raw data into a usable format by applying data cleansing, aggregation, filtering, and enrichment techniques.
Optimizes data pipelines and data processing workflows for performance, scalability, and efficiency.
Implement large-scale data ecosystems including data management, governance, and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
What You´ll Need
Education
: A bachelor’s degree in computer science, data science, software engineering, information systems, or related quantitative field; master’s degree preferred
Experience: At least 4 years of experience working as a data engineer. Proven project experience developing and maintaining data warehouses in big data solutions. Ability to design, build, and deploy data solutions that capture, explore, transform, and utilize data to support AI, ML, and BI. Great communication and collaboration skill. Loves thinking out of the box to solve challenges at an unprecedented scale. Knowledge of data mining, machine learning tools, such as Databricks will be considered a plus. Knowledge of data governance solutions such as Informatica, collibra, allation and etc. will be considered a plus. Functional expertise: [i] You are able to work in a high-pressure environment while juggling multiple tasks, and effectively prioritize tasks, [ii] you are able to understand and translate clients’ (business units) needs into solutions, providing services that meet clients’ expectations for quality and performance; [iii] you listen actively, obtain necessary input, share ideas, speak persuasively, and convey information in a clear, objective, and concise manner.
Languages: You are proficient in written and verbal communication in English and in a second official language of the IDB Group (Spanish, Portuguese or French).
Requirements
Consanguinity
: You have no family members (up to the fourth degree of consanguinity and second degree of affinity, including spouse) working at the IDB, IDB Invest, or IDB Lab.
Type Of Contract And Duration
Type of contract:
Consultant Full-Time.
Length of contract: 18 months.
Location: Washington DC.
Responsible person: Head of Data Management & Analytics
What We Offer
The IDB group provides benefits that respond to the different needs and moments of an employee’s life. These benefits include a
competitive compensation
package comprised of:
Leaves and vacations: 2 days per month of contract + gender- neutral parental leave.
Health Insurance: the IDB Group provides a monthly allowance for the purchase of health insurance.
Savings plan: The IDB Group cares about your future, depending on the length of the contract, you will receive a monthly savings plan allowance.
Assistance with relocation and visa applications for you and your family, if applicable.
Work Schedules: Hybrid and flexible work schedules.
Development support: We offer learning opportunities to boost your professional profile such as seminars, one-to-one professional counseling, and much more.
Health and wellbeing: Access to our Health Services Center which provides preventive care and health education for all employees.
Other perks: Lactation Room, Daycare Center, Gym, Bike Racks, Parking, and others.
Our culture
At the IDB Group we work so everyone brings their best and authentic selves to work, willing to try new approaches without fear, and where they are accountable and rewarded for their actions.
Diversity, Equity, Inclusion and Belonging (DEIB) are at the center of our organization. We celebrate all dimensions of diversity and encourage women, LGBTQ+ people, persons with disabilities, Afro-descendants, and Indigenous people to apply.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job interview process. If you are a qualified candidate with a disability, please e-mail us at diversity@iadb.org to request reasonable accommodation to complete this application.
Our Human Resources Team reviews carefully every application.
About IDB Invest
IDB Invest, a member of the IDB Group, is a multilateral development bank committed to promoting the economic development of its member countries in Latin America and the Caribbean through the private sector. IDB Invest finances sustainable companies and projects to achieve financial results and maximize economic, social, and environmental development in the region. With a portfolio of $14.1 billion in asset management and 325 clients in 25 countries, IDB Invest provides innovative financial solutions and advisory services that meet the needs of its clients in a variety of industries.
Follow Us
https://www.linkedin.com/company/idbinvest/
https://www.facebook.com/IDBInvest
https://twitter.com/BIDInvest
Additional Information
External Opening Date: Dec 5, 2023
External Closing Date: Dec 21, 2023
External Contact Email: hrsc@iadb.org
External Contact Name: HR Service Center
Job Field: Technical Support
Show more
Show less","Data Engineering, Data Mining, Machine Learning, Business Intelligence, SQL, Data Warehousing, Data Pipelining, Data Integration, Data Cleansing, Data Transformation, Data Enrichment, Cloud Computing, Databricks, Informatica, Collibra, Allation, Data Governance, English, Spanish, Portuguese, French","data engineering, data mining, machine learning, business intelligence, sql, data warehousing, data pipelining, data integration, data cleansing, data transformation, data enrichment, cloud computing, databricks, informatica, collibra, allation, data governance, english, spanish, portuguese, french","allation, business intelligence, cloud computing, collibra, data engineering, data enrichment, data governance, data integration, data mining, data transformation, databricks, datacleaning, datapipeline, datawarehouse, english, french, informatica, machine learning, portuguese, spanish, sql"
Data Analyst,"ALTA IT Services, LLC","Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-at-alta-it-services-llc-3768084873,2023-12-17,Sterling,United States,Associate,Onsite,"IF INTERESTED REACH OUT DIRECTLY TO KSPICER@ALTAITS.COM OR CALL/TEXT 301-252-8762
DEPT OF STATE /HYBRID
Data/Business Intelligence Analyst
Required: SECRET can hold higher
Position Description
Seeking a Data Analyst to support a large federal government agency.
Description Of Duties
Utilize enterprise IT management systems and tools (e.g. PowerBI and ServiceNow) to collect and analyze data, develop, organize, and present dashboards, ad-hoc reports and other data visualizations for internal and external customers.
Apply statistical methodologies to identify trends and patterns in the data. Interpret findings and make recommendations based on industry standard IT management processes.
Design, develop, create and maintain visually intuitive, appealing and interactive dashboards, reports and visualizations using tools such as Microsoft Power BI, Tableau or similar platforms.
Prepare and brief senior level management, effectively communicating complex information to technical and non-technical stakeholders.
Collaborate with cross-functional teams and other external organizations in the performance of responsibilities and duties.
Required Experience/Skills/Attributes
Bachelor’s in computer science, data science, related field, or equivalent work experience
3-5+ years of work experience in data management and analysis
Desired Experience/Skills/Attributes
Prior work experience in government
ITIL v4 Foundations
ISO 20K
Show more
Show less","Data Analysis, PowerBI, ServiceNow, Dashboards, Statistical methodologies, Data visualizations, Microsoft Power BI, Tableau, Data management, ITIL v4 Foundations, ISO 20K","data analysis, powerbi, servicenow, dashboards, statistical methodologies, data visualizations, microsoft power bi, tableau, data management, itil v4 foundations, iso 20k","dashboard, data management, data visualizations, dataanalytics, iso 20k, itil v4 foundations, microsoft power bi, powerbi, servicenow, statistical methodologies, tableau"
Data Security Specialist / Security Engineer,ECS,"Washington, DC",https://www.linkedin.com/jobs/view/data-security-specialist-security-engineer-at-ecs-3737595664,2023-12-17,Sterling,United States,Associate,Onsite,"ECS is seeking a
Data Security Specialist / Security Engineer
to work in our
Washington, DC
office / Remote .
Job Description:
ECS is seeking an experienced Data Security Specialist.In this role you will work directly with administrators to architect secure systems. Independently assess systems for secure configuration and compliance to Federal, NIST 800-53, and Treasury directives.Thoroughly and accurately write security documentation including System Security Plans and Security Assessment Reports.Brief management and administrators on findings.Expect to work directly with administrators to identify and remedy findings and self-direct work to meet OFR deadlines.Design, develop, engineer, and implement solutions to MLS requirements. Perform complex risk analyses which also include risk assessment. Establish and satisfy information assurance and security requirements based upon the analysis of user, policy, regulatory, and resource demands. Support customers at the highest levels in the development and implementation of doctrine and policies. Apply know-how to government and commercial common user systems, as well as to dedicated special purpose systems requiring specialized security features and procedures. Perform analysis, design, and development of security features for system architectures.
Engage directly with administrators and advise them on how to securely configure and administer their applications and operating systems
Perform risk assessments on major applications and technologies and advise management of risks involved in system operation
Perform the work to complete and write from scratch all of the following documentation in a thorough, accurate, and grammatically correct manner:
Security Assessment Reports
Security Impact Assessments
System Security Plans
Risk Assessments
Security Risk Compliance Matrix
Certification Memos
Accreditation Memos
Risk Acceptance Memos
POAMs
Architect secure systems by direct engagement with system and application administrators
Assess system compliance with federal information security mandates, Treasury Directives, and NIST guidance—and advise management when compliance is deficient
Read results of vulnerability scans, identify false positives, and work with administrators to resolve vulnerabilities
Brief management and administrators on findings and recommendations
Be proactive, self-directed, and align schedule to meet OFR deadlines
Required Skills:
10+ years experience in Information Security
5+ years writing security documentation such as System Security Plans (SSP) and System Assessment Reports
3+ years performing risk assessments.
3+ years as security system architect—advising administrators/developers on how to create and configure secure applications.
3+ years as a system or network administrator
Expert knowledge of NIST 800 Special Publications, Federal Information Processing Standards, and OMB memos on Information Security
Desired Skills:
CISSP or like certification desired
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Information Security, NIST 800, System Security Plans (SSP), Security Assessment Reports, Risk Assessments, Security System Architect, System Administrator, Network Administrator, Vulnerability Scans, CISSP","information security, nist 800, system security plans ssp, security assessment reports, risk assessments, security system architect, system administrator, network administrator, vulnerability scans, cissp","cissp, information security, network administrator, nist 800, risk assessments, security assessment reports, security system architect, system administrator, system security plans ssp, vulnerability scans"
DoD Cleared - Data Analyst,ISN Corp,"Bethesda, MD",https://www.linkedin.com/jobs/view/dod-cleared-data-analyst-at-isn-corp-3763926887,2023-12-17,Sterling,United States,Associate,Onsite,"Company Description
Headquartered in Bethesda, Maryland, ISN Corporation is a nationwide provider of specialized professional services to Federal government agencies. ISN's portfolio includes work with over 100 Federal agencies.
ISN Corporation has been awarded a place in the
Washington Business Journal's list of the 50 Fastest Growing Government Contractors.
ISN is was also on the
Business Inc. Magazine's 5000 List of Fastest Growing Private Companies
for two years.
ISN Corporation offers many competitive benefits to its employees, including:
Major medical insurance with prescription coverage
Dental plan
Flexible spending
Short-term and long-term disability benefits at no cost to employees
Basic life insurance at no cost to employees
Retirement Plan
Paid leave with accrual beginning at Date of Hire
We are looking for an energetic, self-starting, organized, detail-oriented candidate with the ability to multitask in a fast-paced environment to join our team.
Headquartered in Bethesda, Maryland, ISN Corporation is a nationwide provider of specialized professional services to Federal government agencies. ISN's portfolio includes work with over 100 Federal agencies.
ISN Corporation has been awarded a place in the
Washington Business Journal's list of the 50 Fastest Growing Government Contractors.
ISN is was also on the
Business Inc. Magazine's 5000 List of Fastest Growing Private Companies
for two years.
ISN Corporation offers many competitive benefits to its employees, including:
Major medical insurance with prescription coverage
Dental plan
Flexible spending
Short-term and long-term disability benefits at no cost to employees
Basic life insurance at no cost to employees
Retirement Plan
Paid leave with accrual beginning at Date of Hire
We are looking for an energetic, self-starting, organized, detail-oriented candidate with the ability to multitask in a fast-paced environment to join our team.
Job Description
The Department of Defense (DOD), Office of Inspector General (OIG), established in 1982 by the Inspector General Act of 1978, as amended, DOD OIG is an independent, agency that conducts oversight and identifies, deters, and detects fraud, waste, and abuse in DOD operations and programs. The investigative arm of the DOD OIG is the Defense Criminal Investigative Service (DCIS) that works closely with law enforcement agencies to conduct criminal investigations of matters critical to DOD property, programs, and operations with an emphasis on life, safety, and readiness. The importance of the DOD OIG oversight work is signified by the enormity of the Department’s mission, the numerous assets that DOD utilizes to accomplish its mission, the magnitude of the $600 billion budget, and the over 3 million personnel who are part of the DOD family. The DOD OIG’s oversight work balances the needs and requests of both the DOD and Congress. The purpose of this contract effort is intended to acquire various levels of support to assist each of the components accomplish prospective activities. The ISN DoD OIG IT Contract Support Services Data Analyst, will provide analytical data support to various components as needed (floater), with minimal government assistance. The contractor shall serves as the subject matter expert in the creation and maintenance of the external (public) and internal OIG websites. This position serves as the subject matter expert in the configuration and maintenance of the Issuetrak application. This position researches and recommends web analytical tools to support internal and external OIG websites. The position recommends strategies for search engine optimization to ensure the OIG content is searchable across all Internet search engines for greater visibility of OIG website and content. Works with OIG's Office of the Chief Information Officer to assess and address any technical challenges in displaying the content. The contractor shall assist with data queries, cleanup, and database maintenance to ensure data integrity and reliability consistent with Government Accountability Office guidelines, and OIG standards. The following tasks serve to provide the contactor with a better understanding of the Objective. The tasks are not all inclusive as this is a performance based work statement designed to allow the contractor to develop the total solution for meeting the needs of the objective: Task 1: Respond to user questions and concerns regarding access, data entry, data quality and application issues. Task 2: Write new requirements for development, modification to forms within the system. Task 3: Engage with OCIO to recommend system changes or enhancements for the development and modification of pages and forms within the application. Task 4: Interpret complex data results and communicate orally and in writing to non-technical users such as headquarters managers and other personnel. Task 5: Develop, modify, and manage complex queries using a customized version of Microsoft Dynamics CRM to support user requirements. Task 6: Review results of queries and resolve data discrepancies. Task 7: Develop searchable reports, views and dashboards that meet the needs of stakeholders. Task 8: Create, modify, configure, and maintain Microsoft Dynamics CRM entities, attributes, data views, and forms using functional requirements and process maps to satisfy the business case.
Qualifications
Data analysts/technical support should have the following skills in order to perform the above functions: 1. Experience with application development using Oracle Developer Suite (form and report), ASP .NET, ColdFusion, or C#. 2. Technical proficiency in Microsoft products including but not limited to: SharePoint, Office (especially Excel), Visio, and Project. As well as other software products including Web Site Design and functionality, Adobe Captivate and Lexis CaseSoft Suite. 3. Proficiency in basic database structure, parent-child entity relationships, or permissions-based systems roles. 4. Experience with Microsoft Dynamics CRM. 5. Certified in Microsoft Dynamics CRM.
Must pass and maintain Active DoD Secret clearance throughout the life of the contract.
Bachelors in technical field. Masters preferred.
Additional Information
ISN Corporation is an Equal Opportunity Employer
ISN Corporation is an Equal Employment Opportunity/Affirmative Action employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, protected veteran status, disability status, marital status, genetic information, or any other characteristic protected by law.
ISN Corporation is a Drug-Free Workplace. Candidates are required to pass a pre-employment background investigation before beginning employment.
Show more
Show less","ASP.NET, ColdFusion, C#, Oracle Developer Suite, SharePoint, Microsoft Office, Visio, Project, Web Site Design, Adobe Captivate, Lexis CaseSoft Suite, Microsoft Dynamics CRM, Database Structure, Oracle","aspnet, coldfusion, c, oracle developer suite, sharepoint, microsoft office, visio, project, web site design, adobe captivate, lexis casesoft suite, microsoft dynamics crm, database structure, oracle","adobe captivate, aspnet, c, coldfusion, database structure, lexis casesoft suite, microsoft dynamics crm, microsoft office, oracle, oracle developer suite, project, sharepoint, visio, web site design"
Data Analyst,ECS,"Arlington, VA",https://www.linkedin.com/jobs/view/data-analyst-at-ecs-3787116972,2023-12-17,Sterling,United States,Associate,Hybrid,"ECS is seeking a
Data Analyst
to work in our
Arlington, VA
office .
Job Description:
Job Description: ECS is looking to hire a Data Analyst to join our Federal Practice in the DC area. This role requires you to interface with customers and other stakeholders to develop quantitative measurements and data insights at a large organization. Responsibilities: Assist with the development of concrete and demonstrable metrics and measures of program outputs and outcomes Mine quantifiable metrics from interviews and written narratives from program stakeholders Develop and review program and project documentation Support efforts to improve access to data and reporting tools through improved and standardized customer experience (CX) design Provide support for agile practices and ceremonies Support project management planning and task execution Provide analysis and solution recommendations for continual process improvement, data organization and integrity, security compliance and assessments for new technologies Develop processes for data analysis inclusion in new project planning
Required Skills:
US Citizenship is required.
Ability to obtain government security clearance (Public Trust/ Sensitive)
Must hold a Bachelor’s degree 3+ years of experience in a relevant field.
Experience with data analysis/reporting/visualization.
Desired Skills:
Strong requirement elicitation, analysis, and documentation skills.
Ability to manage multiple responsibilities across multiple workstreams with tight deadlines in a fast-paced environment.
Ability to effectively communicate and share knowledge with contacts at all levels. Skilled in developing collaborative relationships.
Ability to operate under minimum supervision where appropriate and understand when to escalate issues leadership Proficient at consolidating data from numerous sources to create presentations for all levels of management and other stakeholders.
Ability to build dashboards and visualize data using tools such as PowerBI or Tableau.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Data Analysis, Data Visualization, PowerBI, Tableau, Data Mining, Metrics Development, Reporting, Project Management, Process Improvement, Data Integrity, Security Compliance, Agile Development, Customer Experience Design, Requirement Elicitation, Data Consolidation, Presentation Skills, Leadership, Communication, Collaboration","data analysis, data visualization, powerbi, tableau, data mining, metrics development, reporting, project management, process improvement, data integrity, security compliance, agile development, customer experience design, requirement elicitation, data consolidation, presentation skills, leadership, communication, collaboration","agile development, collaboration, communication, customer experience design, data consolidation, data integrity, data mining, dataanalytics, leadership, metrics development, powerbi, presentation skills, process improvement, project management, reporting, requirement elicitation, security compliance, tableau, visualization"
Senior Data Analyst,"The Clearing, Inc.","District of Columbia, United States",https://www.linkedin.com/jobs/view/senior-data-analyst-at-the-clearing-inc-3780469062,2023-12-17,Sterling,United States,Associate,Hybrid,"Description
COMPANY DESCRIPTION
The Clearing is a management consulting firm that helps leaders identify underlying causes of organizational obstacles, resolve highly complex challenges, prioritize the fewest, most important initiatives to tackle regardless of conflicting needs, and make informed decisions in the context of an agreed upon mission, vision, and strategy.
We help create peak performance organizations through strategy, organizational development, leadership training, and change management. Our experienced consultants bring a powerful blend of analytic and creative skills from diverse fields, including change management, finance, engineering, communications, education, policy, design, and corporate leadership.
POSITION DESCRIPTION
In this position, data meets consulting. Senior Data Analysts infuse scientific insight into TC’s change strategy consulting work by viewing things with a scientific, data-centric lens.
This position requires an experienced data analyst who can lead client relations with a customer-centric focus. He/she/they leads opportunities for complex data to be leveraged to put analytics into the hands and brains of multiple clients. The Senior Data Analyst improves client decision-making and processes through complex data insights, accuracy, and integrity.
WHAT YOU’LL GET TO DO
Use complex data analysis to drive change strategies and process improvement, infusing efficiency and automation into many analytical tasks for customers
Think creatively about customer data sources, proxy measures, and novel means for extracting data
Develop and deliver data-focused presentations and reports, including a variety of data visualizations (dashboards, charts, graphs, and plots) and partner with visual consulting team as needed
Develop and write code for both analysis and visualization purposes. May oversee or review code development of more junior colleagues.
Proactively identify opportunities for and provide support to drive data-based decision-making with customers
Reevaluate and update processes and clean datasets when needed
Ensure data quality and integrity by performing extensive peer Quality Assurance (QA) reviews
Translate client objectives into clear data-centered change and other strategies, using non-technical language
Keep a pulse on where data tools are headed
Develop and maintain client relationships
Identify and support opportunities to expand project scope and actively share information about emerging customer support needs and trends with team members and management
Design, facilitate, and manage client meetings to successfully achieve identified and desired outcomes
Work on a full-time project or juggle multiple (2-4) part-time projects
Validate requirements and act as a supporting writer on proposals with a data-centric lens
Oversee work of others, which may include leading medium-sized project teams
Develop short- and long-term project plans
Deliver work on time, within budget, and to requested specifications; deliver error proof files at the completion of a project
Requirements
WHAT YOU BRING
Minimum of 6 years experience, at least 4 of which involve working as a data analyst and/or data scientist, in a consultative capacity, preferably with a background in management consulting, client relations, data analytics, data science, and/or coding
Master’s Degree in a highly analytical field required for the position (e.g., STEM or social sciences, data analytics or data science).
Strong curiosity, initiative, and willingness to learn, including an interest in exploring, learning, and upskilling data science capabilities
Incredible detail-orientation in all data and consulting tasks
Strong analytical and critical thinking abilities, including creative, out of the box thinking
Excellent ability to communicate complex data into digestible, simple ways to non-technical customers
Strong ability to engage and communicate with senior leaders and/or multiple stakeholders
A passionate commitment to quality
Ability to work with autonomy, but knowledge of when to reach to senior leadership as needed
Ability to review the work of peers and others and provide constructive, detailed feedback
Proven ability to adapt quickly to emerging requirements and prioritize as appropriate
Comfort working through times of ambiguity and in an environment of rapid growth
Excellent verbal and written communication skills; ability to strategically and effectively present data work with supporting reasoning
Ability to understand issues quickly and recommend and apply problem solving skills to data-centered solutions
Strong time management and organization skills and willingness to be accountable for deadlines and project outcomes
Willingness and ability to obtain and maintain a U.S. security clearance
Willingness and ability to travel to client sites in the surrounding DC and Baltimore metro areas; occasional travel outside of these regions could be expected (5%)
WHAT TOOLS/LANGUAGES YOU HAVE DEMONSTRATED EXPERIENCE APPLYING:
Advanced In
Intermediate In
Bonus Points For
Excel, Google Sheets
Tableau, SQL
Python, R
Statistics
Predictive Analytics
PHYSICAL AND MENTAL REQUIREMENTS
While performing duties of this job, an employee may be required to perform any, or all of the following: Attend meetings in and out of the office, travel (sometimes extensively); Occasional evening and weekend work may be required as job duties demand; Communicate effectively (both orally and in writing); Ability to effectively use computers and other electronic and standard office equipment; Occasionally exerting up to 10 pounds to lift, carry, push, pull or otherwise move objects, including the human body. Work is primarily sedentary and involves sitting for several hours at a time. Occasionally walking, climbing stairs, and standing occur in this role. Additionally, this job requires certain mental demands, including the ability to use judgment, withstand moderate amounts of stress, and maintain attention to detail. The Clearing is committed to partnering with all candidates and employees to ensure reasonable accommodations are made to meet these requirements.
EEO
The Clearing is committed to providing equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, religion, color, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, family medical history or genetic information, political affiliation, military service, or other non-merit based factors. In addition to federal legal requirements, The Clearing complies with applicable state and local laws governing nondiscrimination in employment. These protections extend to all terms and conditions of employment, including recruiting and hiring practices, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training and career development programs.
Show more
Show less","Data analysis, Consulting, Data scientist, Data analytics, Data science, Coding, Data visualization, SQL, Tableau, Python, Excel, Statistics, Predictive analytics, Machine learning, R","data analysis, consulting, data scientist, data analytics, data science, coding, data visualization, sql, tableau, python, excel, statistics, predictive analytics, machine learning, r","coding, consulting, data science, data scientist, dataanalytics, excel, machine learning, predictive analytics, python, r, sql, statistics, tableau, visualization"
Sr. Data Analyst (Hybrid),Fannie Mae,"Reston, VA",https://www.linkedin.com/jobs/view/sr-data-analyst-hybrid-at-fannie-mae-3783190153,2023-12-17,Sterling,United States,Associate,Hybrid,"Company Description
At Fannie Mae, futures are made. The inspiring work we do helps make a home a possibility for millions of homeowners and renters. Every day offers compelling opportunities to use tech to tackle housing’s biggest challenges and impact the future of the industry. You’ll be a part of an expert team thriving in an energizing, flexible environment. Here, you will grow your career and help create access to fair, affordable housing finance.
Job Description
As a valued colleague on our team, you will enhance and/or refine existing applications and systems to meet new technical requirements with support from leadership, and perform data analysis at various stages of projects to ensure development of a quality application.
THE IMPACT YOU WILL MAKE
Responsibilities
The Sr Data Analyst role will offer you the flexibility to make each day your own, while working alongside people who care so that you can deliver on the following responsibilities:
Collaborate with application owners to review defined business processes.
Review and define technical and/or data requirements for moderately complex application enhancements.
Independently establish and communicate standards for project teams.
Contribute to application test plan and evaluate test results.
Qualifications
THE EXPERIENCE YOU BRING TO THE TEAM
Minimum Required Experiences
At least 2 years of relevant professional work experience
Experience in querying databases with the use of relevant software such as Toad and DBeaver
Communication skills including communicating in writing or verbally, copywriting, planning and distributing communication, etc.
Desired Experiences
Bachelor's degree or equivalent
4+ years of relevant professional work experience
Experience working as a Product Owner
Ability to work independently as well as part of a bigger team.
Working knowledge of the mortgage banking industry and secondary mortgage market processes and systems is desired but not essential.
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Product Development skills including designing products, developing product roadmaps, translating design requirements, prototyping, etc.
Skilled in documentation and database reporting for the purposes of analysis, data discovery, and decision-making with the use of relevant software such as Crystal Reports, Excel, or SSRS
Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand
Experience in the process of analyzing data to identify trends or relationships to inform conclusions about the data
Operational Excellence skills including improving and overseeing operations
Determining causes of operating errors and taking corrective action
Experience gathering accurate information to explain concepts and answer critical questions
Working with people with different functional expertise respectfully and cooperatively to work toward a common goal
Skilled in the graphical representation of information in the form of a charts, diagrams, pictures, and dashboards with programs and tools such as Excel, Tableau, or Power BI
Experience using JIRA
Experience using SharePoint
Additional Information
The future is what you make it to be. Discover compelling opportunities at careers.fanniemae.com.
Fannie Mae is an Equal Opportunity Employer, which means we are committed to fostering a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, religion, national origin, gender, gender identity, sexual orientation, personal appearance, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation in the application process, email us at careers_mailbox@fanniemae.com.
The hiring range for this role is set forth on each of our job postings located on Fannie Mae's Career Site. Final salaries will generally vary within that range based on factors that include but are not limited to, skill set, depth of experience, certifications, and other relevant qualifications. This position is eligible to participate in a Fannie Mae incentive program (subject to the terms of the program). As part of our comprehensive benefits package, Fannie Mae offers a broad range of Health, Life, Voluntary Lifestyle, and other benefits and perks that enhance an employee’s physical, mental, emotional, and financial well-being. See more here.
Show more
Show less","Data Analysis, Software Development, Technical Writing, Product Development, Database Reporting, Data Visualization, Project Management, Communication, AWS, Excel, Tableau, Power BI, JIRA, SharePoint, Toad, DBeaver, Crystal Reports, SSRS","data analysis, software development, technical writing, product development, database reporting, data visualization, project management, communication, aws, excel, tableau, power bi, jira, sharepoint, toad, dbeaver, crystal reports, ssrs","aws, communication, crystal reports, dataanalytics, database reporting, dbeaver, excel, jira, powerbi, product development, project management, sharepoint, software development, ssrs, tableau, technical writing, toad, visualization"
Data Scientist,Burtch Works,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-scientist-at-burtch-works-3778877258,2023-12-17,Sterling,United States,Associate,Hybrid,"New Data Science opening in Washington, D.C.! Our client, an exciting, established startup, is looking to bring on a Data Scientist to join their growing analytics team. This highly-visible role will give you the chance to learn from a team of great mentors, working on analytics and data science projects while also liaising with team members across the business all the way up to the top floor! Ideal candidates will have solid Python or R skills and experience utilizing these tools to analyze large datasets to build models from the ground up.
Requirements:
• Master’s degree in a quantitative field
• Strong proficiency in Python (preferred) and/or R
• Statistical modeling experience – time series, regression, forecasting, etc.
• The ability to draw insights and tell stories from data
• Strong communication required – presenting to C-Suite and outside organizations
• Knowledge of Artificial Intelligence a plus
Base salary up to around $90K. Unfortunately, only US Citizens and Green Card holders can be considered at this time. This HYBRID position is based in Washington, D.C. and is in office 2 days a week.
Keywords:
data science, R, python, SQL, Tableau, analytics, time series, forecasting, optimization, machine learning, ML, AI, artificial intelligence, modeling, statistical, data mining, AWS, reporting, Power BI
Show more
Show less","Data Science, Python, R, Statistical Modeling, Time Series, Regression, Forecasting, Data Analysis, Data Mining, Artificial Intelligence, Machine Learning, Cloud Computing, AWS, Reporting, Data Visualization","data science, python, r, statistical modeling, time series, regression, forecasting, data analysis, data mining, artificial intelligence, machine learning, cloud computing, aws, reporting, data visualization","artificial intelligence, aws, cloud computing, data mining, data science, dataanalytics, forecasting, machine learning, python, r, regression, reporting, statistical modeling, time series, visualization"
Data Visualization Analyst,ECS,"Arlington, VA",https://www.linkedin.com/jobs/view/data-visualization-analyst-at-ecs-3764263419,2023-12-17,Sterling,United States,Associate,Hybrid,"ECS is seeking a
Data Visualization Analyst
to work in our
Arlington, VA
office.
ECS is seeking a passionate and driven visualization-focused data analyst to help our public sector customer secure the federal civilian IT enterprise, specifically focusing on the cybersecurity health of High Value Assets. Strong candidates will take advantage of modern approaches to business analysis, user centered design (UCD), information presentation, and the use of data analytics to improve client mission outcomes. This front line, operationally focused, client facing consultant develops deep understanding of the client’s mission and priorities, anticipates client needs, and translates them into mission-impactful analytics questions that can be addressed by data analytics using data available in the operational environment. Collaborating with other data ECS colleagues and co-contractor peers, the ideal candidate builds out polished information (data) presentations — dashboards, excel files, other media as appropriate — that are intuitively comprehensible for “regular people”, those without the word “data” in their job titles.
Job Responsibilities:
Assemble and maintain a holistic point of view about the client’s mission and operations. Document this perspective using industry standard business analysis methods.
Decompose this holistic perspective into a portfolio of distinct, separable use cases addressable by analytics. Work with client stakeholders to prioritize portfolio elements in terms of mission benefit. Consult data technology peers to estimate technical complexity for each.
Sequentially and incrementally build out information visualization use cases according to an Agile-like delivery methodology. Apply user centered design methods to achieve information visualization solutions that are fit to the way that client stakeholders perform their missions. Make optimum use of data analytics and visualization tools available in client’s tightly security-constrained IT environment.
Respond to ad hoc client needs arising from a dynamic, continually changing operating tempo. Reprioritize work activities on the fly and clearly and proactively communicate the resulting impacts to stakeholders awaiting other deliverables in backlog.
Required Skills:
Must be a US Citizen
Must be able to obtain Public Trust Suitability
Bachelor’s degree (or equivalent experience) in computer science, information systems management, mathematics, engineering, or other relevant discipline.
5+ years front-line technology consulting experience with a proven track record of delivering data analytics work products that address questions and decisions of central importance to clients’ missions.
Fluency in leading approaches to user-centered design (UCD). Implementation and execution of repeatable UCD methods that capture analytics consumers operations and inject relevant information into them.
Experience generating data products and reports that organize data analysis in a consumable format for a broad range of stakeholders, including interactive data visualizations and infographics.
Proven ability in using tools like Tableau, PowerBI, kibana, and Grafana to present mission information in intuitive, useful interfaces. Realization of user journeys and workflows through mission information (data) that allow analytics consumers to quickly navigate through a sequence of questions.
Proficiency with analytics information assembly via relational databases and other structured-information technologies. Fluency in SQL with the ability to quickly construct queries from denormalized and normalized data structures.
Keen understanding of the value of data quality and its importance to effective data analytics. Ability to articulate essential quality aspects of available data to clients and limitations of what can be learned from them.
Desired Skills:
Business and logical data modeling. Ability to recognize mission-essential information in operational activities and procedures. Experience in systematically deconstructing information into distinct domains in the form of conceptual data models.
Conversance with business analytics techniques, particularly descriptive analytics. Familiarity with basic statistical methods. Ability to clearly discern which statistical methods are best suited to a given question, and to articulate limitations in certainty from statistical results.
Project management leadership in an Agile development context. Ability to assemble team member inputs in terms of task scope and duration as well as interdependencies, then incorporate them into sprint and epic plans. Confidence to track work performance progress and communicate risk and issues to client stakeholders.
Business process modeling and mining. Experience with methods and techniques to document operational workflows comprised of interactions between humans and systems. Knowledge in methods to identify mission information exchanges within those workflows and associate them with technical data elements in client’s mission information environment.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3000+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Business Analysis, User Centered Design (UCD), Data Analytics, Dashboard Creation, Data Visualization, Agile Delivery, SQL, Relational Databases, Tableau, PowerBI, Kibana, Grafana, Data Quality, Business Intelligence, Business Process Modeling, Business Process Mining, Agile Development, Project Management","business analysis, user centered design ucd, data analytics, dashboard creation, data visualization, agile delivery, sql, relational databases, tableau, powerbi, kibana, grafana, data quality, business intelligence, business process modeling, business process mining, agile development, project management","agile delivery, agile development, business analysis, business intelligence, business process mining, business process modeling, dashboard creation, data quality, dataanalytics, grafana, kibana, powerbi, project management, relational databases, sql, tableau, user centered design ucd, visualization"
Unstructured Data Analyst,ITAC Solutions,"Alexandria, VA",https://www.linkedin.com/jobs/view/unstructured-data-analyst-at-itac-solutions-3359693007,2023-12-17,Sterling,United States,Associate,Hybrid,"ITAC Solutions is assisting one of our clients in Alexandria, VA, in their search for an
Unstructured Data Analyst
.
What you’ll be doing (duties of this position):
Follow detailed instructions on all work
Complete assigned metadata creation related tasks that are routine in nature
Review documents
Identify metadata information from the documents
Key in the metadata information
Perform other duties as assigned.
What you’ll need to be considered (requirements):
Trained and skilled in overall computer systems and applications (i.e., Microsoft Word, PowerPoint, and Excel)
Meet certification level requirements to the degree defined in the GSA contract
FS08, the individual should have at a minimum, a Bachelor’s Degree with two years of experience
FS09, the individual should have at a minimum, an Associate’s Degree with three years of related experience
FS10, the individual should have at a minimum, an Associate’s Degree with five years of experience.
Hold current United States of America citizenship
Minimum Top Secret security clearance is required to perform this task.
What could set you apart from the rest (preferred skills / experience / knowledge):
Previous military or DOD experience is a plus
Show more
Show less","Microsoft Word, Microsoft PowerPoint, Microsoft Excel, Metadata Creation, Document Review, Data Entry","microsoft word, microsoft powerpoint, microsoft excel, metadata creation, document review, data entry","data entry, document review, metadata creation, microsoft excel, microsoft powerpoint, microsoft word"
Data Scientist - Advana,Zencon Group,"Arlington, VA",https://www.linkedin.com/jobs/view/data-scientist-advana-at-zencon-group-3758132346,2023-12-17,Sterling,United States,Associate,Hybrid,"Currently looking for Secret cleared and higher Data Scientists local to the DC Metro.  On-site 2-3 days/week, some flexibility, but preference given to candidates willing to go on-site more.
Booz Allen is seeking a skilled and highly motivated Data Scientist to join our Advana team. As a Data Scientist, you will play a crucial role in building data pipelines to efficiently collect and process data from various external sources. Your expertise in developing validation and analytics processes to ensure our team is fully developed to provide data accuracy and reliability for executing mission-critical work.
Responsibilities:
Collaborate with analysts and product leads to identify and implement data solutions.
Optimize data pipelines to ensure efficient performance with minimal human intervention.
Build data pipelines for collecting and processing data from various external sources.
Work with data engineers to manage data pipelines and troubleshoot issues to maintain data quality and reliability.
Develop validation and analytics processes that are consistent with business needs and strategic goals.
Assist with machine learning and data visualization initiatives as needed.
Continuously improve data solutions to effectively address customer needs.
Communicate and coordinate with teams to accomplish objectives and deliver excellence.
Requirements:
Secret clearance or higher is required.
Minimum of 4 years with
SQL and modern Big Data ETL technologies like NiFi or StreamSets.
Bachelor's degree in a related field.
Strong background in a modern programming language, such as Python or Java, with at least 4 years of experience working in a big data and cloud environment.
Ability to quickly grasp technical concepts and collaborate with multiple functional groups
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Detail-oriented mindset with a commitment to delivering high-quality results.
Must be in the DC Metro area and available to work onsite (Crystal City, VA and Alexandria, VA) 2-3 days per week.
Nice to Have:
Recent DoD or IC-related experience.
Experience in working in an agile development environment.
Previous experience with Advana is a plus.
Show more
Show less","Data Scientist, Secret Clearance, SQL, Apache NiFi, StreamSets, ETL, Python, Java, Big Data, Cloud, Data Pipelines, Machine Learning, Data Visualization, Bachelor's degree, Agile Development","data scientist, secret clearance, sql, apache nifi, streamsets, etl, python, java, big data, cloud, data pipelines, machine learning, data visualization, bachelors degree, agile development","agile development, apache nifi, bachelors degree, big data, cloud, data scientist, datapipeline, etl, java, machine learning, python, secret clearance, sql, streamsets, visualization"
Privacy Data Analyst,Venable LLP,"Washington, DC",https://www.linkedin.com/jobs/view/privacy-data-analyst-at-venable-llp-3725943397,2023-12-17,Sterling,United States,Associate,Hybrid,"Venable LLP’s Technology & Innovation Group seeks a Privacy Data Analyst to join the Venable Blue team in the Washington, D.C., New York, Los Angeles, or San Francisco office. The Privacy Data Analyst, works on and supports all aspects of client-based projects related to privacy program management, privacy operations, and regulatory response. This is a non-attorney position.
This role will help bring efficiency to how clients respond to regulatory compliance requests, including data gathering, automation of regulatory responses, and measuring operational effectiveness of privacy safeguards. This will include gap analysis and actionable recommendations for improvements in quality and accuracy.
The Privacy Data Analyst will work on and support the delivery of various workstreams such as Safeguard Maintenance, Safeguard Development, Program Management, and Audit Support.
Key responsibilities include working with clients to:
Formulate responses to privacy compliance requests within SLA, perform data quality reviews, and conduct operational effectiveness testing against privacy safeguards.
Develop and communicate best practices for regulatory response.
Develop automation for compliance responses to increase efficiency and to provide consistency and quality in responses.
Articulate data and technology gaps and their compliance impact to a variety of technical and non-technical stakeholders, including product and engineering teams, risk and compliance partners, assessors and regulators.
Recommend process improvements and strategic initiatives as related to privacy compliance response.
Coordinate and drive client privacy response activities for both inbound and outbound relationships.
Support business relationships with internal and external auditors and regulators.
Qualifications:
B.A. or B.S. degree; Master’s degree (or equivalent) preferred
3 years of minimum work experience in high profile settings, such as presenting to leadership and driving cross-functional teams
3 years of minimum experience in a quantitative role related to data, reporting, and analytical problem solving
3 years of minimum experience writing complex SQL queries to drive analysis and insights
Experience coding in Python
Preferred Qualifications:
Experience coding in R, PHP and/or similar programming language
Experience with developing security/privacy reporting and recommendations that are meaningful, defensible and actionable for a variety of audiences
Experience developing and submitting audit and compliance reports to governing bodies, legal entities, and/or external authorities
Experienced in processes for assessing and designing internal controls for large scale organizations
Experience performing risk assessments or safeguard/control operational effectiveness testing
Venable offers full-service solutions to everything from routine to novel privacy and cybersecurity challenges. Our team brings to bear significant experience and industry knowledge to help clients satisfy data privacy and security laws and maximize their business potential. Fully immersed in all aspects of data privacy, cybersecurity, and information governance, Venable is unique among privacy and cybersecurity practices.
We participate in legislative advocacy, rulemakings, and development of new legal standards. Our team advises organizations with regard to industry best practices and drafting codes of conduct and standards, helping them stay compliant with federal, state, international, and self-regulatory requirements.
We strengthen the integrity of our clients’ data, ecommerce security, and customer or user records; develop internal data collection and use practices; and ensure the creation of sound privacy policies and procedures.
Venable Blue helps organizations and individuals manage and mitigate risk in the online space. Whether it’s an issue of data access, account takeover, cyber harassment, child safety, or a government or regulatory investigation, we build, operationalize, and deploy integrated programs and systems designed with people and products in mind.
For additional information about the Technology & Innovation Group, Venable Blue team see: www.venableblue.com
Venable LLP is an
American Lawyer
Global 100 law firm headquartered in Washington, D.C., with offices in California, Delaware, Florida, Illinois, Maryland, New York, and Virginia. Our lawyers and legislative advisors serve domestic and international clients in all areas of corporate and business law, complex litigation, intellectual property, regulatory matters, and government affairs. Additional information can be found at Venable.com.
The salary range for Privacy Data Analyst positions is $85,000 - $130,000 per year. This is the minimum and maximum salary that Venable in good faith believes at the time of this posting that it is willing to pay for the advertised position. Exact compensation will be determined based on individual candidate qualifications and location.
We comply with the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance. Conviction of a crime will not necessarily be a bar to employment at the Firm. Factors such as age at the time of the offense, type of the offense, seriousness of the offense, remoteness of the offense in time, position applied for, rehabilitation, overall record, and other relevant factors will be taken into account in determining effect on suitability for employment.
V
enable LLP is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age, protected Veteran status and any other protected basis under applicable law.
Show more
Show less","Privacy Data Analyst, SQL, Python, R, PHP, Data Analysis, Reporting, Analytical Problem Solving, Regulatory Response, Privacy Compliance, Data Gathering, Automation, Data Quality Reviews, Operational Effectiveness Testing, Process Improvement, Strategic Initiatives, Risk Assessment, Safeguard Testing, Internal Controls, Legislative Advocacy, Rulemakings, Code of Conduct, Corporate Law, Complex Litigation, Intellectual Property, Regulatory Matters, Government Affairs","privacy data analyst, sql, python, r, php, data analysis, reporting, analytical problem solving, regulatory response, privacy compliance, data gathering, automation, data quality reviews, operational effectiveness testing, process improvement, strategic initiatives, risk assessment, safeguard testing, internal controls, legislative advocacy, rulemakings, code of conduct, corporate law, complex litigation, intellectual property, regulatory matters, government affairs","analytical problem solving, automation, code of conduct, complex litigation, corporate law, data gathering, data quality reviews, dataanalytics, government affairs, intellectual property, internal controls, legislative advocacy, operational effectiveness testing, php, privacy compliance, privacy data analyst, process improvement, python, r, regulatory matters, regulatory response, reporting, risk assessment, rulemakings, safeguard testing, sql, strategic initiatives"
Data Analytics Engineer,Eco Financial,"McLean, VA",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-eco-financial-3787934572,2023-12-17,Sterling,United States,Mid senior,Onsite,"Eco Financial is looking for an analytical and detail-oriented research analyst to assist in improving the operations and decision-making of our business. The research analyst's responsibilities include researching, collecting, analyzing, and interpreting data, and using the data to guide the business's decision-making. A research analyst should be prepared to work in various sectors, such as marketing, business operations, and finance.
To be a successful research analyst, you should have excellent mathematical, critical thinking, and communication skills. You should be flexible and able to work independently or in a team.
Responsibilities:
Analyzing past operations' results and performing variance analyses.
Identifying and analyzing trends and forecasts and recommending improvements to the business processes.
Researching market trends, conducting surveys, analyzing data from competitors, and analyzing the business's operations, expenditures, and customer retention to identify patterns of potential issues or improvements.
Using data analysis and interpretations to guide the decision-making of the business.
Using operations data to develop pricing models and identify areas for improvement.
Using statistical, economic, and data modeling techniques and tools.
Organizing and analyzing data, creating charts and graphs, and presenting your findings to the leadership team.
Providing recommendations to improve future business operations.
Organizing and storing data for future research projects.
Testing processes, policies, and protocols for efficiency and improvements.
Requirements:
A bachelor's or associate's degree in economics, finance, statistics, computer science, or related field.
Experience in applied research or data management may be advantageous.
Strong mathematical, analytical, and data modeling skills.
The ability to manipulate large, complex data sets into manageable, understandable reports.
Excellent problem-solving, communication, and team-working skills.
Familiarity with data modeling software and Excel software.
Attention to detail and organizational skills.
Powered by JazzHR
iI30NJICox
Show more
Show less","Data analysis, Data management, Research, Data modeling, Statistics, Economics, Computer science, Excel, Data visualization, Problemsolving, Communication, Teamwork","data analysis, data management, research, data modeling, statistics, economics, computer science, excel, data visualization, problemsolving, communication, teamwork","communication, computer science, data management, dataanalytics, datamodeling, economics, excel, problemsolving, research, statistics, teamwork, visualization"
Data Engineer,Jobs for Humanity,"McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3785372771,2023-12-17,Sterling,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Job Title: Data Engineer** **Location: McLean, Virginia, United States** Are you passionate about using technology to solve complex problems and thrive in a collaborative, inclusive, and fast-paced environment? Join the Capital One team and be part of a group of innovative problem solvers who prioritize meeting customer needs. We are currently looking for a Senior Associate, Data Engineer to drive a major transformation within Capital One's Finance Tech team. **What You'll Do:** - Seek out opportunities to address customer needs and build solutions that solve important problems. - Support the design and development of scalable data architectures and systems for extracting, storing, and processing large amounts of data. - Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity. - Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling. - Implement testing, validation, and pipeline observability to ensure data pipelines meet customer SLAs. **Basic Qualifications:** - Bachelor's Degree. - At least 2 years of application development experience. - At least 1 year of experience in big data technologies. **Preferred Qualifications:** - 3+ years of experience developing data pipelines using Python or Scala. - 2+ years of experience with distributed computing tools like Spark, EMR, and Hadoop. - 2+ years of experience with UNIX/Linux and basic commands and shell scripting. - 1+ years of experience with a public cloud platform like AWS, Microsoft Azure, or Google Cloud. - 1+ years of data warehousing experience with technologies like Redshift or Snowflake. - 1+ years of experience with Agile engineering practices. Please note that at this time, Capital One will not sponsor a new applicant for employment authorization for this position. **Why Join Capital One:** Capital One offers a comprehensive set of health, financial, and other benefits that support your total well-being. Learn more about our benefits on the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. **How to Apply:** To apply for this position, please [click here](link to application page). **Equal Opportunity Employer:** Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We welcome applicants from all backgrounds and do not discriminate based on sex, race, age, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state, or local law. As a company, we promote a drug-free workplace and will consider applicants with a criminal history in accordance with applicable laws. **Accessibility Accommodations:** If you require an accommodation to access our website or to apply for a position, please contact Capital One Recruiting at 1-800-304-9102 or email RecruitingAccommodation@capitalone.com. We will ensure that any accommodations needed are provided in a confidential manner and used only as required to provide needed reasonable accommodations. **Technical Support:** For technical support or questions regarding our recruiting process, please email Careers@capitalone.com. **Disclaimer:** Capital One does not provide, endorse, or guarantee third-party products, services, educational tools, or other information available through this site. Please note that positions posted in Canada are for Capital One Canada, positions posted in the United Kingdom are for Capital One Europe, and positions posted in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Python, Scala, Spark, EMR, Hadoop, UNIX/Linux, AWS, Microsoft Azure, Google Cloud, Redshift, Snowflake, Agile","python, scala, spark, emr, hadoop, unixlinux, aws, microsoft azure, google cloud, redshift, snowflake, agile","agile, aws, emr, google cloud, hadoop, microsoft azure, python, redshift, scala, snowflake, spark, unixlinux"
Sr Engineer - Data Science,Sonitalent Corp,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-at-sonitalent-corp-3718978162,2023-12-17,Sterling,United States,Mid senior,Onsite,"Role: Sr Engineer - Data Science
Location: Gaithersburg , MD Onsite 5x weekly (NEED TO LIVE IN A 15-20M RADIUS OF Gaithersburg MD)
Highlights in Yellow candidate must have
Required
Resume can be no longer than 2 pages
Needs to have degree/name of university/and year of graduation
Place of Birth
Conversion salary after 6m
Do not send any candidate that are not living in Fredrick or Montgomery County Maryland- or 15/20m radius of Gaithersburg Md
Needs to be a Senior candidate.
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Experience working across varying business and technical functional units
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Python, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Git, AWS, Google Cloud, MySQL, BigQuery, Tableau, Shiny","data science, machine learning, artificial intelligence, python, sql, pandas, scikitlearn, tensorflow, keras, git, aws, google cloud, mysql, bigquery, tableau, shiny","artificial intelligence, aws, bigquery, data science, git, google cloud, keras, machine learning, mysql, pandas, python, scikitlearn, shiny, sql, tableau, tensorflow"
"Looking for AWS Data Engineer - Reston, VA( Once in a Month) - Contract",Extend Information Systems Inc.,"Reston, VA",https://www.linkedin.com/jobs/view/looking-for-aws-data-engineer-reston-va-once-in-a-month-contract-at-extend-information-systems-inc-3755516291,2023-12-17,Sterling,United States,Mid senior,Onsite,"Hi,
I hope you are doing well!
We have an opportunity for
AWS Data Engineer
with one of our clients for
Reston, VA( Once in a Month)
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
AWS Data Engineer
Location: Reston, VA( Once in a Month)
Terms:
Contract
Job Description
And try for AWS Certified Profile(Good to Have).
Essential Skills
Strong Python development experience with Python, Pandas , NumPy, Pyspark
Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
Having prior working experience in Fannie Mae will be added advantage
Good Knowledge on Java and Database (Oracle Postgres)
Essential Qualification
Bachelors is a must
Thanks & Regards
Priyanka tiwari
Extend Information System Inc
Phone: (703) 956-1120
Email: priyanka1@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Show more
Show less","Python, Pandas, NumPy, PySpark, AWS, S3, RDS, EC2, Lambda, SQS, SNS, Redshift, Java, Oracle, Postgres, AWS Certified Profile","python, pandas, numpy, pyspark, aws, s3, rds, ec2, lambda, sqs, sns, redshift, java, oracle, postgres, aws certified profile","aws, aws certified profile, ec2, java, lambda, numpy, oracle, pandas, postgres, python, rds, redshift, s3, sns, spark, sqs"
AWS Data Engineer with Splunk,Futran Solutions,"Reston, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-with-splunk-at-futran-solutions-3687059599,2023-12-17,Sterling,United States,Mid senior,Onsite,"Role : AWS Data Engineer with Splunk
Location: Reston, VA( Once in a month)
Experience : 10+yrs
Job Description
Please don't Submit any Profile without Splunk exp.
AWS Data Engineer with splunk
Strong Python, AWS skills and expereince with Splunk development.
Advanced knowledge of AWS Services/Architecture
Experience in AWS Compute such as EC2, Lambda, Beanstalk, Batch or ECS
Experience with AWS Storage services such as: S3, EFS, Glacier.
Experience in AWS Management and Governance suite of products such as CloudTrail, CloudWatch
Experience in AWS Analytics such as Athena, EMR, Glue, Redshift, Kinesis
Strong knowledge in Python object-oriented programming
Strong experience with AWS Database services such as: RDS, DynamoDB
Experience using APIs for developing or programming software
Experience using AWS Application Integration Services such as: Simple Notification Service (SNS), Simple Queue Service (SQS), Step Functions.
Experience with AWS Developer tools such as: CodeDeploy, CodePipeline
Experience with JSON
Strong experience with SQL
Experience with enterprise data lakes, data warehouses, data marts, and big data.
Strong experience with data migration, cloud migration, and ETL processes.
Experience determining causes of operating errors and taking corrective action
Show more
Show less","Splunk, Python, AWS, EC2, Lambda, Beanstalk, Batch, ECS, S3, EFS, Glacier, CloudTrail, CloudWatch, Athena, EMR, Glue, Redshift, Kinesis, Objectoriented programming, RDS, DynamoDB, APIs, SNS, SQS, Step Functions, CodeDeploy, CodePipeline, JSON, SQL, Data lakes, Data warehouses, Data marts, Big data, Data migration, Cloud migration, ETL processes","splunk, python, aws, ec2, lambda, beanstalk, batch, ecs, s3, efs, glacier, cloudtrail, cloudwatch, athena, emr, glue, redshift, kinesis, objectoriented programming, rds, dynamodb, apis, sns, sqs, step functions, codedeploy, codepipeline, json, sql, data lakes, data warehouses, data marts, big data, data migration, cloud migration, etl processes","apis, athena, aws, batch, beanstalk, big data, cloud migration, cloudtrail, cloudwatch, codedeploy, codepipeline, data lakes, data marts, data migration, data warehouses, dynamodb, ec2, ecs, efs, emr, etl, glacier, glue, json, kinesis, lambda, objectoriented programming, python, rds, redshift, s3, sns, splunk, sql, sqs, step functions"
Database Engineer,Red Arch Solutions,"Chantilly, VA",https://www.linkedin.com/jobs/view/database-engineer-at-red-arch-solutions-3787773126,2023-12-17,Sterling,United States,Mid senior,Onsite,"Red Arch Solutions (RAS)
is building a DIA Enterprise Data Platform (EDP) that will facilitate the transition and retirement of duplicative and legacy data services and provide an automated, modernized, enterprise data hub within a secure environment. EDP will enable DIA to expose its data to a wider audience through a zero-trust based, data centric architecture. Based on a robust data governance policy approach, EDP will free data from siloes and allow more users to gain valuable insights from DIA data assets. RAS will maintain and improve the EDP to ensure DIA has high data availability, accessibility, veracity, integration, and discoverability while enabling data analytics, visualization, security, governance, and management across the enterprise.
Basic Job Responsibilities:
Data Onboarding Architecture: Data Engineers design and create the overall data onboarding architecture for DDH. They work closely with stakeholders to understand their data requirements and translate them into scalable data models and structures.
Data Analysis: Inspect data with the goal of discovering useful information needed to create data schemas.
Data Pipeline Development: Data Engineers build data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses. They develop efficient and reliable ETL processes to ensure data quality, integrity, and timeliness.
Data Integration: Data Engineers integrate disparate data sources and systems, both internal and external, to enable seamless data flow across different platforms. They may work with APIs, databases, cloud services, and third-party tools to establish data connections and ensure smooth data integration.
Data Transformation and Processing: Data Engineers transform raw data into a format suitable for analysis or consumption. This involves cleaning, filtering, aggregating, and structuring the data using tools like SQL, Python.
Data Quality and Governance: Ensuring data quality and integrity is a crucial responsibility of Data Engineers. They implement data quality checks, data validation rules, and data governance practices to maintain accurate and consistent data throughout the data lifecycle.
Scalability and Performance: Data Engineers optimize data pipelines and infrastructure for scalability and performance.
Data Security: Data Engineers play a role in securing data assets by validating security policies are implemented for the data.
Monitoring and Troubleshooting: Data Engineers monitor data pipelines and systems to identify and resolve issues proactively.
Collaboration: Data Engineers collaborate with internal DDH teams and cross-functional teams (CDO, MARs).
Customer Engagement: Engage in regular meetings and communication channels with Data Stewards and Data Steward Representatives to gather information needed to appropriately understand and secure their data in preparation for data onboarding.
Provide “white glove” service to Data Stewards for data onboarding questionnaire assistance.
Training and User Support: Provide training to Data Stewards and end users on DDH UI.
Documentation: Author or update documentation such as the Data Onboarding Execution Plan, data onboarding Standard Operating Procedure, and any associated DDH documentation (CONOPS, CDD).
Data Engineering Skills:
Data Modeling and Database Design: Proficiency in designing efficient and scalable data models, understanding relational and non-relational databases, and knowledge of SQL
ETL and Data Integration: Experience with Extract, Transform, Load (ETL) processes, data integration techniques, and tools like Apache NiFi.
Programming and Scripting: Strong programming skills in languages such as Python.
Data Warehousing: Understanding of data warehousing concepts and experience with data warehouse technologies.
Data Pipeline and Workflow Tools: Proficiency in tools for building and managing data pipelines and workflows.
Data Streaming: Knowledge of real-time data streaming technologies such as Apache Kafka for processing and analyzing streaming data.
Data Quality and Governance: Understanding of data quality frameworks, data validation techniques, and experience implementing data governance practices.
Cloud Platforms: Familiarity with cloud platforms like Amazon Web Services (AWS and their relevant data services.
Data Security: Knowledge of data security best practices, data encryption techniques, and understanding of privacy regulations.
Data Visualization: Ability to work with data visualization tools like Tableau, Power BI, or QlikView to create meaningful visualizations and reports.
Problem Solving and Troubleshooting: Strong analytical and problem-solving skills to identify and resolve issues in data pipelines, databases, or data infrastructure.
Collaboration and Communication: Effective communication skills to collaborate with cross-functional teams, understand requirements, and present technical solutions to non-technical individuals.
Continuous Learning: A mindset of continuous learning and keeping up-to-date with evolving technologies, tools, and industry trends in the field of data engineering.
Qualifications:
Bachelor Degree + 12 years or Master Degree + 10 years experience or 16 years of experience.
Experience developing requirements based on needs obtained from users.
Experience building scripts to manipulate data for ingestion into databases.
TS/SCI with Poly is required for this position**
All Red Arch Solutions openings require US Citizenship.
Red Arch Solutions is a proven and effective small business integrator and consultant, recognized as a leading provider of IT development to the Federal Government. We offer excellent benefits, including 20 days PTO, 10 holidays, up to 10% 401k contribution, and reimbursement for tuition/certifications. Top of the line PPO Medical, Dental, Vision, and Short and Long-Term disability are also offered.
Red Arch Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, national origin, age, marital status, disability, or protected veteran status. Red Arch Solutions takes affirmative action in support of its policy to advance in employment individuals who are minorities, women, protected veterans and individuals with disabilities.
Powered by JazzHR
09kC53NTiZ
Show more
Show less","Data Engineering, Data Modeling, Database Design, SQL, ETL, Data Integration, Apache NiFi, Python, Data Warehousing, Data Pipeline Tools, Workflow Tools, Data Streaming, Apache Kafka, Data Quality, Data Governance, Data Validation, Data Security, Data Encryption, Privacy Regulations, Data Visualization, Tableau, Power BI, QlikView, Problem Solving, Troubleshooting, Collaboration, Communication, Continuous Learning","data engineering, data modeling, database design, sql, etl, data integration, apache nifi, python, data warehousing, data pipeline tools, workflow tools, data streaming, apache kafka, data quality, data governance, data validation, data security, data encryption, privacy regulations, data visualization, tableau, power bi, qlikview, problem solving, troubleshooting, collaboration, communication, continuous learning","apache kafka, apache nifi, collaboration, communication, continuous learning, data encryption, data engineering, data governance, data integration, data pipeline tools, data quality, data security, data streaming, data validation, database design, datamodeling, datawarehouse, etl, powerbi, privacy regulations, problem solving, python, qlikview, sql, tableau, troubleshooting, visualization, workflow tools"
Big Data Engineer BDE21-1,"DataSync Technologies, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/big-data-engineer-bde21-1-at-datasync-technologies-inc-3361817635,2023-12-17,Sterling,United States,Mid senior,Onsite,"We are in search of a Big Data Engineer who can seamlessly mesh tech know-how with business acumen to help us navigate our clients big data computing needs. Not only are you experienced in cloud-based tech, but you have a firm grasp of platforms and applications, and have knowledge of how best to customize these attributes to our customers business demands. Not only can you help us connect the dots and evolve our capabilities over time, but you are also evolving your own capabilities as an engineer and keeping an eye on developments.
Big Data Engineer Responsibilities
Gather and process raw data at scale.
Design and develop data applications using selected tools and frameworks as required and requested.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.
Process unstructured data into a form suitable for analysis.
Monitoring data performance and modifying infrastructure as needed.
Define data retention policies.
Big Data Engineer Requirements
Bachelors Degree or more in Computer Science or a related field.
A solid track record of data management showing your flawless execution and attention to detail.
Programming experience, ideally in Python, Spark, Kafka or Java , and a willingness to learn new programming languages to meet goals and objectives.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
A willingness to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.
Experience with Data Lakes a plus.
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.
_______________________
Interested in Joining Our Team?
- Check out this YouTube video!
Be a part of an award-winning, employee friendly company in Northern VA and have the satisfaction of helping keep America safe. DataSync Technologies, Inc is a veteran owned small business providing consulting excellence and real time solutions for customers with complex information technology needs within Intelligence Community. Our cleared consultants bring real world experience with a common sense approach to their jobs whether they are creating complex analytic dashboards, architecting new cloud technology infrastructures, securing sensitive data or streamlining business processes for efficiency.
Equal Employment Opportunity
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at the Department of Labor's Website .
DataSync is committed to providing veteran employment opportunities to our service men and women.
Show more
Show less","Big Data, Cloud Computing, Data Mining, Data Lake, Python, Spark, Kafka, Java, SQL, Data Retention Policies, Data Management, Data Processing, Data Innovations, Data Analysis, Web Scraping","big data, cloud computing, data mining, data lake, python, spark, kafka, java, sql, data retention policies, data management, data processing, data innovations, data analysis, web scraping","big data, cloud computing, data innovations, data lake, data management, data mining, data processing, data retention policies, dataanalytics, java, kafka, python, spark, sql, web scraping"
Data Engineer ETL Expert (2021-0140),Acclaim Technical Services,"Chantilly, VA",https://www.linkedin.com/jobs/view/data-engineer-etl-expert-2021-0140-at-acclaim-technical-services-3787773691,2023-12-17,Sterling,United States,Mid senior,Onsite,"Acclaim Technical Services, founded in 2000, is a leading language and intelligence services company supporting a wide range of U.S. Federal agencies. We are an Employee Stock Ownership Plan (ESOP) company, which is uncommon within our business sector. We see this as a significant strength, and it shows: ATS is consistently ranked as a top workplace among DC area firms and continues to grow.
We are actively hiring a
Data Engineer (ETL) Expert with TS/SCI clearance and polygraph
to join our team working in Chantilly, Virginia. In this position, you will manipulate data and data flows for both existing and new systems. You will also provide support in data extraction, transformation and load (ETL), data mapping, analytical support, operational support, and maintenance support of data and associated systems. As a member of this team, you will work in a fast-paced, dynamic team-based environment.
Responsibilities
Research, design, develop and/or modifies enterprise-wide systems and/or application software.
Develop complex data flows, or makes significant enhancements to existing pipelines.
Resolves complex hardware/software compatibility and interface design considerations.
Conducts investigations and tests of considerable complexity.
Researches emerging technologies to determine impact on application execution.
Provides input to staff involved in writing and updating technical documentation.
Troubleshoots complex problems and provides customer support for the ETL process
Advises hardware engineers on machine characteristics that affect software systems, such as storage capacity, processing speed, and input/output requirements.
Required Education & Experience
Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
8-10 years of related software engineering and ETL experience.
Experience building and maintaining data flows in NiFi or Pentaho.
Excellent organizational, coordination, interpersonal and team building skills.
Familiarization with NoSQL datastores
Familiarization executing jobs in Big Data Technologies (i.e., Hadoop or Spark)
Excellent organizational, coordination, interpersonal and team building skills.
Familiarization with NoSQL datastores
Familiarization executing jobs in Big Data Technologies (i.e., Hadoop or Spark)
Desired Experience
Experience with the following languages: Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML/XHTML, CSS, Python, Shell Scripting, JSON
Knowledge of servers operating systems; Windows, Linux, Distributed Computing, Blade Centers, and cloud infrastructure
Strong problem solving skills
Ability to comprehend database methodologies
Focus on continual process improvement with a proactive approach to problem solving
Ability to follow directions and finish task
Equal Employment Opportunity / Affirmative Action
ATS is committed to a program of equal employment opportunity without regard to race, color, ethnicity, national origin, ancestry, citizenship, sex, pregnancy, marital status, sexual orientation, gender identity, age, religion/creed, hairstyles and hair textures, handicap/disability, genetic information/history, military/veteran status, or any other characteristic or condition protected by federal, state or local law. It is the policy of ATS not merely to refrain from employment discrimination as required by the various federal, state, and local enactments, but to take positive affirmative action to realize for women, people of color, individuals with disabilities and protected veterans full equal employment opportunity. We support the employment and advancement in employment of individuals with disabilities and of protected veterans, and we treat qualified individuals without discrimination on the basis of their physical or mental disability or veteran status.
Powered by JazzHR
yto09kmPBt
Show more
Show less","Data Engineering, ETL, Data Extraction, Data Transformation, Data Loading, Hadoop, Spark, Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML, XHTML, CSS, Python, Shell Scripting, JSON, Windows, Linux, Distributed Computing, Blade Centers, Cloud Infrastructure, NoSQL","data engineering, etl, data extraction, data transformation, data loading, hadoop, spark, javaj2ee, c, c, sql, xml, xquery, xpath, ruby on rails, html, xhtml, css, python, shell scripting, json, windows, linux, distributed computing, blade centers, cloud infrastructure, nosql","blade centers, c, cloud infrastructure, css, data engineering, data extraction, data loading, data transformation, distributed computing, etl, hadoop, html, javaj2ee, json, linux, nosql, python, ruby on rails, shell scripting, spark, sql, windows, xhtml, xml, xpath, xquery"
Senior Data Engineer,"IT Concepts, Inc","Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-it-concepts-inc-3767063622,2023-12-17,Sterling,United States,Mid senior,Onsite,"Description
Founded in 2003, IT Concepts’ core values – customer-centricity, teamwork, driven to deliver, innovation, and integrity – ensure we work together to be the best, realize objectives, and make a positive impact in our communities. We intentionally created and sustain our ITC culture that embraces change, experimentation, continuous learning, and improvement. We bring our design thinking problem solving approach that challenges assumptions, prioritizes curiosity, and invites complexity to deliver innovative, efficient, and effective solutions. As we continue to grow in the support of our government customers, we are looking for driven and innovative individuals to join our team.
IT Concepts is looking for an experienced Senior-Level Data Engineer to support our intelligence customer. The Data Engineer and team will also provide the following functions:
Design and develop end-to-end data pipelines in SQL Server database; oversee build, implementation, and optimization of these pipelines.
Monitor and ensure data pipelines meet performance, availability, and quality standards.
Implement data orchestration solutions for data pipelines.
Manipulate large volumes of data into formats suitable for analytics and reporting.
Adhere to data governance policies and best practices for protecting data.
Research, analyze, and document various technical approaches (including implementation, benefits, constraints, outcomes, and associated risks).
Collaborate with data architect/database administrator to address and resolve problems in the case of failed jobs.
Develop metadata documentation and architecture documents.
Engage with technical and functional program subject matter experts and stakeholders to understand business processes, data requirements, critical data elements, data mappings, data protection guidelines, etc.
Provide recommendations for improvements in data strategies, including governance, integration, and standardization changes.
Assist in developing briefing packages to communicate complex ideas to non-technical customers.
Location: Work location is JBAB, DC or Reston, VA
Requirements
Bachelor’s degree in computer science, mathematics, engineering, information systems, or related.
7+ years of experience as a data engineer (e.g., building data pipelines, coding best practices, maintaining data quality)
Demonstrated experience with ETL processes and concepts in relational (SQL) database management systems.
Expertise in Python and/or SQL
Proficient in Agile development, git operations
Experience translating requirements into technical tasks and developing data structures to support the generation of business insights and strategy.
Innovative problem-solving and root cause identification skills.
Knowledge of data modeling
Experience with data orchestration tools (e.g., Dagster, Prefect, Apache Airflow)
Experience communicating and presenting data to non-technical stakeholders, written and verbally.
Must be self-directed, detail-oriented, and able to coordinate and work with a dynamic team and multiple stakeholders.
Clearance requirements:
Must have an active TS/SCI at the time of hire, and willing/able to get CI Polygraph
Benefits
The Company
We believe in generating success collaboratively, enabling long-term mission success, and building trust for the next challenge. With you as our partner, let’s solve challenges, think innovatively, and maximize impact. As a valued member of our ITC community, you have the unique opportunity to work in a diverse range of technology and business career paths, all while supporting our nation and delivering innovative technology solutions. We are a close community of experts that pride ourselves on creating an environment defined by teamwork, dedication, and excellence.
We hold three ISO certifications (27001:2013, 20000-1:2011, 9001:2015) and two CMMI ML 3 ratings (DEV and SVC).
Industry Recognition
Growth | Inc 5000’s Fastest Growing Private Companies, DC Metro List Fastest Growing; Washington Business Journal: Fastest Growing Companies, Top Performing Small Technology Companies in Greater D.C.
Culture | Northern Virginia Technology Council Tech 100 Honoree; Virginia Best Place to Work; Washington Business Journal: Best Places to Work, Corporate Diversity Index Winner – Mid-Size Companies, Companies Owned by People of Color; Department of Labor’s HireVets for our work helping veterans transition; SECAF Award of Excellence finalist; Victory Military Friendly Brand; Virginia Values Veterans (V3); Cystic Fibrosis Foundation Corporate Breath Award
Benefits
We offer great benefits – Competitive Paid Time Off, Medical, Dental and Vision Insurance, Identity Protection, Pet Insurance, 401(k) with company matching.
We invest in our employees – Every employee is provided with a stipend to invest in certifications, a master’s degree, or even a doctorate. We want you to grow as an expert and a leader and offer flexibility for you to take a course, a certification, or attend a conference. We are committed to supporting your curiosity and sustaining a culture that prioritizes commitment to continuous professional development.
We work hard, we play hard. ITC is committed to injecting fun into every day. We dedicate funds for activities – virtual and in-person – e.g., we have four season tickets to Nationals games that are available every month, we host happy hours, holiday events, fitness events, and annual celebrations. In alignment with our commitment to our communities, we host and attend charity galas/events. We believe in appreciating your commitment and building a positive workspace for you to be creative, innovative, and happy.
AAEO & VEVRAA
IT Concepts is an Affirmative Action/Equal Opportunity employer and a VEVRAA (Vietnam Era Veterans' Readjustment Assistance Act) Federal Contractor. As such, any personnel decisions (hire, promotion, job status, etc.) on applicants and/or employees are based on merit, qualifications, competence and business needs, not on race, color, citizenship status, national origin, ancestry, , sexual orientation, gender identity, age, religion, creed, physical or mental disability, pregnancy, childbirth or related medical condition, genetic information of the employee or family member of the employee, marital status, veteran status, political affiliation, or any other factor protected by federal, state or local law.
IT Concepts maintains a strong commitment to compliance with VEVRAA and other applicable federal, state, and local laws governing equal employment opportunity. We have developed comprehensive policies and procedures to ensure that our hiring practices align with these requirements.
As a part of our VEVRAA compliance efforts, [Company Name] has established an affirmative action plan that outlines our commitment to the recruitment, hiring, and advancement of protected veterans. This plan is regularly reviewed and updated to ensure its effectiveness.
We encourage protected veterans to self-identify during the application process. This information is strictly confidential and will only be used for reporting and compliance purposes as required by law. Providing this information is voluntary, and it will not impact your eligibility for employment.
Our commitment to equal employment opportunity extends beyond legal compliance. We are dedicated to fostering an inclusive workplace where all employees, including protected veterans, are treated with dignity, respect, and fairness.
How To Apply
To apply to IT Concept Position- Please click on the: “Apply for this Job” button at the bottom of this Job Description or the button at the top: “Application.” You can upload your resume and complete all the application steps. You must submit the application for IT Concepts to receive. If you need alternative application methods, please email careers@useitc.com and request assistance.
Accommodations
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.
Show more
Show less","SQL, Python, Data pipelines, Data governance, Data orchestration, Data modeling, ETL processes, Data analysis, Data strategizing, Agile development, Git, Cloud computing, Presentation skills","sql, python, data pipelines, data governance, data orchestration, data modeling, etl processes, data analysis, data strategizing, agile development, git, cloud computing, presentation skills","agile development, cloud computing, data governance, data orchestration, data strategizing, dataanalytics, datamodeling, datapipeline, etl, git, presentation skills, python, sql"
Expert Data Engineer,ANSER,"Washington, DC",https://www.linkedin.com/jobs/view/expert-data-engineer-at-anser-3693335224,2023-12-17,Sterling,United States,Mid senior,Onsite,"Company Overview
ANSER enhances national and homeland security by strengthening public institutions. We provide thought leadership for complex issues through independent analysis, and we deliver practical, useful solutions. ANSER values collaboration, integrity, and initiative and we are client focused in all that we do. Because we were established for the purpose of public service and not for profit, we measure our success in the impact of our service.
Job Description
ANSER is seeking an Expert Data Engineer (Operational Suitability Specialist) to support programs at (Joint Base Anacostia-Bolling JBAB).
Responsibilities
Designs, implements, and operates data management systems for intelligence needs. Designs how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems. Works with data users to determine, create, and populate optimal data architectures, structures, and systems. Plans, designs, and optimizes data throughput and query performance. Participates in the selection of backend database technologies (e.g., SQL, NoSQL, HPC, etc.), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.
Experience providing operational suitability test support to test programs or support DIA in testing procedure development for externally funded developments.
Possesses extensive logistics and suitability experience, particularly in operations analysis and support, logistics, and maintenance.
Possesses a minimum 4 years of academic course work w/ current logistics/suitability experience with space systems or 6 years practical knowledge of space systems ground systems, ground segments, manufacturing and maintenance, and mission planning.
Experience serving as the suitability expert for issues relating to reliability, availability, compatibility, transportability, interoperability, maintainability, safety, human factors, manpower supportability, logistics supportability, environmental effects, system documentation, and training requirements; contribute to all facets of test design and planning using in-depth knowledge of system under test, test policies, and test range capabilities.
Experience assisting the Test Director in timely preparation of high-quality suitability analysis across the varied test venues during all test phases to inform system development and operational evaluation; developing space system suitability specific data collection, data processing, data evaluation and data reporting as well as proactively identify suitability gaps, risks, or other issues across several test programs.
Qualifications
Current TS/SCI with ability to pass a polygraph.
Minimum 20 years of experience conducting analysis relevant to the specific labor category with at least a portion of the experience within the last 2 years.
Master’s degree in an area related to the labor category from a college or university accredited by an agency recognized by the U.S. Department of Education.
Disclaimer
In compliance with the Americans with Disabilities Act Amendment Act (ADA), if you have a disability and would like to request an accommodation in order to apply for a position with ANSER, please call 703-416-2000 or e-mail Recruiting@anser.org.ANSER is proud to be an Equal Opportunity Employer. We seek individuals from a broad variety of backgrounds with varying levels of experience who have a desire to do meaningful work. We recruit, employ, train, compensate, and promote regardless of race, color, gender, religion, national origin, ancestry, disability, age, veteran status, sexual orientation, or any other characteristic protected by law.
Show more
Show less","SQL, NoSQL, HPC, Data management systems, Data architectures, Data structures, Data systems, Data throughput, Query performance, Backend database technologies, ETL, Test design, Test planning, Test policies, Test range capabilities, Data collection, Data processing, Data evaluation, Data reporting, Suitability analysis, System development, Operational evaluation, Reliability, Availability, Compatibility, Transportability, Interoperability, Maintainability, Safety, Human factors, Manpower supportability, Logistics supportability, Environmental effects, System documentation, Training requirements","sql, nosql, hpc, data management systems, data architectures, data structures, data systems, data throughput, query performance, backend database technologies, etl, test design, test planning, test policies, test range capabilities, data collection, data processing, data evaluation, data reporting, suitability analysis, system development, operational evaluation, reliability, availability, compatibility, transportability, interoperability, maintainability, safety, human factors, manpower supportability, logistics supportability, environmental effects, system documentation, training requirements","availability, backend database technologies, compatibility, data architectures, data collection, data evaluation, data management systems, data processing, data reporting, data structures, data systems, data throughput, environmental effects, etl, hpc, human factors, interoperability, logistics supportability, maintainability, manpower supportability, nosql, operational evaluation, query performance, reliability, safety, sql, suitability analysis, system development, system documentation, test design, test planning, test policies, test range capabilities, training requirements, transportability"
Sr. Software/Systems/Data Engineer - SixMap (DC Area),DataTribe,"Washington, DC",https://www.linkedin.com/jobs/view/sr-software-systems-data-engineer-sixmap-dc-area-at-datatribe-3787755630,2023-12-17,Sterling,United States,Mid senior,Onsite,"Do you want to help build the next generation network security solution?
SixMap is working on leading edge network intrusion detection technology that enables enterprises and network operators to gain insights into their complete network attack surface and identify network vulnerabilities at unheard of speed and comprehensiveness. SixMap’s platform can complete IPv4 scans with deep and configurable service interrogation that is orders of magnitude faster than anything currently available. The team is building the world’s first platform to perform comprehensive IPv6 scans, previously thought to be impossible. Position Summary: We are looking for a systems-oriented senior software engineer to help build the core network mapping and interrogation engine. Candidates should have deep hands-on experience working on complex systems, data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking. The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development.
Come join us, if you are ready to change the world of network security while having some fun along the way.
Position Requirements:
To be considered for this position, you must:
Have at least 5 years’ experience and have a passion in understanding users’ needs and system requirements and turning them into working software
Have a BS degree or higher in computer science, electrical/computer engineering, or related technical field or equivalent work experience
Be fully fluent in Python and common data analysis Python libraries, C++, SQL, Airflow or other ETL / data pipeline tools, and preferably be a polyglot comfortable in many additional programming languages.
Be experienced in using relational databases and NoSQL data stores - PostgreSQL experience is a must.
Be experienced with Linux environments.
Have experience working on container-based cloud infrastructure frameworks such as Docker or Kubernetes within common cloud service providers such as AWS, GCP, or Azure.
Be experienced using Agile methodologies, operating cloud dev-ops, and coordinating with product development teams
Have the ability to thrive when presented a complex challenge in a fast-paced, performance-oriented culture with intelligent people
Have exceptional level of integrity, raw intelligence, creativity, energy, and passion
Operate efficiently with individual responsibility in a highly collaborative environment
Powered by JazzHR
UjAcy3uGZM
Show more
Show less","TCP/IP, Python, C++, SQL, Data Analysis, ETL, Airflow, PostgreSQL, NoSQL, Linux, Docker, Kubernetes, AWS, GCP, Azure, Agile Methodologies, Cloud DevOps","tcpip, python, c, sql, data analysis, etl, airflow, postgresql, nosql, linux, docker, kubernetes, aws, gcp, azure, agile methodologies, cloud devops","agile methodologies, airflow, aws, azure, c, cloud devops, dataanalytics, docker, etl, gcp, kubernetes, linux, nosql, postgresql, python, sql, tcpip"
Data Engineer/DBA,Cohere Technology Group,"Bethesda, MD",https://www.linkedin.com/jobs/view/data-engineer-dba-at-cohere-technology-group-3787903680,2023-12-17,Sterling,United States,Mid senior,Onsite,"Cohere is seeking fully cleared, dynamic technical professionals to join our team as a Member of an agile team you will work with the government customer to help shape a new phase of applications into the cloud environment. You will work closely with multiple contractor teams and the customer to transform approved requirements into features during each sprint. You will help to develop the new suite of applications one sprint at a time. As a Database Administrator/ Engineer, you will deliver services within a cloud environment, while working within an Agile project team. You will also be responsible for participating in collaborative discussions and helping to establish DB administration norms and practices to ensure consistency across the multiple development and integration efforts.
As a Database Administrator/ Engineer you will be responsible for the following:
Providing technical expertise and support in the use of PostgreSQL DBMS
Creating and maintaining database objects such as tables, views, index, constraints for data platform and custom applications
Developing, implementing, and maintaining database backup and recovery procedures for the processing environments, and ensures that data integrity, security, and recoverability are built into the DBMS applications.
Creating views to drive front-end data analytics and reporting.
Driving end-to-end availability, performance monitoring, and capacity planning for PostgreSQL, using native and 3rd party tools
Implementing automated methods and industry best practices for consistent installation and configuration of PostgreSQL for production, pre-production, and non-production environments
Administering PostgreSQL databases throughout the non-production (Sandbox, development, test, UAT, Training), pre-production (Staging) and production lifecycles in an AWS environment
Enhancing and maintaining logical and physical database designs from application requirements
Designing and implementing automation using SQL scripts and database functions
Responding to user reported errors in a timely manner.
Monitoring systems performance and identifying problems that may arise.
Liaison with developers and IT project managers
Recommending changes and enhancements for database performance and protection
Maintaining an awareness of trends and developments in database development and administration
Leveraging experience to provide support in the areas of data mapping, data extraction, transformation, and loading (ETL)
The ideal Database Administrator/ Engineer candidate should demonstrate the following skills:
Active TS/SCI w/FSP
Bachelor’s degree in computer science, software engineering, or related field of study
6+ years of experience in administering on-prem/cloud based multiuser environment with expertise in planning, designing, building, and implementing complex database systems.
Experience with AWS RDS or Aurora for PostgreSQL
Experience with optimizing database performance through exhaustive proactive testing and ongoing real-time monitoring
Experience with SQL scripting and database functions
Experience testing database security and enterprise policy guidelines according to established best practices.
Monitored and maintained the backups for database recoverability for all production databases.
Experience working in the database domain managing all operations of RDBMS databases such as Oracle/SQL Server/DB2 or PostgreSQL DB.
Familiar with open-source DBMS tools such as pgAdmin, pgModeler, or DBeaver.
Experience with data modeling and Entity-Relationship Diagrams (ERDs)
Experience with Structured Query Language (SQL)
Possess analytic problem-solving skills.
Knowledge of computer software applications, including database technologies and programming languages
Excellent verbal and written communication skills
Additional desired skills for the Database Administrator/ Engineer candidate include the following:
Experience implementing and deploying databases in Amazon Web Services (AWS)
Experience with AGILE / DEVOPS a plus
Working knowledge of COTS tools supporting data governance.
Experience with Financial Management Systems.
Powered by JazzHR
hsD9u2YHCB
Show more
Show less","PostgreSQL, SQL, AWS RDS, Aurora, pgAdmin, pgModeler, DBeaver, EntityRelationship Diagrams (ERDs), Structured Query Language (SQL), Amazon Web Services (AWS), AGILE, DEVOPS, COTS tools, Financial Management Systems","postgresql, sql, aws rds, aurora, pgadmin, pgmodeler, dbeaver, entityrelationship diagrams erds, structured query language sql, amazon web services aws, agile, devops, cots tools, financial management systems","agile, amazon web services aws, aurora, aws rds, cots tools, dbeaver, devops, entityrelationship diagrams erds, financial management systems, pgadmin, pgmodeler, postgresql, sql, structured query language sql"
Azure Cloud Data Engineer,"Harmonia Holdings Group, LLC","McLean, VA",https://www.linkedin.com/jobs/view/azure-cloud-data-engineer-at-harmonia-holdings-group-llc-3706226281,2023-12-17,Sterling,United States,Mid senior,Remote,"We have an exciting opportunity for a
Azure
Cloud Data Engineer
to join our team.
Design and implement cloud data solutions on Microsoft Azure, migrating on-premises data to cloud data warehouses like Azure Synapse Analytics, and Azure Databricks.
Build data pipelines to move data between a variety of sources and sinks such as SQL Server, Azure Data Lake Storage, and Cosmos DB using tools like Azure Data Factory.
Develop Python scripts and notebooks for ETL processes, data transformations, and visualization using services like Databricks and Azure HDInsight.
Implement data analytics solutions on the Azure platform leveraging services like Stream Analytics, Power BI, and Time Series Insights.
Engineer and deploy end-to-end big data solutions on Azure using technologies like Spark, Logic Apps, Function Apps, Databricks, and Azure Kubernetes Containers.
Automate deployment and infrastructure-as-code using ARM templates, Terraform, Ansible, and PowerShell.
Monitor and optimize performance of cloud data systems using tools like Azure Monitor, Log Analytics, and Application Insights.
Design schemas and implement ETL processes in data warehouses and data lakes for analytical workloads.
Design solutions using emerging technologies to meet business requirements.
Translate the requirements into functional or technical specifications, prototype, configuration of the specific emerging technology solution.
Support data scientists by provisioning and managing infrastructure like GPU VMs for machine learning and Azure Databricks
Work in an Agile environment collaborating with stakeholders and teams to deliver data projects.
Qualifications:
7+ years of experience in cloud data engineering with a focus on Microsoft Azure.
5+ years of experience as a Solution Architect.
Analysis level UML, activity diagrams, business use cases, business rules, and system level features.
Experience in enterprise architecture frameworks (TOGAF is preferred).
Experience with data warehouse and big data technologies such as Azure Synapse Analytics, Azure Data Lake Storage, Spark, Kafka, Databricks, Azure Logic Apps, Azure Function apps, Azure Cognitive Services, and Containers.
Hand-on experience with ETL tools such as Azure Data Factory and Databricks.
Experience with data analytics tools such as Stream Analytics, Power BI, and Time Series Insights.
Extensive Experience with cloud infrastructure automation tools such as ARM templates,
Terraform
, Ansible, and PowerShell.
Experience with monitoring and optimizing Azure cloud data systems.
Experience with designing and implementing data schemas.
Experience with working in an Agile environment.
Master’s or bachelor’s degree in computer science, Information Technology, or a related field.
As per the Executive Order on Ensuring Adequate Covid Safety Protocols for Federal Contractors and regulations as detailed by
www.saferfederalworkforce.gov
, it is recommended that all federal government contractors be vaccinated against Covid-19, unless approved for an exemption/ accommodation on the basis of a sincerely held religious belief or medical circumstance.
Show more
Show less","Azure, Cloud Data Engineer, Microsoft Azure, Data Pipeline, Python, ETL Processes, Data Transformations, Databricks, Azure HDInsight, Azure Monitor, Log Analytics, Application Insights, Data Warehouses, Data Lakes, SQL Server, Azure Data Lake Storage, Cosmos DB, Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Stream Analytics, Power BI, Time Series Insights, Apache Spark, Logic Apps, Function Apps, Azure Kubernetes Containers, ARM Templates, Terraform, Ansible, PowerShell, UML, Activity Diagrams, Business Use Cases, Business Rules, Enterprise Architecture Frameworks, TOGAF, Experience with ETL tools, Experience with data analytics tools, Experience with cloud infrastructure automation tools, Designing and implementing data schemas, Working in an Agile environment","azure, cloud data engineer, microsoft azure, data pipeline, python, etl processes, data transformations, databricks, azure hdinsight, azure monitor, log analytics, application insights, data warehouses, data lakes, sql server, azure data lake storage, cosmos db, azure data factory, azure synapse analytics, azure databricks, stream analytics, power bi, time series insights, apache spark, logic apps, function apps, azure kubernetes containers, arm templates, terraform, ansible, powershell, uml, activity diagrams, business use cases, business rules, enterprise architecture frameworks, togaf, experience with etl tools, experience with data analytics tools, experience with cloud infrastructure automation tools, designing and implementing data schemas, working in an agile environment","activity diagrams, ansible, apache spark, application insights, arm templates, azure, azure data factory, azure data lake storage, azure databricks, azure hdinsight, azure kubernetes containers, azure monitor, azure synapse analytics, business rules, business use cases, cloud data engineer, cosmos db, data lakes, data pipeline, data transformations, data warehouses, databricks, designing and implementing data schemas, enterprise architecture frameworks, etl, experience with cloud infrastructure automation tools, experience with data analytics tools, experience with etl tools, function apps, log analytics, logic apps, microsoft azure, powerbi, powershell, python, sql server, stream analytics, terraform, time series insights, togaf, uml, working in an agile environment"
Senior Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Herndon, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-wfh-at-get-it-recruit-information-technology-3778807374,2023-12-17,Sterling,United States,Mid senior,Remote,"Since our founding in 2008, our company has been fueled by a simple yet powerful idea: understanding people. We empower leading brands and agencies with deep consumer intelligence, fostering unbreakable relationships through better connections, more meaningful engagement, and compelling customer experiences. At the core of our mission is a ""people-centric"" approach, revealing the Human Element—a holistic understanding of individuals, starting with their values and motivations.
As we continue to grow, we are seeking a talented individual to join us as a Senior Data Engineer. This role involves mentoring our Data Integration Engineering team, collaboratively designing and implementing product features. If you're an engineer who thrives in a highly skilled team, values ownership of your work, and has a passion for coding, this opportunity is ideal for you.
Key Responsibilities
Design, implement, and enhance data integration and data streaming systems.
Develop data pipelines and Spring-based microservices as part of an agile/scrum team.
Support and maintain existing features, debugging throughout the stack, and applying fixes in a timely manner.
Apply best practices in continuous integration and delivery.
Design and develop APIs using RESTful principles.
Write unit, integration, and full end-to-end tests for new features.
Participate in the release of new features to production.
Collaborate with product managers and other engineers to implement and document complex and evolving requirements.
Act as on-call high-availability support for triage/bug fixing periodically throughout the year.
Required Qualifications & Experience
Experience with languages like Scala, Java, or similar.
Proficiency in cloud technologies (AWS) such as EMR/EC2, Lambda, and CloudWatch.
Experience with Big Data technologies like Hive/Spark.
Master's degree or equivalent relevant work experience.
5+ years of Scala and data processing development experience.
Experience in developing RESTful web services and JSON.
Proficiency in relational DB table design, implementation, and tuning (pl/SQL, ETL, etc).
Experience in database integration, especially ORM tools (Hibernate, JPA, etc).
Familiarity with build and deployment tools (Maven, Gradle, or SBT).
Practical knowledge of OOP/JS design patterns.
Understanding of ""12-Factor App"" concepts.
Experience with distributed system development for large-scale applications.
Familiarity with continuous integration and testing.
Experience with agile methodologies and short release cycles.
Strong communication skills, attention to detail, good work ethic, and ability to manage multiple projects simultaneously.
Knowledge of Continuous Integration & Continuous Deployment tools and processes.
Experience with large-scale SQL databases is a strong plus.
Desired Qualifications & Experience
Experience working on a SaaS product in a commercial environment.
Worked with Postgres DB.
Real-time data processing experience, such as Kafka.
Knowledge of Splunk, Grafana, and other AWS technologies.
Experience with Athena and AWS cost optimization.
Background in digital media, online advertising, or reporting/analytical applications.
Benefits
In addition to the opportunity to work with smart, fun, hard-working team members, we offer uncapped growth potential, a work/life balance, and a competitive suite of benefits.
Location:
Our flexible work environment combines the best of both worlds, with team members seamlessly collaborating across physical locations. Whether you prefer working from home or from our state-of-the-art offices, you'll have access to the tools and resources you need to succeed.
We are headquartered in Reston, VA, with offices in New York City and Washington, D.C.
Our EEO Statement
We are an equal opportunity employer committed to diversity and inclusion in the workplace, prohibiting discrimination and harassment based on various characteristics outlined by federal, state, or local laws.
Learn more about our story at our website.
Employment Type: Full-Time
Show more
Show less","Data Integration, Data Streaming, Spring Microservices, Agile/Scrum, Continuous Integration, Continuous Delivery, RESTful APIs, Unit Testing, Integration Testing, EndtoEnd Testing, Production Release, Product Management, Complex Requirements, Oncall Support, Scala, Java, AWS, EMR/EC2, Lambda, CloudWatch, Hive/Spark, Big Data, Master's Degree, Relational DB, Table Design, PL/SQL, ETL, ORM tools, Hibernate, JPA, Maven, Gradle, SBT, OOP/JS Design Patterns, 12Factor App Concepts, Distributed Systems, Continuous Integration, Continuous Testing, Agile Methodologies, Communication Skills, Attention to Detail, Work Ethic, Project Management, Continuous Integration & Continuous Deployment tools, Largescale SQL databases, SaaS, Postgres DB, Kafka, Splunk, Grafana, Athena, AWS cost optimization, Digital Media, Online Advertising, Reporting/Analytical Applications","data integration, data streaming, spring microservices, agilescrum, continuous integration, continuous delivery, restful apis, unit testing, integration testing, endtoend testing, production release, product management, complex requirements, oncall support, scala, java, aws, emrec2, lambda, cloudwatch, hivespark, big data, masters degree, relational db, table design, plsql, etl, orm tools, hibernate, jpa, maven, gradle, sbt, oopjs design patterns, 12factor app concepts, distributed systems, continuous integration, continuous testing, agile methodologies, communication skills, attention to detail, work ethic, project management, continuous integration continuous deployment tools, largescale sql databases, saas, postgres db, kafka, splunk, grafana, athena, aws cost optimization, digital media, online advertising, reportinganalytical applications","12factor app concepts, agile methodologies, agilescrum, athena, attention to detail, aws, aws cost optimization, big data, cloudwatch, communication skills, complex requirements, continuous delivery, continuous integration, continuous integration continuous deployment tools, continuous testing, data integration, data streaming, digital media, distributed systems, emrec2, endtoend testing, etl, gradle, grafana, hibernate, hivespark, integration testing, java, jpa, kafka, lambda, largescale sql databases, masters degree, maven, oncall support, online advertising, oopjs design patterns, orm tools, plsql, postgres db, product management, production release, project management, relational db, reportinganalytical applications, restful apis, saas, sbt, scala, splunk, spring microservices, table design, unit testing, work ethic"
Senior Data Engineer,"Changeis, Inc.","Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-changeis-inc-3778344143,2023-12-17,Sterling,United States,Mid senior,Remote,"Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.
The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.
Roles And Responsibilities
Identifying new datasets and integrate them, focusing on enhancing product capabilities.
Conducting experiments with analytical techniques to solve complex problems across various domains.
Recognizing relevant data sources and gather structured and unstructured data to meet client needs.
Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.
Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.
Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.
Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.
Collaborating with software developers and machine-learning engineers to implement models in production environments.
Requirements
15+ years of experience in data analysis, data management, data science, or operations research.
7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.
5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).
5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.
Experience with data ETL (Extract, Transform, and Load).
Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).
Ability to formulate business needs and translate them into technical functional and non-functional requirements.
Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.
Experience in data modeling, including transactional and data warehouse design.
Prefer candidates holding active AWS certifications
Show more
Show less","Data analysis, Data management, Data science, Operations research, Data engineering, ETL, Data analytics, Big data, Machine learning, Software engineering, AWS, Python, JavaScript, Node.js, React, Vue.js, C#, ELK Stack, Elastic Search, Logstash, Kibana, Docker, Kubernetes, Data modeling, Transactional design, Data warehouse design","data analysis, data management, data science, operations research, data engineering, etl, data analytics, big data, machine learning, software engineering, aws, python, javascript, nodejs, react, vuejs, c, elk stack, elastic search, logstash, kibana, docker, kubernetes, data modeling, transactional design, data warehouse design","aws, big data, c, data engineering, data management, data science, data warehouse design, dataanalytics, datamodeling, docker, elastic search, elk stack, etl, javascript, kibana, kubernetes, logstash, machine learning, nodejs, operations research, python, react, software engineering, transactional design, vuejs"
AWS Data Engineer,IVY TECH SOLUTIONS INC,"Washington, DC",https://www.linkedin.com/jobs/view/aws-data-engineer-at-ivy-tech-solutions-inc-3787771995,2023-12-17,Sterling,United States,Mid senior,Remote,"CLIENT : AMTRAK
Location : DC
AWS, AWS GLUE, ETL, Informatica, KINESIS, LAMBDA
This position is for a Mid level AWS/Data Engineer, responsible for designing and implementing data processing pipelines using AWS Glue and other AWS services. The qualified individual will work closely with clients/onsite counterparts as needed.
Must-haves:
Familiarity with AWS data services and modules.
5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline)
3+ years of experience with data migration, data analysis, and SQLs
3+ years of experience with informatica
Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets
Ability to work in globally distributed teams
Knowledge of IT processes, including quality assurance, release management, and production support
Excellent analytical, troubleshooting, and problem-solving skills
Excellent communicator (written and verbal, formal, and informal).
Flexible and proactive/self-motivated working style with strong personal ownership.
Ability to multi-task and prioritize under pressure.
Ability to work independently with minimal supervision as well as in a team environment.
Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required
Please feel free to share resumes to sangeetha.sirivala@ivytechsol.us ,Adarsh@ivytechsol.com
Powered by JazzHR
vINzbmxXts
Show more
Show less","AWS, AWS GLUE, ETL, Informatica, KINESIS, LAMBDA, AWS Data Services, AWS Modules, AWS Services (Lambda S3 RDS Aurora DynamoDB Kinesis AWS Glue AWS Data Pipeline), Data Migration, Data Analysis, SQL, SQLs, IT Processes, Quality Assurance, Release Management, Production Support, Analytical Skills, Troubleshooting Skills, ProblemSolving Skills, Communication Skills (Written Verbal Formal Informal), Flexible Work Style, Proactive Work Style, SelfMotivated Work Style, Personal Ownership, MultiTasking Skills, Prioritization Skills, Independent Work Style, Team Environment, Computer Science Degree, Data Science Degree, Equivalent Education/Professional Experience","aws, aws glue, etl, informatica, kinesis, lambda, aws data services, aws modules, aws services lambda s3 rds aurora dynamodb kinesis aws glue aws data pipeline, data migration, data analysis, sql, sqls, it processes, quality assurance, release management, production support, analytical skills, troubleshooting skills, problemsolving skills, communication skills written verbal formal informal, flexible work style, proactive work style, selfmotivated work style, personal ownership, multitasking skills, prioritization skills, independent work style, team environment, computer science degree, data science degree, equivalent educationprofessional experience","analytical skills, aws, aws data services, aws glue, aws modules, aws services lambda s3 rds aurora dynamodb kinesis aws glue aws data pipeline, communication skills written verbal formal informal, computer science degree, data migration, data science degree, dataanalytics, equivalent educationprofessional experience, etl, flexible work style, independent work style, informatica, it processes, kinesis, lambda, multitasking skills, personal ownership, prioritization skills, proactive work style, problemsolving skills, production support, quality assurance, release management, selfmotivated work style, sql, sqls, team environment, troubleshooting skills"
Senior Data Engineer,"Changeis, Inc.","Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-changeis-inc-3759558676,2023-12-17,Sterling,United States,Mid senior,Remote,"Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.
The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.
Roles And Responsibilities
Identifying new datasets and integrate them, focusing on enhancing product capabilities.
Conducting experiments with analytical techniques to solve complex problems across various domains.
Recognizing relevant data sources and gather structured and unstructured data to meet client needs.
Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.
Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.
Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.
Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.
Collaborating with software developers and machine-learning engineers to implement models in production environments.
Requirements
15+ years of experience in data analysis, data management, data science, or operations research.
7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.
5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).
5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.
Experience with data ETL (Extract, Transform, and Load).
Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).
Ability to formulate business needs and translate them into technical functional and non-functional requirements.
Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.
Experience in data modeling, including transactional and data warehouse design.
Prefer candidates holding active AWS certifications
Show more
Show less","Data Engineering, Python, JavaScript, C#, Node.js, React, Vue.js, AWS, ETL, ELK Stack, Docker, Kubernetes, Data Analytics, Machine Learning, Big Data, Algorithms, Statistical Models, Cloud Computing, Database Services, Data Modeling, AWS Certifications","data engineering, python, javascript, c, nodejs, react, vuejs, aws, etl, elk stack, docker, kubernetes, data analytics, machine learning, big data, algorithms, statistical models, cloud computing, database services, data modeling, aws certifications","algorithms, aws, aws certifications, big data, c, cloud computing, data engineering, dataanalytics, database services, datamodeling, docker, elk stack, etl, javascript, kubernetes, machine learning, nodejs, python, react, statistical models, vuejs"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751643391,2023-12-17,Sterling,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Data Pipelines, Cloud Computing, AWS, Azure, GCP, ETL, Data Orchestration, Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion, Snowflake, Redshift, Databricks, SQL, NoSQL, PostgreSQL, MySQL, SQL Server, Oracle, Aurora, Presto, BigQuery, Star Schema, Snowflake Schema, De/Normalized Designs, Python, Java, R, C, C#, C++, Shell, Git, Jenkins, CI/CD, Jira, DevOps, Big Data, Open Source, Data Streaming, SQL, Database Design, Data Structure","data engineering, data pipelines, cloud computing, aws, azure, gcp, etl, data orchestration, azure data factory, ssis, informatica, alteryx, ab initio, pentaho, talend, matillion, snowflake, redshift, databricks, sql, nosql, postgresql, mysql, sql server, oracle, aurora, presto, bigquery, star schema, snowflake schema, denormalized designs, python, java, r, c, c, c, shell, git, jenkins, cicd, jira, devops, big data, open source, data streaming, sql, database design, data structure","ab initio, alteryx, aurora, aws, azure, azure data factory, big data, bigquery, c, cicd, cloud computing, data engineering, data orchestration, data streaming, data structure, database design, databricks, datapipeline, denormalized designs, devops, etl, gcp, git, informatica, java, jenkins, jira, matillion, mysql, nosql, open source, oracle, pentaho, postgresql, presto, python, r, redshift, shell, snowflake, snowflake schema, sql, sql server, ssis, star schema, talend"
Senior Data Engineer,Resonate,"Reston, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-resonate-3773575605,2023-12-17,Sterling,United States,Mid senior,Remote,"Senior Data Engineer
Since Resonate’s founding in 2008, the company has been driven by a simple but powerful idea: understanding people. Resonate empowers leading brands and agencies with deep consumer intelligence that ignites unbreakable relationships through better connections, more meaningful engagement and compelling customer experiences.
We are “people-centric.” It is at the heart of what we do and drives how we operate. We reveal the Human Element—a holistic understanding of a person that starts with what makes us the most human—our values and motivations. In other words, we help you understand the “what” that drives the “why” people decide to choose, buy, endorse or abandon a brand or cause. By combining this human, person-based ""why"" with relevant data about your industry, brand or product, we help you create powerful marketing engagement that drives results.
As a senior data engineer, you will be working as a mentor of our Data Integration Engineering team to jointly design and implement product features. This is an ideal job if you are an engineer who wants to be part of an intensely skilled team, values total ownership of your work, and can’t imagine a day without coding.
If you are a skilled developer, with professional experience with data processing technologies, and distributed systems we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both designs as well as working hands-on with the code.
Key Responsibilities
Design, implement, and improve the data integration, data streaming systems
Design and develop Data pipeline, Spring-based microservices, as part of an agile/scrum team
Support and maintain existing features, debugging throughout the stack, and applying fixes in a timely manner
Apply best practices in continuous integration and delivery
Design and develop API’s using RESTful principles
Write unit, integration, and full end-to-end tests for new features
Participate in the release of new features to production
Work with product managers and other engineers to implement and document complex and evolving requirements
Act as on-call high-availability support for triage/bug fixing periodically throughout the year
Required Qualifications & Experience Requirements
Experience with languages like Scala, Java, or similar language
Experience with cloud technologies (AWS) like EMR/EC2, Lambda and Cloud watch
Experience with Big Data technologies such as Hive/Spark.
Masters' degree or equivalent relevant work experience
5+ years of Scala and data processing development experience
Experience developing RESTful web services Java restful web services and JSON
Experience in relational DB table design, implementation, and tuning (pl/SQL, ETL, etc)
Experience in database integration, especially ORM tools (Hibernate, JPA, etc)
Experience in build and deployment tools such as Maven or Gradle or SBT (Simple Build Tool)
Practical knowledge of OOP/JS design patterns
Understanding of “12-Factor App” concepts
Distributed System Development for large-scale applications
Experience with continuous integration and testing
Experience with agile methodologies and short release cycles
Demonstrate strong communication skills, strong attention to detail, good work ethic, and ability to work on multiple projects simultaneously
Strong knowledge of Continuous Integration & Continuous Deployment tools and processes
Experience with large scale SQL databases is a strong plus
Ability to work with shifting deadlines in a fast paced environment
Desired Qualifications & Experience Requirements
Experience working on a SAAS Product in a commercial environment
Worked with Postgres DB
Realtime data processing like Kafka
Knowledge of Splunk, Grafana and other AWS technologies
Experience with Athena and AWS cost optimization
Experience in digital media, online advertising, or reporting/analytical applications
Benefits
Besides the opportunity to work with smart, fun, hard-working Resonate employees, you will have uncapped growth potential, a work/life balance, and a competitive suite of benefits.
Location
At Resonate, we're proud to offer a flexible work environment that combines the best of both worlds. Our team is made up of talented individuals who collaborate seamlessly across physical locations, thanks to our innovative hybrid and remote work policies. Whether you're working from home or from one of our state-of-the-art offices, you'll have access to the tools and resources you need to succeed.
Resonate is headquartered in Reston, VA with offices in New York City, and Washington, D.C. Be a part of the team that changes the industry!
Our EEO Statement:
Resonate is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outline by federal, state, or local laws.
Find out more about our story at www.resonate.com.
Show more
Show less","Scala, Java, Spring, Kafka, Database integration, Hive, Spark, Big Data, AWS, Data streaming, Data engineering, Agile, Continuous integration, Unit testing, Integration testing, Endtoend testing, Continuous deployment, SQL, PostgreSQL, Splunk, Grafana, Athena","scala, java, spring, kafka, database integration, hive, spark, big data, aws, data streaming, data engineering, agile, continuous integration, unit testing, integration testing, endtoend testing, continuous deployment, sql, postgresql, splunk, grafana, athena","agile, athena, aws, big data, continuous deployment, continuous integration, data engineering, data streaming, database integration, endtoend testing, grafana, hive, integration testing, java, kafka, postgresql, scala, spark, splunk, spring, sql, unit testing"
IT ENGINEERING DATA ENGINEER,IVY TECH SOLUTIONS INC,"Vienna, VA",https://www.linkedin.com/jobs/view/it-engineering-data-engineer-at-ivy-tech-solutions-inc-3787781068,2023-12-17,Sterling,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
IT ENGINEERING ( DATA ENGINEER)
Location: VA
Duration: 12+Months
Please send the resume to
or 847- 350-1008
DATAWAREHOUSE, DB2, ETL, ORACLE, POWER BI, SQL
We will only accept the following:
US Citizens, Green Card Holders, TN Visa
Description:
Below are the primary skills required for the role:
SQL, Oracle, DB2 server
Hands on experience of building and maintaining artifacts in Datawarehouses, Data lakes etc for both on-prem (Teradata) and on-cloud (preferably on Azure)
Hands on experience of using ETL tools such as Informatica
Hands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOps
Business analytics and Intelligence tools such as Tableau, Power BI etc
Experience with Agile frameworks
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
5dhv59Zytw
Show more
Show less","SQL, Oracle, DB2, Datawarehouses, Data lakes, Teradata, Azure, Informatica, Azure Data Factory, Azure Data Bricks (PySpark), Synapse, Azure DevOps, Tableau, Power BI, Agile","sql, oracle, db2, datawarehouses, data lakes, teradata, azure, informatica, azure data factory, azure data bricks pyspark, synapse, azure devops, tableau, power bi, agile","agile, azure, azure data bricks pyspark, azure data factory, azure devops, data lakes, datawarehouses, db2, informatica, oracle, powerbi, sql, synapse, tableau, teradata"
"Senior Data Engineer, Mortgage Backed Securities (REMOTE) - (CS2023-2153)","3 Key Consulting, Inc.","Bethesda, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-mortgage-backed-securities-remote-cs2023-2153-at-3-key-consulting-inc-3594799930,2023-12-17,Sterling,United States,Mid senior,Remote,"Job Title:
Senior Data Engineer, Mortgage Backed Securities - (CS2023-2153)
Location:
Bethesda, MD. 20814 (100% remote)
Employment Type:
Permanent
Business Unit:
Data Management Group
Pay Range:
Range $101,575 to $173,341
Posting Date:
05//01/23
Notes:
Only qualified candidates need apply.
3 Key Consulting is hiring a
Senior Data Engineer
for a permanent position with our client, a mortgage securitization solution company based in Bethesda, MD.
Job Description
Our client built and operates the largest and most advanced mortgage securitization platform in the world, supporting the Uniform Mortgage-Backed Security (UMBS) of Fannie Mae and Freddie Mac. Supporting 70% of the mortgage-backed securities in the market, they provide best-in-class single-family issuance, bond administration, disclosure, and tax services. The support a broad portfolio of products for our clients with full lifecycle management.
Their market-leading, cloud-based, end-to-end platform executes transactions on an extraordinary scale which has bolstered liquidity in the secondary mortgage market, one of the largest and most important financial markets in the world. Their unique approach to securitization combines the best minds in financial services with the know-how, flexibility, and innovation of leading technologists.
The Data Engineer is responsible for solution engineering of enterprise scale data management best practices. This includes modern data integration frameworks, building of scalable distributed systems using emerging cloud-based data design patterns. This role will be responsible for developing data integration tasks in data and analytics space. This position will report to director of data management group under Data Operations organization. This is an individual performer role.
Top Must Have Skills
Python -AWS (Build data pipeline in AWS environment, Foundational AWS services (s3, VPC, IAM).
Programming in AWS ).
Snowflake or Redshift.
Data Integration tool – PySpark or Glue
Day To Day Responsibilities
Demonstrate expert ability in implementing Data Warehouse solutions using Snowflake.
Building data integration solutions between transaction systems and analytics platform.
Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs
Create security policies in Snowflake to manage fine grained access control
Develop tasks for a multitude of data patterns, e.g., real-time data integration, Advanced Analytics, Machine Learning, BI and Reporting.
Lead POC efforts to build foundational AI/ML services for Predictive Analytics.
Building of data products by data enrichment and ML.
Be a team player and share knowledge with the existing team members.
Basic Qualifications
Bachelor’s degree in Computer Science or a related field
Minimum of 4 years of experience in building data driven solutions.
Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future.
Client does not offer H-1B sponsorship for this position.
Employee Value Proposition
You will be joining a great team and company. Client is a fully virtual company, meaning this is a remote role.
Interview Process
Video Skype Panel Interview
We invite qualified candidates to send your resume to resumes@3keyconsulting.com. If you decide that you’re not interested in pursuing this particular position, please feel free to take a look at the other positions on our website www.3keyconsulting.com/careers. You are also welcome to share this opportunity with anyone you think might be interested in applying for this role.
Regards,
3KC Talent Acquisition Team
Show more
Show less","Python, AWS, Snowflake, Redshift, PySpark, Glue, Tableau, Data Warehouse, Data Integration, Data Enrichment, Machine Learning, AI, Predictive Analytics, Data Products, Computer Science, Data Analytics","python, aws, snowflake, redshift, pyspark, glue, tableau, data warehouse, data integration, data enrichment, machine learning, ai, predictive analytics, data products, computer science, data analytics","ai, aws, computer science, data enrichment, data integration, data products, dataanalytics, datawarehouse, glue, machine learning, predictive analytics, python, redshift, snowflake, spark, tableau"
"Senior Software Engineer, FHIR Data",For People,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-software-engineer-fhir-data-at-for-people-3787934429,2023-12-17,Sterling,United States,Mid senior,Remote,"For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.
Your Impact
We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare & Medicaid Services (CMS) to serve Medicare beneficiaries' demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will
directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive
.
Our Culture
For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!
This position is
100% remote
. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!
This position's published base salary range is between
$125,000 and $160,000
annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.
Your Opportunities
Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.
Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.
Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.
Design and maintain the data warehouse's FHIR-based data model to meet the needs of downstream API systems.
Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.
Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.
Stay informed about industry best practices and evolving FHIR standards.
You Bring
A humble and caring attitude
In-depth knowledge and experience with FHIR standards and resource types.
Expert-level Java programming abilities, alongside some familiarity with Python and Bash scripts.
Proficiency in designing and implementing data ingestion and transformation processes.
Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.
A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.
Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.
If you're passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.
Some fine print.
You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.
For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status.
Powered by JazzHR
UWyjSwTOgS
Show more
Show less","Healthcare technology, FHIR (Fast Healthcare Interoperability Resources), Java, Python, Bash scripts, Data ingestion, Data transformation, Database design, Data modeling, ETL (Extract Transform Load), HIPAA, Security measures, Access controls, Data mapping","healthcare technology, fhir fast healthcare interoperability resources, java, python, bash scripts, data ingestion, data transformation, database design, data modeling, etl extract transform load, hipaa, security measures, access controls, data mapping","access controls, bash scripts, data ingestion, data mapping, data transformation, database design, datamodeling, etl extract transform load, fhir fast healthcare interoperability resources, healthcare technology, hipaa, java, python, security measures"
"Principal, Data Engineer Architect",Lovelytics,"Arlington, VA",https://www.linkedin.com/jobs/view/principal-data-engineer-architect-at-lovelytics-3787784770,2023-12-17,Sterling,United States,Mid senior,Remote,"Lovelytics is seeking a skilled consultant with experience delivering and solutionging strategic Databricks and data engineering client engagements.
This Principal Consultant will play the role of a data and analytics solution architect, primarily focusing on leading the technical sales process related to data warehousing, ETL development, data integrations, and data modeling. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data through thought leadership and execution.
Role Location: Arlington, VA, or Remote in the US (MD, DC, CA, IA, ID, IN, NC, SC, TX, TN, GA, CO, NY, NJ, VA, FL, PA)
Primary Job Responsibilities:
Fill the role of data engineer solutions architect for our largest and most complex client engagements focused on developing data warehouses, optimizing back-end performance, and integrating data sources to systems using Databricks
Partner with our Technical Directors to identify emerging trends in the market, design and implement internal initiatives, etc.
Provide thought leadership in the form of blogs, technical ideation for Lovelytics' new offerings, and solutions for client's problems across various verticals.
Work hand-in-hand with our other capability teams to successfully sell, plan, and deliver engagements
Gather requirements from clients and develop creative and effective technical solutions
Partner with our sales team to play an extensive role in the presales and pursuit process, applying knowledge to potential client problems, enabling Lovelytics to expand the business
Apply your skills with Databricks, using Python, and big data streaming to pioneer client technologies and data
Manage projects to ensure project milestones are reached within the given timeline and budget allocated
Support other team members on projects, which can oftentimes mean wearing many different hats
Integrate Databricks with 3rd-party applications to support customers' architectures
Troubleshooting data issues on the fly with prospects and clients
Our Ideal Candidate's Skills and Experiences:
B.S. in Computer Science or equivalent
6+ years in data engineering working with cloud-based data analytics architectures and 4+ yrs' experience working directly with clients.
2+ years of involvement in technology pre-sales
Extensive knowledge of data warehousing and data lake concepts and hands-on experience deploying pipelines using Databricks
Experience developing Machine Learning models or ML Ops processes a plus
Excellent communication skills are a MUST, all our employees are client-facing, and this role requires both written and verbal client management skills.
Proven success in client development and partnering with Sales to develop solutions in the presales and discovery phase
Databricks Data Engineer Professional and Databricks Machine Learning Professional certifications a plus
Experience designing architectures within a public cloud (AWS or Azure)
Hands-on experience with Big Data technologies, including Spark, Hadoop, Cassandra, and others
Ability to extract and transform data via Python, deep exposure and understanding of data warehousing, ETL pipelines, etc.
Overall understanding of analytics from analytic engineering to visualization tools
What We Promise You:
Exciting projects with great clients in varying departments and verticals across the world
The ability to work closely with experienced data engineers and quickly grow and expand your skillset
The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. The salary range for candidates in the US for this position is $148,000-$202,000 depending on level, experience, and skills. Note, that a realistic starting salary is the mid point of the provided range.
Powered by JazzHR
MDKwLSZjWa
Show more
Show less","Databricks, Data Warehousing, ETL Development, Data Integration, Data Modeling, Python, Big Data Streaming, Machine Learning, ML Ops, Public Cloud (AWS or Azure), Spark, Hadoop, Cassandra, Data Extraction, Data Transformation, Analytic Engineering, Visualization Tools","databricks, data warehousing, etl development, data integration, data modeling, python, big data streaming, machine learning, ml ops, public cloud aws or azure, spark, hadoop, cassandra, data extraction, data transformation, analytic engineering, visualization tools","analytic engineering, big data streaming, cassandra, data extraction, data integration, data transformation, databricks, datamodeling, datawarehouse, etl development, hadoop, machine learning, ml ops, public cloud aws or azure, python, spark, visualization tools"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/data-engineer-aws-azure-gcp-at-captech-3774195945,2023-12-17,Sterling,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
Specific responsibilities for the Data Engineer – Cloud position include:
Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly
Utilizing your skills in engineering best practices to solve complex data problems
Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization.
Articulating architectural differences between solution methods and the advantages/disadvantages of each
Qualifications
Typical experience for successful candidates includes:
Experience delivering solutions on a major cloud platform
Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture
Experience in the design and implementation of data architecture solutions
A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines
Ability to assess and utilize traditional and modern architectural components required based on business needs.
A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning.
Skills
Successful candidates usually have demonstrable experience with technologies in some of these categories:
Languages: SQL, Python, Java, R, C# / C++ / C
Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle
DevOps: git, docker, subversion, Kubernetes, Jenkins
Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR
Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
#LM-Remote
Show more
Show less","Python, Java, SQL, R, C#, C++, C, SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle, Git, Docker, Subversion, Kubernetes, Jenkins, Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR, AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer","python, java, sql, r, c, c, c, sql server, postgresql, snowflake, redshift, aurora, presto, bigquery, oracle, git, docker, subversion, kubernetes, jenkins, spark, databricks, kafka, kinesis, hadoop, lambda, emr, aws cloud practitioner, microsoft azure data fundamentals, google associate cloud engineer","aurora, aws cloud practitioner, bigquery, c, databricks, docker, emr, git, google associate cloud engineer, hadoop, java, jenkins, kafka, kinesis, kubernetes, lambda, microsoft azure data fundamentals, oracle, postgresql, presto, python, r, redshift, snowflake, spark, sql, sql server, subversion"
Azure Data Engineer,CC Pace,"Vienna, VA",https://www.linkedin.com/jobs/view/azure-data-engineer-at-cc-pace-3779239091,2023-12-17,Sterling,United States,Mid senior,Remote,"​​​​
Job Description:
You will be working with all levels of technology from backend data processing technologies (Databricks/Apache Spark) to other Cloud computing technologies / Azure Data Platform. You should be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development. Must be a hand-on technologist passionate about learning new technologies and help improve the ways we can better leverage Advanced Analytics and Machine Learning.
Responsibilities
• Build end-to-end direct capabilities.
• Create and maintain optimal data pipeline architecture.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
• Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business.
• Derive insights to differentiate member and team member experiences.
• Collaborate with cross-functional teams.
• Analyze and define with product teams the data migration and data integration strategies.
• Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems.
• Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights.
• Integrate source systems with information management solutions and target systems for automated migration processes.
• Create proof-of-concepts to demonstrate viability of solutions under consideration.
Qualifications
1. Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions.
2. Bachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience.
3. Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming)
4. Significant hands-on experience guiding technical teams as a mentor while leading collaboration with multiple teams across the organization.
5. Significant hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources.
6. Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala)
7. Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills.
8. Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics
Desired
• Proficiency in collaborative coding practices, such as pair programming, and ability to thrive in a team-oriented environment
• The following certifications:
o Microsoft Certified Azure Data Engineer
o Microsoft Certified Azure Solutions Architect
o Databricks Certified Associate Developer for Apache 2.4/3.0
Show more
Show less","Data Engineering, Data Analytics, Machine Learning, Data Visualization, Data Modeling, Data Integration, Data Migration, Data Processing, Big Data, Cloud Computing, Azure, Azure Data Factory, Azure Databricks, Azure Data Lake Storage, Azure SQL, Apache Spark, PySpark, Java, Scala, Python, Data Structures, Algorithms, Design Patterns, Problem Solving, Collaborative Coding, Pair Programming","data engineering, data analytics, machine learning, data visualization, data modeling, data integration, data migration, data processing, big data, cloud computing, azure, azure data factory, azure databricks, azure data lake storage, azure sql, apache spark, pyspark, java, scala, python, data structures, algorithms, design patterns, problem solving, collaborative coding, pair programming","algorithms, apache spark, azure, azure data factory, azure data lake storage, azure databricks, azure sql, big data, cloud computing, collaborative coding, data engineering, data integration, data migration, data processing, data structures, dataanalytics, datamodeling, design patterns, java, machine learning, pair programming, problem solving, python, scala, spark, visualization"
Sr. Azure Data Engineer (100% Remote),Eliassen Group,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/sr-azure-data-engineer-100%25-remote-at-eliassen-group-3768087153,2023-12-17,Sterling,United States,Mid senior,Remote,"Eliassen Group is looking for a Sr. Azure Data Engineer to support a brand new data migration project as the lead SME that will be working with a highly prestigious logistics company located in the DMV area. You will work as the first Data Engineer and play a pioneering role in establishing and shaping our data engineering function, leveraging Microsoft Azure and its associated services. You will be responsible for designing, developing, and maintaining our data infrastructure, building robust data pipelines, ensuring data quality and availability, and supporting the analytics and data science teams in extracting actionable insights from our data. Your expertise in data engineering and architecture, specifically within the Microsoft Azure ecosystem, will be instrumental in charting the course of our data journey.
Eliassen Group offers a wide range of benefits from 401k match to medical, dental and vision insurance for those who qualify
I cannot do C2C on this position
Key Responsibilities:
Data Engineering Leadership:
Spearhead the establishment of data engineering practices, standards, and processes at Peak Technologies, with a primary focus on the Microsoft Azure platform.
Data Pipeline Development:
Design, develop, and maintain scalable and reliable data pipelines utilizing Microsoft Azure services to collect, transform, and load (ETL) data from various sources into our data warehouse.
Data Modeling:
Create and maintain data models, ensuring data accuracy, consistency, and efficiency within the Microsoft Azure environment. Optimize data structures for performance and ease of access.
Data Integration:
Collaborate with cross-functional teams to integrate data from various systems and sources, including APIs, databases, and third-party services, leveraging Microsoft Azure tools.
Data Quality:
Establish data quality controls and monitoring processes using Microsoft Azure services to ensure the integrity and accuracy of our data.
Performance Optimization:
Continuously improve data pipeline and query performance within the Microsoft Azure framework for optimal speed and efficiency in a global context.
Data Security:
Implement and maintain data security and privacy best practices, ensuring data compliance with relevant regulations and standards across diverse geographical regions with Microsoft Azure security features.
Documentation:
Create and maintain clear and comprehensive documentation of data pipelines, data models, and processes, emphasizing the use of Microsoft Azure services.
Mentorship:
Provide guidance and mentorship to junior data engineers, fostering a culture of knowledge sharing and skill development within the team, with a focus on Microsoft Azure expertise.
Stay Current:
Keep up-to-date with industry trends and emerging technologies in data engineering, particularly within the Microsoft Azure ecosystem, and apply them where relevant on a global scale.
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
10+ years of experience in data engineering, with a strong focus on ETL processes, data modeling, and data warehousing within Microsoft Azure.
Proficiency in one or more programming languages commonly used in data engineering (e.g., Python, Java, Scala).
Hands-on experience with Microsoft Azure and its associated data services (e.g., Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse, Azure Synapse Analytics).
Expertise in SQL and database management systems within the Microsoft Azure environment.
Strong knowledge of data integration methods, data quality, and data governance best practices in a global supply chain context with a Microsoft Azure focus.
Excellent problem-solving skills and the ability to work independently or in a team.
Strong communication and collaboration skills, with the ability to convey technical concepts to non-technical stakeholders.
Certifications in relevant data engineering and Microsoft Azure technologies are a plus.
US-based remote position with the willingness for occasional travel.
Show more
Show less","Data Engineering, Azure Architecture, Microservices, Data Warehousing, Data Modeling, Data Integration, Data Quality, Performance Optimization, Data Security, Documentation, Mentorship, Industry Trends, Emerging Technologies, SQL, Python, Java, Scala, Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse, Azure Synapse Analytics, Data Governance, Communication, Collaboration","data engineering, azure architecture, microservices, data warehousing, data modeling, data integration, data quality, performance optimization, data security, documentation, mentorship, industry trends, emerging technologies, sql, python, java, scala, azure data factory, azure databricks, azure sql data warehouse, azure synapse analytics, data governance, communication, collaboration","azure architecture, azure data factory, azure databricks, azure sql data warehouse, azure synapse analytics, collaboration, communication, data engineering, data governance, data integration, data quality, data security, datamodeling, datawarehouse, documentation, emerging technologies, industry trends, java, mentorship, microservices, performance optimization, python, scala, sql"
Data Warehouse Engineer,Venturi,"McLean, VA",https://www.linkedin.com/jobs/view/data-warehouse-engineer-at-venturi-3784574672,2023-12-17,Sterling,United States,Mid senior,Hybrid,"A large hotel & hospitality client of mine is looking to fill a Sr Product/Project Manager position. Please see below for the job description.
Description:
Experience in analyzing data integration requirements, sources, targets, business rules and transformation logic.
Very strong experience designing and architecting data warehouses.
Experience in designing and developing data extraction, cleansing, transformation, and loading processes.
More than 10 years of experience in creating advanced mappings, transformations, and workflows using Informatica PowerCenter with warehouse implementations.
Exposure in building on-premise to cloud-based ETLs.
Well versed in SQL, dimensional modeling and relational databases.
Strong communication, analytical and problem solving skills.
If this sounds like an opportunity you would be interested in, please apply with an up to date resume.
Venturi is an equal opportunity employer, committed to supporting and creating a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve.All qualified applicants will receive consideration for employment without regard to sex, race, color, national origin, sexual orientation, gender, gender identity, genetic information, religion, disability, age, veteran status, or any other legally protected status under national, federal, state, or local law.
Show more
Show less","Informatica PowerCenter, SQL, Dimensional modeling, Relational databases, Data warehouses, Data integration, Data extraction, Data cleansing, Data transformation, Data loading, ETLs, Cloudbased ETLs","informatica powercenter, sql, dimensional modeling, relational databases, data warehouses, data integration, data extraction, data cleansing, data transformation, data loading, etls, cloudbased etls","cloudbased etls, data extraction, data integration, data loading, data transformation, data warehouses, datacleaning, dimensional modeling, etls, informatica powercenter, relational databases, sql"
Data Engineer,Robert Half,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-engineer-at-robert-half-3780029888,2023-12-17,Sterling,United States,Mid senior,Hybrid,"This is a Full time Direct Hire positions. This is a Manager, Data Engineering role that will manage two people.
This will be a hybrid role and will only need to come to the office for all hands meetings and other reasons once every couple of months.
Top Must Haves:
Azure is a Must for the backend data and getting it in the right format to Power Bi. Power Bi is a Must. Create and maintain Power BI apps, reports, dashboards that assist various departments and communities to use analytics to increase organizational efficiencies. Snowflake is a very nice to have.
Team lead or someone who has managed people. This will be 75% hands on 25% management. They will have two people under them. One which is strong with Power Bi and one that is strong with the ETL Development.
Improve governance of our data assets including our PowerBi Datasets, Dataflows. Experience with ETL / ELT tools such as: SSIS, Azure Data Factory.
The Data Engineering Manager is a key role responsible for leveraging expertise in data warehousing, analytical reporting, and team leadership to ensure the effectiveness of business intelligence systems. This multifaceted position involves collaboration across teams, departments, and with business stakeholders. The Manager oversees end-to-end solutions to democratize data, ensuring reliability, accuracy, and timeliness of data sources. The role demands collaboration with both technical and non-technical team members to support organizational operations.
Key Responsibilities:
Contribute significantly to establishing the ETL/ELT framework for the Data Analytics Team.
Design and develop pipelines for data movement between the Data Warehouse and other analytical solutions.
Provide support for existing pipelines and troubleshoot issues as they arise.
Develop and maintain PowerBI apps, reports, and dashboards to enhance organizational efficiency and senior care.
Enhance governance of data assets, including PowerBI Datasets and Dataflows.
Collaborate with other teams within the organization on data engineering initiatives.
Job Description:
Identify new areas of data, research, and data technology to address business problems.
Utilize effective project planning techniques to manage project scope, break down complex projects into tasks, and ensure timely completion.
Apply data best practices and lessons learned to develop technical solutions for descriptive analytics, ETL, predictive modeling, and prescriptive ""real-time decisions"" analytics.
Develop technical solutions using data techniques in data and analytics processes.
Build frameworks/prototypes integrating data and advanced analytics for business decision-making.
Implement new data technologies (ingestion, processing, distribution) and research delivery methods to solve business problems.
Understand data-related problems and requirements to identify optimal technical approaches.
Collaborate with peers to ensure efforts within assigned tracks meet their needs.
Identify and develop data sources and techniques to address business problems.
Co-mingle data sources to lead work on data and problems across departments, driving improved business and technical results through design.
Maintain compliance with required training applicable to the role.
Experience and Qualifications:
Successful candidates should meet the following criteria:
Bachelor's Degree in Computer Science, Computer Engineering, or a related field.
6+ years of IT experience.
4+ years of experience as a Data Engineer or 1+ years as a Manager of Data Engineering (or equivalent).
Strong communication skills.
Project management experience.
Technical mastery of systems core to the Data Platform.
Hands-on experience with databases like SqlServer, Oracle, Snowflake.
Ability to create reports using PowerBI or other Business Intelligence Software.
Experience with ETL/ELT tools such as SSIS, Azure Data Factory, Pentaho, Talend, etc.
DevOps experience using GIT, developing, deploying code to production.
Proficiency in working with Unix/Linux.
Show more
Show less","Azure, Power Bi, Snowflake, SSIS, Azure Data Factory, ETL / ELT tools, Data Engineering Manager, Governance, PowerBi Datasets, Dataflows, Technical solutions, Frameworks / Prototypes, Data techniques, Advanced analytics, Datarelated problems, Data sources, Comingle data sources, Compliance, SQL Server, Oracle, Business Intelligence Software, Pentaho, Talend, GIT, Unix/Linux, DevOps, RDBMS, ETL, ELT, Data Warehouse, Analytical solutions, Data pipelines, Data movement, Reporting, Dashboards, Organizational Efficiency, Senior Care, Project Planning, Project Management, Data Best Practices, Descriptive Analytics, Predictive Modeling, Prescriptive Analytics, Data and Analytics Processes, Data Ingestion, Data Processing, Data Distribution","azure, power bi, snowflake, ssis, azure data factory, etl elt tools, data engineering manager, governance, powerbi datasets, dataflows, technical solutions, frameworks prototypes, data techniques, advanced analytics, datarelated problems, data sources, comingle data sources, compliance, sql server, oracle, business intelligence software, pentaho, talend, git, unixlinux, devops, rdbms, etl, elt, data warehouse, analytical solutions, data pipelines, data movement, reporting, dashboards, organizational efficiency, senior care, project planning, project management, data best practices, descriptive analytics, predictive modeling, prescriptive analytics, data and analytics processes, data ingestion, data processing, data distribution","advanced analytics, analytical solutions, azure, azure data factory, business intelligence software, comingle data sources, compliance, dashboard, data and analytics processes, data best practices, data distribution, data engineering manager, data ingestion, data movement, data processing, data sources, data techniques, dataflows, datapipeline, datarelated problems, datawarehouse, descriptive analytics, devops, elt, etl, etl elt tools, frameworks prototypes, git, governance, oracle, organizational efficiency, pentaho, powerbi, powerbi datasets, predictive modeling, prescriptive analytics, project management, project planning, rdbms, reporting, senior care, snowflake, sql server, ssis, talend, technical solutions, unixlinux"
"Data Engineer, Data Platform",Grammarly,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656898066,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Data Lakehouse, AWS, Azure, GCE, SQL, APIs, Microservices, System Design, Internal Tools, Data Lakes, Admin Sites","python, scala, java, data lakehouse, aws, azure, gce, sql, apis, microservices, system design, internal tools, data lakes, admin sites","admin sites, apis, aws, azure, data lakehouse, data lakes, gce, internal tools, java, microservices, python, scala, sql, system design"
Data Engineer (Python/SQL),Motion Recruitment,"Chevy Chase, MD",https://www.linkedin.com/jobs/view/data-engineer-python-sql-at-motion-recruitment-3783218809,2023-12-17,Sterling,United States,Mid senior,Hybrid,"An Applied AI company is looking for a Data Engineer to help manage and clean data from various databases. Ideal candidates for this position will have 3-4+ years of professional experience, preferably in a start-up or small company, and be proficient with Python and SQL. Candidates will also be working hybrid in the office. Required Skills & Experience
3-4+ years of professional experience
Python, SQL
The Offer
Competitive Salary
Full-Health Benefits
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k)
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Sean Thompson
Show more
Show less","Data Engineering, Python, SQL, Databases","data engineering, python, sql, databases","data engineering, databases, python, sql"
"Digital, Data Engineer",Madison Energy Infrastructure,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/digital-data-engineer-at-madison-energy-infrastructure-3762695053,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Madison Energy Infrastructure (“MEI”) believes in the power of clean energy infrastructure and has quickly emerged as a preeminent developer, investor, asset owner, and operator of distributed generation. The Digital Data Engineer will actively support the Digital Team and will assist with development and implementation of data security policies and classifications according to best practices. This position presents significant growth potential within our dynamic organization, offering exposure to Distributed Generations (DG) solar and Battery Energy Storage Systems (BESS) nationwide.
What You’ll Be Doing:
Designing, building, maintaining, and optimizing infrastructure for data collection, storage, management, transformation, and delivery to MEI teams.
Identifying and implementing internal process improvements to ensure greater scalability, optimize data delivery, and automate manual processes.
Building required infrastructure for optimally extracting, transforming, and loading raw data from a variety of sources using AWS and SQL technologies.
Building pipelines to collect, store, transform, and deliver raw data in usable formats for visualization, analysis, business intelligence, and reporting.
Building analytical tools that leverage data pipelines to provide actionable insight into key business performance metrics including financial planning and analysis as well as operational efficiency.
Working with the Digital Director to develop, implement, and improve internal data security policies and classifications.
Working with MEI teams to support data infrastructure needs while assisting with data related technical issues.
What We Are Looking For
:
Demonstrates a steady, even pace to ensure precision and high-quality work outcomes.
Possess a technical and analytical focus, working within established systems, standards, and procedures, with job-related communication based on knowledge and expertise.
Engages in decision-making within a well-defined job scope, adhering to established policies and procedures, with managerial support.
Emphasizes job-related knowledge and expertise, maintaining helpful and supportive communication with both management and peers in a structured work environment.
Demonstrates leadership focused on consistent, accurate, and high-quality work output, utilizing a supportive and non-threatening leadership style. Delegates tasks to others appropriately, providing training, coaching, and on-the-job experience.
Location: Vienna, VA or Charlottesville, VA
Madison Energy Investments is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
Show more
Show less","AWS, SQL, Data security, Data classification, Data infrastructure, Data transformation, Data visualization, Data analysis, Business intelligence, Financial planning, Operational efficiency, Data pipelines","aws, sql, data security, data classification, data infrastructure, data transformation, data visualization, data analysis, business intelligence, financial planning, operational efficiency, data pipelines","aws, business intelligence, data classification, data infrastructure, data security, data transformation, dataanalytics, datapipeline, financial planning, operational efficiency, sql, visualization"
Azure Data Engineer,Seneca Resources,"Vienna, VA",https://www.linkedin.com/jobs/view/azure-data-engineer-at-seneca-resources-3782029952,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Position Title:
Azure Data Engineer
Location:
Hybrid in Vienna, VA (1 day onsite each week)
Position Status:
Contract to Hire
Position Description:
Our client is looking for
Azure Data Engineer
candidates for a hybrid position located in
Vienna, VA
.
Responsibilities:
Build end-to-end direct capabilities.
Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business.
Derive insights to differentiate member and team member experiences.
Collaborate with cross-functional teams.
Analyze and define with product teams the data migration and data integration strategies.
Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems.
Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights.
Integrate source systems with information management solutions and target systems for automated migration processes.
Create proof-of-concepts to demonstrate viability of solutions under consideration.
Required Education:
Bachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience.
Required Skills:
Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions.
Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming)
Significant hands-on experience guiding technical teams as a mentor while leading collaboration with multiple teams across the organization.
Significant hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources.
Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala)
Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills.
Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics
Desired Skills:
The following certifications:
Microsoft Certified Azure Data Engineer
Microsoft Certified Azure Solutions Architect
Databricks Certified Associate Developer for Apache 2.4/3.0
About Seneca Resources:
Seneca Resources is a client driven provider of strategic Information Technology consulting services and Workforce Solutions to government and industry. Seneca Resources is a leading IT services provider with offices in Virginia, Alabama, Georgia, Florida, and North Carolina that service clients throughout the United States.
We are an Equal Opportunity Employer and value the benefits of diversity in our workplace.
Show more
Show less","Azure, Azure Data Factory, Azure Databricks, Azure Data Lake Storage, Azure SQL, Data Management, Information Architecture, Performance Management, Information Delivery, Advanced Analytics, Apache Spark, PySpark, Java, Scala, Data Structures, Algorithms, Design Patterns, Analytics, Data Visualization, Modeling, Python","azure, azure data factory, azure databricks, azure data lake storage, azure sql, data management, information architecture, performance management, information delivery, advanced analytics, apache spark, pyspark, java, scala, data structures, algorithms, design patterns, analytics, data visualization, modeling, python","advanced analytics, algorithms, analytics, apache spark, azure, azure data factory, azure data lake storage, azure databricks, azure sql, data management, data structures, design patterns, information architecture, information delivery, java, modeling, performance management, python, scala, spark, visualization"
Data Engineer/Sr Data Engineer,DNV,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-sr-data-engineer-at-dnv-3779262392,2023-12-17,Sterling,United States,Mid senior,Hybrid,"About Us
We are the independent expert in assurance and risk management. Driven by our purpose, to safeguard life, property, and the environment, we empower our customers and their stakeholders with facts and reliable insights so that critical decisions can be made with confidence.
As a trusted voice for many of the world’s most successful organizations, we use our knowledge to advance safety and performance, set industry benchmarks, and inspire and invent solutions to tackle global transformations.
About The Role
EVOLVE Intelligence accelerates the transition towards a carbon-free future through software and analytics.
We are looking for a
Data Engineer/Sr. Data Engineer
to help us accomplish this mission.
The Analytics & Data Science team in DNV – Energy Management’s Technology group is a remote-first team. We offer more than just a job; we provide a community where you can learn, grow, and thrive your way. Join a dynamic and diverse technology team that values relationships and the environment as much as results. Help us create software that empowers utility clean energy customers to combat climate change!
This is a remote position open to any location in the continental United States.
What You’ll Do
As a Data Engineer/Sr. Data Engineer, you will design, develop, and maintain data architecture, pipelines, and systems that play a vital role in how our utility partners steward clean energy programs. Your impact will be immediate and will directly enable pathways to decarbonization through energy efficiency, demand response, storage, electric vehicles, and renewable energy technologies. You will solve a variety of problems that leverage your deep understanding of data engineering principles.
How You’ll Succeed
Collaborate with cross-functional teams, including machine learning engineers, software developers, analytics engineers, and product managers to translate business requirements into highly available data solutions
Leverage your creative problem-solving skills to architect, develop, and maintain scalable and efficient data processing pipelines using PySpark and other distributed computing technologies
Create and optimize data models that support reporting, analytics, artificial intelligence, and software
Optimize data storage and retrieval by designing and implementing efficient storage systems that use technologies such as Timescale and Apache Spark
Apply data validation, data profiling, and data cleansing to ensure data quality and integrity
Write clean, efficient, maintainable code, and actively engage with team members in code reviews
Create and maintain technical documentation covering data models, flows, views, dictionaries, and mapping schemes
Serve as a mentor and resource to other members of the team
What we offer
Generous paid time off (vacation, sick days, company holidays, personal days)
Multiple Medical and Dental benefit plans to choose from, Vision benefits
Spending accounts – FSA, Dependent Care, Commuter Benefits, company-seeded HSA
Special programs – Employee Assistance Program, ID theft protection, and accident and critical illness options for you and your family
Employer-paid, therapist-led, virtual care services through Talkspace
Company provided life insurance, short-term, and long-term disability benefits
Tuition assistance
Flexible work schedule with hybrid/remote opportunities
Advancement opportunities
Benefits may vary based on position, tenure, location, and employee election
Immigration-related employment benefits, for example visa sponsorship, are not available for this position**
How We Do It
We Care, We Dare, We Share
DNV is a proud equal opportunity employer committed to building an inclusive and diverse workforce. All employment is decided on the basis of qualifications, merit or business need, without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
DNV is committed to ensuring equal employment opportunity, including providing reasonable accommodations to individuals with a disability. US applicants with a physical or mental disability who require a reasonable accommodation for any part of the application or hiring process may contact the North America Recruitment department (hrrecruitment.northamerica@dnv.com). Information received relating to accommodations will be addressed confidentially.
For more information
https://www.eeoc.gov/know-your-rights-workplace-discrimination-illegal
Read more here
Diversity at DNV
Meet our Employees
About DNV
Careers in DNV
Please visit our website at www.dnv.com
As required by the Pay Transparency laws in Colorado, New York City, California, Washington, and Connecticut, DNV provides a reasonable compensation range for roles that may be hired in those locations. Actual compensation is influenced by a wide array of factors, including but not limited to skill set, level of experience, and specific office location. For the states of Colorado, New York City, NY, California, Washington, and Connecticut only, the range of starting pay for this role is
$105,000 - $150,000.
About You
What is Required
An undergraduate or advanced degree in a quantitative field
Proven professional experience as a Data Engineer
Strong proficiency in SQL, Python, and Spark/PySpark
Hands on experience with distributed computing frameworks such as Apache Spark
Familiarity with working in Azure – experience with Data Factory is strongly desired
Experience with version control systems (e.g., Git) and familiarity with agile development practices
Excellent communication and teamwork skills, with the ability to effectively collaborate with cross-functional teams
Experience working on an agile product team is a bonus
Someone who is eager to learn new things and is coachable
Most importantly, this team member should display a positive, team-oriented attitude to match our friendly and enthusiastic work environment
Willingness and the ability to undergo a background investigation and drug screening
Excellent written and verbal English communication skills
We conduct pre-employment drug and background screening
Show more
Show less","Data engineering, Apache Spark, ETL, Data quality, Data validation, Data profiling, Data cleansing, Data storage, Data retrieval, Data pipelines, Data models, Data architecture, Analytics, Reporting, Python, SQL, TimeScale, Azure, Git, Agile, English, Teamoriented","data engineering, apache spark, etl, data quality, data validation, data profiling, data cleansing, data storage, data retrieval, data pipelines, data models, data architecture, analytics, reporting, python, sql, timescale, azure, git, agile, english, teamoriented","agile, analytics, apache spark, azure, data architecture, data engineering, data models, data profiling, data quality, data retrieval, data storage, data validation, datacleaning, datapipeline, english, etl, git, python, reporting, sql, teamoriented, timescale"
"Job Opportunity :: AWS Data Platform Engineer :: 6 months Contract :: Stamford, CT, Charlotte, CT (Hybrid)",Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/job-opportunity-aws-data-platform-engineer-6-months-contract-stamford-ct-charlotte-ct-hybrid-at-steneral-consulting-3762482758,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Title- AWS Data Platform Engineer
Location: Hybrid,
NYC,
Lawrenceville, NJ or Washington DC
– 3 Days a week onsite :: Local must
Linkedin must
JD-
Notes
This is an AWS Platform Engineer who is building the underlying platform that other Data Engineers will use. So, they need experience building out the Platform and a focus on Infrastructure as code.
Here is some specific details from the hiring manager. First 2 are mandatory and 3rd can be optional:
Infrastructure as Code(IaC) is required. AWS CDK with Typescript as language for CDK development
Working Experience/Expertise creating AWS infrastructure for AWS services
Some experience in Databricks environment
Must have a heavy background in AWS Platform Engineering with a focus on setting up AWS environment for other engineers to use. Strong background with Infrastructure as Code and building out ETL Engines.
I’m not looking for a Data Application Engineer.
Please don’t send someone who’s last job was not AWS specific or is a Big Data Engineer.
Our client, a leading provider of audio entertainment products, is seeking an AWS Data Platform Engineer to join their data team.
In this role you will be a member of a team responsible for designing, developing and supporting the data platform which will be used across data organization and other groups.
What You’ll Do
Build on the AWS data platform which supports Datalake, Job Orchestration, ETL template, ETL Compute, integration with third party tools like fivetran, Monte Carlo
Design, code and maintain infrastructure as a code (IaC) using CDK, typescript, CDKTF.
Build and improve workflow orchestration tooling to support efficient data pipelines E.g., airflow plugins, systems integration, deployments.
What You’ll Need
7+ years’ experience developing infrastructure as Code working with AWS CDK, typescript/or Python.
3+ years of experience working on AWS building out the cloud platform
AWS CDK with Typescript as language for CDK development
Working Experience for Job Orchestration tool – Airflow/MWAA
Working Experience/Expertise creating AWS infrastructure for AWS services including but not limited to: S3 Datalake, Kms keys, IAM Role/Policy, MWAA, RDS, Lambda function
Knowledge/Expertise on tools viz (Fivetran, Monte-Carlo, Datadog, Tableau/Looker)
Plus Skills
Experience/Expertise on Databricks and Monte Carlo is a plus.
Show more
Show less","AWS, AWS CDK, Typescript, Infrastructure as Code (IaC), AWS Cloud Platform, ETL Engines, Data Lake, Job Orchestration, ETL Templates, ETL Compute, Third Party Tools (Fivetran Monte Carlo), Airflow Plugins, Systems Integration, Deployments, Airflow, MWAA, S3 Data Lake, KMS Keys, IAM Role/Policy, RDS, Lambda Function, Fivetran, Monte Carlo, Datadog, Tableau/Looker, Databricks","aws, aws cdk, typescript, infrastructure as code iac, aws cloud platform, etl engines, data lake, job orchestration, etl templates, etl compute, third party tools fivetran monte carlo, airflow plugins, systems integration, deployments, airflow, mwaa, s3 data lake, kms keys, iam rolepolicy, rds, lambda function, fivetran, monte carlo, datadog, tableaulooker, databricks","airflow, airflow plugins, aws, aws cdk, aws cloud platform, data lake, databricks, datadog, deployments, etl compute, etl engines, etl templates, fivetran, iam rolepolicy, infrastructure as code iac, job orchestration, kms keys, lambda function, monte carlo, mwaa, rds, s3 data lake, systems integration, tableaulooker, third party tools fivetran monte carlo, typescript"
Sr. Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-steampunk-inc-3770815197,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around.
We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
5-7 years industry experience coding commercial software and a passion for solving complex problems.
5-7 years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, SQL, Big Data, Agile, DevOps, Cloud Computing, Data Warehousing, Data Modelling, Data Streaming, Machine Learning, Data Analytics","python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, sql, big data, agile, devops, cloud computing, data warehousing, data modelling, data streaming, machine learning, data analytics","agile, airflow, aws, azkaban, big data, cassandra, cloud computing, data modelling, data streaming, dataanalytics, datawarehouse, devops, ec2, elasticsearch, emr, hadoop, kafka, lucene, luigi, machine learning, postgres, python, rds, redshift, solr, spark, sparkstreaming, sql, storm"
Data Engineer Manager,Robert Half,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/data-engineer-manager-at-robert-half-3782267010,2023-12-17,Sterling,United States,Mid senior,Hybrid,"The Manager of Data Engineering plays a pivotal role in overseeing the operational efficiency of business intelligence systems through a collaborative, end-to-end approach. This multifaceted position involves team leadership and effective collaboration with various stakeholders to democratize data access while maintaining stringent governance for reliability and accuracy.
Key Responsibilities:
ETL/ELT Framework Development: Spearhead the establishment of robust ETL/ELT frameworks for the Data Analytics Team, making significant contributions to set a solid foundation.
Pipeline Design and Maintenance: Design and develop data pipelines, ensuring seamless data flow between the Data Warehouse and other analytical solutions. Provide support for existing pipelines, promptly troubleshooting issues as they arise.
PowerBI Expertise: Create and maintain PowerBI apps, reports, and dashboards to facilitate departments and communities in leveraging analytics for organizational efficiency and enhancing senior care.
Data Governance Enhancement: Improve governance of data assets, including PowerBi Datasets and Dataflows, to ensure data integrity and compliance with standards.
Collaborative Data Engineering: Collaborate with other Sunrise teams on data engineering efforts, fostering a unified approach to data-related challenges and opportunities.
Additional Responsibilities:
Innovative Data Solutions: Identify new areas of data, research, and data technology to address business problems, implementing effective project planning techniques for complex projects.
Technical Expertise: Leverage and contribute to data best practices, applying lessons learned to develop technical solutions for descriptive analytics, ETL, predictive modeling, and real-time decision analytics.
Data Technology Integration: Develop frameworks and prototypes that integrate data and advanced analytics, supporting informed business decisions.
Continuous Improvement: Implement new data technologies, research delivery methods, and understand data-related problems and requirements to identify optimal technical approaches.
Qualifications:
Bachelor's Degree in Computer Science, Computer Engineering, or a related field.
6+ years of IT experience, with 4+ years as a Data Engineer or 1+ years as a Manager of Data Engineering.
Strong communication skills and proven project management experience.
Technical mastery of core systems in the Sunrise Data Platform.
Hands-on experience with databases (e.g., SqlServer, Oracle, Snowflake) and BI software (especially PowerBI).
Expertise in ETL/ELT tools (e.g., SSIS, Azure Data Factory, Pentaho, Talend) and DevOps using GIT.
Proficiency in Unix/Linux environments.
Show more
Show less","ETL/ELT Frameworks, Pipelines, PowerBI, Data Governance, Collaborative Data Engineering, Data Solutions, Data Best Practices, Data Technology Integration, Continuous Improvement, SQL, Oracle, Snowflake, PowerBI, SSIS, Azure Data Factory, Pentaho, Talend, GIT, Unix/Linux","etlelt frameworks, pipelines, powerbi, data governance, collaborative data engineering, data solutions, data best practices, data technology integration, continuous improvement, sql, oracle, snowflake, powerbi, ssis, azure data factory, pentaho, talend, git, unixlinux","azure data factory, collaborative data engineering, continuous improvement, data best practices, data governance, data solutions, data technology integration, etlelt frameworks, git, oracle, pentaho, pipelines, powerbi, snowflake, sql, ssis, talend, unixlinux"
Data Mining and Analytics Engineer (Junior),ICF,"Arlington, VA",https://www.linkedin.com/jobs/view/data-mining-and-analytics-engineer-junior-at-icf-3726721043,2023-12-17,Sterling,United States,Mid senior,Hybrid,"ICF International seeks a Junior Data Mining and Analytics Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate. Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale.
As the Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.
The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!
This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the National Capital Region.
What You Will Be Doing
Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions
Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners
Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training
Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment
Develop custom data modeling procedures to assist with data mining, modeling, and production
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Develop processes and tools to monitor and analyze model performance and data accuracy
Interpret and communicate results to non-technical customers
What You Must Have
Active high-level security clearance required as part of client contract requirements
Bachelor’s degree in Computer Science, Mathematics, Engineering, or related field
US Citizenship required as part of client contract requirements
Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.
Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
Preferred Skills/Experience
Master’s degree in Computer Science, Mathematics, Engineering, or related field
Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details
Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise
Experience with computational notebook software such as Zeppelin or Jupyter
Experience with the application of visual analytics to computational analytic results
Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)
Experience with database querying like SQL
Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products
Scaled Agile Framework (SAFe) experience
Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired
CompTIA Security+ or higher cybersecurity certification preferred
#cybsr1
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.
Pay Range
- There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$64,372.00 - $109,432.00
Arlington, VA (VA31)
Show more
Show less","Data Mining, Analytics, Security Clearance, Computer Science, Mathematics, Engineering, Data Modeling, Data Analysis, Programming Languages, SQL, Scaled Agile Framework (SAFe), Amazon Web Services (AWS), CompTIA Security+, Zeppelin, Jupyter, Cybersecurity","data mining, analytics, security clearance, computer science, mathematics, engineering, data modeling, data analysis, programming languages, sql, scaled agile framework safe, amazon web services aws, comptia security, zeppelin, jupyter, cybersecurity","amazon web services aws, analytics, comptia security, computer science, cybersecurity, data mining, dataanalytics, datamodeling, engineering, jupyter, mathematics, programming languages, scaled agile framework safe, security clearance, sql, zeppelin"
Senior Data Engineer (AWS),Harnham,Washington DC-Baltimore Area,https://www.linkedin.com/jobs/view/senior-data-engineer-aws-at-harnham-3756540852,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Senior Data Engineer (AWS)
Hybrid (D.C. area)
$100,000 -$150,000
You will be working with a rapidly growing government contracting technical solutions firm in the US. They work on extremely impactful projects with multiple government agencies. They are looking to bring on a full stack software engineer who will hit the ground running and have major growth opportunities in their organization.
Responsibilities
Design, code, test, and debug software applications and integrations.
Collaborate with cross-functional teams to define software requirements and system specifications.
Work closely with team members, stakeholders, and clients to understand requirements, provide updates, and address concerns, fostering a collaborative work environment.
Create and maintain technical documentation for efficient collaboration and knowledge transfer.
Write and execute software tests to ensure reliability, quality, functionality, and performance.
Utilize software development tools, version control systems, and SDLC methodologies to deliver high-quality software within timelines.
Write documentation to support DOD mission system compliance and approval.
Requirements
Proven experience in software engineering, with a strong understanding of software development principles, methodologies, and best practices.
Proficiency in programming languages such as Pyspark, databricks, Python, and familiarity with modern development frameworks and tools.
Excellent problem-solving and analytical thinking abilities, with a focus on delivering innovative and scalable solutions.
Experience collaborating with cross-functional teams and key stakeholders.
Knowledge of cloud-native technologies and experience with cloud-based development and deployment platforms (e.g., AWS)
Understanding of DOD CC SRG, impact levels such as IL-5, NIST 800-53 and Fedramp.
Active Secret Clearance.
Security+ certification.
Preferred Qualifications
Active CAC card or have previously obtained one.
Desired Traits
Takes initiative, proactively manages multiple priorities, and works well under pressure.
Analytical, creative thinker, and strategic problem-solver.
Delivers fast but carefully thought-out, high-quality results.
Structured, organized, and efficient work style.
Factual, direct communication style.
Aligned with our core values: honesty, humility, hard work, commitment, innovation, and exceptionalism.
Seniority Level
Mid-Senior level
Industry
TelecommunicationsComputer & Network SecurityInformation Technology and Services
Employment Type
Full-time
Job Functions
AnalystMarke
Show more
Show less","Software Engineering, Pyspark, Databricks, Python, Modern Development Frameworks, Development Tools, Version Control Systems, SDLC Methodologies, DOD Mission System Compliance, Programming Languages, Analytical Thinking, Problem Solving, Cloudnative Technologies, Cloudbased Development, Cloudbased Deployment Platforms, AWS, Security+ Certification, DOD CC SRG, NIST 80053, FedRamp","software engineering, pyspark, databricks, python, modern development frameworks, development tools, version control systems, sdlc methodologies, dod mission system compliance, programming languages, analytical thinking, problem solving, cloudnative technologies, cloudbased development, cloudbased deployment platforms, aws, security certification, dod cc srg, nist 80053, fedramp","analytical thinking, aws, cloudbased deployment platforms, cloudbased development, cloudnative technologies, databricks, development tools, dod cc srg, dod mission system compliance, fedramp, modern development frameworks, nist 80053, problem solving, programming languages, python, sdlc methodologies, security certification, software engineering, spark, version control systems"
Graph Data Developer #: 23-07299,HireTalent - Diversity Staffing & Recruiting Firm,"Rockville, MD",https://www.linkedin.com/jobs/view/graph-data-developer-%23-23-07299-at-hiretalent-diversity-staffing-recruiting-firm-3784903682,2023-12-17,Sterling,United States,Mid senior,Hybrid,"The our client Knowledge Graph team is looking for an experienced Graph Data Developer to join our team developing the next generation of graph-driven data solutions at our client. Our products connect the people, places, and things enabling pattern detection, self-exploration, and machine learning applications. The right candidate thrives in a team based environment focused on value and outcomes
Required Skills
5 or more years experience with Python or Scala development (OOP and scripting)
3 or more years of experience performing data analysis tasks in big data, graph, business intelligence, or similar environments
2 or more years of experience in AWS
Demonstrated expert level skills in SQL
1 or more years of experience in Neo4j
Experience with large scale data processing engines (Spark, Presto, or equivalent)
Naturally inquisitive and enjoys exploring data and business outcomes
Nice to Have
Financial industry experience
Experience in machine learning environments
Typical Job Activities
Data analytics and processing in Spark
Data profiling new data sources
Design and support of graph architecture
Identification of patterns in data
Engage with upstream and downstream engineering partners in value pipeline improvements
Agile ceremonies and team engagement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, identity, national origin, disability, or protected veteran status.
Show more
Show less","Python, Scala, OOP, SQL, Neo4j, Spark, Presto, AWS, Data analytics, Data profiling, Graph architecture, Pattern identification, Agile, Machine learning","python, scala, oop, sql, neo4j, spark, presto, aws, data analytics, data profiling, graph architecture, pattern identification, agile, machine learning","agile, aws, data profiling, dataanalytics, graph architecture, machine learning, neo4j, oop, pattern identification, presto, python, scala, spark, sql"
Senior Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-steampunk-inc-3734165558,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years experience with a Bachelor's Degree or 5+ years of experience with a Master's Degree
5-7 years industry experience coding commercial software and a passion for solving complex problems.
5-7 years direct experience in Data Engineering with experience in tools such as
Big data tools DataBricks, Confluent Kafka, Collibra, Spark, etc.
Relational SQL and NoSQL databases, including SQL, Postgres and Oracle.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, S3 (or Azure equivalents)
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a Change Agent in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our Human-Centered delivery methodology, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an employee owned company, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, Java, C++, Scala, SQL, PostgreSQL, Oracle, Airflow, Spark, DataBricks, Confluent Kafka, Collibra, EC2, EMR, RDS, S3, Data Processing, Data Modeling, Data Engineering, Agile Development, Big Data, Databases, Data Warehouses, Data Mining, Cloud Computing, Enterprise Architecture, Machine Learning","python, java, c, scala, sql, postgresql, oracle, airflow, spark, databricks, confluent kafka, collibra, ec2, emr, rds, s3, data processing, data modeling, data engineering, agile development, big data, databases, data warehouses, data mining, cloud computing, enterprise architecture, machine learning","agile development, airflow, big data, c, cloud computing, collibra, confluent kafka, data engineering, data mining, data processing, data warehouses, databases, databricks, datamodeling, ec2, emr, enterprise architecture, java, machine learning, oracle, postgresql, python, rds, s3, scala, spark, sql"
Senior Database Engineer,Mastercard,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785140981,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","MySQL, Aurora, MongoDB, AWS, Sharding, Clustering, HA, Scaling, Monitoring, Deploying, Tuning, DBE, DBA, DevSecOps, Cloud Native","mysql, aurora, mongodb, aws, sharding, clustering, ha, scaling, monitoring, deploying, tuning, dbe, dba, devsecops, cloud native","aurora, aws, cloud native, clustering, dba, dbe, deploying, devsecops, ha, mongodb, monitoring, mysql, scaling, sharding, tuning"
Senior Data Engineer,Motion Recruitment,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-motion-recruitment-3765926117,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Our client is seeking a Senior Data Engineer to collaborate with their internal and external requirements. You would be assisting with architecting, implementing, and improving end-to-end pipelines and automation for database processes. Their products are powered by extensive database to enable their clients with insights to make an impact at the local, state, and national level.
Requirements
5+ years with tech
SQL and NoSQL databases
Elasticseach
Spark, Java, or Python
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
LI#-DP1
Posted By:
Derek Progin
Show more
Show less","SQL, NoSQL, Elasticsearch, Spark, Java, Python","sql, nosql, elasticsearch, spark, java, python","elasticsearch, java, nosql, python, spark, sql"
Data Engineer - Python/Java/NoSQL/Big Data/ELK/Hybrid,Motion Recruitment,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-python-java-nosql-big-data-elk-hybrid-at-motion-recruitment-3764709687,2023-12-17,Sterling,United States,Mid senior,Hybrid,"A metro-DC-based company is searching for a Mid-Level Data Engineer for a full-time opportunity working in the government/info systems realm. This company sits on the cutting edge of technology, delivering innovative products and services through the strategic use of data, software and analytics. Their dual customer base of political organizations and commercial clients presents a unique variety of business requirements that drive faster innovation and encourage cross-application of practices between verticals.
This is a position for a data engineer who wants to be involved in every step of the process. You will help design, architect, and build end-to-end database solutions. The role itself is less of a traditional Data Engineering position focused around ETL work, but instead includes a mix of Data Engineering, Big Data, and Software Engineering responsibilities.
This opportunity is 4 days a week on-site in Clarendon, VA and offers excellent pay and benefits.
Required Skills & Experience
BA/BS in Computer Science, Computer Engineering, Statistics, or similar
3+ years working with Python and Java
Proficiency in NoSQL technologies
Proficiency with messaging/streaming technologies (Kafka/RabbitMQ)
Proficiency in version control technologies (Git/SVN/TFS)
Familiarity with Linux/Unix
Familiarity with Elasticsearch, Logstash, and Kibana (ELK stack)
Desired Skills & Experience
Masters or higher in Computer Science, Computer Engineering, Statistics, or similar
Experience with Spring framework, AOP, JPA and REST
Experience with building RESTful web services
Experience with Cloud system architecture and design, large scale streaming data processing
Experience in automation area for database technologies using Chef or Puppet
Experience coding in C#, R, GO, Rust etc.
The Offer You Will Receive The Following Benefits
Medical Insurance, including optional dental and vision benefits
HSA and flexible spending account
3 weeks PTO
401(k) w/ match
Paid parental leave
Infertility/adoption assistance
Bonus eligibility
Posted By:
Lindsay Troyer
Show more
Show less","Data Engineer, Data Engineering, Big Data, Software Engineering, ETL, Python, Java, NoSQL, Kafka, RabbitMQ, Git, SVN, TFS, Linux, Unix, Elasticsearch, Logstash, Kibana, Spring framework, AOP, JPA, REST, Cloud system architecture, SQL, Chef, Puppet, C#, R, GO, Rust","data engineer, data engineering, big data, software engineering, etl, python, java, nosql, kafka, rabbitmq, git, svn, tfs, linux, unix, elasticsearch, logstash, kibana, spring framework, aop, jpa, rest, cloud system architecture, sql, chef, puppet, c, r, go, rust","aop, big data, c, chef, cloud system architecture, data engineering, dataengineering, elasticsearch, etl, git, go, java, jpa, kafka, kibana, linux, logstash, nosql, puppet, python, r, rabbitmq, rest, rust, software engineering, spring framework, sql, svn, tfs, unix"
Data Analyst,BryceTech,"Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-at-brycetech-3753018072,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Company Description
BryceTech has partnered with technology and R&D clients to deliver mission and business success since 2017. Bryce combines core competencies in analytics and engineering with domain expertise. Our teams help government agencies, Fortune 500 firms, and investors manage complex programs, develop IT tools, and forecast critical outcomes. We offer clients proprietary, research-based models that enable evidence-based decision-making. Bryce cultivates a culture of engagement and partnership with our clients. BryceTech is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Job Description
We are looking for an experienced data analyst with strong experience in Python, SQL, OLAP concepts to join our team and support our Federal Government Customer. Responsibilities include:
Develop new data storage schema for workforce data records and migrate existing database records, including developing data extraction, transform, and load routines from systems of record to the newly designed tool
Ensure transition and preservation of historical records from existing database.
Develop quality assurance and data validation routines, and support data collection, integration, cleaning, and processing
Develop data presentations and visualizations based on existing database capabilities, conduct data analysis, develop analytic tools using Tableau and/or Power BI
Develop data access and sharing guidelines for internal and external communities, to include options for public facing data site
Ensure a seamless transition for users to a new system, including documenting usage and maintenance guidelines
Candidate must be proficient in the functionality of Python, SQL, OLAP concepts. Tableau and PowerBI is highly desired to create dashboards for analysis. Knowledge of the R programming language, Shiny, Dash or Streamlit are helpful.
Qualifications
Relevant Bachelor’s degree or higher in data analysis, computer science, or related analytic, engineering, math, science, or technology field.
4+ years of experience in data analysis
Additional Information
BryceTech offers a full range of benefits, including competitive salary, a comprehensive health plan including dental and vision coverage, company-paid life & disability insurance policies, 401(k) plan with company match, and an educational reimbursement program.
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Python, SQL, OLAP, Tableau, Power BI, R, Shiny, Dash, Streamlit, Data visualization, Data analysis, Data storage schema, Data extraction, Data transformation, Data loading, Data validation, Data cleaning, Data processing, Data presentations, Data access guidelines, Data sharing guidelines, User documentation","python, sql, olap, tableau, power bi, r, shiny, dash, streamlit, data visualization, data analysis, data storage schema, data extraction, data transformation, data loading, data validation, data cleaning, data processing, data presentations, data access guidelines, data sharing guidelines, user documentation","dash, data access guidelines, data cleaning, data extraction, data loading, data presentations, data processing, data sharing guidelines, data storage schema, data transformation, data validation, dataanalytics, olap, powerbi, python, r, shiny, sql, streamlit, tableau, user documentation, visualization"
Senior Database Administrator / Database Engineer,CoStar Group,"Washington, DC",https://www.linkedin.com/jobs/view/senior-database-administrator-database-engineer-at-costar-group-3762966267,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Company Overview
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives.
We have been living and breathing the world of real estate information and online marketplaces for over 35 years, giving us the perspective to create truly unique and valuable offerings to our customers. We’ve continually refined, transformed and perfected our approach to our business, creating a language that has become standard in our industry, for our customers, and even our competitors. We continue that effort today and are always working to improve and drive innovation. This is how we deliver for our customers, our employees, and investors. By equipping the brightest minds with the best resources available, we provide an invaluable edge in real estate.
Position Overview
CoStar Group serves up over 1.8PB of data from more than 800 SQL Servers that span four On-Premise data centers, two continents, and six different time zones and we are expanding to AWS Cloud solution rapidly and start to support Aurora MySQL, PostgreSQL, DynamoDB, Databricks, Snowflake and MongoDB Atlas and other DB engines on the cloud. Our highly motivated, extremely talented DBA group is looking for an exceptionally bright
Senior Database Administrators / Database Engineers
both On-prem and AWS Cloud that will help the company continue to grow to the next level. If you are passionate about automating and programming to support databases, this is your job! Our goal is to provide an exceptional experience for our customers through continued development, refinement, and improvement of our industry-leading applications.
We are a data-centric; Microsoft focused shop that uses the latest SQL Server, AWS RDS, DynamoDB and .NET technologies for major applications and also supports MongoDB, and more. As a member of the team, you will be relied on for your problem-solving, communication, collaboration, project management, and process adherence skills.
This position is located in Washington, DC and offers a hybrid schedule of 3 days onsite and 2 days remote per week.
Responsibilities
Demonstrate extensive knowledge of all aspects of Microsoft SQL Server, including administration, replication, backup/restore procedures and related components such as Reporting Services, Analysis Services, and Integration Services.
Implement and Maintain Automated Processes using TSQL/PowerShell/Python scripts
Understand AWS-based architecture and closely work with DevOps, SecOps and develop scripts to deploy and maintain the environment.
Install, configure, monitor, and maintain databases in the production, development, and quality assurance environments.
Recommend and implement solutions for SQL performance monitoring, tuning, and automation.
Understand SAN-based storage and related technologies, specifically as applicable and useful to MS SQL Server installations.
Work with the Operations team planning server hardware, storage, and OS configuration upgrades to help get the most out of SQL Server for specific application requirements and performance metrics.
Process database change requests, including the creation and modification of databases, tables, views, stored procedures, triggers, jobs, etc. in accordance with change control policies.
Analyze problems, anticipate future problem areas, and implement solutions.
Work outside of normal business hours as required to complete projects, work on deployments, and resolving system outages.
To be placed on on-call rotation for a week every 3 months and respond to system alerts
Basic Qualifications
Bachelor's degree required from an accredited, not for profit university or college
5+ years of experience with Microsoft SQL Server 2016 / 2017 / 2019 supporting high-traffic websites.
Experience with the implementation and support of database replication.
Experience working with very large databases, upgrades, troubleshooting performance, etc.
Experience with the implementation and support of SQL database replication via script
Experience with SQL Server High Availability Architecture (AlwaysOn) and troubleshooting.
Understanding of Windows Server configuration and management, specifically as it relates to MS SQL Server installations and mission-critical systems.
Excellent understanding of coding with Microsoft T-SQL and stored procedures.
Desire to learn and administrate new cloud technologies in order to support a variety of database engines such as AWS Database Engines (RDS, DynamoDB, Atlas MongoDB)
Experience with adhering to and implementing best practices within an enterprise environment that includes using source control for all database-related artifacts (including jobs, SSIS packages, and replication configuration) and following a change request/notification policy.
Experience with either Python scripting and/or PowerShell is required
Experience with TFS/Visual Studio Database Projects, Azure DevOps, Terraform, CloudFormation, or any other CI/CD with a Source Control system is required.
Experience with SANs (3PAR/EMC/Pure), specifically with aspects related to MS SQL Server.
Understands and is able to properly implement backup and disaster recovery procedures.
Excellent work ethic and willingness to put in significant extra time with large team members are required.
Preferred Qualifications
Strong AWS experience
Deep Knowledge of Relational Database Management experience such as MS SQL, Aurora / RDS PostgreSQL, MySQL, and Redshift is preferred.
Deep knowledge of Azure DevOps / Terraform / GitHub is a big plus
NoSQL database experience (MongoDB/Hadoop/Redis/DynamoDB/Elastic Search)
Experienced with automation and deployment tools & Scripts: MS Team Foundation Service, ELK, PRTG, Lambda, CloudFormation, etc.
AWS solution architect certification
Any other programming skills are valuable, such as C#
Having knowledge of all aspects of Azure Database Engines (Managed Instances, Data Warehouse), including administration
Azure solutions architect associate certification
Excellent technical expertise in relevant state-of-the-art data technologies, database warehouse data security, data systems, and its design. Able to formulate and present various solutions for business initiatives and discuss pros/cons for each option.
Knowledge of monitoring tools such as Kibana, CloudWatch, DataDog, or any other tools, etc.
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar is not able to provide visa sponsorship for this position.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","SQL Server, SQL Services, Reporting Services, Analysis Services, Integration Services, TSQL, PowerShell, Python, AWS, RDS, DynamoDB, .NET, MongoDB, SANbased storage, Server hardware, Storage, OS configuration, TSQL, Stored procedures, Cloud technologies, Source control, Change request/notification policy, Python scripting, PowerShell, TFS/Visual Studio Database Projects, Azure DevOps, Terraform, CloudFormation, CI/CD, Source Control, NoSQL databases, Automation and deployment tools, Scripts, MS Team Foundation Service, ELK, PRTG, Lambda","sql server, sql services, reporting services, analysis services, integration services, tsql, powershell, python, aws, rds, dynamodb, net, mongodb, sanbased storage, server hardware, storage, os configuration, tsql, stored procedures, cloud technologies, source control, change requestnotification policy, python scripting, powershell, tfsvisual studio database projects, azure devops, terraform, cloudformation, cicd, source control, nosql databases, automation and deployment tools, scripts, ms team foundation service, elk, prtg, lambda","analysis services, automation and deployment tools, aws, azure devops, change requestnotification policy, cicd, cloud technologies, cloudformation, dynamodb, elk, integration services, lambda, mongodb, ms team foundation service, net, nosql databases, os configuration, powershell, prtg, python, python scripting, rds, reporting services, sanbased storage, scripts, server hardware, source control, sql server, sql services, storage, stored procedures, terraform, tfsvisual studio database projects, tsql"
Senior Big Data Engineer,Homes.com,"Washington, DC",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-homes-com-3785820520,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
Job Description
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives.
Homes.com is already one of the fastest growing real estate portals in the industry, we are driven to be #1. Just ask Brad Bellflower, Chief Change Officer at Apartments.com. After its acquisition in 2014, Apartments.com quickly turned into the most popular place to find a place. Proven success at the highest level – and we’re doing it again with the new Homes.com. Homes.com is a CoStar Group company with 20+ years' experience in leading and growing digital marketplaces. We pride ourselves on continually improving, innovating, and setting the standard for property search and marketing experiences. With Homes.com we’re building a brand on the cusp of defining the industry.
We’re looking for big thinkers, brave leaders, and creative advertising wizards ready to influence a new age of homebuying within a tried-and-true, award-winning company.
Learn more about Homes.com.
We are hiring a talented Senior Big Data Engineer to build cloud-based data pipelines for machine learning, data processing with Apache Spark, and database development.
This position is located in Washington, DC and offers a hybrid schedule of 3 days onsite, 2 days remote.
Responsibilities
Designing, building, testing and deploying scalable, reusable and maintainable applications that handle substantial amounts of data.
Taking full ownership of your work from development and testing to eventual deployment and support in production.
Collaborating with other engineers, product owners, designers, and leadership.
Becoming a trusted team member in matters of technical architecture, design and code.
Advocating for evolution and improvement - both technical and non-technical - within our teams. Includes new tech, tools and best practices.
Basic Qualifications
Bachelor’s Degree required from an accredited, not for profit university or college, with degree preferably in Computer Science, Data Science, or related field. MSc or PhD is a plus.
A track record of commitment to prior employers
A demonstrable record of accomplishment of building and launching successful products that use terabytes of data
5+ years of data pipeline engineering experience, and/or deep database engineering experience
Ability to analyze technical requirements and design new architectures, data models and ETL strategies
Hands-on experience with cloud-based relational and non-relational databases and proficiency in SQL.
Deliver work products that meet specifications, are free of defects, with excellent performance
Define Architecture and Development best practices
Preferred Skills
Performance tuning of database queries (SQL Server or PostgreSQL), database design, monitoring, and analysis
Experience with No-SQL databases (e.g. DynamoDB)
Experience with data pipeline tools (e.g. Glue, Step Functions, Lambda)
Experience using Confluent Kafka
Knowledge and/or experience working with Apache Spark/Databricks
Monitoring & dashboard metric management (e.g. CloudWatch, Kibana)
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
On-site fitness center and/or reimbursed fitness center membership costs (location dependent), with yoga studio, Pelotons, personal training, group exercise classes
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar Group is not able to provide visa sponsorship for this position.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","Cloudbased data pipelines, Machine learning, Data processing, Apache Spark, Database development, SQL, Cloudbased relational databases, Nonrelational databases, NoSQL databases, Data pipeline tools, Confluent Kafka, Apache Spark/Databricks, Monitoring & dashboard metric management, CloudWatch, Kibana, Performance tuning, Database design, Monitoring, Analysis, ETL strategies","cloudbased data pipelines, machine learning, data processing, apache spark, database development, sql, cloudbased relational databases, nonrelational databases, nosql databases, data pipeline tools, confluent kafka, apache sparkdatabricks, monitoring dashboard metric management, cloudwatch, kibana, performance tuning, database design, monitoring, analysis, etl strategies","analysis, apache spark, apache sparkdatabricks, cloudbased data pipelines, cloudbased relational databases, cloudwatch, confluent kafka, data pipeline tools, data processing, database design, database development, etl strategies, kibana, machine learning, monitoring, monitoring dashboard metric management, nonrelational databases, nosql databases, performance tuning, sql"
Data Integration Engineer/ Developer,Nine Mind Solutions,"Dulles, VA",https://www.linkedin.com/jobs/view/data-integration-engineer-developer-at-nine-mind-solutions-3768021546,2023-12-17,Sterling,United States,Mid senior,Hybrid,"We are seeking a qualified Cyber Security Data Integration Engineer/ Developer to support the design, development, and deployment of advanced cybersecurity capabilities.
Eligibility
Must be a US Citizen
Must have an active TS/SCI clearance.
Must be able to obtain Client Suitability prior to starting employment
6+ years of directly relevant experience
4+ years of experience with administration of enterprise SIEM technologies ( Splunk primarily)
Responsibilities
Responsibilities :
The Security Engineer is to play a key role in supporting a statewide program providing cyber assessment services and management that will protect 20+ affiliates from growing and evolving cyber threats. The engineering effort will focus on cloud security, SIEM and log management, and endpoint detection/response protecting customers from the ever growing and evolving cyber threats. This person will also work with customers to ensure the organization's compliance standards are met and maintained while also driving solid customer relationships to the next level.
This position requires a thorough understanding of network architecture fundamentals, protocols, routing, firewalls, cloud, and DevOps. This position is part of a larger team; however, the candidate is expected to work well on his or her own under general supervision, be self-directed, able to multi-task, and prioritize work.
Required Skills
Splunk Cloud experience: Architect, design, engineer, support, configure, administer content and maintain infrastructure for a highly available and disaster recovery configuration
Splunk experience: Administer Splunk and Splunk Application for Enterprise Security log or event management
Expertise with EDR toolsets administration, analysis, and integrations preferably CrowdStrike
Familiarity with SOAR Products include Phantom and ThreatConnect
Experience with scripting (e.g., PowerShell, bash/ksh/sh,python)
Ability to assist team with Incident response and handling
Excellent demonstrated experience in communicating technical information to non-technical and technical audiences.
Experience working directly with senior leadership and management.
Desired Skills
Automation: Experience related to Ansible for performing administration using code and Git/Gitlab for workflow management
Familiarity with Windows and Linux integration, SQL database technologies, troubleshooting, deployment, patching, and administration
Experience with Logstash and ability to collect, parse, and transform logs
Experience with the standards compliance process (e.g., NIST) and writing network security documentation
Desired Certifications : Splunk IT Service Intelligence Certified Admin, Splunk Enterprise Security Certified Admin, Splunk Cloud Certified Admin, CCNA, CCNP), NCSP 800-53 Practitioner
Required Education : Bachelor's degree in Information Security, Cyber Engineering or a related discipline is required. Two years of related work experience may be substituted for each year of degree-level education.
Show more
Show less","Cybersecurity, Data Integration, Splunk Cloud, SIEM, EDR, CrowdStrike, Phantom, ThreatConnect, PowerShell, Bash, Ksh, Sh, Python, Incident Response, Ansible, Git, Gitlab, SQL, Logstash, NIST, CCNA, CCNP, NCSP 80053 Practitioner, Information Security, Cyber Engineering","cybersecurity, data integration, splunk cloud, siem, edr, crowdstrike, phantom, threatconnect, powershell, bash, ksh, sh, python, incident response, ansible, git, gitlab, sql, logstash, nist, ccna, ccnp, ncsp 80053 practitioner, information security, cyber engineering","ansible, bash, ccna, ccnp, crowdstrike, cyber engineering, cybersecurity, data integration, edr, git, gitlab, incident response, information security, ksh, logstash, ncsp 80053 practitioner, nist, phantom, powershell, python, sh, siem, splunk cloud, sql, threatconnect"
Senior Cloud Data Engineer,BDO USA,"Potomac, MD",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470287,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, SQL, C#, Python, Java, Scala, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Data Definition Language, Data Manipulation Language, Views, Functions, Stored Procedures, Performance Tuning, Semantic Model Definition, Star Schema Construction, Tabular Modeling, Microsoft Fabric, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, data warehousing, data modeling, sql, c, python, java, scala, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, data definition language, data manipulation language, views, functions, stored procedures, performance tuning, semantic model definition, star schema construction, tabular modeling, microsoft fabric, azure data factory, redshift, uipath, cloud, rpa, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, automation tools, aws, aws lake formation, azure analysis services, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud, computer vision, data definition language, data lake medallion architecture, data manipulation language, data ops, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, functions, git, java, kinesis, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, quicksight, redshift, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, streaming data ingestion, tabular modeling, terraform, uipath, views"
Senior Cloud Data Engineer,BDO USA,"Washington, DC",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467839,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data analytics, Business intelligence, AI, Machine learning, Data modeling, Data ingestion, Semantic modeling, Automation, RPA, DevOps, Cloud computing, Azure, AWS, SQL, C#, Python, Java, Scala, Git, Linux, Data lake, Star schema, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, ai, machine learning, data modeling, data ingestion, semantic modeling, automation, rpa, devops, cloud computing, azure, aws, sql, c, python, java, scala, git, linux, data lake, star schema, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai, automation, aws, azure, bicep, business intelligence, c, cloud computing, data ingestion, data lake, data ops, dataanalytics, datamodeling, dbt, delta, devops, git, java, linux, machine learning, microsoft fabric, pandas, purview, python, rpa, scala, semantic modeling, spark, spark sql, sql, ssas, ssis, ssrs, star schema, terraform"
Senior Data Engineer - Data Quality/Governance,SiriusXM,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-quality-governance-at-siriusxm-3752310020,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role, you will be a member of SiriusXM’s Data Quality (DQ) Team within the larger Data Organization, and will be responsible for designing, developing and supporting data quality tools, applications and data marts for use by the DQ Team and other groups.
What You’ll Do
Define technical requirements for DQ tools and applications that support profiling, controls, alerts, KPIs and dashboards for business-critical partner and internal data utilized across SXM’s systems.
Design, develop and improve applications and tools to monitor data quality using data science algorithms and methodologies. This includes cloud-based data pipeline frameworks and workflow orchestration tooling.
Design and develop a data quality analytics framework to standardize the detection of data issues, identify root cause, and drive improvements in data processing procedures.
Develop subject matter expertise in SXM’s data domains (e.g., car/radio lifecycle, customer, marketing, streaming, contact center) to develop advanced quality controls and more easily navigate anomalies affecting a broad cross-functional audience.
Become proficient in using in-house and off-the-shelf DQ tools and applications.
Write documentation to encourage adoption of these tools and support users in their use; support analysts as needed with query/ETL optimization.
Strengthen corporate best practices around data engineering software development processes.
What You’ll Need
BS/MS or above in Computer Science or related field, or relevant experience
5+ years of overall work experience, including demonstrated understanding of the software development life cycle
5+ years of experience in a subscription services or data-driven industry or environment
5+ years of experience developing data ETL pipelines and data tools in Scala and/or Python
Experience with data technologies: e.g., Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies: e.g., Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing: e.g., Google Cloud Platform, Amazon Web Services
Experience with Data Visualization or Data Notebook tools: e.g., Jupyter, Zeppelin, Tableau, etc.
Experience developing and deploying machine learning algorithms
Experience with workflow management systems: e.g., Airflow, Composer, Luigi
Experience with unit and integration testing frameworks
Experience with API design/development: e.g., RPC, REST
Experience with DevOps tools and practices: e.g., Version control, CI/CD, Infrastructure as Code, build/deployment systems, performance monitoring
Experience with data serialization systems: e.g., Avro, Protobuf
Good public speaking and presentation skills
Interpersonal skills and ability to interact and work with staff at all levels
Excellent written and verbal communication skills
Ability to work independently and in a team environment
Attention to detail and organizational skills
Ability to project professionalism over the phone and in person
Ability to handle multiple tasks in a fast-paced environment
Commitment to “internal client” and customer service principles
Willingness to take initiative and to follow through on projects
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $195,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-05-126
Show more
Show less","Data Quality, Data Tools, Data Marts, Data Pipelines, Data Analytics Framework, Machine Learning Algorithms, SQL Applications, Cloud Computing, Data Visualization Tools, Data Notebook Tools, Workflow Management Systems, Unit and Integration Testing, API Design/Development, DevOps Tools, Data Serialization Systems, Scala, Python, Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Jupyter, Zeppelin, Tableau, Airflow, Composer, Luigi, Version Control, CI/CD, Avro, Protobuf","data quality, data tools, data marts, data pipelines, data analytics framework, machine learning algorithms, sql applications, cloud computing, data visualization tools, data notebook tools, workflow management systems, unit and integration testing, api designdevelopment, devops tools, data serialization systems, scala, python, databricks, mapreduce, hdfs, hive, tez, spark, sqoop, kafka, kafka connect, kstreams, ksql, kinesis, beam, flink, jupyter, zeppelin, tableau, airflow, composer, luigi, version control, cicd, avro, protobuf","airflow, api designdevelopment, avro, beam, cicd, cloud computing, composer, data analytics framework, data marts, data notebook tools, data quality, data serialization systems, data tools, data visualization tools, databricks, datapipeline, devops tools, flink, hdfs, hive, jupyter, kafka, kafka connect, kinesis, ksql, kstreams, luigi, machine learning algorithms, mapreduce, protobuf, python, scala, spark, sql applications, sqoop, tableau, tez, unit and integration testing, version control, workflow management systems, zeppelin"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3688219277,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Preferred
Experience with Informix and Data Stage
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Scala, SQL, Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, SQL Server, Big Data, Agile, Informix, Data Stage","data engineering, python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, scala, sql, amazon s3, athena, redshift spectrum, aws glue, aws glue catalog, aws functions, sql server, big data, agile, informix, data stage","agile, airflow, amazon s3, athena, aws, aws functions, aws glue, aws glue catalog, azkaban, big data, cassandra, data engineering, data stage, ec2, elasticsearch, emr, hadoop, informix, kafka, lucene, luigi, postgres, python, rds, redshift, redshift spectrum, scala, solr, spark, sparkstreaming, sql, sql server, storm"
Data Engineer,IntraFi,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-at-intrafi-3757234803,2023-12-17,Sterling,United States,Mid senior,Hybrid,"What is the role?
You will be a key member of our IT Data Engineering team responsible for providing the company with accurate and timely data gathered from our business processes and external sources. Your primary responsibility will be to assist in establishing the security, architecture, engineering, operations, and governance of the company’s data pipelines, data stores, and associated cloud-based infrastructure.
Your Responsibilities Will Include
Designing cloud infrastructure architecture options for the company’s data pipelines and stores
Researching and implementing best practices derived from industry experiences and standards
Facilitating and leading cross-functional teams to define, expand, document, and implement the company's data pipelines and data lake resources
Further developing subject matter expertise in data and cloud architecture and technologies to support initiatives across the company
Managing identity and access management solutions for the cloud infrastructure
Refining data governance and compliance standards to ensure the security and auditability of the data platform
Learning, understanding, and assisting with the operational management of the existing on-premises data applications and infrastructure
You Should Possess The Following Experience, Skills, And Qualifications
3+ years of progressive systems infrastructure and cloud experience
5+ years of infrastructure as code or software development experience; Python experience is preferred
3+ years of ETL and data pipeline experience using tools such as Talend, Airflow, SSIS, and AWS Glue
3+ years of database experience, including database administration, queries, and data modelling
Bachelor’s degree in computer science or equivalent experience; a Master’s degree is preferred
Experience designing, deploying, and supporting cloud technologies using AWS with expertise in AWS storage, security, data lake, database, and serverless compute tools
An infectious sense of exploration and experimentation, allowing you to learn new technologies and functional areas quickly
Ability to create standards and procedures for the production use of sufficiently mature technologies
Exceptional presentation, written, and verbal communication skills directed to both technical and nontechnical audiences
Outstanding interpersonal skills for listening to the input and needs of clients and building consensus among the team
Excellent organizational, planning, and project management skills with the ability to demonstrate mature, timely, and professional problem-solving abilities
Must demonstrate initiative and be customer-focused, self-directed, and results/goal-oriented
Show more
Show less","Cloud Infrastructure Architecture, Data Pipelines, Data Stores, Data Governance, Compliance Standards, Identity and Access Management, Data Applications, Infrastructure as Code, Software Development, Python, ETL, Data Pipeline, Talend, Airflow, SSIS, AWS Glue, Database Administration, Queries, Data Modelling, AWS Storage, AWS Security, AWS Data Lake, AWS Database, Serverless Compute Tools, DevOps, Agile, Scrum","cloud infrastructure architecture, data pipelines, data stores, data governance, compliance standards, identity and access management, data applications, infrastructure as code, software development, python, etl, data pipeline, talend, airflow, ssis, aws glue, database administration, queries, data modelling, aws storage, aws security, aws data lake, aws database, serverless compute tools, devops, agile, scrum","agile, airflow, aws data lake, aws database, aws glue, aws security, aws storage, cloud infrastructure architecture, compliance standards, data applications, data governance, data modelling, data pipeline, data stores, database administration, datapipeline, devops, etl, identity and access management, infrastructure as code, python, queries, scrum, serverless compute tools, software development, ssis, talend"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3757463845,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned
Data Engineer
to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
10+ years industry experience coding commercial software and a passion for solving complex problems.
10+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Python, AWS, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Java, C++, Scala, SQL, Agile","python, aws, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, java, c, scala, sql, agile","agile, airflow, aws, azkaban, c, cassandra, ec2, elasticsearch, emr, hadoop, java, kafka, lucene, luigi, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3734162947,2023-12-17,Sterling,United States,Mid senior,Hybrid,"In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
US Citizen Only
Ability to hold a position of public trust with the US government.
2-4 years industry experience coding commercial software and a passion for solving complex problems.
2-4 years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Python, AWS, Data Exploitation, HumanCentered Design, DevSecOps, ETL, BI tools, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, Hadoop, Spark, Kafka, SQL, NoSQL, Java, C++, Scala, Data modeling, Agile, Data warehousing","data engineering, python, aws, data exploitation, humancentered design, devsecops, etl, bi tools, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, hadoop, spark, kafka, sql, nosql, java, c, scala, data modeling, agile, data warehousing","agile, airflow, aws, azkaban, bi tools, c, cassandra, data engineering, data exploitation, datamodeling, datawarehouse, devsecops, ec2, elasticsearch, emr, etl, hadoop, humancentered design, java, kafka, lucene, luigi, nosql, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3757460949,2023-12-17,Sterling,United States,Mid senior,Hybrid,"Overview
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.
For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.
Contributions
We are looking for seasoned
Data Engineer
to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications
US Citizen Only
Ability to hold a position of public trust with the US government.
5+ years industry experience coding commercial software and a passion for solving complex problems.
5+ years direct experience in Data Engineering with experience in tools such as
Big data tools Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra.
Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)
Data streaming systems Storm, Spark-Streaming, etc.
Search tools Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Data Engineering, Data Platforms, Data Pipelines, ETL Jobs, BI Tools, Data Products, AWS, Azure, GCP, Python, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Solr, Lucene, Elasticsearch, SQL, NoSQL, Java, C++, Scala, Data Modeling, Agile, DevOps, Machine Learning","data engineering, data platforms, data pipelines, etl jobs, bi tools, data products, aws, azure, gcp, python, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, ec2, emr, rds, redshift, storm, sparkstreaming, solr, lucene, elasticsearch, sql, nosql, java, c, scala, data modeling, agile, devops, machine learning","agile, airflow, aws, azkaban, azure, bi tools, c, cassandra, data engineering, data platforms, data products, datamodeling, datapipeline, devops, ec2, elasticsearch, emr, etl jobs, gcp, hadoop, java, kafka, lucene, luigi, machine learning, nosql, postgres, python, rds, redshift, scala, solr, spark, sparkstreaming, sql, storm"
Lead Data Analyst,Texas Tech University,"Lubbock, TX",https://www.linkedin.com/jobs/view/lead-data-analyst-at-texas-tech-university-3733827590,2023-12-17,Lubbock,United States,Mid senior,Onsite,"Lubbock
Lead Data Analyst
35349BR
Data Management Division
Position Description
Responsibilities
Performs specialized analytical duties in the operation and maintenance of assigned area. Responsibilities include collecting, analyzing and developing data relative to area, making recommendations and assisting in implementation of projects. Work is performed under general supervision with latitude for independent judgment in accordance with established policies and procedures. While some management oversight may be expected for specific projects, this position is expected to exercise discretion and independent judgment in the performance of the following duties:
Performs data analysis and develops database actions defined in project plans to meet customer requirement and/or marketing objectives.
About The Department And/or College
Through collaboration and coordination with campus partners, the Data Management Division will support all areas of Texas Tech University with the development, execution, and supervision of plans, policies, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their lifecycles.
Major/Essential Functions
Oversight for Data Governance Projects (Data Quality and Audits).
Evaluate/understand high level data flow and data exchange between systems.
Evaluate technical standards for critical systems.
Coordinate with technical staff on metadata, data dictionaries, data protection, and data quality management.
Understands data across systems and alignment with business processes.
Supports change controls related to critical data.
Manages initial triage work for data related issues and projects before it is escalated to the Data Stewardship Council.
Identify and track critical dependencies between business requirements and data.
Required Qualifications
Bachelor's degree plus four years progressively responsible related experience; OR a combination of education and/or experience to equal eight years
Preferred Qualifications
Experience developing data quality measures that align with business processes. Understanding of Master Data, Metadata, Reference Data, Data Warehousing, and BI principles and processes including technical architecture. Familiarly with enterprise information tools like SQL Server, Power BI, Oracle, etc. Excellent soft skills, including the ability to communicate well with various levels. Experience with Data Management Principles or Data Governance and Technical Writing.
Safety Information
Adherence to robust safety practices and compliance with all applicable health and safety regulations are responsibilities of all TTU employees.
Occasional Duties
Occasional work outside of normal hours required to meet project deadlines and travel to attend conferences or training.
Does this position work in a research laboratory?
No
Required Attachments
Cover Letter, Professional/Personal References, Resume / CV
Optional Attachments
Transcript
Job Type
Full Time
Pay Basis
Hourly
Minimum Hire Rate
20.76
Pay Statement
Compensation is commensurate upon the qualifications of the individual selected and budgetary guidelines of the hiring department, as well as the institutional pay plan. For additional information, please reference the institutional pay plan by visiting www.depts.ttu.edu/hr/payplan.
Travel Required
Up to 25%
Shift
Day
Schedule Details
M-F, 8AM-5PM
Grant Funded?
No
Job Group
Information and Records Clerks
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age, disability, genetic information or status as a protected veteran.
Show more
Show less","Data Analysis, Data Governance, Data Quality, Data Warehousing, Business Intelligence, SQL Server, Power BI, Oracle, Master Data, Metadata, Reference Data, Technical Writing, Data Management Principles, Data Stewardship","data analysis, data governance, data quality, data warehousing, business intelligence, sql server, power bi, oracle, master data, metadata, reference data, technical writing, data management principles, data stewardship","business intelligence, data governance, data management principles, data quality, data stewardship, dataanalytics, datawarehouse, master data, metadata, oracle, powerbi, reference data, sql server, technical writing"
data engineer iv,Skiltrek,"Rockledge, FL",https://www.linkedin.com/jobs/view/data-engineer-iv-at-skiltrek-3785509484,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"Job Summary:
Are you a process driven IT professional? Are you looking to work for a company where you can grow and learn and be a part of an outstanding health and IT organization? Our client in Rockledge, FL is looking for a highly motivated Data Engineer with Salesforce Development experience as well as as well as ETL and data integration experience to add to their quickly growing IT organization. This is a 13 week contract role with the possibility of extension. 100% remote. Visa sponsorship is not available for this opportunity.
location: ROCKLEDGE, Florida
job type: Contract
salary: $50 - 75 per hour
work hours: 8am to 5pm
education: Bachelors
Responsibilities:
POSITION SUMMARY
The Data Engineer IV serves as a senior technical member of an engineering team. The position will work across multiple disciplines to satisfy key business initiatives by delivering data and analytics solutions that scale efficiently while improving performance and minimizing costs. The Data Engineer IV works independently, with minimal supervision and is the lead for their technical domain or discipline. This role provides technical leadership to the engineering team, is able to work across multiple disciplines, and is looked to by others for guidance.
The Data Engineer IV is a lead contributor in developing strategies, standards, new capabilities, and design to enable faster and easier data access, automated workflows, and technical proofs of concept. The Data Engineer IV will have experience in design, implementation and support of data integration and ETL/ELT solutions within healthcare or a comparable industry. The position will require experience and working knowledge of Agile and/or Lean methodologies to deliver information services in an efficient and repeatable manner.
The Data Engineer IV proactively works with customers, stakeholders, analysts, architects, and other engineers to troubleshoot and find root cause of solution defects. They need to be excellent team players and have demonstrated experience across multiple systems.
Primary Accountabilities
Lead contributor to the development and design of end-to-end enterprise data delivery and analytic solutions using varying technologies (ETL, API, SQL, cloud), conducts reviews and validation of systems, code, and security.
Develops data solution and security standards for the organization.
Drives resolution of complex issues and implements corrective actions to prevent future occurrences.
Leads automation initiatives for testing and deployment.
Collaborates with business stakeholders to institute and advance data governance and quality frameworks.
Leads and coordinates cross functional teams and vendors to ensure the health of client's IT systems.
Reviews and jointly develops specifications and requirements for complex data integration and delivery products with users, vendors, and other engineers.
Will manage/lead complex technical projects. Proactively communicates with users and management the status of projects and requests.
Supports, maintains, and troubleshoots enterprise data delivery solutions with the ability to perform root cause analysis (RCA).
Shares expertise with a variety of business data domains, source systems, and business processes to enable delivery of solutions applicable across the business' clinical and business functions.
Minimum Qualifications
Education: Bachelor's Degree in Computer Science or equivalent STEM field
Licensure: None Required
Certification: Advanced data delivery-specific technical certification within one year of hire.
For Example:
Certified Data Vault 2.0 Practitioner (CDVP2)
TDWI Certified Business Intelligence Professional (CBIP)
DAMA Certified Data Management Professional (CDMP)
Work Experience: Six years of experience in delivery of data engineering solutions on large, heterogeneous datasets, to include data modeling, database/data mart development, ETL/ELT/data integration, and data security.
Work Experience In Lieu Of Education:
Associates degree plus six years of applicable technical experience in areas as noted above.
Or, in lieu of no degree, ten years of applicable technical experience in areas as noted above.
Knowledge/Skills/Abilities:
strong understanding of current developments and trends in Information Technology in one or more of the following domains:
Data Warehousing
ETL/ELT/Data integration
Data modeling / Database design
Ability to work in SQL, Oracle SQL and DB2
Data integration technologies such as data replication/CDC, message-oriented data movement, API, data stream integration, or CEP.
Dimensional modeling / Star Schema
Data governance framework (DMBOK, TDWI, etc.)
strong soft skills, verbal and written communication, including the ability to translate complex technical topics into understandable terminology, is a necessity.
This position must have the proven ability to lead large projects and initiatives from kick off to completion.
This position builds trust among colleagues, must be a continuous learner, and focus on delivering capabilities for the customer.
Preferred Qualifications
Education: None
Licensure: None
Certification: Existing advanced data delivery-specific technical certification. For example:
Certified Data Vault 2.0 Practitioner (CDVP2)
TDWI Certified Business Intelligence Professional (CBIP)
DAMA Certified Data Management Professional (CDMP)
Work Experience: 6+ years of experience preferred.
Knowledge/Skills/Abilities:
Cloud services, especially Azure
Hadoop/Mongo DB experience
R, Python, C#, Java or other OO development
Knowledge of MUMPS
Formal training in Lean or Agile
Physical Requirements
Sedentary
Majority of time involves sitting or standing; occasional walking, bending, stooping
Long periods of computer time or at workstation
Light work that may include lifting or moving objects up to 20 pounds with or without assistance.
May be exposed to inside environments with varied temperatures, air quality, lighting and/or low to moderate noise
Communicating with others to exchange information.
Visual acuity and hand-eye coordination to perform tasks
Workspace may vary from open to confined, on site or remote
May require travel to various facilities within and beyond county perimeter; may require use of personal vehicle
Qualifications:
Experience level: Experienced
Minimum 6 Years Of Experience
Education: Bachelors (required)
Skills:
Data Warehouse
ETL
About Us
Skiltrek is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US.
At Skiltrek, we promise you the perfect opportunity of building technical excellence, understand business performance and nuances,
be abreast with the latest happenings in technology world and enjoy a satisfying work life balance.
Skiltrek is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender,
race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law.
Skiltrek is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Show more
Show less","Data Engineering, Salesforce Development, ETL, Data Integration, Agile, Lean, SQL, Oracle SQL, DB2, Data Replication, CDC, MessageOriented Data Movement, API, Data Stream Integration, CEP, Dimensional Modeling, Star Schema, Data Governance Framework, DAMA, Hadoop, Mongo DB, R, Python, C#, Java, MUMPS","data engineering, salesforce development, etl, data integration, agile, lean, sql, oracle sql, db2, data replication, cdc, messageoriented data movement, api, data stream integration, cep, dimensional modeling, star schema, data governance framework, dama, hadoop, mongo db, r, python, c, java, mumps","agile, api, c, cdc, cep, dama, data engineering, data governance framework, data integration, data replication, data stream integration, db2, dimensional modeling, etl, hadoop, java, lean, messageoriented data movement, mongo db, mumps, oracle sql, python, r, salesforce development, sql, star schema"
Senior Principal Data Engineer (Melbourne FL),Northrop Grumman,"Melbourne, FL",https://www.linkedin.com/jobs/view/senior-principal-data-engineer-melbourne-fl-at-northrop-grumman-3768861310,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Are you motivated to work in an environment that will challenge you, force you to continuously innovate, and work on solutions that make a difference for your customers?
Northrop Grumman Aerospace Systems is looking for a passionate
Senior Principal Data Engineer
in
Melbourne Florida
to design and develop automated, end-to-end, ETL pipelines and Data Analytics/Visualization solutions from disparate data sources. You will share in the ownership of the technical vision and direction for advanced analytics systems that change the way we see and use data. We are looking for people who are self-motivated, hardworking, and have demonstrated the ability to find innovative solutions to complex technical problems.
Job Responsibilities
Design, develop, and maintain a scalable ETL pipelines.
Enable storing, searching, processing, and securing of extremely large structured or unstructured data sets.
Data preparation/cleaning, integration, and automation from heterogeneous sources
Ensure data integrity and system availability
Identify, evaluate, and recommend core technologies and strategies
Monitor and optimize system performance
Decomposition of user requirements into logical functions/components
Basic Qualifications for Senior Principal Data Engineer:
Bachelor's in a STEM related field with 9 years of experience; Master's in a STEM related field with 7 years of experience. PhD with 4 years of experience.
5+ years of experience working with ETL techniques and frameworks
Understanding of elastic data storage and archive storage lifecycle management
Experience with integration of data from multiple sources
Experience with Python, SQL (structure query language), and relational databases.
Experience with data visualization tools (e.g. Tableau)
Experience with Agile Software Development
US Citizenship with the ability to obtain/maintain an active DoD Secret Clearance.
Must be able to obtain Program Access (PAR) within a reasonable amount of time
Preferred Qualifications:
Master's degree with 7 years of relevant experience.
5+ years of experience working with ETL techniques and frameworks.
Expert in Python, SQL, and data visualization tools.
Development experience utilizing Hadoop, Spark, PowerShell, and automation scripts
Familiarity with Data Virtualization and Data Cataloging tools (e.g. Denodo, Collibra).
Experience with NoSQL Databases (e.g., MongoDB, Neo4J, etc.)
CompTIA Security+ Certification.
Current DOD Top Secret clearance.
Salary Range:
$112,000 - $168,000
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Hadoop, Spark, NoSQL Databases, SQL, Python, Databases, ETL Pipelines, Data Analytics, Visualization, Relational Databases, Data Visualization Tools, Tableau, Agile Software Development, Denodo, Collibra, Automation Scripts, PowerShell, CompTIA Security+ Certification, Data Virtualization, Data Cataloging","hadoop, spark, nosql databases, sql, python, databases, etl pipelines, data analytics, visualization, relational databases, data visualization tools, tableau, agile software development, denodo, collibra, automation scripts, powershell, comptia security certification, data virtualization, data cataloging","agile software development, automation scripts, collibra, comptia security certification, data cataloging, data virtualization, data visualization tools, dataanalytics, databases, denodo, etl pipelines, hadoop, nosql databases, powershell, python, relational databases, spark, sql, tableau, visualization"
Sr Data Operations Engineer,Wabtec Corporation,"West Melbourne, FL",https://www.linkedin.com/jobs/view/sr-data-operations-engineer-at-wabtec-corporation-3748730547,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"Wabtec Corporation is a leading global provider of equipment, systems, digital solutions and value-added services for freight and transit rail. Drawing on nearly four centuries of collective experience across Wabtec, GE Transportation and Faiveley Transport, the company has unmatched digital expertise, technological innovation, and world-class manufacturing and services, enabling the digital-rail-and-transit ecosystems. Wabtec is focused on performance that drives progress, creating transportation solutions that move and improve the world. Wabtec has approximately 27,000 employees in facilities throughout the world. Visit the company’s new website at: http://www.WabtecCorp.com.
It’s not just about your career… or your job title…it’s about who you are and the impact you are going to make on the world. Do you want to go into uncharted waters…do things that haven’t been done to make yours and someone else's life better? Wabtec has been doing that for decades and we will continue to do so! Through our people, leadership development, services, technology and scale, Wabtec delivers better outcomes for global customers by speaking the language of industry.
Who will you be working with?
Our system architecture team combines deep knowledge of Linux, Windows, Networking, and Database Technologies. We deliver excellence and automation in deploying and maintaining many customer environments and projects. Here you will grow in your specialized field and partner with other key stakeholders to support our customers and their operations.
How will you make a difference?
As a member of the Digital Operations group, The Database Administrator will have deep, focused, and varied experience specifically with supporting production Oracle Database environments as well as aptitude and desire for growth into other database arenas such as Cockroach or PostgreSQL.
The Oracle expert will plan, coordinate, and administer Oracle database(s) in an environment that requires deep hands-on design, implementation, and maintenances of a mature database infrastructure in a fully automated environment using Ansible.
What do we want to know about you?
BS/BA in Computer Science or related field, or equivalent work experience
3 to 5 years of deep experience in Cockroach Database Administration and design.
Working knowledge of Oracle Database Administration.
Experience with migration of data cross database platforms.
Working knowledge of incident and service management. ITIL certified (preferred).
Hands on, production experience with troubleshooting of database issues.
The ability to seamlessly work with application developers, other strong technical leaders, and non-technical staff.
Extremely strong written and verbal skills. Documentation will be a large part of the job.
Desire and flexibility to be on-call to support customers at varying contractual agreement levels.
What will your typical day look like?
As a part of a cross-functional IT team, the Oracle DBA will create and maintain all databases required for development, testing, and production usage.
Performs capacity planning required to create and maintain Oracle databases. The DBA works closely with System Administration Engineers to build capacity plans.
Performs ongoing tuning of all Oracle databases.
Installs new versions of Oracle and its tools.
Patches Oracle on a consistent cycle.
Plans and implements backup and recovery for Oracle databases.
Implements and enforces security for all Cockroach databases.
Use knowledge of Oracle internals to help develop and implement application design and coding use of the database.
Provide technical support to the application development team.
Automation of database related activities using Ansible.
Maintain Oracle Goldengate infrastructure and processes.
Oracle Dataguard monitoring, trouble-shooting, and creation.
Oracle RMAN backup experience.
Other duties as assigned
LI-TD1
Wabtec Corporation is committed to taking on the world’s toughest challenges. In order to fulfill that commitment we rely on a culture of leadership, diversity and inclusiveness. We aim to employ the world’s brightest minds to help us create a limitless source of ideas and opportunities. We believe in hiring talented people of varied backgrounds, experiences and styles…people like you! Wabtec Corporation is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, or protected Veteran status. If you have a disability or special need that requires accommodation, please let us know.
Show more
Show less","Linux, Windows, Networking, Database Technologies, Oracle Database, Cockroach, PostgreSQL, Ansible, ITIL, Incident and service management, Troubleshooting, Database administration, Capacity planning, Tuning, Patching, Backup and recovery, Security, Application design and coding, Technical support, Automation, Oracle Goldengate, Oracle Dataguard, Oracle RMAN","linux, windows, networking, database technologies, oracle database, cockroach, postgresql, ansible, itil, incident and service management, troubleshooting, database administration, capacity planning, tuning, patching, backup and recovery, security, application design and coding, technical support, automation, oracle goldengate, oracle dataguard, oracle rman","ansible, application design and coding, automation, backup and recovery, capacity planning, cockroach, database administration, database technologies, incident and service management, itil, linux, networking, oracle database, oracle dataguard, oracle goldengate, oracle rman, patching, postgresql, security, technical support, troubleshooting, tuning, windows"
Senior Principal Data Engineer (Melbourne FL) with Security Clearance,ClearanceJobs,"Melbourne, FL",https://www.linkedin.com/jobs/view/senior-principal-data-engineer-melbourne-fl-with-security-clearance-at-clearancejobs-3770046062,2023-12-17,Cocoa Beach,United States,Mid senior,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Responsibilities
Are you motivated to work in an environment that will challenge you, force you to continuously innovate, and work on solutions that make a difference for your customers? Northrop Grumman Aerospace Systems is looking for a passionate Senior Principal Data Engineer in Melbourne Florida to design and develop automated, end-to-end, ETL pipelines and Data Analytics/Visualization solutions from disparate data sources. You will share in the ownership of the technical vision and direction for advanced analytics systems that change the way we see and use data. We are looking for people who are self-motivated, hardworking, and have demonstrated the ability to find innovative solutions to complex technical problems. Job Responsibilities:
Design, develop, and maintain a scalable ETL pipelines.
Enable storing, searching, processing, and securing of extremely large structured or unstructured data sets.
Data preparation/cleaning, integration, and automation from heterogeneous sources
Ensure data integrity and system availability
Identify, evaluate, and recommend core technologies and strategies
Monitor and optimize system performance
Decomposition of user requirements into logical functions/componentsBasic Qualifications for Senior Principal Data Engineer:
Bachelor's in a STEM related field with 9 years of experience; Master's in a STEM related field with 7 years of experience. PhD with 4 years of experience.
5+ years of experience working with ETL techniques and frameworks
Understanding of elastic data storage and archive storage lifecycle management
Experience with integration of data from multiple sources
Experience with Python, SQL (structure query language), and relational databases.
Experience with data visualization tools (e.g. Tableau)
Experience with Agile Software Development
US Citizenship with the ability to obtain/maintain an active DoD Secret Clearance. * Must be able to obtain Program Access (PAR) within a reasonable amount of timePreferred Qualifications:
Master's degree with 7 years of relevant experience.
5+ years of experience working with ETL techniques and frameworks.
Expert in Python, SQL, and data visualization tools.
Development experience utilizing Hadoop, Spark, PowerShell, and automation scripts
Familiarity with Data Virtualization and Data Cataloging tools (e.g. Denodo, Collibra).
Experience with NoSQL Databases (e.g., MongoDB, Neo4J, etc.)
CompTIA Security+ Certification.
Current DOD Top Secret clearance.Salary Range: $112,000 - $168,000 The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions. Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","ETL, Elastic data storage, Data visualization, Agile software development, Python, SQL, Hadoop, Spark, PowerShell, Data virtualization, Data cataloging, NoSQL databases, MongoDB, Neo4J","etl, elastic data storage, data visualization, agile software development, python, sql, hadoop, spark, powershell, data virtualization, data cataloging, nosql databases, mongodb, neo4j","agile software development, data cataloging, data virtualization, elastic data storage, etl, hadoop, mongodb, neo4j, nosql databases, powershell, python, spark, sql, visualization"
Sr. Data Analyst,Eversource Solutions LLC,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-analyst-at-eversource-solutions-llc-3787922706,2023-12-17,West Springfield,United States,Mid senior,Onsite,"RESPONSIBILITIES:
Improve use case implementation and reporting through data modeling and design.
Ability to grasp, document, and articulate revenue assurance and fraud analysis pertaining to customers. Additionally, possess the skills to create reports for both Directors and field personnel, which hold potential for legal action against criminals.
Proficiency in SAS, Oracle, Teradata, Excel, and database analytics is a prerequisite.
Capability to write intricate SQL queries for report generation.
The aptitude to craft visually captivating presentations using elements such as (graphics, graphs, clipart, MS Visio diagrams, and MS PowerPoint).
Successfully engages in multiple initiatives simultaneously.
Collaborates autonomously with users to formulate concepts while operating under the guidance of project managers.
Act as the intermediary linking the customer community (both internal and external customers) with the software development team, facilitating the seamless flow of requirements.
Formulates requirement specifications in accordance with established templates, employing natural language as and when required.
Collaborates with developers and subject matter experts to establish the data sources, and the technical vision and analyze tradeoffs between usability and performance needs.
Be the liaison between the business units, technology teams, and support teams.
Assigned responsibilities will encompass assisting in the QA process and contributing to the development of training materials.
Applicants should either hold U.S. citizenship or possess the authorization to work full-time in the United States for any employer.
Should be capable of obtaining a public trust clearance from the US government.
Strong proficiency in verbal, written, and interpersonal communication skills is a prerequisite for this role.
Requires on-site presence in Washington DC with the client, at least 2-3 days a week.
QUALIFICATIONS:
Minimum Qualifications:
Must have a bachelor’s degree in computer science or information systems or science-related field or related experience.
The Sr. Data Analyst must have minimum of 6+ years of professional experience analyzing data needs, business needs, performing data analysis and performing ETL against a multitude of data sources and platforms, business analytics, proposing business solutions, and generating reports.
Must have 5+ years’ business and technical analysis experience in a client-facing role with the ability to use PowerPoint, excel, and other graphical tools such as Qlik, PowerBI, Tableau or Microstrategy to report to upper management the results of the analysis.
Must be proficient and at an advanced level in SAS, Teradata, and Oracle ETL processes and reporting.
Knowledge of Internet technologies and web applications is useful including Jason, XML and web services.
Must understand data modeling data entities and data communication and processing.
Complete ad hoc report requests from business stakeholders leveraging necessary Business Intelligence (Bl) tools.
Must have strong skills with Excel, SAS, SQL queries, functions, procedures, views.
Must understand relational databases; past database development and data analysis experience is needed.
Must have served as a data analyst in 3-5 projects in the past as a team member and in a lead capacity on at least one project.
Must have prior customer support experience to be able to work with customers and explain the reports and decisions and trends to them.
Must be able to solve problems and help modelers with the right type of cleaned data.
Good interpersonal skills are also required.
Must have strong written and verbal communication skills.
Desired Qualifications:
Master’s Degree in the related field is a plus.
Prior experience of working with Teradata or other data warehouse databases is a plus.
Big data experience is a huge plus.
Understanding supply chain management communication and business transactions is a big plus.
WHY YOU SHOULD JOIN:
Collaborative & Inclusive Culture:
Our client’s colleagues work in small, self-organized and cross-functional teams that determine the best tactics to support the vision and strategy set by leadership. They encourage and welcome all ideas, taking an agile approach to creating an amazing product.
Design Thinking for Innovation:
With focus on their colleague’s growth, they have developed a Learning Management System (LMS) that trains all colleagues on Design Thinking for Innovation, Security, BI & Analytics, and the business they are about to support. More trainings are added every year.
Their Colleagues are recognized for their innovative problem solving, goodwill, candor with respect and excellence. We also give out spot bonuses besides the yearly performance bonus.
Workplace:
While their headquarters is in Sterling, Virginia, and our largest client in Washington DC (on-site), their team lives across the globe. They partner with their team to help them achieve personal and professional alignment, helping their colleagues to define their own blend of work-life balance.
Office Perks
: Colleagues that work out of one of their main offices enjoy beverages, snacks, lunches, and happy hour benefits.
Vacation & Holiday Schedule: They believe this is a partnership and they trust their colleagues to build their own time-off schedule and encourage their teams to take time for their well-being while minimizing impacts to Client’s deliverables.
Growth Opportunity: Through hands-on learning and development, the opportunities are endless.
Focus on Well-Being:
They pride themselves on offering numerous benefit options to best fit the needs of our colleagues and their families, including domestic partners. Benefits include medical (PPO & HDHP-HSA), dental, vision, flexible spending account, commuter benefit, discount programs, life & disability insurance, and accident insurance. They also offer a 401k program with company match and paid parental leave.
Live your Passion:
The client encourages and promotes monetary and personnel involvement in the causes around the globe. They believe in their Value of Goodwill, and work to find new ways to give back to their communities and make a positive impact. They focus on initiatives in the following areas - technology, charity, volunteerism, and honorary contributions.
The duties and responsibilities listed in this job description generally cover the nature and level of work being performed by individuals assigned to this position. This is not intended to be a complete list of all duties, responsibilities, and skills required. Subject to the terms of an applicable collective bargaining agreement, the company management reserves the right to modify, add, or remove duties and to assign other duties as may be necessary. We wish to thank all applicants for their interest and effort in applying for the position; however, only candidates selected for interviews will be contacted.
EverSource Solutions LLC. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Powered by JazzHR
XJq2wmUi2L
Show more
Show less","Data modeling, Data design, Revenue assurance, Fraud analysis, SAS, Oracle, Teradata, Excel, Database analytics, SQL, PowerPoint, Visio, MS Visio diagrams, Natural language, ETL, Business analytics, Data sources, Technical vision, Usability, Performance needs, Business Intelligence (BI) tools, Relational databases, Database development, Data analysis, Data analyst, Customer support, Data communication, Data processing, Jason, XML, Web services, Data entities, Ad hoc report requests, Business stakeholders, Data modeling, Data cleaning, Interpersonal skills, Written communication, Verbal communication","data modeling, data design, revenue assurance, fraud analysis, sas, oracle, teradata, excel, database analytics, sql, powerpoint, visio, ms visio diagrams, natural language, etl, business analytics, data sources, technical vision, usability, performance needs, business intelligence bi tools, relational databases, database development, data analysis, data analyst, customer support, data communication, data processing, jason, xml, web services, data entities, ad hoc report requests, business stakeholders, data modeling, data cleaning, interpersonal skills, written communication, verbal communication","ad hoc report requests, business analytics, business intelligence bi tools, business stakeholders, customer support, data cleaning, data communication, data design, data entities, data processing, data sources, dataanalytics, database analytics, database development, datamodeling, etl, excel, fraud analysis, interpersonal skills, jason, ms visio diagrams, natural language, oracle, performance needs, powerpoint, relational databases, revenue assurance, sas, sql, technical vision, teradata, usability, verbal communication, visio, web services, written communication, xml"
Data Analyst I,SimIS Inc.,"Quantico, VA",https://www.linkedin.com/jobs/view/data-analyst-i-at-simis-inc-3787741409,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Position
Data Analyst I
Security Clearance
Secret
Work Location
Hybrid, contractor/remote site and government on-site, Quantico, VA
Education
BS
Position Description
Serve as the Data Analyst supporting a team of Enterprise Architects and Business Analysts developing, updating, and archiving Department of Defense Architecture Framework (DoDAF) products and services that inform customer enterprise-level system integration and capability requirements and documentation.
Experience
2 or more years of experience, within the past 5 years providing data analysis/data analytical procedures for a DOD agency.
Current experience in conducting data collection activities including data analysis methodology, data validation techniques, and quality assurance for IT support systems.
Current experience in the application of Agile procedures to support development, analysis, and refinement of system capabilities to support client requirements.
Desired – USMC military or civilian Service.
Desired - Prior TECOM experience.
Desired - Current experience in and understanding of military operations and in conducting in-depth research, analysis, and documentation of analytical findings and conclusions.
Desired - Experience in Joint and Combined operations, inter-agency operations, or combat development/defense acquisition
Knowledge
Current knowledge and experience in data modeling techniques, data aggregation methodologies, and statistical analysis procedures.
Skills
Analyzes data in various formats to support assessment of enterprise trends and developing requirements.
Uses a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining.
Uses Defense Collaborative System, WebEx and other collaborative planning tools.
Uses MS Office/365 office, MS SharePoint, MS Windows server and desktop operating environments.
Abilities
Maintain excellent writing, analytic, statistical, and communications skills in order to coordinate effectively, document analytical procedures and products, and communicate with customer leadership and stakeholders.
Credentials
No specific requirements
SimIS Offers:
Flexible Spending Account (FSA)
Medical, Dental, and Vision
Short Term Disability (SimIS provides Short-Term Disability benefits at no cost to you)
LTD
Life Insurance
401(k) Savings Plan
Tuition Assistance Program
Paid Time Off (PTO)
10 Holidays each year
SimIS, Inc. is an AA / EOE / M / F / Disability / Vet / V3 certified / Drug Free Employer
Powered by JazzHR
y4QwInQNDW
Show more
Show less","Data Analysis, Data Analytics, Data Aggregation, Data Mining, Statistical Analysis, Agile, Data Modeling, Defense Collaborative System, WebEx, MS Office/365, MS SharePoint, MS Windows, DoD Architecture Framework (DoDAF)","data analysis, data analytics, data aggregation, data mining, statistical analysis, agile, data modeling, defense collaborative system, webex, ms office365, ms sharepoint, ms windows, dod architecture framework dodaf","agile, data aggregation, data mining, dataanalytics, datamodeling, defense collaborative system, dod architecture framework dodaf, ms office365, ms sharepoint, ms windows, statistical analysis, webex"
Data Analyst - Secret,"KBR, Inc.","Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-secret-at-kbr-inc-3770440580,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Title:
Data Analyst - Secret
The Data Analyst supports operations and conducts data analytics projects to provide information and insights to stakeholders for decision-making or solution development; implements data standards and deploys automation tools to extract, synthesize, and validate data from different sources and transform data into usable metrics; constructs data sets, monitors data quality, troubleshoots and resolves database issues to ensure data integrity; utilizes scripting and querying tools like Python, R or SQL, data visualization/BI tools, statistical methods, and data modeling to produce reports, data files, and dashboards; collaborates with stakeholders to understand their needs, objectives, and requirements.
The Data Analyst shall:
Work independently or under general direction to perform analysis, preparation, and entry of data into prescribed data formats.
Directly support the Senior Data Analyst as well as Subject Matter Experts (SMEs) with analytical support services and related studies.
Submit recommendations for solutions.
Prepare written reports to management.
Required skills:
Have at least 2 years but no more than 6 years of experience in support of government acquisition and knowledge of requirements to collect, store, organize, and assess data related to acquisition programs.
Demonstrated capability to work both independently and under the direction of experienced analysts.
Demonstrated skill in defining, developing, and maintaining databases and working with spreadsheets and complex data models.
Demonstrated ability to work in a dynamic work environment and ability to coordinate and perform multiple assignments.
Possession of a government or industry standard data science certification is preferred but not required.
Experience utilizing scripting and querying tools like Python, R or SQL
Experience with data visualization/BI tools such as Power BI and Tableau
Bachelors Degree or higher
Experience with the production of reports, data files, and dashboards
Clearance Requirement:
Secret Clearance Required, Top Secret Preferred.
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Data analytics, Data visualization, Python, R, SQL, Power BI, Tableau, Statistics, Data modeling, Data mining, Data warehousing, Data governance, Machine learning, Artificial intelligence, Databases, Spreadsheets, Data science, Report writing, Project management, Team work, Communication","data analytics, data visualization, python, r, sql, power bi, tableau, statistics, data modeling, data mining, data warehousing, data governance, machine learning, artificial intelligence, databases, spreadsheets, data science, report writing, project management, team work, communication","artificial intelligence, communication, data governance, data mining, data science, dataanalytics, databases, datamodeling, datawarehouse, machine learning, powerbi, project management, python, r, report writing, spreadsheets, sql, statistics, tableau, team work, visualization"
Data Engineer,NR Consulting,"Herndon, VA",https://www.linkedin.com/jobs/view/data-engineer-at-nr-consulting-3768016667,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Job Requirements:
Database engineering and management activities associated with high volume, multi-format data management services
Designs large-scale data management solutions including developing efficient storage, indexing, and access approaches
Updates data enterprise architecture models (if needed) and data abstraction layer to enable access to new data sources
Designs and implements indexing strategies for ingested data.
Designs and implements storage and indexing strategies across the multiple repositories to provide efficient storage and retrieval
Supports design and development of data access APIs
Basic Qualifications
Bachelor's degree with 9+ years of relevant experience OR a master's degree with 7 years of relevant experience OR A PhD; an additional 4 years of experience maybe considered in lieu of a degree requirement
Active TS/SCI clearance
Experience with Postgres, SQL, PL/pgSQL
Experience installing application software in an operational environment
Experience with Agile, AWS, RDBMS technology, Elasticsearch, Linux
Security+
Active TS/SCI CI poly clearance (can start unclass before indoc)
Preferred
Knowledge of data modeling, Cameo
Familiarity with NoSQL technology (Hadoop and/or EMR)
CentOS System Administration
Linux
Comments for Suppliers: TS/SCI CI poly required; however selected candidate can start unclass before indoc
Show more
Show less","Database engineering, Data management, Data architecture, Data abstraction, Indexing, Storage, Retrieval, Data access APIs, Postgres, SQL, PL/pgSQL, Agile, AWS, RDBMS, Elasticsearch, Linux, Security+, Data modeling, Cameo, NoSQL, Hadoop, EMR, CentOS System Administration","database engineering, data management, data architecture, data abstraction, indexing, storage, retrieval, data access apis, postgres, sql, plpgsql, agile, aws, rdbms, elasticsearch, linux, security, data modeling, cameo, nosql, hadoop, emr, centos system administration","agile, aws, cameo, centos system administration, data abstraction, data access apis, data architecture, data management, database engineering, datamodeling, elasticsearch, emr, hadoop, indexing, linux, nosql, plpgsql, postgres, rdbms, retrieval, security, sql, storage"
Senior Data Engineer - Principal Associate,Jobs for Humanity,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-principal-associate-at-jobs-for-humanity-3768928383,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Job Title: Senior Data Engineer - Principal Associate Are you passionate about technology and solving complex business problems? Do you enjoy working in a collaborative and inclusive environment? At Capital One, we are a diverse group of makers, breakers, doers, and disruptors who strive to meet the needs of our customers. We are currently looking for Data Engineers who are excited about combining data with emerging technologies. As a Data Engineer at Capital One, you will play a vital role in driving a major transformation within our organization. What You'll Do: - Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies. - Work with a team of experienced developers specializing in data pipelines, distributed microservices, and full stack systems. - Utilize programming languages like Java, Scala, Python, and open-source databases to develop cloud-based data stores such as DynamoDB, Elasticache, and Snowflake. - Stay up-to-date with tech trends, experiment with new technologies, participate in technology communities, and mentor other members of the engineering community. - Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans to achieve financial well-being. - Design and implement effective data models optimized for search, query, reporting, or analytics. - Identify and select high-quality data sources for target data models. - Develop data pipelines, including ETL programs, to extract data from various sources and transform it to fit the target model. - Test and deploy data pipelines to ensure compliance with data governance and security policies. - Implement audits and checks to ensure data pipelines are functioning correctly. - Perform unit tests and code reviews to ensure high-quality and well-performing code. Basic Qualifications: - Bachelor's Degree. - At least 4 years of experience in application development (Internship experience does not apply). - At least 1 year of experience in big data technologies. Preferred Qualifications: - 5+ years of experience in application development, including Java, Python, NoSQL, and SQL. - 2+ years of experience with a public cloud platform (AWS, Microsoft Azure, Google Cloud). - 3+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL). - 2+ years of experience with real-time data and streaming applications. - 2+ years of experience with NoSQL implementations (DynamoDB, Redis, Elasticache). - 2+ years of experience with data warehousing (Redshift or Snowflake). - 3+ years of experience with UNIX/Linux including basic commands and shell scripting. - 2+ years of experience with Agile engineering practices. At Capital One, we believe in creating an inclusive and diverse workplace. We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to sex, race, age, color, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other prohibited basis under applicable law. We promote a drug-free workplace and consider qualified applicants with a criminal history in accordance with applicable laws. To apply for this position or request accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. For technical support or questions about our recruiting process, please email Careers@capitalone.com. Learn more about our comprehensive benefits package at the Capital One Careers website (www.capitalonecareers.com/benefits). Note: This salary information is specific to the location mentioned in the job posting. Salaries for part-time roles will be prorated based on hours worked regularly. Candidates hired in different locations will receive compensation based on the associated pay range for that location. Please note that Capital One Financial is comprised of multiple entities. Positions posted in Canada are for Capital One Canada, in the United Kingdom are for Capital One Europe, and in the Philippines are for Capital One Philippines Service Corp. (COPSSC). Thank you for considering a career at Capital One!
Show more
Show less","Agile, AWS, cloud computing, data analytics, data modeling, data pipelines, data warehousing, DynamoDB, Elasticache, ELK Stack, ETL, Google Cloud, Hadoop, Hive, Java, Kafka, MapReduce, Microsoft Azure, MySQL, NoSQL, Python, Redshift, Redis, Scala, Snowflake, Spark, SQL, UNIX/Linux","agile, aws, cloud computing, data analytics, data modeling, data pipelines, data warehousing, dynamodb, elasticache, elk stack, etl, google cloud, hadoop, hive, java, kafka, mapreduce, microsoft azure, mysql, nosql, python, redshift, redis, scala, snowflake, spark, sql, unixlinux","agile, aws, cloud computing, dataanalytics, datamodeling, datapipeline, datawarehouse, dynamodb, elasticache, elk stack, etl, google cloud, hadoop, hive, java, kafka, mapreduce, microsoft azure, mysql, nosql, python, redis, redshift, scala, snowflake, spark, sql, unixlinux"
Data Engineer Senior / Sr. ETL developer (min 12 Yrs exp) (LOCAL Candidates preferred) (Webcam interviews),Prohires,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-senior-sr-etl-developer-min-12-yrs-exp-local-candidates-preferred-webcam-interviews-at-prohires-3617892415,2023-12-17,West Springfield,United States,Mid senior,Onsite,"We are looking for
Data Engineer Senior / Sr.
ETL developer
(min 12 Yrs exp) (LOCAL Candidates preferred) (Webcam interviews)
POSITION DESCRIPTION
DIRECT CLIENT Position
Number of positions: 1
Length: 15 Months +
Location: Washington, DC 20019
Immediate interviews either Webcam or In-person interviews
Please note that most of the resources are currently working remote. However, depending on the project situation, this position may require 1 -2 days onsite and rest REMOTE.
12+ years of experience needed to serve as the subject matter expert on data architecture for the client's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse.
We are looking for the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by client to ensure that the client has accurate and reliable data maintained by high quality systems, all client data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
Specific Duties:
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist
for the client's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of data standards.
Participate in the development and maintenance of data security, privacy, policies, procedures, and best practices.
Bachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Candidates must have ALL the ""Required"" skills in order to be considered for the position. ""Desired"" or ""Highly Desired"" skills are a PLUS but may NOT be required.
Skill Matrix
Experience with Business workflow processes
Required / Desired
Amount
of Experience
Experience in developing, testing, maintaining the process of transaction, transformation, and load (ETL) data processes
Required
12
Years
Experience with command-line scripting (Ex: Linux-Bash, GREP, SED, AWK, Kerberos, LDAP, CRON, Windows PowerShell, Windows Active Directory, Windows
Required
12
Years
Experience with programing scripting languages (Ex: Python, R/RStudio, JavaScript, JSON/CVS, Perl, Scala, C/C++)
Required
12
Years
Experience leveraging SQL relational databases to manage complex datasets and analytical reporting
Required
12
Years
Bachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred. .
Required
BS in CS or IS or overall exp of 16 yrs
Years
Experience with Linux-based Operating Systems and Servers
Required
8
Years
Experience with Windows-based Operating Systems and Servers
Required
8
Years
Experience with System Administration of Microsoft SQLServer
Required
8
Years
Experience with System Administration of PostGresSQL
Required
8
Years
Experience in the maintenance and monitoring of system & network related activities associated with data security and controls.
Required
8
Years
Understanding of the Software Development Lifecycle (SDLC) in Agile and Waterfall environments
Required
5
Years
Proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing
Required
5
Years
Communication skills, both written and spoken, business level English mandatory
Required
5
Years
Writing System Guides, Manuals, and User Guides for complex systems
Required
5
Years
Strong communication skills - ability to articulate technical terms and complex data clearly to non-technical audience both verbally and in writing
Required
5
Years
Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions.
Required
5
Years
Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives.
Desired
5
Years
Familiarity with CKAN, DKAN, and/or ArcGIS
Desired
2
Years
Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy)
Desired
2
Years
Familiarity with CKAN, DKAN, and/or ArcGIS
Required
2
Years
Experience writing SQL queries for checking expected results and setting up test data;
Required
12
Years
Show more
Show less","Data Engineer, ETL Developer, Data Architecture, Data Warehouse, Data Governance, Data Gap Analysis, Data Catalog, Data Quality, Data Analysis, Data Reporting, Data Security, Data Privacy, Agile Development, Waterfall Development, Agile Testing, Automation Testing, Blackbox Testing, Unit Testing, CrossBrowser Testing, Communication, Writing, Consultative Skills, CKAN, DKAN, ArcGIS, Tableau, MicroStrategy, SQL","data engineer, etl developer, data architecture, data warehouse, data governance, data gap analysis, data catalog, data quality, data analysis, data reporting, data security, data privacy, agile development, waterfall development, agile testing, automation testing, blackbox testing, unit testing, crossbrowser testing, communication, writing, consultative skills, ckan, dkan, arcgis, tableau, microstrategy, sql","agile development, agile testing, arcgis, automation testing, blackbox testing, ckan, communication, consultative skills, crossbrowser testing, data architecture, data catalog, data gap analysis, data governance, data privacy, data quality, data reporting, data security, dataanalytics, dataengineering, datawarehouse, dkan, etl developer, microstrategy, sql, tableau, unit testing, waterfall development, writing"
Data Engineer,"KBR, Inc.","Reston, VA",https://www.linkedin.com/jobs/view/data-engineer-at-kbr-inc-3770445008,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Title:
Data Engineer
KBR is seeking a Data Engineer to support one of our government customers in Reston, Virginia. The Data Engineer utilizes a diverse range of data-focused skills, experience, deep technical and analytical problem solving, and team-centered solutioning for complex problems to satisfy priority information enabling operations. This position requires strong attributes for problem solving, data analysis, troubleshooting, analytical thinking, and experimentation, often under time sensitive decisions.
Required Qualifications:
Develop and implement methods of automation and optimization of data and products to present to upper management
Demonstrate understanding of statistical analysis such as regression, anomaly detection and clustering
Create data packages in the form of databases (DBs), reports, and interactive visualizations
Demonstrate effective communication skills to relay data science activities, technical findings, and data products for both technical and non-technical customers
Ability to combine a diverse set of data sources containing multiple forms of information including, but not limited to, Open Source, Publicly Available Information (PAI), Commercially Available Information (CAI), and intelligence records to provide technical information to produce data packages
Demonstrated ability to use technical and analytic skills to solve complex problem
Bachelor of Science Degree in Science, Technology, Engineering, or Mathematics
5+ years of progressive, relevant work experience.
Preferred Qualifications:
Hands-on experience using commercially available data exploitation and visualization tools for analysis (e.g., MS Excel, Tableau, Power BI, Wireshark)
Knowledge of National Intelligence Agencies and Department of Defense elements
Advanced Statistical knowledge and analysis methods (e.g., Linear models, multivariate analysis)
Experience in analytical tool development, identification, and integration
Master's degree in a science related field
Travel: CONUS and OCONUS travel may be required. (10%)
Security Requirements:
Active TS/SCI security clearance w/CI Poly
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Data engineering, Automation, Optimization, Data analysis, Statistical analysis, Regression, Anomaly detection, Clustering, Data visualization, Databases, Reports, Communication, Open Source, Publicly Available Information (PAI), Commercially Available Information (CAI), MS Excel, Tableau, Power BI, Wireshark, National Intelligence Agencies, Department of Defense, Linear models, Multivariate analysis, Analytical tool development, Identification, Integration, CONUS, OCONUS, TS/SCI security clearance, CI Poly","data engineering, automation, optimization, data analysis, statistical analysis, regression, anomaly detection, clustering, data visualization, databases, reports, communication, open source, publicly available information pai, commercially available information cai, ms excel, tableau, power bi, wireshark, national intelligence agencies, department of defense, linear models, multivariate analysis, analytical tool development, identification, integration, conus, oconus, tssci security clearance, ci poly","analytical tool development, anomaly detection, automation, ci poly, clustering, commercially available information cai, communication, conus, data engineering, dataanalytics, databases, department of defense, identification, integration, linear models, ms excel, multivariate analysis, national intelligence agencies, oconus, open source, optimization, powerbi, publicly available information pai, regression, reports, statistical analysis, tableau, tssci security clearance, visualization, wireshark"
Data Analyst Senior,Prometheus Federal Services (PFS),"Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-senior-at-prometheus-federal-services-pfs-3747078112,2023-12-17,West Springfield,United States,Mid senior,Onsite,"Data Analyst Senior
Prometheus Federal Services (PFS), a trusted partner to federal health and social services agencies,has anopening for a Data Analyst Senior with Federal Consulting experience. You will work closely with and support Federal health and social services agency clients. You will collaborate with client leadership to shape work product development and ensure quality delivery.
The Data Analyst Senior will be responsible for driving defined PowerBI dashboards, data visualization reports, data models, automation processes, and general integration. As the PowerBI expert, this role will collaborate with stakeholders to define requirements, develop powerful data driven solutions, and support PowerBI training as needed (i.e., Dashboard Training).All applicants must reside in the U.S.
Essential Duties and Responsibilities
Conceptualize and develop integrated data solutions including task trackers, dashboards, customized application, and automation scripts to aid in data capture through products that support a variety of workstreams.
Design and deploy a continuum of integrated data management solutions within NDS authorized datasets and platforms including CDW, Pyramid Analytics, TMS, PowerBI, PowerApps, Power Automate.
Minimum Qualifications
Bachelor’s degree from an accredited institution
Minimum 8 years of working with large, complex health-related data sets
Ability to run scenarios
Ability to conduct data analysis
Experience with Data visualization (e.g., PowerSuite)
Hands-on experience building visually appealing Power BI dashboards
Experience using SQL and RDBMS
Strong analytical skills with ability to identify alternatives
Excellent oral and written communication skills
Proven ability to work independently and as part of a team
Proficiency with MS Office
Ability to work in the US indefinitely without sponsorship
Ability to obtain a US security clearance if needed
Preferred Qualifications
Knowledge of Department of Veterans Affairs databases and datasets
Ambition to bring creative ideas to the table, pursue new learning opportunities and delight clients
Microsoft Certifications within the Power Platform, AZURE, or other closely related Microsoft Products
Six Sigma and/or continuous improvement background
Experience with SQL query design and database architecture through Microsoft SQL Server Management Studio
Experience with PowerBI Paginated Reporting or similar SSRS reporting capabilities
Show more
Show less","Data Analysis, Data Visualization, PowerBI, CDW, Pyramid Analytics, TMS, PowerApps, Power Automate, SQL, RDBMS, Microsoft Office, MS SQL Server Management Studio, PowerBI Paginated Reporting, SSRS","data analysis, data visualization, powerbi, cdw, pyramid analytics, tms, powerapps, power automate, sql, rdbms, microsoft office, ms sql server management studio, powerbi paginated reporting, ssrs","cdw, dataanalytics, microsoft office, ms sql server management studio, power automate, powerapps, powerbi, powerbi paginated reporting, pyramid analytics, rdbms, sql, ssrs, tms, visualization"
Healthcare Data Analyst,"Northwest Human Services, Inc.","Salem, OR",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-northwest-human-services-inc-3755785764,2023-12-17,Silverton,United States,Associate,Onsite,"Northwest Human Services is a non-profit leader in providing advocacy, quality healthcare and social services in Marion and Polk counties since 1970. We are a mission focused organization providing compassionate and professional medical, dental, psychiatry, mental health, and wraparound services for those in our community who need it most - uninsured individuals, families, the homeless, and migrant workers. As a Community Health Center we value a culture of equity, diverse perspectives, and life experiences. Our organization embraces innovation, collaboration, and work-life harmony.
HEALTHCARE DATA ANALYST
Location: West Salem Clinic, 1233 Edgewater Street NW, Salem OR 97304 (Not a remote position)
Job Status: Full-time, Monday - Friday
YOUR ROLE:
This position is responsible for producing clinical and operational business intelligence (BI) from many complex data sources, using various analytic methods. The Healthcare Data Analyst provides guidance and advice regarding the availability and validity of data to answer questions regarding organizational and provider performance. The Healthcare Data Analyst provides interpretation of trends and drivers of performance and evaluates the effect of improvement projects.
SPECIFIC DUTIES
Conduct routine and exploratory analysis to describe performance, evaluate programs/projects. Serves as an internal consultant for identifying improvement opportunities and potential strategic opportunities.
Apply advanced analysis techniques to generate insights about drivers, correlations, historical trends, and predictions.
Generate BI reports to assist with decision-making, help leaders interpret and use reports and dashboards effectively, and provide innovative data visualizations that promote understanding and decision-making around key issues.
Collaborate with IT, Quality Management, and Finance teams to ensure NWHS’ raw information assets contain high-quality data that can be used for effective reporting and decision-making.
Remain informed of best practices, national and state trends, tools, data sources, and techniques in the field of data, analytics, and business intelligence in the healthcare industry. Maintains thorough understanding of current and future performance measurement requirements from regulatory agencies.
Qualifications
Bachelor’s degree in Business Information Systems, Computer Science, Data Analytics, or related field with coursework in quantitative analysis.
3-5 years of analysis and reporting experience in an ambulatory, primary care medical group, or community health center environment.
Knowledge and experience with SQL and PowerBI, Tableau, or an analytics automation platform such as Alteryx.
Knowledge and experience using statistical analysis.
Knowledge of healthcare data sources, concepts, and metrics.
Experience working within a health center or clinical setting.
SUMMARY OF BENEFITS: Our Agency strives to provides a benefits program that is comprehensive and competitive within our industry.
Competitive Salary
Comprehensive Health Plans: Dental and Vision
Flex Spending Account
Group Life: Short-Term & Long-Term Disability 100% paid by employer
403(b) retirement plan with 3% employer match
10 hours of monthly Paid Time Off based on FTE
7½ paid holidays each year + 2 paid floating holiday
Continuing Education Plan
Employee Healthy Living Program
TO APPLY:
If you are interested in joining a team that makes a difference in the lives of many, apply online at: Employment (northwesthumanservices.org)
For more information, contact the HR/Recruiting Department at: HR@nwhumanservices.org | 503.588.5828
All candidates who receive a written offer of employment will be required to undergo a criminal records check.
Equal Opportunity Employer | We celebrate diversity and are committed to creating an inclusive environment for all employees.
Show more
Show less","Healthcare Data Analytics, SQL, PowerBI, Tableau, Alteryx, Statistical Analysis, Healthcare Data Sources, Healthcare Concepts, Healthcare Metrics, Business Intelligence","healthcare data analytics, sql, powerbi, tableau, alteryx, statistical analysis, healthcare data sources, healthcare concepts, healthcare metrics, business intelligence","alteryx, business intelligence, healthcare concepts, healthcare data analytics, healthcare data sources, healthcare metrics, powerbi, sql, statistical analysis, tableau"
Database Administrator,Clayton Services,"Houston, TX",https://www.linkedin.com/jobs/view/database-administrator-at-clayton-services-3783989397,2023-12-17,Houston,United States,Associate,Onsite,"Clayton Services is searching for a
Database Administrator
to join a thriving company in Houston. The
Database Administrator
will administer, maintain, and optimize company databases, utilizing Python, SQL, and Power BI expertise to support data management initiatives and collaborate with the data engineering team.
Pay Rate:
$75,000 - $80,000/annually
Benefits:
Fully covered medical and dental benefits with excellent PTO available!
Location:
Spring Branch Central
Work Setting:
onsite, 5 days per week
Database Administrator Responsibilities:
Assist in installing, configuring, and maintaining database software (e.g., MySQL, PostgreSQL, SQL Server).
Aid in data migration and transformation tasks using Python and SQL.
Collaborates with the data engineer to ensure smooth data integration and reporting within Power BI.
Monitor database systems, optimize SQL queries, and troubleshoot performance issues.
Perform routine database backups, implement disaster recovery plans, and uphold data security.
Create and maintain comprehensive documentation for database configurations, processes, and procedures.
Stay updated on emerging trends in database technologies, Power BI advancements, and developments in data analytics.
Database Administrator Skills and Abilities:
Excellent problem-solving and troubleshooting skills.
Excellent communication and teamwork abilities.
Familiarity with database management tools and utilities.
Basic understanding of database management systems (DBMS).
Knowledge of database security best practices.
Certification in database administration (e.g., Oracle Certified Associate, Microsoft Certified: Azure Database Administrator Associate).
Strong proficiency in Python and SQL for data manipulation and analysis.
Ability to take on new challenges and adapt to evolving technologies.
Ability to take and pass a technical assessment based on SQL and coding skills.
Database Administrator Education and Experience:
Bachelor's degree in Computer Science, Information Technology, or related field.
2+ years of hands-on experience.
Experience with cloud-based databases (e.g., AWS RDS, Azure SQL Database).
Experience with the SDLC (front-end and/or back-end development).
Experience with data visualization and reporting.
Show more
Show less","Python, SQL, Power BI, MySQL, PostgreSQL, SQL Server, Data migration, Data transformation, Data integration, Data reporting, Database backups, Disaster recovery, Data security, Database documentation, Database management tools, Database management systems (DBMS), Database security best practices, Oracle Certified Associate, Microsoft Certified: Azure Database Administrator Associate, Cloudbased databases (AWS RDS Azure SQL Database), SDLC (frontend and/or backend development), Data visualization, Data reporting","python, sql, power bi, mysql, postgresql, sql server, data migration, data transformation, data integration, data reporting, database backups, disaster recovery, data security, database documentation, database management tools, database management systems dbms, database security best practices, oracle certified associate, microsoft certified azure database administrator associate, cloudbased databases aws rds azure sql database, sdlc frontend andor backend development, data visualization, data reporting","cloudbased databases aws rds azure sql database, data integration, data migration, data reporting, data security, data transformation, database backups, database documentation, database management systems dbms, database management tools, database security best practices, disaster recovery, microsoft certified azure database administrator associate, mysql, oracle certified associate, postgresql, powerbi, python, sdlc frontend andor backend development, sql, sql server, visualization"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX",https://www.linkedin.com/jobs/view/data-qa-engineer-locals-at-steneral-consulting-3762496226,2023-12-17,Houston,United States,Mid senior,Hybrid,"Spring TX
USC or GC Holder
12 month contract
Hybrid (onsite Tuesday – Thursday)
Top 3 Skills
Experience extracting from user stories
SQL (needs to be a 3-5 out of 10)
Experience working in a continuous integration team – must know the principles
Must have tested data warehouses
Data QA Engineer
The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.
Responsibilities
Utilize Azure DevOps for test case management and issue/defect tracking.
Write clear and concise defect details describing actual versus expected behaviors.
Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines.
Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies.
Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues.
Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration.
Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications
Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration.
5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications
Show more
Show less","SQL, Azure DevOps, ETL/ELT, Data Warehouses, Data Pipelines, Data Quality, Data Validation, Analytics Testing, Regression Testing, Continuous Integration, Test Case Management, Defect Tracking, Software Testing Methodologies, Data Analytics, Data Warehousing","sql, azure devops, etlelt, data warehouses, data pipelines, data quality, data validation, analytics testing, regression testing, continuous integration, test case management, defect tracking, software testing methodologies, data analytics, data warehousing","analytics testing, azure devops, continuous integration, data quality, data validation, data warehouses, dataanalytics, datapipeline, datawarehouse, defect tracking, etlelt, regression testing, software testing methodologies, sql, test case management"
Data Governance Manager,Harnham,"Houston, TX",https://www.linkedin.com/jobs/view/data-governance-manager-at-harnham-3786550637,2023-12-17,Houston,United States,Mid senior,Hybrid,"DATA GOVERNANCE MANAGER
$135,000 – $145,000 + BONUS + BENEFITS
HYBRID – OFFICE LOCATED IN HOUSTON, TX
This automotive/retail company is seeking a Data Governance Manager to implement, execute, and manage the data governance strategy for the organization. This is a pivotal role for the company and the strategy that is created by the Data Governance Manager will have high visibility across the company.
ROLE OVERVIEW:
Partner closely with senior leadership within the Data and Analytics group to implement and manage the overall Data Governance strategy.
Lead critical initiatives, including intake and requirement processes, data and KPI standardization, Governance committee implementation, Master Data Management, Data Security, Data Quality, Data Retention, Data Glossary, and Self-Service reporting.
RESPONSIBILITIES:
Collaborate with stakeholders, IT teams, and various business units.
Understand data quality and BI tools, particularly their interaction with data virtualization systems.
Discover and connect with data across different business units, focusing on KPI metrics and business acumen.
Develop and execute a comprehensive data governance strategy based on business unit needs.
Lay out a plan and roadmap for the implementation of the data governance strategy.
SKILLS AND EXPERIENCE:
Proven experience building a data governance model and executing it from the ground up.
5+ years of industry experience
Background in data (data analyst, engineer, or product manager) before transitioning to implementing data governance.
Strong communication skills, and ability to influence and command a room.
Familiarity with master data, domains, transactions, data warehouses, and quality standards.
BENEFITS
$135,000 - 145,000 Base Salary + Bonus + Benefits
HOW TO APPLY
Please register your interest by sending your Resume to Emma Spagnola via the Apply link on this page or at emmaspagnola@harnham.com.
Show more
Show less","Data Governance, Data Analyst, Data Engineer, Data Product Manager, Data Quality, Data Retention, Data Security, Data Virtualization, Data Warehouse, Data Standardization, SelfService Reporting, Business Acumen, KPI Metrics, Master Data Management, Master Data, Domains, Transactions","data governance, data analyst, data engineer, data product manager, data quality, data retention, data security, data virtualization, data warehouse, data standardization, selfservice reporting, business acumen, kpi metrics, master data management, master data, domains, transactions","business acumen, data governance, data product manager, data quality, data retention, data security, data standardization, data virtualization, dataanalytics, dataengineering, datawarehouse, domains, kpi metrics, master data, master data management, selfservice reporting, transactions"
Data Governance Manager,Harnham,"Houston, TX",https://www.linkedin.com/jobs/view/data-governance-manager-at-harnham-3787367814,2023-12-17,Houston,United States,Mid senior,Hybrid,"DATA GOVERNANCE MANAGER
$135,000 - $145,000 + BONUS + BENEFITS
HYBRID - OFFICE LOCATED IN HOUSTON, TX
This automotive/retail company is seeking a Data Governance Manager to implement, execute, and manage the data governance strategy for the organization. This is a pivotal role for the company and the strategy that is created by the Data Governance Manager will have high visibility across the company.
ROLE OVERVIEW:
Partner closely with senior leadership within the Data and Analytics group to implement and manage the overall Data Governance strategy.
Lead critical initiatives, including intake and requirement processes, data and KPI standardization, Governance committee implementation, Master Data Management, Data Security, Data Quality, Data Retention, Data Glossary, and Self-Service reporting.
RESPONSIBILITIES:
Collaborate with stakeholders, IT teams, and various business units.
Understand data quality and BI tools, particularly their interaction with data virtualization systems.
Discover and connect with data across different business units, focusing on KPI metrics and business acumen.
Develop and execute a comprehensive data governance strategy based on business unit needs.
Lay out a plan and roadmap for the implementation of the data governance strategy.
SKILLS AND EXPERIENCE:
Proven experience building a data governance model and executing it from the ground up.
5+ years of industry experience
Background in data (data analyst, engineer, or product manager) before transitioning to implementing data governance.
Strong communication skills, and ability to influence and command a room.
Familiarity with master data, domains, transactions, data warehouses, and quality standards.
BENEFITS
$135,000 - 145,000 Base Salary + Bonus + Benefits
Show more
Show less","Data Governance, Data Analyst, Data Quality, Data Retention, Master Data Management, Data Security, Data Virtualization, BI tools, KPI Metrics, Business Acumen, Data Warehouse, Quality Standards, Data Visualization, Data Engineering, Data Product Management, Data Strategy, Data Governance Model, Business Intelligence, Data Governance Committee, Data Glossary, SelfService Reporting","data governance, data analyst, data quality, data retention, master data management, data security, data virtualization, bi tools, kpi metrics, business acumen, data warehouse, quality standards, data visualization, data engineering, data product management, data strategy, data governance model, business intelligence, data governance committee, data glossary, selfservice reporting","bi tools, business acumen, business intelligence, data engineering, data glossary, data governance, data governance committee, data governance model, data product management, data quality, data retention, data security, data strategy, data virtualization, dataanalytics, datawarehouse, kpi metrics, master data management, quality standards, selfservice reporting, visualization"
Data Science Specialist,Hydrogen Group,"Houston, TX",https://www.linkedin.com/jobs/view/data-science-specialist-at-hydrogen-group-3780021087,2023-12-17,Houston,United States,Mid senior,Hybrid,"Ob Description
We are currently seeking an experienced data scientist to join the Big Data and Advanced Analytics department. As part of the Data Analytics team, the Lead Data Scientist will work closely with the Data Engineering team and business functions to solve real-world oil and gas midstream problems using machine learning, data science algorithms and artificial intelligence.
Responsibilities Include
Work independently on optimization projects for multiple business functions
Identify and frame the optimization opportunity from understanding the business problem / opportunity
Gather, cleanse, and transform internal and external data
Analyze data and deliver insights via visualizations and dashboards
Create, productionize, and maintain models / solutions that address business problems
Present, explain and defend results from analysis and modeling, and approach taken
Participate in strategic planning discussions around optimizations, data science and big data analytics
The Successful Candidate Will Meet The Following Qualifications
5+ years of practical experience framing and solving optimization problems in supply chain, logistics, or operations
5+ years of hands on experience with applied statistics / math and optimization techniques
Professional experience with optimization tools such as linear programming, integer programming, or heuristic methods
Professional experience programming in Python
Experience with AWS is a plus
Educational background in Operations Research, Applied Mathematics, or Industrial Engineering is a plus
Ability to adapt in a rapidly changing environment
Ability to communicate insights and approaches in a simple, actionable manner
Ability to work independently and with team members from different backgrounds
Excellent attention to detail and problem-solving skills
Show more
Show less","Data Science, Optimization, Machine Learning, Data Analytics, Python, AWS, Operations Research, Applied Mathematics, Industrial Engineering, Linear Programming, Integer Programming, Heuristic Methods, Data Engineering, Business Intelligence, Data Visualization, Dashboarding, Presentation Skills, Communication Skills, Teamwork Skills, Problem Solving Skills","data science, optimization, machine learning, data analytics, python, aws, operations research, applied mathematics, industrial engineering, linear programming, integer programming, heuristic methods, data engineering, business intelligence, data visualization, dashboarding, presentation skills, communication skills, teamwork skills, problem solving skills","applied mathematics, aws, business intelligence, communication skills, dashboard, data engineering, data science, dataanalytics, heuristic methods, industrial engineering, integer programming, linear programming, machine learning, operations research, optimization, presentation skills, problem solving skills, python, teamwork skills, visualization"
Expert Data Engineer,HP,"Spring, TX",https://www.linkedin.com/jobs/view/expert-data-engineer-at-hp-3772976776,2023-12-17,Houston,United States,Mid senior,Hybrid,"We are looking for an
Expert Data Engineer
to join our team and help us build and maintain scalable data pipelines and systems. You will be responsible for designing, developing, testing, and deploying data solutions that meet the needs of our clients and stakeholders. You will also collaborate with data analysts, data scientists, and other data engineers to ensure data quality, reliability, and performance.
If you are interested in working on petabytes of data from millions of devices. If solving complex data issues excites you this is the opportunity for you.
Responsibilities
Leads the team to write, deploy, and maintain software to build, integrate, manage, maintain, and quality-assure data.
Architects, designs, implements, and maintains reliable and scalable data solutions in the AWS cloud environment using Scrum/Agile methodology.
Implement data ingestion, transformation, and processing workflows using ETL tools and frameworks.
Researches and promotes new tools and techniques to shape the future of the data engineering environment.
Ensure data security, privacy, and compliance with relevant regulations and policies.
Monitor, troubleshoot, and debug data issues and performance bottlenecks. Guides team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI / CD pipeline.
Document and communicate data engineering processes and solutions to stakeholders and users.
Represents the data engineering team for all phases of larger and more-complex development projects.
Works with following site-reliability engineering standard methodologies: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Knowledge & Skills
Demonstrable coding expertise in one or more object-oriented programming languages (e.g., Python, Scala, Java, etc.)
Deep and hands-on experience (7+ years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.
Hands on experience with:
Expert in AWS tools and services such as S3, Glue, Lambda, EMR, Redshift, Athena, etc.
Experience with other cloud platforms and services such as Azure, GCP, etc. is a plus.
Experience with data quality, testing, and validation tools and techniques
Experience with data visualization and reporting tools such as QuickSight, Tableau, Power BI, etc.
Strong analytical and problem-solving skills
Excellent communication and collaboration skill
Understanding Data Structures & Algorithms & their performance
Experience designing and implementing large-scale distributed systems.
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Internal client management and ability to lead large organizations via influence .
Ability to effectively communicate product architectures, design proposals and negotiate options at senior management levels.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Python, Scala, Java, AWS, S3, Glue, Lambda, EMR, Redshift, Athena, Azure, GCP, QuickSight, Tableau, Power BI, Data Structures, Algorithms, Distributed Systems, Data Lifecycle, Data Visualization, Communication, Collaboration","python, scala, java, aws, s3, glue, lambda, emr, redshift, athena, azure, gcp, quicksight, tableau, power bi, data structures, algorithms, distributed systems, data lifecycle, data visualization, communication, collaboration","algorithms, athena, aws, azure, collaboration, communication, data lifecycle, data structures, distributed systems, emr, gcp, glue, java, lambda, powerbi, python, quicksight, redshift, s3, scala, tableau, visualization"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX",https://www.linkedin.com/jobs/view/data-qa-engineer-locals-at-steneral-consulting-3759316531,2023-12-17,Houston,United States,Mid senior,Hybrid,"Spring TX
USC or GC Holder
12 month contract
Hybrid (onsite Tuesday – Thursday)
Data QA Engineer
The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.
Responsibilities
Utilize Azure DevOps for test case management and issue/defect tracking.
Write clear and concise defect details describing actual versus expected behaviors.
Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines.
Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies.
Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues.
Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration.
Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications
Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration.
5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications
Show more
Show less","Data QA, ETL/ELT, Azure DevOps, Manual testing, Automated testing, Data validation, Analytics testing, Data lakehouse, Regression testing, Software testing methodologies, Test case management, Defect tracking, Test case automation, Data quality","data qa, etlelt, azure devops, manual testing, automated testing, data validation, analytics testing, data lakehouse, regression testing, software testing methodologies, test case management, defect tracking, test case automation, data quality","analytics testing, automated testing, azure devops, data lakehouse, data qa, data quality, data validation, defect tracking, etlelt, manual testing, regression testing, software testing methodologies, test case automation, test case management"
"Data Analyst-SQL, Python",Zortech Solutions,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-analyst-sql-python-at-zortech-solutions-3763554032,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Analyst-SQL, Python
Location: Los Angeles, CA (Hybrid)
Duration: 6+ Months
Job Description
As a Data Analyst you will:
Partner closely with Business, Product Management, Product Development, and Design teams to leverage the key steps of data analytics: requirement gathering, data acquisition (ETL), presenting findings, driving to impact
Develop clear articulation of business-relevant analysis from data for all levels – executive levels to granular reports
Present to senior leaders within and outside the organization, telling a story with data
Skills
5-7 years relevant experience
Requires a Bachelors degree
Proficient with SQL to perform data acquisition, segmentation, and aggregation from scratch from existing data sources
Knowledge of programming languages (e.g. Python)
Knowledge of DWH concepts
Apply analytics and systems thinking to analyze data, identify emerging trends and turn the data into actionable information/insights.
Strong analytical and problem-solving skills, including hypothesis-driven testing and root-cause analysis to understand drivers of process variations
Able to effectively interact and communicate with all levels of management and with users at all levels of technical/analytical expertise
Drive results through influence, partnership and cross functional teams
Experienced and comfortable applying appropriate statistical mathematical techniques (academic or practical)
Outstanding communication skills with the ability to present the most relevant outcomes
Familiar with agile methodology
Payments domain knowledge – good to have
Show more
Show less","SQL, Python, ETL, DWH, Business Analytics, Data Acquisition, Data Segmentation, Data Aggregation, HypothesisDriven Testing, RootCause Analysis, Statistical Mathematical Techniques, Communication, Agile Methodology, Payments Domain Knowledge","sql, python, etl, dwh, business analytics, data acquisition, data segmentation, data aggregation, hypothesisdriven testing, rootcause analysis, statistical mathematical techniques, communication, agile methodology, payments domain knowledge","agile methodology, business analytics, communication, data acquisition, data aggregation, data segmentation, dwh, etl, hypothesisdriven testing, payments domain knowledge, python, rootcause analysis, sql, statistical mathematical techniques"
Sr. Data Analyst/ETL Developer,Stellar Professionals,"Lansing, MI",https://www.linkedin.com/jobs/view/sr-data-analyst-etl-developer-at-stellar-professionals-3750160030,2023-12-17,Belleville, Canada,Associate,Hybrid,"Skills Required
Bachelor's degree in a related field (or equivalent work experience).
Proven experience as a Data Analyst or ETL Developer.
Expertise in SQL, especially with Microsoft SQL Server.
Proficiency in data integration and migration, with cloud migration experience as a plus.
Understanding of industry data management best practices and standards.
Strong problem-solving and analytical skills.
Ability to mentor and guide junior team members.
Knowledge of data management concepts, including MDM, Data Quality, Metadata Management, Data Security, ILM, and Data Governance.
Excellent stakeholder engagement, negotiation, and leadership skills.
Strong organizational skills with the ability to work independently and collaborate effectively within a team.
Show more
Show less","SQL, Microsoft SQL Server, Data integration, Data migration, Cloud migration, Data management, MDM, Data Quality, Metadata Management, Data Security, ILM, Data Governance, Stakeholder engagement, Negotiation, Leadership, Problemsolving, Analytical","sql, microsoft sql server, data integration, data migration, cloud migration, data management, mdm, data quality, metadata management, data security, ilm, data governance, stakeholder engagement, negotiation, leadership, problemsolving, analytical","analytical, cloud migration, data governance, data integration, data management, data migration, data quality, data security, ilm, leadership, mdm, metadata management, microsoft sql server, negotiation, problemsolving, sql, stakeholder engagement"
Data Engineer,VeeAR Projects Inc.,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-veear-projects-inc-3777319182,2023-12-17,Belleville, Canada,Associate,Hybrid,"Responsibilities
Develop data pipeline to orchestrate the lifecycle of crowd grading tasks, build dashboard to monitoring and reporting the quality of tasks
Programming in one of the following languages: Python, Scala
Audit a wide variety of data for correctness
Required Skills
Programming language Python or Scala
Excellent experience with Spark
Familiarity with Git or similar version control system
Good understanding of data processing concepts and data engineering process.
Able to frequently collaborate closely with a diverse tight-knit team
Experience investigating and/or debugging problems
Able to track, analyze, and report issues in a timely manner
Attention to detail and a commitment to quality, while delivering work on schedule
Highly organized and efficient
Flexibility to respond and react to changing priorities quickly and efficiently
Excellent oral and written communication in English
Highly Desired Skills
Big data processing, e.g. Spark/Hadoop
Cloud based services, e.g. AWS
Workflow management tools, e.g. Apache Airflow
SQL
UI design fundamentals
Web development technologies, such as React for JS
QA/QE
Education
BA or BS degree or equivalent work experience.
Show more
Show less","Python, Scala, Spark, Git, Data processing, Data engineering, Collaboration, Problem solving, Reporting, Attention to detail, Quality assurance, Organization, Efficiency, Flexibility, Communication, Big data processing, Cloud services, Workflow management, SQL, UI design, Web development, React","python, scala, spark, git, data processing, data engineering, collaboration, problem solving, reporting, attention to detail, quality assurance, organization, efficiency, flexibility, communication, big data processing, cloud services, workflow management, sql, ui design, web development, react","attention to detail, big data processing, cloud services, collaboration, communication, data engineering, data processing, efficiency, flexibility, git, organization, problem solving, python, quality assurance, react, reporting, scala, spark, sql, ui design, web development, workflow management"
"Data Analyst-SQL, Tableau",Zortech Solutions,"Mountain View, CA",https://www.linkedin.com/jobs/view/data-analyst-sql-tableau-at-zortech-solutions-3782767161,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Analyst-SQL, Tableau
Location : Mountain View CA (2 days per week)
Duration: 6+ Months
Job Description
Must have:
Strong SQL
Tableau
Roles And Responsibilities
Describe and manipulate data sets, sources, and structures. Develop reports to make data more meaningful to business initiatives, analyze data quantitatively and qualitatively and communicate findings.
Develop data models to transform raw data into meaningful insights
Design, develop, and deploy dashboards using Tableau.
Owning the full BI lifecycle from requirements gathering through design & development, and through release and support/maintenance phases
Identify repeat processes and assist in process documentation and automation
Articulate key process flows, manage multiple inputs and priorities.
Mandatory Qualifications
Bachelor's/master’s degree in engineering, Economics, Finance, Mathematics, Statistics, Business Administration, or a related quantitative field
3-4 years of relevant analytics and BI experience and overall experience of 4 to 7 years.
Basic Knowledge about data warehouse design and data mining
Expert knowledge of SQL and Tableau
Strong experience in creating data-rich dashboards
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Good communication skills and ability to coordinate with multiple stakeholders across multiple functions
Prior work experience in with FinTech will be added advantage.
Show more
Show less","SQL, Tableau, Data visualization, Data analysis, Data modeling, Data manipulation, Business intelligence, Data mining, Data warehouse design, Dashboards, Data mining, Process automation, Data extraction, Data processing, Data communication","sql, tableau, data visualization, data analysis, data modeling, data manipulation, business intelligence, data mining, data warehouse design, dashboards, data mining, process automation, data extraction, data processing, data communication","business intelligence, dashboard, data communication, data extraction, data manipulation, data mining, data processing, data warehouse design, dataanalytics, datamodeling, process automation, sql, tableau, visualization"
Azure Data Engineer,VeeAR Projects Inc.,"Milwaukee, WI",https://www.linkedin.com/jobs/view/azure-data-engineer-at-veear-projects-inc-3767588471,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Descripation
Candidate with experience as Data Architect, Data Engineering, or any related role to Data solutions.
Candidate should have a proven track record in leading and delivering Azure Data Analytics solutions.
Good experience in Developing Advanced Analytics solutions, Applying Data Visualization.
Strong experience in SQL
Hands-on experience solutioning and implementing analytical capabilities using the Azure Data Analytics platform including, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake.
Candidate should be capable to support in all the phases of Analytical Development from identification of key business questions, through Data Collection and ETL.
Strong knowledge of Data Modelling and Data Design is required for the role.
Show more
Show less","Data Architecture, Data Engineering, Azure Data Analytics, Data Visualization, SQL, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Storage, Azure SQL Data Warehouse/Synapse, Azure Data Lake, Data Modelling, Data Design","data architecture, data engineering, azure data analytics, data visualization, sql, azure data factory, azure logic apps, azure functions, azure storage, azure sql data warehousesynapse, azure data lake, data modelling, data design","azure data analytics, azure data factory, azure data lake, azure functions, azure logic apps, azure sql data warehousesynapse, azure storage, data architecture, data design, data engineering, data modelling, sql, visualization"
Database Engineer V,doTERRA International LLC,"Pleasant Grove, UT",https://www.linkedin.com/jobs/view/database-engineer-v-at-doterra-international-llc-3721908821,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job description:
Seeking a highly self-motivated Oracle Database Administrator to join our growing IT Infrastructure Group. The Oracle Database Administrator is responsible for maintaining the integrity and performance of company databases and guarantee that data is stored securely and optimally. Assist with the evaluation of database software purchases and supervise the modification of any existing database software.
Job Responsibilities:
Identifies database requirements by analyzing department applications, programming, and operations; evaluating existing systems and designing proposed systems
Recommends solutions by defining database physical structure and functional capabilities, database security, data back-up, and recovery specifications
Maintains database performance by calculating optimum values for database parameters; implementing new releases; completing maintenance requirements
Evaluating computer operating systems and hardware products
Maintains quality service by establishing and enforcing organization standards
Performs data queries, coordinates testing, trains users and provides documentation
Works with other IT groups to ensure security procedures are maintained and adhered to
Participates in regular security audits
Completes all other tasks as assigned
Job Qualifications:
Oracle Database Administration experience including installing, patching, and upgrading Oracle databases
Experience working in RMAN backup and recovery
Knowledge of security procedures and technologies, and an understanding of audit processes
Strong hands-on experience in monitoring and tuning a database to provide a high availability service and optimum storage capability including: query optimization, initialization parameters, etc.Solid organizational and communication skills
Meticulous attention to detail
Works in an agile environment with the Application Development, Quality Assurance and other IT teams, along with the Business stakeholders to ensure that the necessary changes to the database meet the required deliverables
The DBA team bridges the gap between development and external clients and other IT functional groups
Core Competencies: Oracle; Oracle RAC; Oracle Enterprise Manager; Oracle RMAN, Oracle Enterprise Partitioning, Oracle Performance tuning
Prior experience in various scripting languages Preferred Qualifications:5+ years of Oracle Database Administration
UNIX/Linux system administration
Bachelor's degree in Computer Science/Information Systems or comparable experience
Experience with SAP HANA, MS SQL
Experience with Oracle CloudOracle 12c
Show more
Show less","Oracle Database Administration, Oracle RAC, Oracle Enterprise Manager, Oracle RMAN, Oracle Enterprise Partitioning, Oracle Performance tuning, UNIX/Linux system administration, SQL, SAP HANA, MS SQL, Oracle Cloud, Oracle 12c, RMAN backup and recovery, Query optimization, Initialization parameters, Scripting languages","oracle database administration, oracle rac, oracle enterprise manager, oracle rman, oracle enterprise partitioning, oracle performance tuning, unixlinux system administration, sql, sap hana, ms sql, oracle cloud, oracle 12c, rman backup and recovery, query optimization, initialization parameters, scripting languages","initialization parameters, ms sql, oracle 12c, oracle cloud, oracle database administration, oracle enterprise manager, oracle enterprise partitioning, oracle performance tuning, oracle rac, oracle rman, query optimization, rman backup and recovery, sap hana, scripting languages, sql, unixlinux system administration"
Data Analyst,"Anveta, Inc","Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-at-anveta-inc-3717200523,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: IN - DCS Data Analyst (720257)
Location: Indianapolis, IN 46204--- Webcam only--- Hybrid
Duration: Long term
Client: State of Indiana
Note: Only USC
Resource will be responsible for the activities of Data Analyst for the State of Indiana's child support system. Resource will work under direction of the Indiana Department of Child Services (DCS), Child Support Bureau (CSB) IT office.
Job Duties
Design, develop, and format polished excel and tableau data visualizations (reports and dashboards) to support business requirements. Focus on Visualization Creation using data to perform reporting and direct analysis. Perform analysis that may be descriptive, diagnostic, predictive, or prescriptive. Responsible for maintaining and developing excel and tableau dashboards and reports, preparing data visualizations, and using data to forecast or guide business activity. Present data in a fashion that is easy to understand with proper documentation and user testing for successful adoption. Publish tableau workbooks to appropriate QA, production, and public servers. Meet project deadlines and requirements. Participate in and contribute to the CSB-IT Data Services Team. Performs other related duties as assigned.
Job Requirements
Experience with Analyzing data, identify trends, interpret results, and prepare excel and tableau reports and data visualizations for bureau leadership, county partners, and other stakeholders.
Experience with developing, maintaining, and managing tableau reports and dashboards.
Experience with creating Analytical, Time Series, Metrics, Rankings, Statistical, Christmas Tree, Tracking, and Expenditure reports. Experience with using excel data analysis functions, scripts, reports, and charts.
Experience with writing SQL queries to get data from different databases like DB2 Z/OS; AWS Aurora and AWS RDS and Data Warehouse.
Experience in working with Cross Technical, Functional and Business Teams.
Capable to understand business requirements and develop reports based on requirements.
Capable to work individually and work in a team environment
Thanks & Regards
Charan
Anveta, Inc.
1333 Corporate Drive, Suite #108
Irving, TX 75038
charan@anveta.com
https://www.linkedin.com/in/charan-reddy-ba6450236/
Show more
Show less","SQL, Excel, Data Analysis, Tableau, Data Visualization, Reporting, Data Analytics, Data Warehousing, AWS, DB2 Z/OS","sql, excel, data analysis, tableau, data visualization, reporting, data analytics, data warehousing, aws, db2 zos","aws, dataanalytics, datawarehouse, db2 zos, excel, reporting, sql, tableau, visualization"
Associate Engineer - Database Services,"Susquehanna International Group, LLP (SIG)",Greater Philadelphia,https://www.linkedin.com/jobs/view/associate-engineer-database-services-at-susquehanna-international-group-llp-sig-3456696600,2023-12-17,Belleville, Canada,Associate,Hybrid,"Overview:
Our trading systems production engineering team is looking for a Database Engineer to join our team. You would join a diverse team of database administrators, software developers, and production engineers that are responsible for providing reliable platforms and services for our trading systems, including SQL and NoSQL databases, messaging middleware, reference data distribution, and observability and alerting solutions.
This person would join an existing group of Data Engineers to assist with the administration, configuration, maintenance, and deployment of an existing environment of 150+ servers. This role would allow you to diagnose full stack problems across multiple systems and collaborate with systems engineers, developers, database administrators, and business users to share ideas and recommendations for solutions that increase and expand our database environment.
Day to day responsibilities would include:
Supporting production operation of over 150 database servers used across a wide variety of systems and end-users
Supporting, operating, troubleshooting, and enhancing ETL processes to support both core business functionality and application specific needs
Design and implement highly available fault tolerant database platforms to support existing and new applications and use cases
Working with stakeholders to continuously improve our systems, including software and hardware upgrades, reliability improvements, monitoring and alerting, and performance tuning in order to meet ongoing business needs
Working with developers and end-users to improve the efficiency and performance of their database operations, including SQL tuning and write operations
Working with other teams to assist with diagnosing and remediating production incidents.
Remediating and repairing database systems and production data to ensure trading systems have access to required data and services
Assisting in the archiving, regeneration, and repair of historical data
What we're looking for
Bachelor’s degree in Computer Science, Computer Engineering, Information Technology or a related field
Minimum of 5 years of experience as a MySQL/MariaDB (operating on Linux) database administrator
Experience with large databases with tens of millions of rows and terabytes of data.
Experience with multiple database systems, including MongoDB, Oracle, Postgres, and Microsoft SQL Server
Experience with Linux and Windows operating systems
Ability to learn and maintain a legacy code base
Hands on experience with SQL development, shell scripting, and Python.
Experience with ETL tools such as Informatica a plus
Experience with DevOps and CI/CD tools such as Perforce, GitLab, TeamCity, Jenkins, or Ansible a plus
About SIG
SIG is a global quantitative trading firm founded by a group of friends who share a passion for game theory and probabilistic thinking. We have incorporated this approach into our culture, where you will find relentless problem solvers within each of our core disciplines: Trading, Technology, and Quantitative Research. From offices around the world, our employees collaborate to make optimal decisions and are driven by the desire to achieve winning results together.
‌
What we do
We are experts in trading essentially all listed financial products and asset classes, with a focus on derivatives trading. Through market making and market taking, we handle millions of trading transactions around the world every day, providing liquidity and ensuring competitive prices for buyers and sellers. While our presence in the market is broad, our trading desks are highly specialized, allowing for a deep understanding of unique drivers of each asset class.
SIG does not accept unsolicited resumes from recruiters or search firms. Any resume or referral submitted in the absence of a signed agreement will become the property of SIG and no fee will be paid.
Show more
Show less","MySQL, MariaDB, Linux, MongoDB, Oracle, Postgres, Microsoft SQL Server, Windows, SQL, Shell Scripting, Python, Informatica, DevOps, CI/CD, Perforce, GitLab, TeamCity, Jenkins, Ansible","mysql, mariadb, linux, mongodb, oracle, postgres, microsoft sql server, windows, sql, shell scripting, python, informatica, devops, cicd, perforce, gitlab, teamcity, jenkins, ansible","ansible, cicd, devops, gitlab, informatica, jenkins, linux, mariadb, microsoft sql server, mongodb, mysql, oracle, perforce, postgres, python, shell scripting, sql, teamcity, windows"
Data Engineer with Cloudera,Diverse Lynx,"North Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-with-cloudera-at-diverse-lynx-3768054900,2023-12-17,Belleville, Canada,Associate,Hybrid,"Role: Data Engineer with Cloudera
Experience: 5+ Years
Location: North Chicago, IL 60064- Onsite from day 1
Duration: 12-24 Months
Must-Have**
Demonstrate mastery across a wide variety of data engineering activities, including data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, and data ops.
Demonstrate knowledge of pharmaceutical R&D/Life Sciensssce centric datasets and utilize this knowledge to advance agile, impactful, and cost-effective solutions rapidly.
Bachelor's degree with 7 years of IT experience
Must have experience with data analysis programming languages (e.g., SQL, Python & Apache Spark, SAS & R)
Must have experience with database technologies (e.g., Oracle, Postgres, Hive, and HBase)
Must have experience with ETL/Orchestration tools (e.g., Informatica, Autosys, and Airflow, etc.)
Experience with AWS and Cloudera Public Cloud architecture is preferred.
Experience working with Pharmaceutical R&D industry-centric datasets is preferred.
Good-to-Have
Collaborate & contribute to the architecture, design, development, and maintenance of large-scale data & analytics platforms, system integrations, data pipelines, data models & API integrations to support evolving business strategy.Must have experience with software development life cycle; Experience with DevOps is preferred.
Contribute and maintain the team's methodology to conform and curate data, benchmarking against industry standards. Ensure that data are optimally standardized and analysis-ready.
Prototype emerging business use cases to validate technology approaches and propose potential solutions.
Research and recommend opportunities to adopt new technologies for continuous improvement.
Ensure compliance with applicable Client software development lifecycle policies and procedures.
Responsibility of / Expectations from the Role
To coordinate with Business and transform their needs into IT requirement
To understand Business Vision/goal and to deliver the same from IT side
Experience of working in Phase-0 projects
Knowlede of ETL & reporting tools
Knowldege of working in Data Lake projects from scratch
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Cloudera, Data warehousing, Master data management, Data cataloging, System integration, Data streaming, Data visualization, Data analysis, Data ops, Oracle, Postgres, Hive, HBase, Informatica, Autosys, Airflow, SQL, Python, Apache Spark, SAS, R, AWS, DevOps, ETL, Reporting tools, Data Lake projects","cloudera, data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, data ops, oracle, postgres, hive, hbase, informatica, autosys, airflow, sql, python, apache spark, sas, r, aws, devops, etl, reporting tools, data lake projects","airflow, apache spark, autosys, aws, cloudera, data cataloging, data lake projects, data ops, data streaming, dataanalytics, datawarehouse, devops, etl, hbase, hive, informatica, master data management, oracle, postgres, python, r, reporting tools, sas, sql, system integration, visualization"
Jr/Mid Opensource Database Engineer,"Liberty Personnel Services, Inc.","Philadelphia, PA",https://www.linkedin.com/jobs/view/jr-mid-opensource-database-engineer-at-liberty-personnel-services-inc-3602463706,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Details:
Jr/Mid Opensource Database Engineer
My client is looking for a Jr/Mid Opensource Database Engineer. They would like to see a STEM bachelors/MS degree along with 1-5 years of opensource data engineer experience. Python is required for scripting. Any ETL would be a plus. Opensource databases such as Maria, mysql, mongo, cassandra, nosql. This is a full time perm position that works a Hybrid schedule.
If you are interested please forward your resume in word format to kevin@libertyjobs.com Kevin Mccarthy
#associate
#mid-senior
Show more
Show less","Opensource, Python, ETL, Maria, MySQL, MongoDB, Cassandra, NoSQL","opensource, python, etl, maria, mysql, mongodb, cassandra, nosql","cassandra, etl, maria, mongodb, mysql, nosql, opensource, python"
Data Analyst,Steneral Consulting,"Richardson, TX",https://www.linkedin.com/jobs/view/data-analyst-at-steneral-consulting-3741122848,2023-12-17,Belleville, Canada,Associate,Hybrid,"Hybrid role in either Chicago or Richardson (Must be local and undrer 60 mins to either location)
Need valid LinkedIn
W2 candidates only
Description
This role requires the individual to have experience with collecting, organizing, and analyzing data from various resources. Primary tasks of this position are compilation and analysis of data definitions, meaning, usage, labelling standards and other source system information to produce IT consumable information. They may be required to take on other tasks as needed such as but not limited to: technical expertise with automatic data collection and reporting systems, including a capacity for program troubleshooting and system security measures
Required Qualification(s)
Show more
Show less","Data analysis, Data collection, Data organization, Data compilation, Data usage, Data labelling, System security, Program troubleshooting, Automatic data collection, Reporting","data analysis, data collection, data organization, data compilation, data usage, data labelling, system security, program troubleshooting, automatic data collection, reporting","automatic data collection, data collection, data compilation, data labelling, data organization, data usage, dataanalytics, program troubleshooting, reporting, system security"
Data Analyst,Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/data-analyst-at-stellar-professionals-3757507292,2023-12-17,Belleville, Canada,Associate,Hybrid,"Skills Required
Data retrieval (UI, SQL, API)
Data Engineering (cleaning, preparation, validation)
Data analytics, integration, and visualization
Advanced Excel skills
Visualization platforms: Tableau, PowerBI
Scripting/coding for data science: Python, R
GIS tools: ArcMap, ArcGIS Pro
Exposure to some transportation skills & concepts
Data fusion
Data science (model development, applying machine learning to transportation problems)
Map conflation
Attention to detail
Strong organizational skills
Ability to work independently
Ability to present data in a format understandable to diverse audience
Strong communicati on skills
Show more
Show less","Data retrieval, SQL, API, Data engineering, Data cleaning, Data preparation, Data validation, Data analytics, Data integration, Data visualization, Excel, Tableau, PowerBI, Python, R, GIS, ArcMap, ArcGIS Pro, Transportation skills, Transportation concepts, Data fusion, Data science, Model development, Machine learning, Map conflation","data retrieval, sql, api, data engineering, data cleaning, data preparation, data validation, data analytics, data integration, data visualization, excel, tableau, powerbi, python, r, gis, arcmap, arcgis pro, transportation skills, transportation concepts, data fusion, data science, model development, machine learning, map conflation","api, arcgis pro, arcmap, data cleaning, data engineering, data fusion, data integration, data preparation, data retrieval, data science, data validation, dataanalytics, excel, gis, machine learning, map conflation, model development, powerbi, python, r, sql, tableau, transportation concepts, transportation skills, visualization"
Data Engineer - DataStage,Brickhouse Resources,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-datastage-at-brickhouse-resources-3756662844,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title:                             Data Engineer, DataStage
Position Type:                   Contract
Contract Duration:           12 months (possibility of extension or conversion to FTE)
Workplace Type:              Hybrid (3 days in-office, 2 days remote)
Workplace Schedule:   8am to 5pm (Tues, Wed, Thursday are in-office)
We are seeking an experienced Data Engineer, specifically with IBM Infosphere DataStage/Quality Stage. The Data Engineer will design, develop, automate, and support complex applications to extract, transform, and load data; Implement and automate complex processes; Plan and conduct ETL unit and development tests; Monitor results and take corrective actions; Translate data access, transformation, and movement requirements into functional requirements and mapping designs. Use SQL to design database schemas, optimize data retrieval, and implement ETL processes. Investigate, analyze, and resolve complex technical problems related to system functions, environment, and procedures; and Manage service levels, standards, and configurations; Participate in Agile teams to meet goals; Development and support of Kafka integrations, including topics, producers, consumers, and streaming applications;
Design, develop and maintain Enterprise micro services  security, logging, common APIs and enforcing coding standards.
Required Skills:
Bachelor's degree in Computer Science, Computer Engineering, Electrical/Electronics Engineering, or related technical field and 7 years of software development experience.
Must have extensive hands-on experience in designing, developing and maintaining software solutions on ETL process utilizing IBM Infosphere DataStage/Quality Stage
Strong Skills in SQL, PL/SQL Programming, Query Optimization.
Must have experience with analyzing, tuning and optimizing queries in the RDBMS.
5+ years experience working in a Data Warehouse environment (or equivalent).
6 years of experience with the design, development, automation, and support of applications to extract, transform, and load data;
6 years of experience with databases, including Oracle, DB2, Teradata, SQL Server, and Access;
6 years of experience with IBM InfoSphere DataStage;
6 years of experience with Data warehouse;
6 years of experience with Unix Shell Scripting;
6 years of experience working within an Agile Scrum environment;
Preferred Skills:
2+ years of experience with Spring boot is preferred
2+ years of experience building applications using Java and experience with API integrations is preferred
Knowledge of Kafka and streaming solutions is a plus
Position Responsibilities and Required Knowledge:
Design, build and maintain batch or real-time data pipelines.
Automate data workflows such as data ingestion, aggregation, and ETL processing.
Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.
Monitor data systems performance and implement optimization strategies.
Analyzing, tuning and optimizing queries in the RDBMS.
Must be able to produce thorough technical design documents, test plans and documented results.
Will work in Agile methodologies and a fast-paced environment
Developing API Integrations with cloud and other applications.
Experience building applications using Java and Java based frameworks like Spring Boot is  strongly preferred.
Knowledge of Data Management Strategies (ETL Frameworks, Data Modeling concepts)
Strong understanding of relational databases, oracle functions, procedures, triggers and packages.
Willingness to learn new technologies.
Excellent verbal and written communication skill to communicate complex information clearly and simply.
Show more
Show less","DataStage, SQL, PL/SQL, Java, Spring Boot, Unix Shell Scripting, Kafka, Data Warehousing, ETL, Data Modeling, Data Management Strategies, Agile Development, Relational Databases, Oracle, DB2, Teradata, SQL Server, Access, API Integrations, Cloud Computing","datastage, sql, plsql, java, spring boot, unix shell scripting, kafka, data warehousing, etl, data modeling, data management strategies, agile development, relational databases, oracle, db2, teradata, sql server, access, api integrations, cloud computing","access, agile development, api integrations, cloud computing, data management strategies, datamodeling, datastage, datawarehouse, db2, etl, java, kafka, oracle, plsql, relational databases, spring boot, sql, sql server, teradata, unix shell scripting"
Senior Data Engineer,Brother USA,"Bridgewater, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-brother-usa-3743996500,2023-12-17,Belleville, Canada,Associate,Hybrid,"Let's Grow Together
Our mission is to live our “at your side” promise and simplify and enrich the lives of our customers, employees, and communities. ""At your side"" is more than a slogan to us; it’s the purpose we do our best to fulfill every day. With a legacy spanning over a century, this is a great place to launch or expand any career and push the boundaries of what comes next. We're committed to achieving shared success, and we provide opportunities for you to develop through experience, exposure and education. Our people have always leveraged their unique perspectives to keep us on the right track for a lasting future. If you want to innovate, learn, and grow with a global leader that builds products, services, and a company people love, then we’ll be “at your side” every step of the way.
The Senior Data Engineer is dedicated to advancing analytics capabilities to support our business. This role plays a crucial part in acquiring, cleaning, and transforming data from various sources to facilitate advanced analytics. The responsibilities include data preparation, ML pipeline development, data integration, modeling, performance tuning, data governance, and collaborative documentation. The role's expertise contributes to the success of data-driven initiatives and help drive optimal project outcomes for our stakeholders.
Duties and Responsibilities
Data Engineering - Data Enablement Support
Data Preparation and Cleaning: Acquire, clean, and transform data from various sources, ensuring data quality and suitability for advanced analytics. Handle missing data, outliers, and validate data
Machine Learning Pipeline Development: Collaborate closely with Data Scientists to design, build, and maintain machine learning pipelines. Develop data preprocessing and feature engineering workflows for accurate model training
Data Integration: Integrate data from diverse sources, including databases, APIs, and external data providers, using tools like Alteryx, Databricks, and Azure. Design and implement Extract, Transform, Load (ETL) processes to harmonize data for analytical purposes
Data Modeling and Optimization: Assist in creating data models supporting analytical needs, including data warehousing and data marts. Optimize data structures and databases for query performance.
Performance Tuning: Ensure efficient data processing systems and timely execution of analytical workloads through indexing, query optimization, and parallel processing
Data Governance and Compliance: Incorporate data governance policies and compliance into data engineering processes to maintain data integrity and security. Capture data lineage to meet governance requirements
Collaboration and Documentation: Collaborate with cross-functional teams, documenting data sources, transformations, and data lineage to ensure transparency and reproducibility in analytics projects
Business Knowledge Utilization: Utilize business acumen to contribute to project success and align data engineering efforts with stakeholder objectives. Understand the business problem and how data is used for decision-making
Data Quality Assurance: Develop and implement data quality checks and validation procedures to maintain data accuracy
Data Monitoring: Continuously monitor data pipelines and proactively address issues to ensure data availability and reliability
Technology Evaluation: Stay updated on emerging data engineering technologies and recommend their adoption where appropriate to enhance our capabilities.
Data Security: Implement and enforce data security measures to protect sensitive information throughout the data lifecycle
Business Consultation
Facilitate meetings and/or projects, determine audience and tactics appropriate for a particular discussion
Analyze raw data to find opportunities to implement artificial intelligence into the current business processes
Perform statistical analysis to determine gaps that critically affect the performance of the process and prioritize for review
Identify options to streamline and/or eliminate manual processes, where applicable, by developing innovative solutions
Education
Bachelor's Degree (or equivalent experience) Computer Science, Information Technology, Business Administration, or related field Required
Experience
Minimum 5 years
Experience in data management disciplines demonstrating knowledge of analytics delivery and data integration best practices Required
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative Required
Software/Technical Skills
Strong knowledge of database systems (SQL and NoSQL) and data warehousing Required
Experience with data integration/ETL tools like Alteryx, Databricks, etc. Required
Proven Experience In Data Engineering And ETL Processes Required
Proficiency in programming languages such as Python, Java, or Scala Required
Knowledge of data governance and compliance practices Required
Familiarity With Machine Learning Frameworks And Concepts Preferred
Experience with data security practices and tools Preferred
Other Skills/Knowledge/Abilities
Knowledge of Visualization Tools (Tableau, PowerBI, etc.) Preferred
Data-driven approach with the ability to aid business transformation Required
Strong Critical Thinking Skills Required
Strong communication skills with ability to adapt facilitation style in order to engage a variety of group settings Required
Business acumen and ability to collaborate effectively with cross-functional teams Required
Ability to effectively convey complex and detailed technical information in a timely manner Required
Ability to challenge the status quo and foster shared understanding, transparency, and mastery of the process and/or system Required
Ability to balance multiple priorities and act with resolve in an ambiguous Required
Excellent Problem-solving And Troubleshooting Skills Required
Strong communication and documentation abilities Required
This role will be a hybrid role. Subject to business needs, employees may work remotely up to two days per week. Assigned office days will be determined by managers.
The salary (or hiring) range for this position is $125,000-$145,000 per year
Starting salary to be determined by the education, experience, knowledge, skills and abilities of the applicant, internal equity, location, and alignment with market data
Benefits include, but are not limited to, healthcare and wellness coverage, life and disability insurance, 401K, tuition reimbursement, and Paid Time Off. Details are available at https://mybenefits.nfp.com/Brother/2024/guidebook/
Brother International Corporation has earned its reputation as a premier provider of home office and business products, home appliances for the sewing and crafting enthusiast as well as industrial solutions that revolutionize the way we live and work. Brother International Corporation is a wholly-owned subsidiary of Brother Industries Ltd. With worldwide sales exceeding $6 billion, this global manufacturer was started more than 100 years ago. Bridgewater, New Jersey is the corporate headquarters for Brother in the Americas. It has fully integrated sales, marketing services, manufacturing, research and development capabilities located in the U.S. In addition to its headquarters, Brother has facilities in California, Illinois and Tennessee, as well as subsidiaries in Canada, Brazil, Chile, Argentina, Peru and Mexico. For more information, visit www.brother.com.
Brother International Corporation (""Brother"") is an equal opportunity employer and does not discriminate or make employment decisions on the basis of race, color, religion, sex, disability, or any other characteristic protected by applicable state or federal laws. If you require any physical or other assistance in completing this application, a reasonable accommodation will be made upon request.
Show more
Show less","Data Engineering, Data Preparation, Machine Learning, Data Integration, Data Modeling, Data Governance, Data Quality Assurance, Data Monitoring, SQL, NoSQL, Data Warehousing, Alteryx, Databricks, Python, Java, Scala, Tableau, PowerBI, Visualization Tools, Critical Thinking, Communication, Business Acumen, Problemsolving, Troubleshooting, Documentation","data engineering, data preparation, machine learning, data integration, data modeling, data governance, data quality assurance, data monitoring, sql, nosql, data warehousing, alteryx, databricks, python, java, scala, tableau, powerbi, visualization tools, critical thinking, communication, business acumen, problemsolving, troubleshooting, documentation","alteryx, business acumen, communication, critical thinking, data engineering, data governance, data integration, data monitoring, data preparation, data quality assurance, databricks, datamodeling, datawarehouse, documentation, java, machine learning, nosql, powerbi, problemsolving, python, scala, sql, tableau, troubleshooting, visualization tools"
Java UI / Data Engineer,"The Dignify Solutions, LLC","Boston, MA",https://www.linkedin.com/jobs/view/java-ui-data-engineer-at-the-dignify-solutions-llc-3768013820,2023-12-17,Belleville, Canada,Associate,Hybrid,"PrISM, Landing Page, DA Framework UI
Java
SQL
Oracle or Postgres
React.js/JavaScript
Additional Skills - SSC specific Cloud (CDT)
Show more
Show less","PrISM, Landing Page, DA Framework UI, Java, SQL, Oracle, Postgres, React.js, JavaScript, Cloud, CDT","prism, landing page, da framework ui, java, sql, oracle, postgres, reactjs, javascript, cloud, cdt","cdt, cloud, da framework ui, java, javascript, landing page, oracle, postgres, prism, reactjs, sql"
"Senior Data Analyst________________westfield center,OH",Steneral Consulting,"Ohio, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-westfield-center-oh-at-steneral-consulting-3707345721,2023-12-17,Belleville, Canada,Associate,Hybrid,"Hello,
I am reaching out to you on an exciting job opportunity with one of our clients.
Title:- Senior Data Analyst -locals
Location:-
Westfield Center, OH(Hybrid)
Duration:- 6 Months
Visa:- GC/USC
Interview Mode:-Skype/Video
Description
Title- Senior Data Analyst
Location- Hybrid 3 days a week in office and for PI Planning :
One Park Circle, Westfield Center, OH 44251
Live within 50 mile radius.
Linkedin must.
Jd-
Must Haves
Insurance Claims and Policy
Data Design & Data Modeling
SQL
Description
As a member of the Enterprise Information Management and Analytics (EIMA) team within the IT department, a Data Analyst 4 will have direct impact on important initiatives that enable our business by:
Providing analysis and insights of our data
Driving clarity of business requirements
Assessing whether data exists within the organization to satisfy those requirements and the quality of it
Determining how individuals or systems should use the data
Working in an agile team environment, the Data Analyst 4 partners with other analysts, members of business units and IT to perform data analysis, data profiling, data sourcing and data requirements gathering. The Data Analyst 4 is expected to be an excellent communicator that challenges themselves and has a strong desire to continually improve their knowledge and skills. A Data Analyst 4 works independently and serves as a subject matter expert. They direct the work and serve as a peer mentor to less experienced data analysts.
Essential Functions (primary functions and/or reasons the job exists in order of importance)
Performs data analysis, data profiling and data sourcing utilizing knowledge of data definitions, domain values, data relationships, business rules, data sources and the enterprise integrated data environment.
Gathers and documents functional and non-functional data requirements.
Creates, updates, and validates sources to target mapping documents.
Analyzes impacts of planned new application development or enhancements to existing applications on legacy and other relational data stores and the enterprise integrated data environment.
Creates conceptual, logical, and physical data models in support of requested changes.
Loads and manages reference data in partnership with business customers.
Analyzes and identifies data quality concerns or cleansing opportunities to correct data quality anomalies and facilitate resolution between technical, business, and other stakeholders.
Ensures deliverables are compliant with Westfield's data standards.
Serves as a peer mentor to less experienced data analysts.
Desired Qualifications/Experience/Certification/Education (in Order Of Importance)
10+ years of experience as a Data Analyst.
10+ years of experience understanding business requirements, translating them into data requirements, analyzing and designing data processes and assisting in building a solution to support the business requirements.
Highly skilled at systematically applying logical reasoning techniques to query, inspect, cleanse, profile, and study data in structured, semi-structured, and unstructured formats for the purpose of deriving knowledge to support business decision-making and mapping data between source and target systems to support application integrations.
Highly skilled at SQL Programming with the ability to perform the most complex queries against relational databases in an efficient manner.
Knowledgeable at building conceptual, logical and physical data models which are visual representations of an enterprise's business data. Experience with ErWin or other modeling tools a plus. Able to build and modify simple data models.
Knowledgeable at transforming data into compelling, meaningful and appealing visualizations, such as graphs, dashboards and reports, using intuitive industry visualization tools (e.g. Tableau, PowerBI, Excel, etc.)
Bachelor's degree in Information Technology, Computer Science, Engineering, Mathematics or related field or commensurate experience.
Candidate must have some
Data Modeling
experience (Erwin or other)
Candidate, if selected will be asked during the first 2 weeks trial period to demonstrate and Model a problem successfully.
Insurance Industry Experience Is Required Policy And Claims
Om Prakash
Talent Acquisition - North America
Direct:+13026017357
om@steneral.com
Show more
Show less","Data Analysis, Data Profiling, Data Sourcing, Data Requirements Gathering, Data Modeling, SQL Programming, Data Visualization, Tableau, PowerBI, Excel, ErWin, Data Quality Management, Business Intelligence, Business Requirements Gathering, Data Integration, Data Governance, Data Standards, Data Cleansing, Data Warehousing, Data Mining, Data Analytics, Data Science, Machine Learning, Artificial Intelligence","data analysis, data profiling, data sourcing, data requirements gathering, data modeling, sql programming, data visualization, tableau, powerbi, excel, erwin, data quality management, business intelligence, business requirements gathering, data integration, data governance, data standards, data cleansing, data warehousing, data mining, data analytics, data science, machine learning, artificial intelligence","artificial intelligence, business intelligence, business requirements gathering, data governance, data integration, data mining, data profiling, data quality management, data requirements gathering, data science, data sourcing, data standards, dataanalytics, datacleaning, datamodeling, datawarehouse, erwin, excel, machine learning, powerbi, sql, tableau, visualization"
Senior Data Lake Engineer - Databricks,Sev1Tech LLC,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-lake-engineer-databricks-at-sev1tech-llc-3772665208,2023-12-17,Belleville, Canada,Associate,Hybrid,"We are seeking a highly experienced and skilled Senior Data Lake Engineer to join our team. As the Senior Data Lake Engineer, you will play a critical role in establishing and configuring an enterprise-level Databricks solution to support our federal customer organization's data lake initiatives. This position offers a unique opportunity to work with cutting-edge technologies and shape the future of our federal customer's data infrastructure.
This position requires onsite presence at the customer location (Arlington, VA) one day per week.
If you are a highly skilled and experienced Senior Data Lake Engineer with expertise in Databricks and passion for building scalable and secure data lake solutions, we would like to hear from you.
Responsibilities:
Lead the design, implementation, and configuration of an enterprise Data Lake solution utilizing Databricks, ensuring scalability, reliability, and optimal performance.
Collaborate with cross-functional teams to gather requirements, understand data integration needs, and define data lake architecture and governance policies.
Establish and configure Databricks workspaces, clusters, and storage components, optimizing the solution for efficient data processing, query performance, and data governance.
Design and implement data ingestion pipelines to efficiently extract, transform, and load data from various sources into the data lake using Databricks tools and services.
Develop and maintain data lake security frameworks, including access controls, encryption solutions, and data masking techniques to protect sensitive data.
Collaborate with data engineers and data scientists to optimize data pipelines, develop data transformations, and ensure data quality and integrity.
Monitor and tune Databricks clusters and workloads to ensure performance, reliability, and cost optimization, utilizing automated scaling and resource management techniques.
Implement best practices for data governance, data cataloging, metadata management, and data lineage within Databricks, adhering to regulatory and compliance requirements.
Collaborate with infrastructure teams to ensure data lake infrastructure meets scalability and availability requirements, leveraging Databricks cluster management and AWS/Azure services.
Develop and maintain documentation and guidelines related to the Databricks solution, including architecture diagrams, standards, and processes.
Stay up to date with the latest advancements in Databricks, big data technologies, and cloud platforms, continuously evaluating and implementing new features and capabilities.
Provide technical guidance and mentorship to junior data engineers, promoting best practices and fostering a culture of continuous learning and growth.
Collaborate with stakeholders to understand their data analytics and reporting needs and develop scalable data models and data transformation processes to support these requirements.
Support data lake-related incident resolutions, troubleshooting data quality issues, performance bottlenecks, and other data-related challenges.
Collaborate with data governance and compliance teams to ensure data privacy, security, and compliance guidelines are adhered to within the data lake solution.
Participate in the evaluation and selection of new tools, technologies, and services to enhance the data lake infrastructure.
Minimum Qualifications
Bachelor's degree in computer science, information technology, or a related field. Equivalent experience will also be considered.
Proven experience in building and configuring enterprise-level data lake solutions using Databricks in an AWS or Azure environment.
In-depth knowledge of Databricks architecture, including workspaces, clusters, storage, notebook development, and automation capabilities.
Strong expertise in designing and implementing data ingestion pipelines, data transformations, and data quality processes using Databricks.
Experience with big data technologies such as Apache Spark, Apache Hive, Delta Lake, and Hadoop.
Solid understanding of data governance principles, data modeling, data cataloging, and metadata management.
Hands-on experience with cloud platforms like AWS or Azure, including relevant services like S3, EMR, Glue, Data Factory, etc.
Proficiency in SQL and one or more programming languages (Python, Scala, or Java) for data manipulation and transformation.
Knowledge of data security and privacy best practices, including data access controls, encryption, and data masking techniques.
Strong problem-solving and analytical skills, with the ability to identify and resolve complex data-related issues.
Excellent interpersonal and communication skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
Experience in a senior or lead role, providing technical guidance and mentorship to junior team members.
Relevant certifications such as Databricks Certified Developer or Databricks Certified Professional are highly desirable.
Eligibility/Clearance Requirements
: Must be able to provide proof of U.S. Citizenship.
Desired Qualifications
Clearance Preference
:
Active DHS/CISA suitability - 1st priority
Any DHS badge + DoD Top Secret - 2nd choice
DoD Top Secret + willingness to obtain DHS/CISA suitability - 3rd choice (it can take 10-60 days to obtain suitability – work can only begin once suitability is fully adjudicated).
About Sev1Tech LLC
Founded in 2010, Sev1Tech provides IT, engineering, and program management solutions delivery. Sev1Tech focuses on providing program and IT support services to critical missions across Federal and Commercial Clients. Our Mission is to Build better companies. Enable better government. Protect our nation. Build better humans across the country.
Join the Sev1Tech family where you can achieve great accomplishments while fostering a satisfying and rewarding career progression. Please apply directly through the website at: https://careers-sev1tech.icims.com/#joinSev1tech
For any additional questions or to submit any referrals, please contact: Caitlin.maupin@sev1tech.com
Sev1Tech is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Show more
Show less","Databricks, Data Lake, Apache Spark, Apache Hive, Delta Lake, AWS, Azure, S3, EMR, Glue, Data Factory, Python, Scala, Java, SQL, Hadoop, Data governance, Data modeling, Data cataloging, Metadata management, Data security, Data privacy, Encryption, Data masking, Data integration, Data pipelines, Data transformations, Data quality, Data analytics, Reporting, Business intelligence, Big data, Cloud computing, Automation, DevOps, Software development, Systems engineering, Program management, Project management, Agile, Scrum, Waterfall, Communication, Teamwork, Collaboration, Problemsolving, Analytical thinking, Innovation, Adaptability, Continuous learning","databricks, data lake, apache spark, apache hive, delta lake, aws, azure, s3, emr, glue, data factory, python, scala, java, sql, hadoop, data governance, data modeling, data cataloging, metadata management, data security, data privacy, encryption, data masking, data integration, data pipelines, data transformations, data quality, data analytics, reporting, business intelligence, big data, cloud computing, automation, devops, software development, systems engineering, program management, project management, agile, scrum, waterfall, communication, teamwork, collaboration, problemsolving, analytical thinking, innovation, adaptability, continuous learning","adaptability, agile, analytical thinking, apache hive, apache spark, automation, aws, azure, big data, business intelligence, cloud computing, collaboration, communication, continuous learning, data cataloging, data factory, data governance, data integration, data lake, data masking, data privacy, data quality, data security, data transformations, dataanalytics, databricks, datamodeling, datapipeline, delta lake, devops, emr, encryption, glue, hadoop, innovation, java, metadata management, problemsolving, program management, project management, python, reporting, s3, scala, scrum, software development, sql, systems engineering, teamwork, waterfall"
Data Analyst,ARC Mechanical,"Wilmington, MA",https://www.linkedin.com/jobs/view/data-analyst-at-arc-mechanical-3774011457,2023-12-17,Belleville, Canada,Associate,Hybrid,"Department:
ARC Mechanical
Employment Type:
Full Time
Location:
Wilmington, MA
Description
Kelvin Group is a leader in the field of industrial and commercial mission-critical mechanical services, process safety management and environmental compliance. Our turnkey solutions provide Design/Installation and Ongoing Service and Preventative Maintenance of world-class mechanical systems for leaders in the Food and Beverage, Cold Storage, Pharmaceutical, Educational, Health Care, and Process industries.
We are seeking a Data Analyst to join our team and support our 150 field technicians in our HVAC/R service business as well as work with our corporate team to build a comprehensive automated reporting package. The Data Analyst will play a critical role in gathering, analyzing, and interpreting data to optimize our operations, improve field service efficiency, and enhance overall business processes.
Key Responsibilities
Data Collection and Analysis:
Gather and compile data from various sources, including service reports, work orders, and back-end ERP systems.
Analyze and interpret data to identify trends, performance indicators, and areas for improvement.
Develop data collection protocols and ensure data accuracy and completeness.
Performance Tracking:
Monitor field technician performance and productivity.
Develop and maintain KPIs to measure service quality, response times, and customer satisfaction.
Generate regular reports and dashboards for management.
Operational Efficiency:
Work with the field service team to optimize technician routes and schedules.
Identify opportunities to reduce operational costs and improve resource allocation.
Assist in inventory management to ensure timely availability of spare parts.
Customer Satisfaction:
Analyze billing team feedback and service quality data to enhance the days to bill process.
Identify areas for service improvement and recommend action plans.
Collaborate with customer service teams to address customer concerns.
Technology Integration:
Assist in the integration of new technologies and validation of the data.
Ensure seamless data flow between field technicians and central databases.
Continuous Improvement:
Stay up to date with industry best practices and data analysis tools.
Propose process improvements and innovations to enhance field service operations.
Skills, Knowledge And Expertise
Bachelor's degree in a relevant field (e.g., Data Science, Business, Engineering, Finance).
Proven experience in data analysis and data management.
Proficiency in data analysis tools, such as Microsoft Excel, SQL, and data visualization tools.
Strong analytical and problem-solving skills.
Excellent communication and teamwork skills.
Knowledge of HVAC/R systems and field service operations (preferred but not mandatory).
Power BI or Tableau experience a plus.
Benefits
Check out these industry leading benefits:
Healthcare (medical, dental, vision, prescription drugs)
Health Reimbursement Arrangement (shared cost deductible)
Flexible Spending Account
Dependent Care Account
Accident Insurance
Life Insurance
AD&D Insurance
Short/Long Term Disability
Employer matched 401(k) savings plan
Paid vacation time
Paid sick time
Generous paid holiday schedule
Show more
Show less","Data Analysis, Data Management, Microsoft Excel, SQL, Data Visualization Tools, Analytical Skills, ProblemSolving Skills, Communication Skills, Teamwork Skills, HVAC/R Systems, Field Service Operations, Power BI, Tableau","data analysis, data management, microsoft excel, sql, data visualization tools, analytical skills, problemsolving skills, communication skills, teamwork skills, hvacr systems, field service operations, power bi, tableau","analytical skills, communication skills, data management, data visualization tools, dataanalytics, field service operations, hvacr systems, microsoft excel, powerbi, problemsolving skills, sql, tableau, teamwork skills"
Sr. Data Analyst,Stellar Professionals,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-stellar-professionals-3769267034,2023-12-17,Belleville, Canada,Associate,Hybrid,"Applicant must have 5 years of relevant experience with the following:
Experience as a Data Analyst
Strong SQL skills
Proficiency in Micro Strategy (or similar reporting tools)
Experience using extremely large data sets
Expert in Microsoft Excel
Advanced Access skills
Experience in data mapping and translation on Financial vertical
Show more
Show less","Data Analysis, SQL, Micro Strategy, Large Data Sets, Microsoft Excel, Access, Data Mapping, Financial Vertical","data analysis, sql, micro strategy, large data sets, microsoft excel, access, data mapping, financial vertical","access, data mapping, dataanalytics, financial vertical, large data sets, micro strategy, microsoft excel, sql"
Data Analyst - Operational Assessment,National Grid Renewables,"Bloomington, MN",https://www.linkedin.com/jobs/view/data-analyst-operational-assessment-at-national-grid-renewables-3748053954,2023-12-17,Belleville, Canada,Associate,Hybrid,"National Grid Renewables is a leading North American renewable energy company based in Minneapolis, Minnesota, with satellite offices located in the regions where it develops, constructs, and operates renewable energy projects. As a farmer-friendly and community-focused company, National Grid Renewables develops projects for corporations and utilities that seek to repower America’s electricity grid by reigniting local economies and reinvesting in a sustainable future. National Grid Renewables is part of the competitive, unregulated Ventures division of National Grid. It has a portfolio of solar, wind, and energy storage projects throughout the United States in various stages of development, construction, and operation.
National Grid Renewables partners with communities, farmers, and landowners where we develop. This means it’s not just about projects but about the people we work with, both outside and inside our organization. National Grid Renewables Team Members embody our foundational culture of being entrepreneurial, creative, and nimble and take pride in supporting National Grid’s vision to be at the heart of a clean, fair, and affordable energy future for all.
Data Analyst - Operational Assessment will be responsible for analyzing data from operating wind, solar, and battery storage projects, and supporting performance tracking / optimization efforts.
Duties:
Analyze meteorological and equipment performance data collected at operating sites
Identify field data quality issues and work with operations / engineering teams to resolve
Collaborate with plant operations staff to troubleshoot underperformance issues
Maintain and enhance internal performance monitoring processes
Assist in development of baseline production and loss expectations
Assist in performing operational assessments / production re-forecasts
Review contractual performance testing results
· Travel approximately 10% of the time
Qualifications:
3+ years in renewable energy
Experience with relational databases and large data sets
Experience with scientific programming languages
Familiarity with wind and solar energy assessments and industry-standard modeling software
BS in meteorology, engineering or other scientific field; MS preferred
Attributes:
Self-directed with ability to manage multiple complex projects simultaneously
Natural curiosity and desire for continuous improvement
Detail-oriented
Ability to effectively communicate technical information to non-technical collaborators
Ability to work in a collaborative environment with technical and non-technical associates
Show more
Show less","Data Analytics, Meteorology, Engineering, Scientific Programming, Relational Databases, Performance Monitoring, Production Forecasting, Wind and Solar Energy Assessments, IndustryStandard Modeling Software","data analytics, meteorology, engineering, scientific programming, relational databases, performance monitoring, production forecasting, wind and solar energy assessments, industrystandard modeling software","dataanalytics, engineering, industrystandard modeling software, meteorology, performance monitoring, production forecasting, relational databases, scientific programming, wind and solar energy assessments"
(5602) Data Engineer,"Merit321, Launching Careers","Columbia, MD",https://www.linkedin.com/jobs/view/5602-data-engineer-at-merit321-launching-careers-3700017595,2023-12-17,Belleville, Canada,Associate,Hybrid,"Position:
(5602) Data Engineer
Location:
Columbia, MD (Hybrid)
Clearance:
Active Secret Clearance
Our client is seeking a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network-related missions within military space, as either a developer, analyst, or engineer.
Work is performed mostly on customer site in Columbia, MD with some flexibility to work from home.
Essential Job Responsibilities
The ideal candidate will have experience with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance -
Must have a current Secret level security clearance and be willing to get PLACEMENT MANAGER/SCI and CI Poly and therefore all candidates must be a U.S. Citizen.
5 years’ experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change).
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired.
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Engineering, Big Data Systems, Programming (Python Java), Network Data, Data Extraction, Data Translation, Data Loading, Kibana, Elasticsearch, Log Formats (JSON XML), Data Flow, Data Storage (AWS S3 SQS), Troubleshooting, Security+ Certification, NOSQL Databases (Accumulo), Cyber Security, Network Security","data engineering, big data systems, programming python java, network data, data extraction, data translation, data loading, kibana, elasticsearch, log formats json xml, data flow, data storage aws s3 sqs, troubleshooting, security certification, nosql databases accumulo, cyber security, network security","big data systems, cyber security, data engineering, data extraction, data flow, data loading, data storage aws s3 sqs, data translation, elasticsearch, kibana, log formats json xml, network data, network security, nosql databases accumulo, programming python java, security certification, troubleshooting"
Senior Data Analyst,Steneral Consulting,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-steneral-consulting-3759955188,2023-12-17,Belleville, Canada,Associate,Hybrid,"Candidate’s Linked In Profile:
2 Managerial References:
First Manager’s Name:
Company:
Title:
Email:
Phone Number:
Second Manager’s Name:
Company:
Title:
Email:
Phone Number:
Attached resume
Job Title:
Senior Data Analyst
Location:
Deerfield Beach Florida 33064 – hybrid work environment, must be onsite a minimum of 3 days per week
Duration:
12+ month contract
Other Requirements
: candidates must be willing to live in the Deerfield Beach Florida area and go into the office a minimum of 3 days/week
Number of Positions
: 1
Job Overview
Selected candidate must be onsite a minimum of 3 days per week in Deerfield Beach, FL.*
Your Future Duties And Responsibilities
Responsible for eliciting, understanding, interpreting and representing business requirements and act as the conduit between the customer and technical teams to ensure requirements are understood.
Provide subject matter expertise on the use of data as well as educate teams on business model, metadata and standards.
Responsible for understanding source systems and its data models.
Develop source to target mappings for data lineage.
Document source architecture to include data flows.
Responsible for analyzing data to validate business domains and requirements.
Responsible for data profiling and ensuring data quality requirements are accurate and complete.
Act in an advisory capacity in data model reviews, architecture approach and solution design to ensure high quality deliverables.
Responsible for partnering with management and business units on innovative ways to successfully utilize data and related tools to advance business objectives.
Works with governance council to establish data governance standards and guidelines.
Assist with business data lake testing / experimentation
Assist with coordinating data dictionary completions
Mentor Project DA resources
Required Qualifications To Be Successful In This Role
Validated experience on projects involving data analysis and profiling, data integration, data cleansing, data mapping, and data conversion activities
Proficient in data management concepts, data lifecycle and methodologies
Knowledge and experience with an ERP system highly preferred. Experience using Sales Force a plus.
Experience and hands-on involvement in operational system modernization and transformation. Knowledge of Microsoft Dynamics 360 a big plus.
Excellent analytical, problem-solving, and decision-making skills, demonstrating both logic and creativity
Excellent written and verbal communication, as well as, strong organizational and presentation skills
Highly motivated and a strong desire to understand the organization, its industry, and its strategies
Resourceful at applying business and technical skills to drive innovation and performance improvement
Demonstrated ability to balance multiple contending priorities in a dynamic environment
Demonstrated facilitations and meeting management skills
Proven ability to work with business representatives to understand and detail their business and functional requirements and document it in an organized ‘functional design’ format
Excellent interpersonal skills with the ability to build relationships within and between individuals and multi-functional teams
Must be a self-starter and show strong initiative
Must exhibit strong customer service orientation
Ability to influence and motivate individuals and teams to drive mutually beneficial outcomes
Solid grasp of agile methodology framework is a plus
4+ years experience working as a data analyst using SQL, BI and other data analysis tools
3+ years hands-on experience working with SQL and a solid understanding of different data structures (flat files, relational, etc.)
Understands data modeling concepts and techniques
Experience working BI/Analytics tools such as Power BI and Tableau is a plus
Experience with MICROSOFT DYNAMICS 360 A BIG PLUS
Education
Bachelor’s degree or equivalent plus 5+ years of related professional experience
Degree in Technology and/or Finance related area preferred
Show more
Show less","Data analysis, Data profiling, Data integration, Data cleansing, Data mapping, Data conversion, Data management, Data lifecycle, ERP systems, Sales Force, Microsoft Dynamics 360, Analytical skills, Problemsolving, Decisionmaking, Written communication, Verbal communication, Organizational skills, Presentation skills, Motivation, Innovation, Performance improvement, Agile methodology, SQL, BI tools, Power BI, Tableau, MICROSOFT DYNAMICS 360, Bachelor's degree, Technology, Finance","data analysis, data profiling, data integration, data cleansing, data mapping, data conversion, data management, data lifecycle, erp systems, sales force, microsoft dynamics 360, analytical skills, problemsolving, decisionmaking, written communication, verbal communication, organizational skills, presentation skills, motivation, innovation, performance improvement, agile methodology, sql, bi tools, power bi, tableau, microsoft dynamics 360, bachelors degree, technology, finance","agile methodology, analytical skills, bachelors degree, bi tools, data conversion, data integration, data lifecycle, data management, data mapping, data profiling, dataanalytics, datacleaning, decisionmaking, erp systems, finance, innovation, microsoft dynamics 360, motivation, organizational skills, performance improvement, powerbi, presentation skills, problemsolving, sales force, sql, tableau, technology, verbal communication, written communication"
Data Engineer - Linguistics,Babel Street,"Somerville, MA",https://www.linkedin.com/jobs/view/data-engineer-linguistics-at-babel-street-3750636728,2023-12-17,Belleville, Canada,Associate,Hybrid,"About Babel Street and the Role:
Babel Street illuminates identity and information for a safer, more productive world. Engineered for mission-critical applications, our proven AI-powered products transform data into knowledge, build a more complete picture around identity, and discover digital evidence. Our advanced data analytics and intelligence platform, combined with a robust text analytics engine, helps teams rapidly transform massive amounts of global, multilingual data into actionable and contextual insights so they can act with confidence. Every day, we work with our customers in highly-regulated, high-stakes industries such as financial services, healthcare, legal and law enforcement, and the global public sector. The actionable insights we deliver safeguard lives and protect critical assets around the world. Learn more at babelstreet.com.
In this role you will have the opportunity to work with multiple, discrete engineering teams providing annotated, reliable data to train, develop, and evaluate natural language processing systems as well as consult on the language specific aspects of multilingual text. Join us and help us create the next wave of software for Natural Language Processing and Text Analytics.
What you will do:
Manage large-scale text mining, data acquisition and annotation projects
Train and supervise contractors as they perform manual annotation tasks
Measure reliability of parallel, manual annotations
Survey and catalogue new data releases and best practices in data maintenance, conversion, and analytics
What you will bring:
Strong scripting abilities, especially Python
Data cleaning, conversion, organization
Parsing XML, JSON, tabular data sets
Scraping and collecting text from online resources including web sites and APIs
Ability to write and revise annotation guidelines
Ability to translate product requirements into annotation guidelines
Ability to synthesize clear instructions and instructive examples
Knowledge of Linguistics and NLP applications including
Language identification
Tokenization
Part of speech tagging
Morphological analysis
Entity extraction, disambiguation, and linking
Syntactic parsing
Sentiment analysis
Experience working with manual annotation tools and platforms such as brat, WebAnno, Prodigy, Mechanical Turk, etc.
Nice to have:
Experience with databases such as SQL and Mongo
Experience with SPARQL query language
Proficiency in at least one natural language in addition to English
Experience with conversion, storage, version control and maintenance tasks for large multilingual text collections
Familiarity with prominent linguistic annotation guidelines (e.g., Penn Treebank)
Familiarity with linguistic community resources and data providers such as
Universal Dependencies treebank project
ClueWeb
CommonCrawl
Linguistic Data Consortium
Benefits at Babel Street (just to name a few...)
Health Benefits: Babel Street covers 90-100% monthly premium costs for Medical, Dental, Vision, Life & Disability insurances – for you and your family!
Retirement Plans: Babel Street offers both a Traditional and Roth 401(K) with a very competitive match.
Unlimited Flexible Leave: We trust our employees to manage their own time and balance their personal and work lives. At a minimum, employees need to take a least 15 to 25 days per year.
Holidays: Babel Street provides employees with 12 paid Federal Holidays
Tuition Reimbursement: We are committed to investing in our employees. One way we do that is with our Tuition Reimbursement Program for continuing education.
Babel Street is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Further, Babel Street will not discriminate against applicants for inquiring about, discussing or disclosing their pay or, in certain circumstances, the pay of their co
‐
worker, Pay Transparency Nondiscrimination.
In addition, Babel Street's policy is to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations and ordinances where a particular employee works. Upon request, we will provide you with more information about such accommodations.
Show more
Show less","Python, Data cleaning, Data conversion, Data organization, XML parsing, JSON parsing, Tabular data sets parsing, Web scraping, API scraping, Annotation guidelines writing, Annotation guidelines revision, Linguistics, NLP applications, Language identification, Tokenization, Partofspeech tagging, Morphological analysis, Entity extraction, Entity disambiguation, Entity linking, Syntactic parsing, Sentiment analysis, Manual annotation tools, Manual annotation platforms, SQL, Mongo, SPARQL query language, Natural language proficiency, Multilingual text collection conversion, Multilingual text collection storage, Multilingual text collection version control, Multilingual text collection maintenance, Linguistic annotation guidelines, Linguistic community resources, Linguistic data providers, Universal Dependencies treebank project, ClueWeb, CommonCrawl, Linguistic data consortium","python, data cleaning, data conversion, data organization, xml parsing, json parsing, tabular data sets parsing, web scraping, api scraping, annotation guidelines writing, annotation guidelines revision, linguistics, nlp applications, language identification, tokenization, partofspeech tagging, morphological analysis, entity extraction, entity disambiguation, entity linking, syntactic parsing, sentiment analysis, manual annotation tools, manual annotation platforms, sql, mongo, sparql query language, natural language proficiency, multilingual text collection conversion, multilingual text collection storage, multilingual text collection version control, multilingual text collection maintenance, linguistic annotation guidelines, linguistic community resources, linguistic data providers, universal dependencies treebank project, clueweb, commoncrawl, linguistic data consortium","annotation guidelines revision, annotation guidelines writing, api scraping, clueweb, commoncrawl, data cleaning, data conversion, data organization, entity disambiguation, entity extraction, entity linking, json parsing, language identification, linguistic annotation guidelines, linguistic community resources, linguistic data consortium, linguistic data providers, linguistics, manual annotation platforms, manual annotation tools, mongo, morphological analysis, multilingual text collection conversion, multilingual text collection maintenance, multilingual text collection storage, multilingual text collection version control, natural language proficiency, nlp applications, partofspeech tagging, python, sentiment analysis, sparql query language, sql, syntactic parsing, tabular data sets parsing, tokenization, universal dependencies treebank project, web scraping, xml parsing"
Salesforce Data Analyst,Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-extend-information-systems-inc-3779828511,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title: Salesforce Data Analyst
Location: New York, NY(Hybrid)
Duration: C2C
Experience level : 10+ yrs
Skills
: Salesforce Analytics, reporting, data sceince
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes. This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM
2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: -
571 - 386 - 2431
Email: Anoop@extendinfosys.com
Show more
Show less","Salesforce, Analytics, Data Science, Reporting, SQL, Salesforce Reporting Tools, Tableau, Power BI, Data Visualization Tools, Data Management Best Practices, Data Governance, Data Accuracy, SQL Server, Mainframe Database, SharePoint, Dynamics CRM, SQL Queries, ETL, SSIS, Salesforce Platform, Salesforce API, Salesforce Data Model, Salesforce OutoftheBox Functionality, Data Modeling, Data Documentation","salesforce, analytics, data science, reporting, sql, salesforce reporting tools, tableau, power bi, data visualization tools, data management best practices, data governance, data accuracy, sql server, mainframe database, sharepoint, dynamics crm, sql queries, etl, ssis, salesforce platform, salesforce api, salesforce data model, salesforce outofthebox functionality, data modeling, data documentation","analytics, data accuracy, data documentation, data governance, data management best practices, data science, data visualization tools, datamodeling, dynamics crm, etl, mainframe database, powerbi, reporting, salesforce, salesforce api, salesforce data model, salesforce outofthebox functionality, salesforce platform, salesforce reporting tools, sharepoint, sql, sql queries, sql server, ssis, tableau"
Salesforce Data Analyst,Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-extend-information-systems-inc-3778539353,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title: Salesforce Data Analyst
Location: New York, NY(Hybrid)
Duration: C2C
Experience level : 10+ yrs
Skills
: Salesforce Analytics, reporting, data sceince
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes. This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM
2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: -
571 - 386 - 2431
Email: Anoop@extendinfosys.com
Show more
Show less","Salesforce Analytics, Reporting, Data Science, SQL, SharePoint, Power BI, Tableau, Data Visualization, ETL, SSIS, Salesforce API, Salesforce Data Model, Salesforce Platform","salesforce analytics, reporting, data science, sql, sharepoint, power bi, tableau, data visualization, etl, ssis, salesforce api, salesforce data model, salesforce platform","data science, etl, powerbi, reporting, salesforce analytics, salesforce api, salesforce data model, salesforce platform, sharepoint, sql, ssis, tableau, visualization"
837 Data Analyst-local,Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/837-data-analyst-local-at-steneral-consulting-3736643582,2023-12-17,Belleville, Canada,Associate,Hybrid,"Location : Currently remote – starting next year 2-3 days on site in DC
2 rounds of interviews – 1 will be ON SITE
MUST BE LOCAL TO DC
MUST BE COVID VACCINATED
This if for a 837 Data Analyst
This is mostly a help desk type role where they will be handling production issues for the 837 transactions (some 835) They will monitor applications and coordinate with teams to resolve issues. They will enter tickets into Dynatrace. They use Onestop for their ticketing tool They also use Jenkins
Responsibilities
5-6 years of experience type person
Perform initial triage and full incident life cycle monitoring in support of FEP Bridge Tenant Plans. This position requires technical and analytical skills with a focus on incident definition, isolation, and resolution. It requires deep knowledge of EDI 837 Claims analysis, 999 errors and SQL queries.
Work with various members of the FEP TOS team to provide Tier 1 technical support to tenant Bridge plans. Tier 1 support involves the initial triaging of reported incidents with the FEP Bridge applications with a focus on data collection and prior incident pattern analysis.
Work closely with FEP Bridge Plan Consultants to ensure smooth communication and prompt follow-up on logged incidents with the applications.
Provide some basic technical support for tenant plans, running queries, and making recommendations on changes to the tenant plans interfaces to improve the overall customer experience.
Create and run performance monitoring reports for end customers including SLA tracking reports.
Manipulation of 837/835 EDI /XML files to generate a wide variety of valid claims
Write complex SQL queries for retrieval of required from multi-database environment. (Strong knowledge of Oracle database required).
Accountable for specific deliverables as defined by the Plan Consultants, and FEP TOS leadership.
Document incident resolutions, escalations, and closures within the CareFirst Incident tracking tool.
Document system monitoring design, implementation, as well as SOPs for various support teams in FEP TOS.
Monitor the FEP Bridge Applications through tools like Dynatrace, Jenkins Batch process.
Skills/Requirement – Support Analyst
This position requires a BA/BS in computer science or equivalent experience, and more than 7 years professional software support experience with an emphasis on incident resolution. Specific requirements include, but not limited to:
Detailed knowledge of 837/835 EDI formats including all segments and loops. Ability to manipulate 837/835 EDI /XML files to generate a wide variety of valid claims
Experience and In-depth knowledge of Claim Adjudication process
Experience with Incident management applications like SharePoint and Salesforce.
Experience with SQL queries.
Experience with Blue Plans is a plus
Some exposure to Object Oriented languages a plus.
Excellent communication and teamwork skills.
Experience with Monitoring tools like Jenkins
Show more
Show less","SQL, EDI, Oracle, Jenkins, Dynatrace, Onestop, SharePoint, Salesforce, XML, 837, 835, Object Oriented Programming Languages","sql, edi, oracle, jenkins, dynatrace, onestop, sharepoint, salesforce, xml, 837, 835, object oriented programming languages","835, 837, dynatrace, edi, jenkins, object oriented programming languages, onestop, oracle, salesforce, sharepoint, sql, xml"
837 Data Analyst-local,Steneral Consulting,"Washington, DC",https://www.linkedin.com/jobs/view/837-data-analyst-local-at-steneral-consulting-3736305169,2023-12-17,Belleville, Canada,Associate,Hybrid,"Location : Currently remote – starting next year 2-3 days on site in DC
2 rounds of interviews – 1 will be ON SITE
Contract to hire. Must convert at max salary of $99,500
MUST BE LOCAL TO DC
MUST BE COVID VACCINATED
This if for a 837 Data Analyst
This is mostly a help desk type role where they will be handling production issues for the 837 transactions (some 835) They will monitor applications and coordinate with teams to resolve issues. They will enter tickets into Dynatrace. They use Onestop for their ticketing tool They also use Jenkins
Responsibilities
5-6 years of experience type person
Perform initial triage and full incident life cycle monitoring in support of FEP Bridge Tenant Plans. This position requires technical and analytical skills with a focus on incident definition, isolation, and resolution. It requires deep knowledge of EDI 837 Claims analysis, 999 errors and SQL queries.
Work with various members of the FEP TOS team to provide Tier 1 technical support to tenant Bridge plans. Tier 1 support involves the initial triaging of reported incidents with the FEP Bridge applications with a focus on data collection and prior incident pattern analysis.
Work closely with FEP Bridge Plan Consultants to ensure smooth communication and prompt follow-up on logged incidents with the applications.
Provide some basic technical support for tenant plans, running queries, and making recommendations on changes to the tenant plans interfaces to improve the overall customer experience.
Create and run performance monitoring reports for end customers including SLA tracking reports.
Manipulation of 837/835 EDI /XML files to generate a wide variety of valid claims
Write complex SQL queries for retrieval of required from multi-database environment. (Strong knowledge of Oracle database required).
Accountable for specific deliverables as defined by the Plan Consultants, and FEP TOS leadership.
Document incident resolutions, escalations, and closures within the CareFirst Incident tracking tool.
Document system monitoring design, implementation, as well as SOPs for various support teams in FEP TOS.
Monitor the FEP Bridge Applications through tools like Dynatrace, Jenkins Batch process.
Skills/Requirement – Support Analyst
This position requires a BA/BS in computer science or equivalent experience, and more than 7 years professional software support experience with an emphasis on incident resolution. Specific requirements include, but not limited to:
Detailed knowledge of 837/835 EDI formats including all segments and loops. Ability to manipulate 837/835 EDI /XML files to generate a wide variety of valid claims
Experience and In-depth knowledge of Claim Adjudication process
Experience with Incident management applications like SharePoint and Salesforce.
Experience with SQL queries.
Experience with Blue Plans is a plus
Some exposure to Object Oriented languages a plus.
Excellent communication and teamwork skills.
Experience with Monitoring tools like Jenkins
Show more
Show less","SQL, EDI, 837 Claims, 999 Errors, XML, Oracle, SharePoint, Salesforce, Linux, Jenkins, Dynatrace","sql, edi, 837 claims, 999 errors, xml, oracle, sharepoint, salesforce, linux, jenkins, dynatrace","837 claims, 999 errors, dynatrace, edi, jenkins, linux, oracle, salesforce, sharepoint, sql, xml"
Data Analyst II-locals,Steneral Consulting,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-analyst-ii-locals-at-steneral-consulting-3728264331,2023-12-17,Belleville, Canada,Associate,Hybrid,"Job Title Data Analyst II
Business Sector Data Engineering
Work Location Pittsburgh, PA F2F interview 1 day onsite
Skills:Issue Management and servicenow and Issue Management, business analysis, and tools integration (requirements and UAT)
Duration 12 Months
Job Description The primary responsibilities of the candidate will include:
Issue Management analysis and triage through self-identified issues and data quality issues across the enterprise
Liase between stakeholders to identify and agree upon remediation plans and timelines for open issues
Use of ServiceNow tool for issue management process
Document technical requirements for ServiceNow tool enhancements and perform UAT
Assist with ServiceNow end user training including live demos, user guides, best practices, FAQs, etc. Manager Release Notes:
Is the goal to convert the candidate to FTE? Potentially
Show more
Show less","Issue Management, Business Analysis, ServiceNow, Requirements Gathering, UAT, End User Training, Technical Writing, Data Quality, Data Analysis","issue management, business analysis, servicenow, requirements gathering, uat, end user training, technical writing, data quality, data analysis","business analysis, data quality, dataanalytics, end user training, issue management, requirements gathering, servicenow, technical writing, uat"
MS BI Data Analyst-locals,Steneral Consulting,"The Woodlands, TX",https://www.linkedin.com/jobs/view/ms-bi-data-analyst-locals-at-steneral-consulting-3736683055,2023-12-17,Belleville, Canada,Associate,Hybrid,"Salary- $115,000/yr - $125,000/yr
The BI Developer will be responsible for delivering enterprise analytical reporting, dashboard and solutions. This position will work closely with business users and executives to gather, define, and translate business requirements into BI solutions. The applicant must have excellent collaboration skills with the ability to communicate concepts and solutions effectively and clearly. This position will also play a key technical role requiring expert level skills in the design, development and implementation of BI solutions using Visual Studio, SSMS, SSIS, SSAS, Power BI and SSRS.
Required Experience, Knowledge Skills, And Abilities
Must have the ability to perform complex analysis and be able to identify and resolve probable root cause issues.
Exception communication skills are mandatory as this position is forward facing with our business partners.
Must have strong analytical skills to identity business needs and formulate viable solutions.
Expert in the design, development, automation, and support of complex processes to extract, transform and load data into SQL Data Store and Data Warehouse environments using Visual Studio and Microsoft’s SQL Stack tools (SSMS, SSIS).
Expert in developing complete SSIS packages including development of stored procedures, connections, tasks, control flows, data flows, parameters, event handlers and variables.
Expert in the development of data warehouses and BI solutions with practicing knowledge of the Kimball methodology for data modeling.
Experience in developing, updating, managing, and monitoring SQL Agent Jobs.
Experience in SSAS tabular modeling with a thorough understanding of defining relationships, security and the use of partitions and perspectives.
Experience with writing custom DAX measures with a thorough understanding of the DAX language required to create DAX expressions/statements and queries.
Knowledge in the development of SSRS and Power BI reporting
Knowledge in the configuration and management of Replication, Change Data Capture and Change Tracking processes.
Required use of Azure DevOps for collaboration and code management
Knowledge of JDE/E1 data and table relationships is desirable.
Degree or Certifications required:
Bachelor’s degree or 6 years’ experience in related field.
Show more
Show less","Visual Studio, SSMS, SSIS, SSAS, Power BI, SSRS, SQL Server, SQL Agent Jobs, DAX, JDE/E1, Data Modeling, Data Warehousing, Azure DevOps, Kimball Methodology","visual studio, ssms, ssis, ssas, power bi, ssrs, sql server, sql agent jobs, dax, jdee1, data modeling, data warehousing, azure devops, kimball methodology","azure devops, datamodeling, datawarehouse, dax, jdee1, kimball methodology, powerbi, sql agent jobs, sql server, ssas, ssis, ssms, ssrs, visual studio"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Newyorkuniversity,"Belleville, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-newyorkuniversity-3750809129,2023-12-17,Belleville, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistics, Machine Learning, SQL, R, Python, Data Visualization, Tableau, Power BI, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, statistics, machine learning, sql, r, python, data visualization, tableau, power bi, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistics, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Dukeduchessinternational,"Greater Napanee, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-dukeduchessinternational-3759037979,2023-12-17,Belleville, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hrteam@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization Tools, Data Collection, Data Cleansing, Data Manipulation, SQL, R, Python, Tableau, Power BI, Data Modeling, Hypothesis Testing, A/B Testing, Data Quality, Data Integrity, Data Accuracy, Data Completeness, ETL Processes","data analysis, statistical techniques, data visualization tools, data collection, data cleansing, data manipulation, sql, r, python, tableau, power bi, data modeling, hypothesis testing, ab testing, data quality, data integrity, data accuracy, data completeness, etl processes","ab testing, data accuracy, data collection, data completeness, data integrity, data manipulation, data quality, data visualization tools, dataanalytics, datacleaning, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau"
Data Scientist/AI/ML Engineer (Onsite at MD),VLink Inc,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/data-scientist-ai-ml-engineer-onsite-at-md-at-vlink-inc-3786287501,2023-12-17,Frederick,United States,Mid senior,Onsite,"Responsibilities:
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements:
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications:
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in
Python
/R (Shiny preferably) Or
Tableau
.
Familiarity with software version control with git.
Strong work ethic and intellectual curiosity.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as
My SQL
and
BigQuery
.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units
Show more
Show less","Technical leadership, Solution finding, ML model design, Business goal tracking, Proof of concept execution, Data wrangling, Predictive modeling, Computer science degree, Data science degree, Computer networking, Data science experience, Analysis leading, Problemsolving, Business problem analysis, Solution implementation, Data science application design, DS design knowledge, ML/Ops/DevOps experience, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Data visualization tools, Tableau, Git, AWS, Google Cloud, MySQL, BigQuery, Communication, Problemsolving, Analytical concepts","technical leadership, solution finding, ml model design, business goal tracking, proof of concept execution, data wrangling, predictive modeling, computer science degree, data science degree, computer networking, data science experience, analysis leading, problemsolving, business problem analysis, solution implementation, data science application design, ds design knowledge, mlopsdevops experience, python, r, sql, pandas, scikitlearn, tensorflow, keras, data visualization tools, tableau, git, aws, google cloud, mysql, bigquery, communication, problemsolving, analytical concepts","analysis leading, analytical concepts, aws, bigquery, business goal tracking, business problem analysis, communication, computer networking, computer science degree, data science application design, data science degree, data science experience, data visualization tools, data wrangling, ds design knowledge, git, google cloud, keras, ml model design, mlopsdevops experience, mysql, pandas, predictive modeling, problemsolving, proof of concept execution, python, r, scikitlearn, solution finding, solution implementation, sql, tableau, technical leadership, tensorflow"
Sr Engineer - Data Science - Gaithersburg MD (Onsite) Local only,TekIntegral,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-gaithersburg-md-onsite-local-only-at-tekintegral-3715850372,2023-12-17,Frederick,United States,Mid senior,Onsite,"Title: Sr Engineer - Data Science
Location: Gaithersburg MD (Onsite) Local only
Duration: 6 months C2H
IV: Phone and Video
Visa: USC, GC and GC-EAD; Let me know if you have someone local with other Visas
Rate: $70/hr C2C
Vendor's Note:
I just spoke with the candidate that interviewed and it seems that the team is looking for a candidate that is very heavy on the data engineering side of the house with some experience in AI/ML and data science
NEED TO LIVE IN A 15-20M RADIUS OF Gaithersburg MD (will move fast) will probably close the role today.
Highlights in Yellow candidate must have
Required
Resume can be no longer than 2 pages
Needs to have degree/name of university/and year of graduation
Place of Birth
Conversion salary after 6m
Do not send any candidate that are not living in Fredrick or Montgomery County Maryland- or 15/20m radius of Gaithersburg Md
Needs to be a Senior candidate.
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Experience working across varying business and technical functional units
Show more
Show less","Data Science, AI/ML, Machine Learning, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Data visualization tools, Tableau, Git, AWS, Google, My SQL, BigQuery","data science, aiml, machine learning, python, r, sql, pandas, scikitlearn, tensorflow, keras, data visualization tools, tableau, git, aws, google, my sql, bigquery","aiml, aws, bigquery, data science, data visualization tools, git, google, keras, machine learning, my sql, pandas, python, r, scikitlearn, sql, tableau, tensorflow"
Sr Engineer Data Science,Hughes,"Germantown, MD",https://www.linkedin.com/jobs/view/sr-engineer-data-science-at-hughes-3750890853,2023-12-17,Frederick,United States,Mid senior,Onsite,"EchoStar Corporation (NASDAQ: SATS) is a premier global provider of satellite communication solutions. Headquartered in Englewood, Colo., and conducting business around the globe, EchoStar is a pioneer in communications technologies through its Hughes Network Systems and EchoStar Satellite Services business segments. For more information, visit echostar.com. Follow @EchoStar on Twitter.
EchoStar has an exciting opportunity for a
Sr
Engineer - Data Science
in our Hughes Network Systems division. This position will be located at our headquarters in Germantown, MD.
Responsibilities
Collaborate with cross-functional teams to design, build, and deploy cloud-based solutions and infrastructure on GCP.
Create interactive, meaningful and insightful visualizations, to communicate findings and insights effectively to internal teams and other stakeholders.
Responsible for extracting large quantities of data from On-premises systems, developing efficient ETL and data management processes, and building architectures for rapid ingestion and dissemination of key data.
Design and implement complete and complex data modeling and analytics to measure the performances of Aero and telecommunication systems.
Understanding the data security and privacy concerns when dealing with sensitive data in the Aero and Mobility sector.
Build and maintain data pipelines. Monitor and troubleshoot production systems.
Continuously improve infrastructure and processes to increase reliability, scalability, and security
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Apply the knowledge in statistics, AI/ML coding and development, cloud-based analytics platform to create an integrated data analytics platform for complex communications system.
Collaborate with leaders and managers to determine and address data and reporting needs for various company projects.
Basic Requirements
Bachelor’s degree in data science, Computer Science, Engineering, Statistics, or related technical field.
5+ years of experience as an MLOPS Engineer
5 to 7 years of Data Analytics work which includes data modelling and data visualization.
At least 2 years’ experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
Preferred Qualifications
Understanding of the Aero and Mobility industries relevant technologies.
Understanding of specific data sources and data types in this sector such as flight data, transport data and sensor data.
Strong programming skills commonly used for data analysis and machine learning such as Go, R and Python
Experience with GCP, IAM, DB, streaming, and build and deploy services.
Experience in designing, implementing, and fine-tuning machine learning models for various applications (e.g., classification, regression, clustering, recommendation systems).
Experience with machine learning techniques and algorithms (e.g., TensorFlow, sklearn, numpy, pytorch) and their applications.
Experience with deep learning and neural networks is beneficial.
Experience building data pipelines for batch and streaming applications.
Experience designing applications for observability using metrics, monitoring, logging, and alarming.
Strong understanding of machine learning algorithms, statistical modeling, and data manipulation techniques.
Experience with data visualization tools and libraries such as Power BI & Tableau.
Familiarity with big data technologies like Snowflake, Big query, or other distributed computing frameworks
Knowledge of SQL and databases for data retrieval and manipulation.
Strong problem-solving, analytical and troubleshooting skills.
Excellent communication skills, including the ability to explain complex technical concepts to non-technical stakeholders.
Experience presenting findings and insights to diverse audiences.
Experience with Confluence, Jira, and Bitbucket
Will be eligible for discretionary bonus, with funding based on company performance.
EchoStar is committed to offering a comprehensive and competitive benefits package. Our programs are designed to provide you with the ability to customize your benefits to best meet the needs of you and your family. Our philosophy for these programs is to support and encourage healthy living and wellness. Our benefits package covers it all–from healthcare savings plans to education assistance and more!
Financial: 401(k) retirement savings plan with company match; employee stock purchase plan; profit-sharing; company-paid life insurance, AD&D and disability
Work-Life Balance: Paid Time Off (PTO), company-paid holidays, health and wellness events, exercise and sports facilities (locations may vary)
Employee Incentives: Tuition reimbursement, employee referral program, year round employee events and community programs, discounts on Dish Network and HughesNet
Health: Medical, Dental, Vision, Employee Assistance Program (EAP), Health Savings Account (HSA) with opportunities to earn employer contributions; Health Care, Dependent Care and Transportation Flexible Spending Accounts (FSA)
EchoStar and its Affiliated Companies are committed to hiring and retaining a diverse workforce. We are an Equal Opportunity/Affirmative Action employer and will consider all qualified applicants for employment without regard to race, color, religion, gender, pregnancy, sex, sexual orientation, gender identity, national origin, age, genetic information, protected veteran status, disability, or any other basis protected by local, state, or federal law. U.S. Persons or those able to obtain and maintain U.S. government security clearances may be required for certain positions. EEO is the law.
Show more
Show less","GCP, Machine learning, Data modeling, Data visualization, Data engineering, Data analytics, Data science, Cloud computing, SQL, Python, R, Go, TensorFlow, Sklearn, Numpy, Pytorch, Snowflake, Big query, Power BI, Tableau, Confluence, Jira, Bitbucket","gcp, machine learning, data modeling, data visualization, data engineering, data analytics, data science, cloud computing, sql, python, r, go, tensorflow, sklearn, numpy, pytorch, snowflake, big query, power bi, tableau, confluence, jira, bitbucket","big query, bitbucket, cloud computing, confluence, data engineering, data science, dataanalytics, datamodeling, gcp, go, jira, machine learning, numpy, powerbi, python, pytorch, r, sklearn, snowflake, sql, tableau, tensorflow, visualization"
Senior Database Engineer - MS SQL Server,EMP Trust HR Solutions,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-ms-sql-server-at-emp-trust-hr-solutions-3787739750,2023-12-17,Frederick,United States,Mid senior,Onsite,"Profound knowledge as Database Developer/ Database Architect with experience in working with large databases.
Expert knowledge in T-SQL and Microsoft SQL-Server 2012 / 2014
Ideally you will have experience in object oriented software development with C#.NET and the design of software in the database environment.
Ability to understand the functionality of RDBMS, Capability to build real-time and highly secure DB, Implementation of logic (using Procedures, functions etc)
Good working experience with Visual Studio, SSDT, TFS and SQL Server Management Studio.
Database Cloud hosting
EMP Trust is an Equal Opportunity Employer
Powered by JazzHR
8Gm0U4dILP
Show more
Show less","Database Development, Database Architecture, TSQL, Microsoft SQLServer, C#.NET, ObjectOriented Software Development, Software Design, RDBMS, Realtime Database Development, Secure Database Development, Procedures, Functions, Visual Studio, SSDT, TFS, SQL Server Management Studio, Database Cloud Hosting","database development, database architecture, tsql, microsoft sqlserver, cnet, objectoriented software development, software design, rdbms, realtime database development, secure database development, procedures, functions, visual studio, ssdt, tfs, sql server management studio, database cloud hosting","cnet, database architecture, database cloud hosting, database development, functions, microsoft sqlserver, objectoriented software development, procedures, rdbms, realtime database development, secure database development, software design, sql server management studio, ssdt, tfs, tsql, visual studio"
"Urgent Role || Sr Engineer - Data Science  || Gaithersburg, MD(Onsite) || USC,GC only",Steneral Consulting,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/urgent-role-sr-engineer-data-science-gaithersburg-md-onsite-usc-gc-only-at-steneral-consulting-3697184097,2023-12-17,Frederick,United States,Mid senior,Onsite,"Onsite 100%
Responsibilities
Provide technical leadership and identify solution for complex problem.
Must be able to understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productionized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.
Basic Requirements
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferable in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science , AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and has experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units
Show more
Show less","Machine learning, Data science, Data analysis, Data wrangling, Data visualization, Problem solving, Analytical skills, Python, R, SQL, Pandas, ScikitLearn, TensorFlow, Keras, Git, AWS, Google Cloud, MySQL, BigQuery, Tableau, Shiny","machine learning, data science, data analysis, data wrangling, data visualization, problem solving, analytical skills, python, r, sql, pandas, scikitlearn, tensorflow, keras, git, aws, google cloud, mysql, bigquery, tableau, shiny","analytical skills, aws, bigquery, data science, data wrangling, dataanalytics, git, google cloud, keras, machine learning, mysql, pandas, problem solving, python, r, scikitlearn, shiny, sql, tableau, tensorflow, visualization"
"Data Engineer IV, Day Shift, Information Technology",Adventist HealthCare,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/data-engineer-iv-day-shift-information-technology-at-adventist-healthcare-3739286195,2023-12-17,Frederick,United States,Mid senior,Onsite,"Support Center
If you are a current Adventist HealthCare employee, please click this link to apply through your Workday account.
Adventist HealthCare seeks to hire an experienced Data Engineer IV who will embrace our mission to extend God’s care through the ministry of physical, mental, and spiritual healing.
As a Data Engineer IV you will:
Lead the creation and supports the ETL process to facilitate the on-boarding of data into the data warehouse and distribution of data across data stores from wide variety of sources using different ETL tools and technologies.
Lead the design and develop the physical and virtual Data warehouse table schemas, database, data marts for new and existent data sources for the business’s data warehouse.
Lead the development and maintenance of informative and actionable data visualizations and reports for analysis, problem solving, and business solutions that highlights relevant business trends and opportunities for improvement.
Lead all service operational tasks including but not limited to database setup, maintenance, configuration, programming, troubleshooting, debugging, testing, fulfilling support requests, resolving incidents, and managing problems.
Lead the creation and support of routine and ad hoc data load processes through database refreshes and updates.
Optimize data integration platform to provide optimal performance under increasing data volumes.
Lead the preparation and maintenance of comprehensive understanding of data warehousing and ETL application lifecycle, technical architecture, data, configuration, and operational support requirements.
Maintains detailed charts, diagrams, and flow charts outlining systems, hardware, interfaces, and configurations related to data warehousing, ETL jobs, Data visualization, Data analytics and other similar.
Provide expertise in the design and development of best practice for big data stacking, data warehousing, data lake, ETL architecture, data federation, virtualization, data modeling procedures, data visualization, and workflows including technical documents.
Lead the analytics support to generate insights in the form of dashboards, metrics, or reports for healthcare solutions including the collection of detail business analytics requirements from stakeholders and the development of standardized functional requirement documents.
Lead the preparation of presentations and demonstrates business intelligence solutions to end users.
Lead in the training of end users on new reports and dashboards
Assist in leading the preparation of documents related to data governance, definitions, key metrics, and sample reports, ensuring alignment and consistency in reporting across the organization and standardized data analytics, visualization, and business intelligence reports.
Lead the data Integration and data warehousing application portfolio support in the timely resolution of technology incidents while adhering to SLA prioritization and success measures including 24x7 on call support.
Qualifications include:
Bachelor’s degree in computer science, information technology, or a related field.
7+ years’ experience and knowledge of coding languages, including Java, XML, and SQL to extract data from various relational databases, application systems, flat files, XML documents, and load into data warehouses.
7+ years’ experience in warehousing architecture techniques, including MOLAP, ROLAP, ODS, DM, and EDW.
7+ years’ experience as an ETL developer using.
integration tools such as Talend and SSIS.
Experience working on cloud-based environment such as AWS or Azure.
Experience working with large scale.
Data warehouses and related technologies.
7+ years working on building meaningful data analytics, reports, and visualizations in health care environment.
Snowflake experience is preferred.
Experience in metadata management and related tools.
Advanced knowledge of database security, integrity, backup and recovery, and performance monitoring standards.
3 Years Supervisory Experience.
Preferred Cloud-Data Warehouse certification or ETL Certification or Data Visualization.
Experience with business intelligence software applications such as Tableau visualization, PowerBI, SQL servicer Management studio, IBM Cognos B.
Work Schedule:
Full Time Days
Monday - Friday
At Adventist HealthCare our job is to care for you.
We do this by offering:
Work life balance through nonrotating shifts
Recognition and rewards for professional expertise
403(b) retirement plan
Free Employee parking
Benefits Eligible Positions:
Competitive, comprehensive benefit plans [including health, employer-paid disability and life insurance, PTO]
Employer retirement contribution and match after 1-year of eligible employment with 3 year vesting
Ancillary benefits such as flexible spending, legal and pet insurance to meet the needs of employees and their eligible family members
Subsidized childcare at participating childcare centers
As a faith-based organization, with over a century of caring for the communities in the Maryland area, Adventist HealthCare has earned a reputation for high-quality, compassionate care. Adventist HealthCare was the first and is the largest healthcare provider in Montgomery County.
If you want to make a difference in someone’s life every day, consider a position with a team of professionals who are doing just that, making a difference.
Join the Adventist HealthCare team today, apply now to be considered!
COVID-19 Vaccination
Adventist HealthCare requires all applicants to be fully vaccinated for COVID-19 before commencing employment. Applicants may be required to furnish proof of vaccination and, if needed, may elect to be vaccinated at any community pharmacy or location offering COVID-19 vaccinations.
Tobacco and Drug Statement
Tobacco use is a well-recognized preventable cause of death in the United States and an important public health issue. In order to promote and maintain a healthy work environment, Adventist HealthCare will not hire applicants for employment who either state that they are nicotine users or who test positive for nicotine and drug use.
While some jurisdictions, including Maryland, permit the use of marijuana for medical purposes, marijuana continues to be classified as an illegal drug under the federal Controlled Substances Act. As a result, medical marijuana use will not be accepted as a valid explanation for a positive drug test result.
Adventist HealthCare will withdraw offers of employment to applicants who test positive for Cotinine (nicotine) and marijuana. Those testing positive are given the opportunity to re-apply in 90 days, if they can truthfully attest that they have not used any nicotine products in the past ninety (90) days and successfully pass follow-up testing. (""Nicotine products"" include, but are not limited to: cigarettes, cigars, pipes, chewing tobacco, e-cigarettes, vaping products, hookah, and nicotine replacement products (e.g., nicotine gum, nicotine patches, nicotine lozenges, etc.).
Equal Employment Opportunity
Adventist HealthCare is an Equal Opportunity/Affirmative Action Employer. We are committed to attracting, engaging, and developing the best people to cultivate our mission-centric culture. Our goal is to have a welcoming, equitable, and safe place to work and grow for all employees, no matter their background. AHC does not discriminate in employment opportunities or practices on the basis of race, ethnicity, color, religion, sex, national origin, age, disability, sexual orientation, gender identity, pregnancy and related medical conditions, protected veteran status, or any other characteristic protected by law.
Adventist HealthCare will make reasonable accommodations for applicants with disabilities, in accordance with applicable law. Adventist HealthCare is a religious organization as defined under applicable law; however, it will endeavor to provide reasonable accommodations for applicants’ religious beliefs.
Applicants who wish to request accommodations for disabilities or religious belief should contact the Support Center HR Office.
Show more
Show less","ETL, Data Warehousing, Data Visualization, Database, Data Analytics, Java, XML, SQL, Talend, SSIS, AWS, Azure, Snowflake, Tableau, PowerBI, SQL Servicer Management Studio, IBM Cognos B, CloudData Warehouse Certification, ETL Certification, Data Visualization Certification","etl, data warehousing, data visualization, database, data analytics, java, xml, sql, talend, ssis, aws, azure, snowflake, tableau, powerbi, sql servicer management studio, ibm cognos b, clouddata warehouse certification, etl certification, data visualization certification","aws, azure, clouddata warehouse certification, data visualization certification, dataanalytics, database, datawarehouse, etl, etl certification, ibm cognos b, java, powerbi, snowflake, sql, sql servicer management studio, ssis, tableau, talend, visualization, xml"
Databricks Sr. Systems Engineer -- Security Clearance REQUIRED,Leidos,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/databricks-sr-systems-engineer-security-clearance-required-at-leidos-3744511473,2023-12-17,Frederick,United States,Mid senior,Onsite,"Description
Leidos is currently seeking a Systems Engineer (SE) for the Advanced Analyst Augmentation Analytical Cloud Enablement System (4ACES). The SE will analyze 4ACES requirements and assist with design analysis. The SE will coordinate COTS upgrades, coordinate with COTS vendors, user Subject Matter Experts (SMEs), NGA Analytic Product Owners, and other segments when issues arise throughout sustainment. The Systems Engineer serves as the liaison with the PMO, customer user community, COTS vendor, other segment data providers, and will assist with integrations, migrations, Software Approval Process (SWAP), and accreditation activities. The SE will coordinate corrective maintenance management with the COTS vendor.
The SE’s primary responsibility will be to function as a Content Manager by managing data and data services published for the user community. As the Content Manager the SE will support the analysts with new data and Databricks requests as well as resolving any daily issues.
Position can also be performed in the following locations:
Alexandria, VA
St. Louis, MO
Tucson, AZ
Clearance Level Required
Top Secret/SCI with Polygraph
Primary Responsibilities
Interact with the customer Release Train Engineer, Product Owners, and stakeholders to analyze customer requirements and determine how to include requirements in the operational environment through COTS integration and new data sources being added to the Databricks cluster.
Perform the design and requirements analysis of data visualizations and COTS appliances for capability enhancements.
Coordinate with stakeholder and team to determine how to host new data capabilities within the operational environment and troubleshoot issues as the customer is integrating new data or creating new visualizations.
Identify software and hardware dependencies and capacities.
Basic Qualifications
US citizenship is required per contract.
Bachelor's degree and 8-12 years of prior relevant experience or Master’s with 6-10 years of prior relevant experience.
Experience with Databricks deployment, configuration, and support.
Significant experience in Cloud Technologies (specifically AWS).
Experience interacting with cross functional project teams including Software Development, Test, and Security.
Substantial experience working in Agile, SAFe, and Scrum environments.
Knowledge of Software Configuration Management life cycle deliverables.
Preferred Qualifications
Experience with Databricks deployment, configuration, and support.
AWS certifications.
Experience working with Machine Learning and knowledge of data science concepts.
Experience working with DevOps CI/CD related technologies and deployment automation techniques (e.g., Ansible, Git, Jenkins, Docker, Confluence, and Junit).
This position has a target range for compensation of $130,000 - $150,000. This is a firm fixed price contract and does not allow for negotiations outside of this target range.
Pay Range
Pay Range $101,400.00 - $183,300.00
The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
Original Posting Date
12/11/2023
While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","Systems Engineering, Databricks, AWS, Software Configuration Management, Machine Learning, Data Science, Agile, SAFe, Scrum, DevOps, CI/CD, Ansible, Git, Jenkins, Docker, Confluence, Junit","systems engineering, databricks, aws, software configuration management, machine learning, data science, agile, safe, scrum, devops, cicd, ansible, git, jenkins, docker, confluence, junit","agile, ansible, aws, cicd, confluence, data science, databricks, devops, docker, git, jenkins, junit, machine learning, safe, scrum, software configuration management, systems engineering"
Sr. Data Analyst - Remote | WFH,Get It Recruit - Information Technology,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/sr-data-analyst-remote-wfh-at-get-it-recruit-information-technology-3776621875,2023-12-17,Frederick,United States,Mid senior,Remote,"Rooted in the realm of higher education, our organization collaborates with institutions across North America to enhance financial decision-making, streamline assessment/accreditation processes, and foster a more integrated approach to strategic planning and institutional effectiveness. Comprised of dedicated professionals, we are fervently driven by the transformative impact of innovation on the mission of higher education.
Our Mission
We specialize in empowering colleges and universities to measure and assess their effectiveness in fulfilling their mission. By aligning investments with financial, operational, and student learning outcomes, we guide our partners on a journey toward institutional sustainability.
Our Approach
Transforming institutional data into a powerful asset is at the core of what we do. We combine cutting-edge technology platforms with industry best practices to present data in ways that empower stakeholders in higher education to make informed decisions and mitigate risks. Our unparalleled institutional expertise, coupled with proven data models and innovative software, sets us apart.
Who We're Looking For
We're on the lookout for a Senior Data Analyst to join our team of higher education data experts. As a key member of our organization, the ideal candidate will collaborate closely with client partners, leveraging their analytical skills and superior technical acumen to contribute to the success of our mission.
Key Responsibilities
Develop expertise in our data platform, guiding client partners in using it to address strategic institutional questions.
Deliver standardized analysis playbooks developed by our team.
Analyze financial data benchmarks for higher education institutions.
Conduct ad hoc analysis to uncover actionable insights for strategic decision-making.
Act as the primary liaison between our organization and client partners, working with senior leaders to maximize platform utilization.
Manage timelines and scope for client and internal projects.
Utilize enterprise business intelligence tools, SQL queries, and other tools to develop complex dashboards and reports.
Provide ongoing training and support to client stakeholders.
Contribute to the development of our product roadmap for analytical tools and solutions.
Translate and interpret business and data requirements between clients and our technical/data teams.
Ensure accuracy and quality of data extracts.
Qualifications
To thrive in this role, you should have:
Minimum of 5 years of experience in analytic functions in higher education or consulting within the field.
Deep understanding of key areas in higher education (e.g., financial aid, admissions, student affairs).
Excellent communication skills.
Experience with Student Information systems (e.g., Banner, PeopleSoft, Colleague, Workday).
Proficiency in reporting and data visualization tools (e.g., Tableau and Power BI).
Understanding of relational databases.
Ability to manage multiple priorities independently with strict deliverable dates.
Experience working with both business users and technical development teams.
Desirable Skills
Nice-to-haves include:
Experience with HR, CRM, Financial, and/or Learning Management systems in higher education.
Experience presenting findings to mid to senior-level executives.
Significant experience building reports and dashboards in Tableau.
Strong understanding of data warehouse concepts and working with large datasets.
Project Management experience.
Experience with SQL and in one or more analytic/programming software packages such as R, SAS, Python, or comparable.
Projects You Will Contribute To
Client analysis deliverables for Data Analytics clients and Benchmarking consortium members.
Creation of Tableau templates and playbooks to standardize analysis deliverables.
Process efficiency initiatives to streamline and improve existing delivery processes.
Tableau development work, documentation, and presentation materials to support the product roadmap.
Compensation And Benefits
We offer a competitive salary, paid time off, healthcare, vision, dental, 401(k) with company match, remote work flexibility, and a dynamic and collaborative work environment.
Employment Type: Full-Time
Show more
Show less","Data analytics, Business intelligence, SQL, Tableau, Power BI, Relational databases, Higher education, Financial aid, Admissions, Student affairs, Student Information systems, Reporting tools, Data visualization, Data warehouse concepts, Large datasets, Project Management, R, SAS, Python, Analytic/programming software packages","data analytics, business intelligence, sql, tableau, power bi, relational databases, higher education, financial aid, admissions, student affairs, student information systems, reporting tools, data visualization, data warehouse concepts, large datasets, project management, r, sas, python, analyticprogramming software packages","admissions, analyticprogramming software packages, business intelligence, data warehouse concepts, dataanalytics, financial aid, higher education, large datasets, powerbi, project management, python, r, relational databases, reporting tools, sas, sql, student affairs, student information systems, tableau, visualization"
Databricks Sr. Systems Engineer -- Security Clearance REQUIRED with Security Clearance,ClearanceJobs,"Gaithersburg, MD",https://www.linkedin.com/jobs/view/databricks-sr-systems-engineer-security-clearance-required-with-security-clearance-at-clearancejobs-3753467156,2023-12-17,Frederick,United States,Mid senior,Hybrid,"R-00121177 Description Leidos is currently seeking a Systems Engineer (SE) for the Advanced Analyst Augmentation Analytical Cloud Enablement System (4ACES). The SE will analyze 4ACES requirements and assist with design analysis. The SE will coordinate COTS upgrades, coordinate with COTS vendors, user Subject Matter Experts (SMEs), NGA Analytic Product Owners, and other segments when issues arise throughout sustainment. The Systems Engineer serves as the liaison with the PMO, customer user community, COTS vendor, other segment data providers, and will assist with integrations, migrations, Software Approval Process (SWAP), and accreditation activities. The SE will coordinate corrective maintenance management with the COTS vendor. The SE’s primary responsibility will be to function as a Content Manager by managing data and data services published for the user community. As the Content Manager the SE will support the analysts with new data and Databricks requests as well as resolving any daily issues. Position can also be performed in the following locations:  * Alexandria, VA
St. Louis, MO
Tucson, AZ  Clearance Level Required: Top Secret/SCI with Polygraph Primary Responsibilities: * Interact with the customer Release Train Engineer, Product Owners, and stakeholders to analyze customer requirements and determine how to include requirements in the operational environment through COTS integration and new data sources being added to the Databricks cluster. * Perform the design and requirements analysis of data visualizations and COTS appliances for capability enhancements. * Coordinate with stakeholder and team to determine how to host new data capabilities within the operational environment and troubleshoot issues as the customer is integrating new data or creating new visualizations. * Identify software and hardware dependencies and capacities. Basic Qualifications: * US citizenship is required per contract. * Bachelor's degree and 8-12 years of prior relevant experience or Master’s with 6-10 years of prior relevant experience. * Experience with Databricks deployment, configuration, and support.
Significant experience in Cloud Technologies (specifically AWS).
Experience interacting with cross functional project teams including Software Development, Test, and Security.
Substantial experience working in Agile, SAFe, and Scrum environments.
Knowledge of Software Configuration Management life cycle deliverables. Preferred Qualifications: * Experience with Databricks deployment, configuration, and support.
AWS certifications.
Experience working with Machine Learning and knowledge of data science concepts.
Experience working with DevOps CI/CD related technologies and deployment automation techniques (e.g., Ansible, Git, Jenkins, Docker, Confluence, and Junit). This position has a target range for compensation of $130,000 - $150,000. This is a firm fixed price contract and does not allow for negotiations outside of this target range. Pay Range: Pay Range $101,400.00 - $183,300.00 The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. Original Posting Date: 12/11/2023 While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","Systems Engineering, Databricks, AWS, Cloud Technologies, Machine Learning, Data Science, DevOps, CI/CD, Ansible, Git, Jenkins, Docker, Confluence, Junit, SAFe, Scrum, Software Configuration Management","systems engineering, databricks, aws, cloud technologies, machine learning, data science, devops, cicd, ansible, git, jenkins, docker, confluence, junit, safe, scrum, software configuration management","ansible, aws, cicd, cloud technologies, confluence, data science, databricks, devops, docker, git, jenkins, junit, machine learning, safe, scrum, software configuration management, systems engineering"
Data Analyst,The Walt Disney Company,"Burbank, CA",https://www.linkedin.com/jobs/view/data-analyst-at-the-walt-disney-company-3744045615,2023-12-17,Burbank,United States,Mid senior,Onsite,"Department/Group Overview:
The Disney Music Group (DMG) is the music hub for The Walt Disney Company, encompassing all aspects of music commercialization and marketing: recorded music (Walt Disney Records and Hollywood Records), Disney Music Publishing, and Disney Concerts! DMG distributes music both physically and digitally, and also licenses music throughout the world in various forms of media including television, print, gaming, and consumer products.
Job Summary:
The
Data Analyst
supports data development, reporting and quantitative research, serving as a data analytics partner to cross-functional teams and providing insights to advise planning and decisions across the Disney Music Group business areas.
As a member of the Data & Analytics team, the Data Analyst will contribute to the development of analytics and tools that enable a deep understanding of user behaviors within music marketing, discovery, and consumptions channels.
The right person for this role has a deep curiosity and aptitude in data with applied knowledge of measurement and statistics. The Data Analyst will collaborate with team members to translate business questions into analytical and technical methodologies, driving results through analysis of large transaction level datasets. The role will need to communicate effectively, distilling insights into concise and compelling data-driven stories and socialize broadly with audiences across levels and functions.
Responsibilities and Duties of the Role:
Acquire, organize, cleanse and manipulate complex, large-scale, high-dimensional data from multiple disparate sources using tools such as PySpark and advance SQL
Build automated, scalable, repeatable, and integrated analysis using modern big data architectures in data ecosystems such as Databricks, Snowflake, Google Big Query
Apply advanced analytics techniques (data mining, statistical analysis, machine learning) to identify patterns and uncover opportunities in behavioral data
Conduct campaign, product, content, and audience analysis for company priorities. Develop and report on key metrics to measure success
Partner with Engineering and Product teams to execute on data projects, contributing to solution discovery, design, build and QA
Required Education, Experience/Skills/Training:
Basic Qualifications:
3+ years of analytical experience or equivalent combination of education and experience
Bachelor’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
Knowledge of supervised and unsupervised learning techniques
Proficient in using scripting tools such as PySpark/Python/R and SQL
Proficient in data exploration and visualization tools
Strong time management with ability to work in a fast-paced environment across multiple priorities
Preferred Qualifications:
Experience in applying supervised and unsupervised learning techniques, such as linear and logistic regression, decision trees, or k-means clustering to derive, validate, and quantify conclusions from the data
Experience documenting data requirements, rules and, assumptions
Experience data mining in distributed systems such as Databricks/Google Big Query/Snowflake
Experience manipulating large, transaction level datasets and interpreting data trends from multiple disparate data sources
A passion for music and/or entertainment industry data!
Familiarity with music streaming services (Spotify, Apple, Amazon Music, YouTube), social media platforms (TikTok, FB/Instagram) and new technologies (digital assistants/voice)
Master’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
The hiring range for this position in Burbank, CA is $93,398 to $125,290 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.
Show more
Show less","Data Analytics, Data Mining, Machine Learning, Natural Language Processing, Statistical Analysis, Business Intelligence, Big Data, Data Visualization, Cloud Computing, SQL, Python, PySpark, R, Databricks, Snowflake, Google BigQuery, Tableau, Power BI","data analytics, data mining, machine learning, natural language processing, statistical analysis, business intelligence, big data, data visualization, cloud computing, sql, python, pyspark, r, databricks, snowflake, google bigquery, tableau, power bi","big data, business intelligence, cloud computing, data mining, dataanalytics, databricks, google bigquery, machine learning, natural language processing, powerbi, python, r, snowflake, spark, sql, statistical analysis, tableau, visualization"
Sr. Data Engineer,Ontic,"Chatsworth, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-ontic-3779465207,2023-12-17,Burbank,United States,Mid senior,Onsite,"We are a fast-paced business with ambitious growth plans; so if you are dedicated, enthusiastic and always seeking ways to improve, you'll enjoy a career with us!
Job Purpose:
Data is Ontic’s biggest asset. We’re looking for someone who is passionate about data and who has the technical and soft skills to propel our data platform through an exciting period of growth. We are looking for someone with a unique blend of data platform governance and architecture, data and analytics engineering and data science. Somebody with the ability to communicate effectively across the board in a way that makes the complex seem simple.
As part of the Ontic Global Data Team, you will be responsible for designing, developing, and maintaining our data architecture and pipelines. You will also be responsible for analysing data across multiple systems, producing data models, identifying gaps in the data models, and making recommendation for improvements. Your primary focus will be on leveraging Snowflake, dbt, Fivetran, and Power BI to ensure our data is accessible, reliable, and actionable for stakeholders across the organization. In addition to technical proficiency, this role requires strong stakeholder engagement skills to collaborate effectively with teams across the company. This is a very exciting and varied role and therefore will best suit a self-starter willing to deeply understand the broader company objectives and how proposed data projects support them.
Main responsibilities:
Perform exploratory data analysis on a range of complex business problems.
Interpret business processes into data requirements.
Define and enforce key data attribute governance strategies.
Act as geographical data leader, reporting directly to the global director.
Collaborate on data team initiatives impacting Ontic’s global strategic pillars.
Administer and maintain the Snowflake Data Warehouse.
Contribute to data pipelines that clean, transform and aggregate data from disparate data sources into reporting data stores
Provide input on the company data strategy by suggesting and proving new technologies and innovations.
Contributing to the translation of data into valuable insights that inform data driven decisions.
Develop dashboards and reports to present data insights to stakeholders around the business.
Support end-user technical questions and assist in data education
Host on-site data quality clinics
Participate in and audit data governance activities
Knowledge, Skills And Experience
Proven experience of administering and working in a Data Warehouse (experience of Snowflake desirable)
Ability to influence a team of technical and non-technical stakeholders
Experience building data models and flows from various sources using Kimball Dimensional Modelling techniques.
Good knowledge of data warehousing and data modelling principles.
Experience of administering, creating, and maintaining data pipelines in an ELT application that cleans, transforms, and aggregates data from disparate data sources into reporting data stores.
Experience documenting ELT specifications such as source to target mappings, business rules and data validation requirements, ETL functional and non-functional requirements.
Experience of DBT
Strong SQL programming experience in a commercial setting.
Experience of using BI reporting tools (i.e. Power BI) with an ability to develop reports and dashboards.
Strong analytical and problem-solving skills with a keen attention to detail.
Excellent oral and written communication skills for translating data insights into concise executive summaries and presentations.
Ability to present findings in a structured, clear manner with actionable insights.
Possess ability to both multi-task and manage priorities effectively with the ability to work proactively and independently in meeting stakeholder needs whilst adhering to tight deadlines
Appreciation of data security, CCPA, GDPR and PII data handling principles.
A willingness to be utilised as a developer across other enterprise applications.
Experience of working in the Aerospace, Engineering, Manufacturing sectors
Experience with below toolset preferred:
Snowflake data warehouse
Dbt
FiveTran
Microsoft Fabric/Power BI Analytics
GitHub peer review
Kimball Dimensional Modelling
Dynamics 365 Backend
Infor Syteline and/or Infor Visual ERP Backend
SQL
Key Working Relationships:
A variety of key stakeholders including site and enterprise leadership teams
Geographically spread Global Data Team
External mix of strategic technology partners and niche providers.
Ontic is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Note: This job description is intended to convey information essential to understanding the scope of the position and is not an exhaustive list of skills, efforts, duties, responsibilities, or working conditions associated with it. The company may change responsibilities or requirements as the need arises.
Compensation Pay Range
$110-$130. Budgeted $130
Please click here to review Ontic's California Consumer Privacy Act policy.
Ontic Engineering and Manufacturing Inc. is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran
Status, or any other characteristic protected by applicable federal, state, or local law.
This position requires use of information which is subject to the International Traffic in Arms Regulations (ITAR). All applicants must be U.S. persons within the meaning of the ITAR. The ITAR defines a U.S. person as a U.S. Citizen, U.S. Permanent Resident (i.e. 'Green Card Holder'), Political Asylee, or Refugee.
Show more
Show less","Snowflake, dbt, Fivetran, Power BI, Data architecture, Data modeling, Data analysis, SQL, Kimball Dimensional Modelling, Data warehousing, Data pipelines, Data governance, Data security, CCPA, GDPR, PII, Cloud computing, Data visualization, Data cleaning, Data transformation, Data aggregation, Data reporting, Datadriven decision making, Stakeholder engagement, Communication, Problemsolving, Analytical thinking, Attention to detail, Multitasking, Prioritization, Time management, Microsoft Fabric","snowflake, dbt, fivetran, power bi, data architecture, data modeling, data analysis, sql, kimball dimensional modelling, data warehousing, data pipelines, data governance, data security, ccpa, gdpr, pii, cloud computing, data visualization, data cleaning, data transformation, data aggregation, data reporting, datadriven decision making, stakeholder engagement, communication, problemsolving, analytical thinking, attention to detail, multitasking, prioritization, time management, microsoft fabric","analytical thinking, attention to detail, ccpa, cloud computing, communication, data aggregation, data architecture, data cleaning, data governance, data reporting, data security, data transformation, dataanalytics, datadriven decision making, datamodeling, datapipeline, datawarehouse, dbt, fivetran, gdpr, kimball dimensional modelling, microsoft fabric, multitasking, pii, powerbi, prioritization, problemsolving, snowflake, sql, stakeholder engagement, time management, visualization"
Senior Data Engineer,Activision Blizzard,"Santa Monica, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-activision-blizzard-3772911047,2023-12-17,Burbank,United States,Mid senior,Onsite,"Job Title
Senior Data Engineer
Requisition ID
R022176
Job Description
Welcome to Solid State Studios. We’re a new in-house studio within Activision, dedicated to developing the best AAA mobile games in the world. Our first project is Call of Duty: Warzone for Mobile, and we’re looking for great talent, passionate about their work, who share our belief in what AAA experiences on mobile can and should be. We’re incredibly excited to build out our teams and get to work, and invite you to join us in making something special!
Our first project is Call of Duty Warzone for Mobile and we’re looking for great talent from mobile, console and PC backgrounds passionate about their work, who share our belief in what AAA experiences on mobile canand should be. We’re incredibly excited to build out our teams and get to work, and invite you to join us in making something special.
Your Profile
As a Sr. Data Engineer you will be responsible for the millions of events our game will emit each day.
You will collaborate closely with engineers to instrument events in the game code, work with data analysts to define the processing of data through ETLs into our data architecture, and be responsible for the infrastructure that automates reporting of business insights. You will work cross-title with other data teams to evolve the pipeline that processes events across a large-scale data footprint. You will lead the monitoring solutions used to ensure the availability and reliability of our data and our game, including the machine learning based alerting system for anomaly detection and identify a solution to detect slow moving trends.
As the senior member of the team, you will identify new ways to elevate our data systems, increase the self-serve nature of our data ecosystem, and minimize data and game outages. You will also mentor and guide other members of the team and set roadmaps and priorities.
Main Mission
Contribute to the solutions used for processing and storing of data, from code libraries to ETLs to Databricks Lakehouse, dashboards, and in-grown coding projects used for data analysis
Maintain and expand upon our ETL process, consuming data from third parties where needed, and ensure a clean data architecture
Work with multi-functional team members in defining and documenting single sources of truth to ensure consistent and high-quality data
Reconcile data issues and alerts between various systems, finding opportunities to innovate and drive improvements
Work in a highly collaborative team to devise solutions to business problems, bringing your skills and ideas into every discussion
Learn new technical skills when needed and use them to help the team achieve success
Collaborate with other Call of Duty teams to ensure the consistent and coherent growth of our data and tools capabilities
Assist the data scientist community, helping to analyse the results of AB tests and define predictive models
Minimum Requirements
MS/BS in Computer Science, Data Science, Systems Engineering, Applied Mathematics or equivalent experience
8+ years of experience as a Data Engineer or in a similar role
Expert-level knowledge of SQL
Experience with Python including common data science libraries (e.g. Pandas,NumPy, Jupyter, IPython)
Experience working with Google Cloud Platform OR Exposure to Big Data technologies like Databricks and/or BigQuery
Ability to build API integrations with our internal systems and third-party data sources
Strong communication skills with the ability to translate business needs into technical specifications
Highly collaborative work style, but not afraid to take on solo tasks
Curious and inquisitive mind-set
Bonus Objectives
Experience in solving complex data engineering problems at a really large scale
Familiarity with version control tools (Git commands) and basic understanding of containerization, build, and deployment processes
Experience with big data platforms technologies such as Kafka, Spark, Airflow, and others
Familiarity with live monitoring solutions like Anodot
We love hearing from anyone who is enthusiastic about changing the games
industry. Not sure you meet all qualifications? Let us decide! Research shows that women and members of other under-represented groups tend to not apply to jobs when they think they may not meet every qualification, when, in fact, they often do! At Activision Blizzard, we are committed to creating a diverse and inclusive environment and strongly encourage you to apply.
About Activision
Activision Blizzard, Inc. (NASDAQ: ATVI), is one of the world's largest and most
successful interactive entertainment companies and is at the intersection of media, technology and entertainment. We are home to some of the most beloved entertainment franchises including Call of Duty®, World of Warcraft®, Overwatch®, Diablo®, Candy Crush™ and Bubble Witch™. Our combined entertainment network delights hundreds of millions of monthly active users in 196 countries, making us the largest gaming network on the planet!
Our ability to build immersive and innovative worlds is only enhanced by diverse teams working in an inclusive environment. We aspire to have a culture where everyone can thrive in order to connect and engage the world through epic entertainment. We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered!
The videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as advised by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
Rewards
Requirements
We provide a suite of benefits that promote physical, emotional and financial well-being for ‘Every World’ - we’ve got our employees covered! Subject to eligibility requirements, the Company offers comprehensive benefits including:
Medical, dental, vision, health savings account or health reimbursement account, healthcare spending accounts, dependent care spending accounts, life and AD&D insurance, disability insurance;
401(k) with Company match, tuition reimbursement, charitable donation matching;
Paid holidays and vacation, paid sick time, floating holidays, compassion and bereavement leaves, parental leave;
Mental health & wellbeing programs, fitness programs, free and discounted games, and a variety of other voluntary benefit programs like supplemental life & disability, legal service, ID protection, rental insurance, and others;
If the Company requires that you move geographic locations for the job, then you may also be eligible for relocation assistance.
Eligibility to participate in these benefits may vary for part time and temporary full-time employees and interns with the Company. You can learn more by visiting https://www.benefitsforeveryworld.com/.
In the U.S., the standard base pay range for this role is $101,000.00 - $186,754.00 Annual. These values reflect the expected base pay range of new hires across all U.S. locations. Ultimately, your specific range and offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share this role’s range details for your local geography during the hiring process. In addition to a competitive base pay, employees in this role may be eligible for incentive compensation. Incentive compensation is not guaranteed.
Show more
Show less","Data Engineering, Data Analysis, ETL, Data Architecture, Business Intelligence, Machine Learning, Anomaly Detection, Data Science, Python, Pandas, NumPy, Jupyter, IPython, Google Cloud Platform, Databricks, BigQuery, SQL, API Integration, Git, Kafka, Spark, Airflow, Anodot","data engineering, data analysis, etl, data architecture, business intelligence, machine learning, anomaly detection, data science, python, pandas, numpy, jupyter, ipython, google cloud platform, databricks, bigquery, sql, api integration, git, kafka, spark, airflow, anodot","airflow, anodot, anomaly detection, api integration, bigquery, business intelligence, data architecture, data engineering, data science, dataanalytics, databricks, etl, git, google cloud platform, ipython, jupyter, kafka, machine learning, numpy, pandas, python, spark, sql"
Sr Data Analyst,The Walt Disney Company,"Burbank, CA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-the-walt-disney-company-3744050022,2023-12-17,Burbank,United States,Mid senior,Onsite,"Department/Group Overview:
The Disney Music Group (DMG) is the music hub for TWDC, encompassing all aspects of music commercialization and marketing: recorded music (Walt Disney Records and Hollywood Records), Disney Music Publishing, and Disney Concerts. DMG distributes music both physically and digitally, and also licenses music throughout the world in various forms of media including television, print, gaming, and consumer products.
Job Summary:
The
Sr. Data Analyst
supports data development, reporting and quantitative research, serving as a data analytics partner to cross-functional teams and providing insights to inform planning and decisions across the Disney Music Group business areas.
As a member of the Data & Analytics team, the Sr. Data Analyst will contribute to the development of analytics and tools that enable a deep understanding of user behaviors within music marketing, discovery, and consumptions channels.
The right person for this role has a deep curiosity and aptitude in data with applied knowledge of measurement and statistics. The role will be expected to translate business questions into analytical and technical methodologies, driving results through analysis of large transaction level datasets. The role will need to communicate effectively, distilling insights into concise and compelling data-driven stories and socialize broadly with audiences across levels and functions.
Responsibilities and Duties of the Role:
Develop analytical frameworks, defining the data, measurement, and analysis, to answer business questions across the Disney Music Group business areas, including recorded music, publishing and concerts
Acquire, organize, cleanse and manipulate complex, large-scale, high-dimensional data from multiple disparate sources utilizing tools such as PySpark and advance SQL
Build automated, scalable, repeatable, and integrated analysis using modern big data architectures in data ecosystems such as Databricks, Snowflake, Google Big Query
Apply advanced analytics techniques (data mining, statistical analysis, machine learning) to identify patterns and uncover opportunities in behavioral data
Conduct campaign, product, content, and audience analysis for company priorities. Develop and report on key metrics to measure success
Partner with Engineering and Product teams to execute on data projects, contributing to solution discovery, design, build and QA
Required Education, Experience/Skills/Training:
Basic Qualifications:
5+ years of analytical experience or equivalent combination of education and experience
Bachelor’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field
Experience delivering end-to-end analytics projects
Experience in applying supervised and unsupervised learning techniques, such as linear and logistic regression, decision trees, or k-means clustering to derive, validate, and quantify conclusions from the data
Experience documenting data requirements, rules and, assumptions
Proficient in using scripting tools such as PySpark/Python/R and SQL
Proficient in data exploration and visualization tools
Strong time management with ability to work in a fast-paced environment across multiple priorities
Preferred Qualifications:
Experience data mining in distributed systems such as Databricks/Google Big Query/Snowflake
Experience manipulating large, transaction level datasets and interpreting data trends from multiple disparate data sources
A passion for music and/or entertainment industry data
Familiarity with music streaming services (Spotify, Apple, Amazon Music, YouTube), social media platforms (TikTok, FB/Instagram) and emerging technologies (digital assistants/voice)
Master’s degree in an analytical field including Statistics, Mathematics, Physics, Computer Science, Engineering, or other related quantitative field.
The hiring range for this position in Burbank, CA is $112,586 to $151,030 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.
Show more
Show less","Data analysis, Machine learning, Statistics, PySpark, SQL, Data mining, Data visualization, Data science, Hadoop, Python, R, Data frameworks, Distributed systems, Big data, Data lakes, Data warehousing, Cloud computing, Agile methodologies, SCRUM, Kanban","data analysis, machine learning, statistics, pyspark, sql, data mining, data visualization, data science, hadoop, python, r, data frameworks, distributed systems, big data, data lakes, data warehousing, cloud computing, agile methodologies, scrum, kanban","agile methodologies, big data, cloud computing, data frameworks, data lakes, data mining, data science, dataanalytics, datawarehouse, distributed systems, hadoop, kanban, machine learning, python, r, scrum, spark, sql, statistics, visualization"
Senior Data Analyst,Revenue.io,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-revenue-io-3704074794,2023-12-17,Burbank,United States,Mid senior,Remote,"Revenue.io powers high-performing teams with real-time guidance. By surfacing and recommending what works best, Revenue.io enables customers like HPE, Nutanix, and Fidelity Investments to deliver predictable results and optimize their entire revenue operation.
Loved by customers, employees and analysts, Revenue.io is the only company included in the most recent Forrester Waves for Conversation Intelligence and Sales Engagement, is a Gartner Cool Vendor, and is named one of the ""Best Places to Work"" by BuiltinLA and Comparably.
Revenue.io offers talented candidates a chance to work with executive mentors in a rapid growth environment where teammates are passionate about our transformative technology, vibrant culture and collaborative mindset.
Who We're Looking For
Revenue.io is seeking an amazing, Senior Data Analyst to be a foundational piece of our Data & Analytics (D&A) team. As the first Senior Data Analyst member of the team, you will serve as a critical voice on our reporting and analytics strategy both in product and within Revenue.io. You will work with Leadership to develop a cascading metrics strategy, connecting the benefits our products deliver to how those benefits are realized in our product experiences. You will work with Product Engineering, Operations, and your fellow D&A team members to build and execute on this strategy, leveraging your deep expertise to create the right experiences that generate the most valuable and actionable insights to your customers.
What You'll Do
Collaborate with Leadership to develop a cascading metrics strategy, connecting the benefits our products deliver to how those benefits are realized/experienced within our products. You will leverage your deep statistical expertise to identify correlation and/or causation between experiences and benefits.
Partner with Product Engineering, Operations, and your fellow D&A team members to build and execute on a reporting and analytics strategy that supports our metrics strategy, leveraging your deep expertise to design and create the right experiences in our products that generate the most valuable and actionable insights to your customers
Collaborate with Chief Product Officer and Chief Delivery Officer to develop our product metrics approach, leveraging Pendo, Redshift, and other analytical tools as necessary
Conduct analysis to drive valuable business insights, using internal analytics tools, i.e., SQL to access and manipulate from disparate data sources
Build intuitive data visualizations to tell stories with data that convey key performance metrics, significant trends, etc.
Assist in the design and analysis of A/B tests to improve the user journey in our products
Support others in the development of sales funnel analysis, lifetime value analysis, customer segmentation and forecasting
Be a proactive and amazing teammate!
Qualifications
5+ years of relevant work experience in analytics, data centric role and/or business intelligence related field
Significant analytical skills; background in statistics and/or data science strongly preferred
Strong, demonstrable experience with data visualization tools (Tableau, Powerbi, etc.) and strong storytelling skills for technical and non-technical audiences
Advanced skills in SQL and SQL platforms
Experience in product analytics/telemetry and related platforms - e.g., Amplitude, Pendo, Google Analytics - a strong plus
Experience with prescriptive and predictive analytics a plus
An excellent communicator, with the ability to simplify key messages, present compelling stories and promote technical and personal credibility with internal and external stakeholders
Passion for teamwork and collaboration, adaptability, communication, problem-solving, customer focus, results, and innovation
Compensation
Additional incentive compensation may be offered as stock options and company benefits. Actual compensation within the range will be determined on your skills, experience, qualification, location, and market conditions. As an equal opportunity employer, we are committed to providing a fair and transparent workplace for all employees and applicants.
Salary Range
$120,000—$150,000 USD
Company Benefits Include
Paid parental leave
Flexible time off (US only)
Competitive salary
Multiple medical plans to choose from including HSA and FSA
Work from home flexibility
Anti-Discrimination
We consider applicants without regards to race, color, national origin, sex, age, religion, sexual orientation, gender identity, veteran status, marital status, physical or mental disability, or other protected classes under all local, state, and federal laws and ordinances (AA/EOE/W/M/Vet/Disabled).
California Applicants Only
What Personal Information We Collect
Professional, employment-related, or schooling information. Current or past job history, performance evaluations, and educational background, including grades and transcripts.
How We Use Your Information
For professional, internal analysis, or employment-related purposes, including job applications. all applicants are subject to our Employment Privacy Notice and Global privacy policy.
Global Data Privacy Notice for Job Candidates
Please follow this link to access the document that provides transparency around the way in which Revenue.io handles personal data of employees and job applicants: https://www.revenue.io/privacy-notice-employment
Show more
Show less","Tableau, PowerBI, SQL, Data Analytics, Data Visualization, Data Science, Predictive Analytics, Prescriptive Analytics, Amplitude, Pendo, Google Analytics, Statistics, A/B Testing, Data Science","tableau, powerbi, sql, data analytics, data visualization, data science, predictive analytics, prescriptive analytics, amplitude, pendo, google analytics, statistics, ab testing, data science","ab testing, amplitude, data science, dataanalytics, google analytics, pendo, powerbi, predictive analytics, prescriptive analytics, sql, statistics, tableau, visualization"
Lead Data Scientist (Customer Analytics),Tiger Analytics,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/lead-data-scientist-customer-analytics-at-tiger-analytics-3775698470,2023-12-17,Langley, Canada,Mid senior,Remote,"Tiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce.
We are also market leaders in AI and analytics consulting in the CPG & retail industry with over 40% of our revenues coming from the sector. This is our fastest-growing sector, and we are beefing up our talent in the space.
We are looking for a Lead Data Scientist with a good blend of data analytics background, who holds solid knowledge of Personalization, and Web/Mobile Analytics, and Customer Analytics, quick learner, and has strong coding capabilities to add to our team.
Key Responsibilities
Work on the latest applications of data science to solve business problems in the Customer Analytics space of Retail, in particular omnichannel retailing
Effectively communicate the analytics approach and how it will meet and address objectives to business partners
Lead data analytic and modeling approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts, and developers
Create repeatable, interpretable, dynamic, and scalable models seamlessly incorporated into analytic data products
Collaborate, coach, and learn with a growing team of experienced Data Scientists
Stay connected with external sources of ideas through conferences and community engagements
Support demands from regulators, investor relations, etc., to develop innovative solutions to meet objectives utilizing cutting-edge techniques and tools
Requirements
>7 years of Data Science experience required
Graduate Degree in Data Science, Computer Science, or a related field is required
Deep Knowledge and Understanding of Personalization, Web/Mobile Analytics, and Customer Analytics
Strong python coding with production experience is preferred, MLops knowledge and experience is plus
At least 4 years of experience in CPG and Retail space
Ability to apply various analytical models to business use cases
Exceptional communication and collaboration skills to understand business partner needs and deliver solutions
Bias for action, with the ability to deliver outstanding results through task prioritization and time management
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Python, MLops, Data science, Personalization, Web Analytics, Mobile Analytics, Customer Analytics, Data Engineering, Statistical Modeling, Data Products, Business Intelligence, Communication, Collaboration, Analytical Models, CPG, Retail","python, mlops, data science, personalization, web analytics, mobile analytics, customer analytics, data engineering, statistical modeling, data products, business intelligence, communication, collaboration, analytical models, cpg, retail","analytical models, business intelligence, collaboration, communication, cpg, customer analytics, data engineering, data products, data science, mlops, mobile analytics, personalization, python, retail, statistical modeling, web analytics"
Staff Data Analyst (Product Analytics),Plenty of Fish,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/staff-data-analyst-product-analytics-at-plenty-of-fish-3779921665,2023-12-17,Langley, Canada,Mid senior,Hybrid,"Founded in Vancouver, Canada in 2003, Plenty of Fish is one of the early pioneers in the online dating industry, with one of the largest and most diverse communities of singles. We're one of the top revenue-driving brands in the Match Group (comprised of Tinder, OkCupid, Hinge and Match.com).
We love what we do, and have the ability to profoundly impact millions of people's lives every single day! Named one of BC’s Top Employers, Plenty of Fish is a great place to build friendships, grow your career and collaborate with top talent.
We work hybrid at Plenty of Fish - IRL in our downtown Vancouver office Monday, Tuesday, Wednesday and work remotely the remainder of the week!
We are looking for someone to be a part of our outstanding Product Analytics team, who will be working cross-functionally with the rest of the organization to help identify, understand, and improve business and product challenges facing Plenty of Fish. The Senior Product Analyst will deliver high-quality strategic and tactical analysis; collaborating with the product, engineering, data science, business intelligence, customer service, and marketing teams.
So, what will you do?
Provide a variety of stakeholders from Executives, Product Managers, Engineering Managers, or other Data Analysts, with deep-dive quantitative analyses and present them in an immediately usable format, extracting key insights from extensive, complex data sets
Proactively find opportunities to take analytics framework into the next level and lead stakeholders to adopt new frameworks and methodologies
Quantitatively analyze user behaviour to help determine product strategy and drive business decisions, including driving decisions from A/B test learnings
Take ownership of ambiguous and complex analytical assignments
You'll be a match for this role if you have..
5+ years of progressive experience using quantitative analysis to make business-focused recommendations
Strong communication skills at different levels of organizational hierarchy
Strong understanding of incrementally as well as sizing business problems and opportunities
Proven technical data analytics skills in working with large amounts of data at different levels of aggregation, from raw telemetry to metric stores.
Experience with raw clickstream data (telemetry event data) and datasets of 100 million+ records
Experience with web/digital analytics metrics (Daily Active Users, Retention, Churn)
Understanding of basic statistical concepts such as confidence interval, normal distribution, correlation, linear regression, and multivariate testing
Proficiency with SQL or other query languages
Strong experience with A/B test analysis
In depth experience with data visualization and analytical tools (E.g. Tableau, PowerBI, Looker)
Proactive skill set that encompasses problem identification, analysis, solution definition, results, and communication
Experience with Python or a willingness to learn
$135,000 - $145,000 a year
Factors such as scope and responsibilities of the position, candidate's work experience, education/training, job-related skills, internal peer equity, as well as market and business considerations may influence base pay offered. This salary range is reflective of a position based in Vancouver, British Columbia.
We would love to hear from you, even if you don't match 100% of the requirements**
Why Plenty of Fish?
We're recognized as one of BC's Top Employers 2023 !
Generous vacation, flex days, professional development days
RRSP matching, and employee stock purchase plan
Professional development budget and unlimited access to Udemy from day one
Match Group mentorship program
Parental leave top up and fertility preservation benefits
Extended health & dental benefits from day one
Corporate ClassPass membership and other wellness benefits
And many more on our careers page
Our Values
Be Proud - We own our ship. We see challenges as opportunities and take action.
Make Waves - We profoundly impact millions of peoples' lives, every day.
Dive Deep - We empathize with our members and use data to surface thoughtful decisions.
Crew Together - We're all working towards the same goals and win as a team.
Want To Dive Deeper?
LinkedIn
Glassdoor
We’re committed to creating an equal and inclusive environment; we welcome all crew (and prospective crew) members regardless of race, colour, ancestry, place of origin, political belief, religion, marital status, family status, physical or mental disability, sex, sexual orientation, gender identity or expression, age, conviction unrelated to employment, or any other prohibited ground of discrimination recognized by applicable law. Plenty of Fish is proud to be an equal opportunity workplace.
Show more
Show less","SQL, Tableau, PowerBI, Looker, Python, A/B testing, User behavior analytics, Product strategy, Business intelligence, Statistics, Data visualization, Data analysis, Machine learning, Data science, Raw telemetry data, Web analytics, Digital analytics, Multivariate testing","sql, tableau, powerbi, looker, python, ab testing, user behavior analytics, product strategy, business intelligence, statistics, data visualization, data analysis, machine learning, data science, raw telemetry data, web analytics, digital analytics, multivariate testing","ab testing, business intelligence, data science, dataanalytics, digital analytics, looker, machine learning, multivariate testing, powerbi, product strategy, python, raw telemetry data, sql, statistics, tableau, user behavior analytics, visualization, web analytics"
"Business Analyst, Data & Analytics - BC Cancer - Vancouver",BC Cancer,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/business-analyst-data-analytics-bc-cancer-vancouver-at-bc-cancer-3787517399,2023-12-17,Langley, Canada,Mid senior,Hybrid,"Business Analyst, Data & Analytics
BC Cancer
Vancouver, BC
We are seeking an experienced Business Analyst to join our team and be a key player in supporting analytics and reporting needs related to British Columbia’s 10-year cancer action plan; transforming complex data into actionable insights and reports. In this role, you will help define metrics, develop data collection methodologies, create visually compelling reports/dashboards, and deliver data-driven presentations drawing from your prior experience and proficiency in key developing performance indicators (KPI) , data analysis, visualization, and reporting using tools such as Power BI.
What You’ll Do
Develop and produces program evaluation performance metrics and tools to provide accurate and high quality analysis of key indicators, including outcomes, quality and safety, data standards, in ensuring continuous quality improvement for BC Cancer.
Ensure internal, external and public reporting needs are met by gathering and negotiating reporting requirements from customers and stakeholders, communicating requirements to technical resources and following up with stakeholders regularly to develop and implement an examination process for the final reports.
Analyze operational problems and recommend innovative solutions by critically evaluating information gathered from multiple sources, reconciling conflicts, decomposing high-level information into details, abstracting up from low-level information to a general understanding, distinguishing user requests from the underlying true needs, and driving and challenging business assumptions.
Work in collaboration with program leaders and provincial partners to monitor program plans, measure progress towards goals and objectives, and identify the key factors that account for any deviation from stated goals and objectives.
Identify issues with complex data and takes initiative in working out optimal solutions with appropriate stakeholders to improve accuracy, timelines, coordination and other data issues.
Develop reports, dashboards, presentations and briefing documents for multiple internal and external stakeholders.
Provide health planning, program development, and project management support for new initiatives and business cases. Provide leadership in the development of processes to analyze the functionality and effectiveness of program delivery and recommend improvements.
Keep up-to-date about new initiatives, developments, trends and best practice in evaluation and monitoring through literature reviews and contact with key stakeholders from other organizations. Communicate information and make recommendations to program leaders.
Qualifications
What you bring
A level of education, training and experience equivalent to a Master’s degree in Business, Analytics, Health Informatics, Health Sciences, or related discipline plus a minimum of five (5) years of recent experience in complex data analysis, health evaluation, project/program management, preferably in a large, multi-site health care environment.
You will also have
Demonstrated ability to perform accurate advanced data manipulation, analysis, and information presentation using Microsoft Office suite, database and statistical software.
Knowledge and experience in project management, strategic planning and project implementation.
Strong verbal and written communication skills.
Demonstrated ability in preparing professional reports and presentation materials.
Ability to work under pressure and multi-task without supervision.
Ability to communicate complex and technical ideas in simple language.
Demonstrated ability to exercise tact, good judgment and initiative.
Commitment to upholding the shared responsibility of creating lasting and meaningful reconciliation in Canada as per TRC (2015) and BC's Declaration on the Rights of Indigenous Peoples Act (2019).
Knowledge of social, economic, political and historical realities impacting indigenous communities and familiarity with Indigenous Cultural Safety and anti-racism and accompanying reports (BC DRIPA, TRC, etc.).
What We Bring
Every PHSA employee enables the best possible patient care for our patients and their families. Whether you are providing direct care, conducting research, or making it possible for others to do their work, you impact the lives of British Columbians today and in the future. That’s why we’re focused on your care too – offering health, wellness, development programs to support you – at work and at home.
Join one of BC’s largest employers with province-wide programs, services and operations – offering vast opportunities for growth, development, and recognition programs that honour the commitment and contribution of all employees.
Access to professional development opportunities through our in-house training programs, including +2,000 courses, such as our San’yas Indigenous Cultural Safety Training course, or Core Linx for Leadership roles.
Enjoy a comprehensive benefits package, including municipal pension plan, and psychological health & safety programs and holistic wellness resources.
Annual statutory holidays (13) with generous vacation entitlement and accruement.
PHSA is a remote work friendly employer, welcoming flexible work options to support our people (eligibility may vary, depending on position).
Access to WorkPerks, a premium discount program offering a wide range of local and national discounts on electronics, entertainment, dining, travel, wellness, apparel, and more.
Job Type:
Regular, Full-Time
Salary range:
$72,445 - $104,139 /year.
The starting salary for this position would be determined with consideration of the successful candidate’s relevant education and experience, and would be in alignment with the provincial compensation reference plan. Salary will be prorated accordingly for part time roles.
Location:
601 West Broadway, Vancouver BC
,
V5Z 4C2
Applications will be accepted until position is filled.
Hours of Work:
Monday – Friday, 0830 – 1630
Requisition #
157364E
As per the current Public Health Order, full vaccination against COVID-19 is a condition of employment with PHSA as of October 26, 2021.
What We Do
BC Cancer provides comprehensive cancer control for the people of British Columbia.
BCC is part of the Provincial Health Services Authority (PHSA).
The Provincial Health Services Authority (PHSA) plans, manages and evaluates specialized health services with the BC health authorities to provide equitable and cost-effective health care for people throughout the province. Our values reflect our commitment to excellence and include: Respect people – Be compassionate – Dare to innovate – Cultivate partnerships – Serve with purpose. Learn more about PHSA and our programs: jobs.phsa.ca/programs-and-services
PHSA and BCC are committed to employment equity, encouraging all qualified individuals to apply. We recognize that our ability to provide the best care for our diverse patient populations relies on a rich diversity of skills, knowledge, background and experience, and value a safe, inclusive and welcoming environment.
Reconciliation is an ongoing process and a shared responsibility for all of us. The BC Governments’ unanimous passage of the Declaration on the Rights of Indigenous Peoples Act was a significant step forward in this journey—one that all health authorities are expected to support as we work in cooperation with Indigenous Peoples to establish a clear and sustainable path to lasting reconciliation. True reconciliation will take time and ongoing commitment to work with Indigenous Peoples as they move toward self-determination. Guiding these efforts Crown agencies must remain focused on creating opportunities that implement the Truth and Reconciliation Commission Mandate.
ATTN: PHSA Employees:
To be considered as a PHSA employee (internal applicant) for this position,
you must apply online via your internal profile at
http://internaljobs.phsa.ca
Please note the internal job posting will no longer be accessible after the expiry date of
December 22, 2023
. If the internal job posting has expired, please contact the Internal Jobs Help Desk and advise that you would like to be considered as a late internal applicant for this position.
Please do not apply for the external job posting.
If you have not registered your internal profile, a password is required to log in for the first time. To obtain your password, please contact the
Internal Jobs Help Desk at 604-875-7264 or 1-855-875-7264
. Please note regular business hours are Monday – Friday (excluding stats), 8:30am to 4:30pm. For inquiries outside of regular business hours, please email the Internal Jobs Help Desk at internaljobshelpu@phsa.ca and a Help Desk Representative will contact you the next business day.
Show more
Show less","Microsoft Office Suite, Database software, Statistical Software, Power BI, Data analysis, KPI (key performance indicators), Data visualization, Reporting, Program evaluation, Project management, Strategic planning, Communication, Presentation skills, Public speaking, Tact, Initiative, Reconciliation, Indigenous Cultural Safety, Antiracism, Social economic political and historical realities impacting indigenous communities","microsoft office suite, database software, statistical software, power bi, data analysis, kpi key performance indicators, data visualization, reporting, program evaluation, project management, strategic planning, communication, presentation skills, public speaking, tact, initiative, reconciliation, indigenous cultural safety, antiracism, social economic political and historical realities impacting indigenous communities","antiracism, communication, dataanalytics, database software, indigenous cultural safety, initiative, kpi key performance indicators, microsoft office suite, powerbi, presentation skills, program evaluation, project management, public speaking, reconciliation, reporting, social economic political and historical realities impacting indigenous communities, statistical software, strategic planning, tact, visualization"
Data Scientist,Pragmatic,"Sedgefield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-pragmatic-3780100312,2023-12-17,Teesside, United Kingdom,Associate,Hybrid,"Overview
Advising on or leading analytical projects (including data infrastructure) to enable better capture, understanding, analysis, modelling and reporting/visualization of data – providing actionable insights to stakeholders and decision makers.
Contributing across multi-functional teams, you will promote and drive a data-centric approach to support and enable analysis across operations and production, as well as other key aspects within the business.
Key tasks
Collaborate with engineers and scientists to support or lead key analysis projects into the complex nature of our manufacturing process
Proactively lead projects which capture new data in structured systems: ETL, data wrangling, cleaning and processing – mindful of the impact of clean and structured data systems on analysis
To contribute to a clean and robust code base, using version control, and enabling all members to work collaboratively
Prepare reports/visualisations/dashboards to capture learnings or effectively share/communicate outputs for wider consumption, to support decision making
Work with various technical/cross functional teams to develop data pipelines and dashboard/reports for stakeholder consumption, enabling improved visibility of their KPIs
To proactively suggest case-studies for analyses and modelling; using your interactions with stakeholders (engineers, scientists) look for and pursue opportunities to find correlations, patterns, signals, signatures; and work towards developing models for a production environments
Work with data and ML engineers to enable productionalisation of any relevant predictive models, to support and optimise process development and drive operations
Assist in training team members to enhance their data literacy and promote a data-driven culture within the company
Qualifications and training
Bachelors in STEM discipline and experience in a relevant role within manufacturing
OR Higher degree (masters/PhD) in STEM discipline + minimum 3 years working experience as part of data team
Desirable - Cloud certification (AWS or ideally MS Azure)
Skills and experience
Excellent communication skills (written and spoken) to enable dialogue with stakeholders, requirements gathering and sharing outputs
Highly analytical mindset and personal drive to understand and work to solve problems
Strong Python (or R) & SQL programming knowledge
Good working knowledge of a wide range of statistical techniques and the various modelling approaches esp. dimensionality reduction, supervised/unsupervised, ensemble methods and optimisation strategies
Significant exposure to the standard python DS and visualisation libraries (pandas, numpy, Sklearn, matplotlib/seaborn + Plotly) – or similar in R
Ability to convert exploratory code into robust, functioning, documented modules/libraries for sharing across the team
Knowledge and working experience with Tableau for dashboard design and reporting
Show more
Show less","Data Analysis, Data Visualization, Data Modeling, Data Wrangling, Data Cleaning, Data Processing, ETL, Python, R, SQL, Statistics, Machine Learning, AWS, Azure, Tableau, Dimensionality Reduction, Supervised Learning, Unsupervised Learning, Ensemble Methods, Optimization Strategies, Pandas, NumPy, ScikitLearn, Matplotlib, Seaborn, Plotly","data analysis, data visualization, data modeling, data wrangling, data cleaning, data processing, etl, python, r, sql, statistics, machine learning, aws, azure, tableau, dimensionality reduction, supervised learning, unsupervised learning, ensemble methods, optimization strategies, pandas, numpy, scikitlearn, matplotlib, seaborn, plotly","aws, azure, data cleaning, data processing, data wrangling, dataanalytics, datamodeling, dimensionality reduction, ensemble methods, etl, machine learning, matplotlib, numpy, optimization strategies, pandas, plotly, python, r, scikitlearn, seaborn, sql, statistics, supervised learning, tableau, unsupervised learning, visualization"
Senior Data Engineer,TUI,"Thornaby-on-Tees, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-tui-3778633795,2023-12-17,Teesside, United Kingdom,Mid senior,Hybrid,"It's an exciting time for First Choice as the business looks ahead to a bright future with a bold new strategy. We are on the search for motivated, talented people with a curious outlook, energetic mind-set and can do attitude to join us and help bring the brand to life for our colleagues,customers and collaborators.
We are looking for a talented and dedicated technical enthusiast to join the FirstChoice technology team which Build, Run and Maintain multiple holiday e-commerce websites.
The Senior Data Engineer is a practitioner and an advocate of state-of-the-art data engineering practices. Working in an agile environment and keeping up with the ever-evolving technical landscape the Senior Data Engineer is a lifelong learner and likes to think outside the box.
Please note the closing date for applications is: Sunday 7th January
About The Job
As a Senior Data Engineer, you are a go-to person working as part of a cross-functional team that enables data engineering skills and capabilities across the FirstChoice platform and websites. Being an enthusiast in data engineering, with an advanced DevSecOps mindset, you’ll use your excellent collaboration skills to work with your team to deliver the best answers to our customers’ needs with full responsibility for its data flows, from design to operation. You care diligently about the quality of your work, including proper documentation and security aspects.
You will use your deep technical skills to work closely with your colleagues to ensure an optimal data delivery architecture which is consistent throughout ongoing highly complex projects. You will work on data collection, data pipelines, data delivery to other systems. With your advanced skillset for understanding and solving problems you are able to take full ownership of complex topics or multi-faceted initiatives and outcomes panning across your domain.
You are able to verbalise your thoughts and ideas and take the initiative to translate ideas into outcomes. Together with the domain’s teams as well as the Group Enabler teams you will research, evaluate and test new approaches, processes and tools and help teams to use them effectively. You take part in Communities of Practice, including collaboration in shared initiatives to grow your experience. You regularly coach, guide and develop more junior colleagues.
You love to work in an international, multi-cultural team. You challenge constructively and have high expectations of yourself and others. You always drive for technical excellence, ownership and self-organisation at team and personal level.  You love to learn and acquire new skills and keep up to date with latest developments in your focus areas.
Security is part of everyone’s job. At TUI, we practise secure behaviours first in everything we do.
About You
Experienced in design, implementation and use of different database structures
Outstanding SQL and data profiling skills and experience with building ETL/ELT pipelines (e.g. SSIS)
Strong hands-on experience with modern data storage systems (e.g. AWS RDS, Aurora, AWS S3, DynamoDB, ElastiCache, SOLR and NoSQL)
On the spot conversion of high-level business & technical requirements into technical specs
Customer centric, passionate about delivering great digital products and services
Passionate about continuous improvement, collaboration, and great teams
Compelling coaching and training skills for imparting professional software engineering skills
About Our Offer
Working in the leading global tourism group: We stand for intercultural cooperation and offer the opportunity to work in international projects and teams.
Fantastic holiday benefits including discounts, special offers
Mobile working, flexible working hours and working from abroad: We believe that work is something you do, not where you go. Our offer: TUI Way of Working
Health and Wellbeing support in five key areas – Health, Social, Community, Career and Financial
Development and career opportunities: We offer a wide range of digital training and international career opportunities.
Additional benefits relevant to the local market that you'll be based in
We love to see your uniqueness shine through and inspire the future of travel. If you would like to read more about what Diversity & Inclusion means to us simply visit Our DNA
If you have any questions, please contact the Recruiter for this role via the contact information included in the advert.
Show more
Show less","Data Engineering, Databases, SQL, Data Profiling, ETL/ELT Pipelines, SSIS, Data Storage Systems, AWS RDS, Aurora, AWS S3, DynamoDB, ElastiCache, SOLR, NoSQL, Business Requirements, Technical Specifications, Customer Centricity, Digital Products and Services, Continuous Improvement, Collaboration, Coaching, Training, Software Engineering","data engineering, databases, sql, data profiling, etlelt pipelines, ssis, data storage systems, aws rds, aurora, aws s3, dynamodb, elasticache, solr, nosql, business requirements, technical specifications, customer centricity, digital products and services, continuous improvement, collaboration, coaching, training, software engineering","aurora, aws rds, aws s3, business requirements, coaching, collaboration, continuous improvement, customer centricity, data engineering, data profiling, data storage systems, databases, digital products and services, dynamodb, elasticache, etlelt pipelines, nosql, software engineering, solr, sql, ssis, technical specifications, training"
Systems and Data Analyst,Tempting Recruitment,"Darlington, England, United Kingdom",https://uk.linkedin.com/jobs/view/systems-and-data-analyst-at-tempting-recruitment-3779694546,2023-12-17,Teesside, United Kingdom,Mid senior,Hybrid,"Systems and Data Analyst in Durham- Exciting Career Prospect
Are you a skilled and passionate professional eager to play a key role in a local authority as a Systems and Data Analyst?
Would you like to contribute to a dynamic local authority recognized for its empowering work culture and meaningful initiatives?
If so, we have an excellent opportunity just for you!
About Us
Tempting Recruitment is delighted to present a compelling opportunity with our respected client - a prominent council. We are currently seeking a dedicated Systems and Data Analyst to join their dynamic team.
The Position
The duties you will be performing as part of your role:
Provided high-quality data for Community Safety service by managing and developing ICT software systems, supporting service development, and conducting research.
Gathered, analyzed, and reported relevant community safety data.
Maintained the list of KPIs and PIs for the service.
Developed and managed ICT software systems within Community Safety.
Collaborated with public, private, and voluntary sectors, as well as colleagues, to gather and analyze data supporting efforts to ensure Darlington is a safe place.
Developed and produced reports on service performance as required.
Identified opportunities for data quality improvement without compromising effectiveness.
Presented analysis in an accessible manner to relevant colleagues and partners.
Conducted research, analyzed data, and provided commentary as needed.
Maintained an overview of relevant legislation related to Community Safety.
Produced reports on both planned and ad-hoc bases as required by senior managers.
About You
To be successful in this role, you should possess the following qualifications and skills:
Qualification in a relevant field, educated to degree level or equivalent.
Approximately 2 years experience in data analysis and reporting for diverse audiences.
Experience in developing, implementing, monitoring, and reviewing strategies and policies.
Proven ability to provide comprehensive and concise written reports.
Around 4 years of experience in collating data with a focus on maintaining data quality consistency.
Proficiency in using software packages to input, extract, interpret, and process data for accurate information and detailed reports.
Understanding of community safety principles.
Experience in administering performance management systems.
Familiarity with public service partnership arrangements.
Knowledge of data-sharing protocols and their implementation.
Awareness of data protection regulations.
Flexible approach to working hours to meet business requirements.
Strong communication skills, capable of providing advice in accurate spoken English.
Contract Duration:
3 - 6 Months (likely extension)
Hourly Rate:
£17.12 per Hour (Umbrella)
Commencement Date:
December 2023
Work Hours: Hybrid!
Monday - Friday, 9 am to 5 pm
How To Apply
If you are interested in applying please press the button below, send your CV to desean.pierre-dwyer@temptingrecruitment.co.uk or contact Desean on 02038544913 to discuss further.
To explore other exciting opportunities or learn more about our recruitment process, please visit our website https://www.temptingrecruitment.co.uk or contact Desean on 02038544913.
Show more
Show less","Data Analysis, Data Reporting, ICT Software Systems, Community Safety, KPI and PI Management, Data Quality Improvement, Legislation Maintenance, Report Writing, Data Interpretation and Processing, Performance Management Systems, Public Service Partnership Arrangements, DataSharing Protocols, Data Protection Regulations, English Communication","data analysis, data reporting, ict software systems, community safety, kpi and pi management, data quality improvement, legislation maintenance, report writing, data interpretation and processing, performance management systems, public service partnership arrangements, datasharing protocols, data protection regulations, english communication","community safety, data interpretation and processing, data protection regulations, data quality improvement, data reporting, dataanalytics, datasharing protocols, english communication, ict software systems, kpi and pi management, legislation maintenance, performance management systems, public service partnership arrangements, report writing"
Data Center Engineer,Cloudflare,"Denver, CO",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732380839,2023-12-17,Boulder,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, Jira, Juniper, Cisco, DWDM, Data Center, Networking, Automation, Process Improvement, Incident Management, Remote Contractors, Data Center Operations, Project Management, MS Excel, Google Spreadsheets, Documentation, SQL, Networking, Physical Security, Data Analysis, Server Technologies, RHCSA, CCNA, JNCIA","linux, jira, juniper, cisco, dwdm, data center, networking, automation, process improvement, incident management, remote contractors, data center operations, project management, ms excel, google spreadsheets, documentation, sql, networking, physical security, data analysis, server technologies, rhcsa, ccna, jncia","automation, ccna, cisco, data center, data center operations, dataanalytics, documentation, dwdm, google spreadsheets, incident management, jira, jncia, juniper, linux, ms excel, networking, physical security, process improvement, project management, remote contractors, rhcsa, server technologies, sql"
Big Data Developer,Epsilon,"Westminster, CO",https://www.linkedin.com/jobs/view/big-data-developer-at-epsilon-3782272099,2023-12-17,Boulder,United States,Associate,Onsite,"Job Description
The Data Engineer position will focus on designing, developing, and supporting our Hadoop data solutions in Spark and Python (PySpark) while working with other components of the Hadoop ecosystem such as HDFS, Hive, Hue, Impala, Zeppelin, Jupyter. A successful candidate will work closely with business and portfolio leads to understand requirements then design and build innovative data solutions.
Job Duties & Responsibilities.
Design and development centered around PySpark, Python and Hadoop Framework.
Working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Provide effective solutions to address the business problems – strategic and tactical.
Collaboration with team members, project managers, business analysts and QA teams in conceptualizing, estimating and developing new solutions and enhancements.
Work closely with the stake holders to define and refine the big data platform to achieve sales, product, and strategic objectives.
Collaborate with other technology teams and architects to define and develop cross-function technology stack interactions.
Read, extract, transform, stage and load (ETL) data to multiple targets, including Hadoop and Oracle.
Ingest and streamline incoming files of various layouts/formats as part of Source Prep process.
Develop scripts around Hadoop framework to automate processes and existing flows.
Modify existing programming/code for new requirements.
Estimate work, and track progress through SDLC with JIRA/Confluence
Unit testing and debugging. Perform root cause analysis (RCA) for any failed processes.
Convert business requirements into technical design specifications and execute on them.
Participate in code reviews and keep applications/code base in sync with version control (GIT/Bitbucket).
Effective communication, self-motivation, and ability to work independently while remaining fully aligned within a distributed team environment.
Required Skills
Bachelor’s or Master’s degree in Computer science (or Engineering equivalent).
3+ years of experience with big data ingestion, transformation and staging.
Analysis, design and implementation experience with Hadoop distributed frameworks, including Python & Spark (SparkSQL, PySpark), HDFS, Hive, Impala, Hue, Cloudera Hadoop, Zeppelin, Jupyter, etc.
Extensive experience handling large volumes of data (measured in Terabytes/Billions of Transactions)
Proficient knowledge of SQL with any RDBMS
Familiarity with RDD and Data Frames within Spark
Working knowledge of data analytics
Troubleshooting and complex problem-solving skills
Knowledge of Oracle databases and PL/SQL
Working knowledge of Linux/Unix environments and comfort with Unix Shell scripts (ksh, bash)
Basic Hadoop administration knowledge.
DevOps Knowledge is an advantage
Ability to work within deadlines and effectively prioritize and execute on tasks
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels
Preferred Skills
Working knowledge of Oracle databases and PL/SQL.
Hadoop Admin & Dev-Ops.
ETL Skills (Familiarity with Talend or other ETL tools a plus.
Good analytical thinking and problem-solving skills.
Ability to diagnose and troubleshoot problems quickly.
Motivated to learn new technologies, applications, and domains.
Possess appetite for learning through exploration and reverse engineering.
Strong time management skills.
Ability to take full ownership of tasks and projects.
Team player with excellent interpersonal skills.
Good verbal and written communication.
Possess Can-Do attitude to overcome any kind of challenges.
Preferred Certifications (Any Of These)
CCA Spark and Hadoop Developer.
MapR Certified Spark Developer (MCSD).
MapR Certified Hadoop Developer (MCHD).
HDP Certified Apache Spark Developer.
HDP Certified Developer.
Salary Range $95,000.00 - $100,000.00/year
Additional Information
About Epsilon
Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.
When you’re one of us, you get to run with the best.
For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC
Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
Epsilon is an Equal Opportunity Employer.
Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.
REF216743S
Show more
Show less","Hadoop, Spark, Python (PySpark), HDFS, Hive, Hue, Impala, Zeppelin, Jupyter, SQL, NoSQL, Data Analytics, Big Data, RDD, Data Frames, Hadoop, DevOps, Linux, Unix, Unix Shell scripts (ksh bash), PL/SQL, ETL, CCA Spark and Hadoop Developer, MapR Certified Spark Developer (MCSD), MapR Certified Hadoop Developer (MCHD), HDP Certified Apache Spark Developer, HDP Certified Developer","hadoop, spark, python pyspark, hdfs, hive, hue, impala, zeppelin, jupyter, sql, nosql, data analytics, big data, rdd, data frames, hadoop, devops, linux, unix, unix shell scripts ksh bash, plsql, etl, cca spark and hadoop developer, mapr certified spark developer mcsd, mapr certified hadoop developer mchd, hdp certified apache spark developer, hdp certified developer","big data, cca spark and hadoop developer, data frames, dataanalytics, devops, etl, hadoop, hdfs, hdp certified apache spark developer, hdp certified developer, hive, hue, impala, jupyter, linux, mapr certified hadoop developer mchd, mapr certified spark developer mcsd, nosql, plsql, python pyspark, rdd, spark, sql, unix, unix shell scripts ksh bash, zeppelin"
"Lead, Data Analyst",Strive Health,"Denver, CO",https://www.linkedin.com/jobs/view/lead-data-analyst-at-strive-health-3785774361,2023-12-17,Boulder,United States,Mid senior,Onsite,"What We Strive For
Strive Health is built for purpose — to transform a broken kidney care system. We are fundamentally changing the lives of kidney disease patients through early identification, engagement and comprehensive coordinated care. Strive’s model is driven by a high-touch care team that integrates with local providers and spans the entire care journey from chronic kidney disease through end-stage kidney disease, leveraging comparative and predictive data and analytics to identify at-risk patients. Strive Health’s interventions significantly reduce the rate of emergent dialysis crash, cut inpatient utilization and significantly improve patient outcomes and experience. Come join our journey as we create THE destination for top talent in the healthcare community and set a new standard for how kidney care should be done.
Why We’re Worth the Application
We Strive for excellence and were recognized as one of America’s Best Startup Employers for 2023 by Forbes, Circle of Excellence, American Society of Transplantation, 2023, Best Places to Work – Denver, Comparably.com, 2022, Best Places to Work – Denver, Built in Colorado, 2022
We derive innovation and ideas from through authentic diversity intentionally building a team that represents the populations we serve in partnership with our Employee Resource Groups:
Strive Forward - LGBTQ
Underrepresented Minorities
Women and Allies
We care and support our Strivers within and beyond work to feel fully charged and empowered through our generous wellbeing offerings including:
Flexible time off
Companywide wellbeing days
Volunteer time off
Leave packages including a sabbatical, parental leave and eight weeks paid for living donor
Professional development
A dedicated certified financial planner
Headspace, Carrot Fertility and Gympass for all Strivers
We like to have fun by celebrating our successes as a team through team building, company gatherings, trivia, wellbeing raffles, pajama days, a companywide book club and more.
We value tenacity to help us overcome obstacles with grit and determination to deliver compassionate kidney care.
Lead Data Analyst, Partner Analytics
The Lead Data Analyst, Partner Analytics will partner with the market Operator, Clinical, Implementation, Data Engineering, Provider Integration leaders in the organization to support new and ongoing initiatives. This person will be a leader of an experienced and passionate team and will play a key role in defining the organization's future. The lead analyst’s primary focus will be to provide in-depth analysis of ongoing and developing new initiative across multiple markets, supporting and advancing initiatives across analytics and operational performance in collaboration with both the new market & business development team, as well as the organization overall.
Essential Functions
Synthesize data stories through innovate analytical deep dives by factoring in data science, external, internal, operational, and clinical initiatives.
Research, hypothesize and develop analytical approaches for identifying, analyzing, and interpreting trends or patterns in complex data sets, particularly in healthcare claims & lab data
Conceptualize and design innovative models to assess viability of new and ongoing initiatives and clinical programs across multiple markets and partners
Collaborate with team members from strategy, clinical and new market functions to identify data driven opportunities to add value to existing and potential partners
Establish best practices for more junior analysts and assist in development of team members
Provide insight to design and build self-service dashboards to measure performance and assist in directing team’s efforts towards initiatives with the largest impact
Identify, communicate, and lead the resolution of data gaps that impede the fulfillment of operational reporting and oversight
Minimum Qualifications
Bachelor’s degree with strong academic achievement
4+ years’ experience in healthcare data analysis, preferably with experience in value-based care or population health analytics
Business travel
Preferred Qualifications
Working knowledge of analyzing medical claims, labs, EMR, and clinical data
Understanding of value-based care and risk contracts, including operational and financial levers for success
High degree of proficiency in Microsoft SQL, Excel & PowerPoint
Moderate degree of proficiency in a data visualization tool (preferably Quicksight)
Comfortable speaking and leading analytical discussions with partners
Annual Salary Range:
$93,400.00 - $116,700.00
Strive Health offers competitive compensation and benefits, including Health insurance, Dental insurance, Vision insurance, 401k Retirement Plan with Employer Match, Life and Accidental Death & Dismemberment insurance, Disability insurance, Health Savings Account, Flexible Spending Account, paid company holidays, in addition to Vacation Time Off. An annual performance bonus, determined by company and individual performance, is available for many roles as aligned to Strive Health guidelines.
Strive Health is an equal opportunity employer and drug free workplace. At this time Strive Health is unable to provide work visa sponsorship. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Please apply even if you feel you do not meet all qualifications. If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to talentacquisition@strivehealth.com
Show more
Show less","Data Science, SQL, Claims, EMR, Risk Contracts, Excel, PowerPoint, Quicksight, Tableau, Health Insurance, Dental Insurance, Vision Insurance, 401k Retirement Plan, Life Insurance, Disability Insurance, Health Savings Account, Flexible Spending Account, Python, R","data science, sql, claims, emr, risk contracts, excel, powerpoint, quicksight, tableau, health insurance, dental insurance, vision insurance, 401k retirement plan, life insurance, disability insurance, health savings account, flexible spending account, python, r","401k retirement plan, claims, data science, dental insurance, disability insurance, emr, excel, flexible spending account, health insurance, health savings account, life insurance, powerpoint, python, quicksight, r, risk contracts, sql, tableau, vision insurance"
Principal Data Engineer-Snowflake,InnovAge,"Denver, CO",https://www.linkedin.com/jobs/view/principal-data-engineer-snowflake-at-innovage-3708484076,2023-12-17,Boulder,United States,Mid senior,Onsite,"Responsibilities
Work-from-Home position supporting an 8am to 5pm MST schedule. Less than 10% travel to InnovAge centers and headquarters.
The
Principal Data Engineer
will be responsible for data design, management and development building new and vital data platforms and products for the Data and Intelligence team. Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models, and in implementing and operating machine learning models and services. Leverage internal client relationships to resolve complex business issues. Serve as an expert in their field of data engineering, providing technical guidance to less experienced team members.
Architect cutting edge solutions that are fit for purpose and mentors more junior staff to ensure overall quality.
Collaborate and make proposals, and help their team make informed decisions in alignment with the department’s strategic plans.
Provide technical guidance and oversee all the phases of projects, including research, development and design
Develop new product design and modify design of existing product as required. Review and decide upon improvements to existing system designs. Determine new designs to eliminate future issues.
Analyze product requirements and accordingly decide on technical and functional direction. Design and deliver solutions to improve system performance and reliability.
Be analytical and data driven to understand and apply knowledge from different domains to new technical areas. Lead the definition of coding standards and lead the Peer Code Review processes
Drive the continuous improvement of product development and deployment processes
Lead technical support of system acceptance testing and validation activities
Respond to customer queries and concerns in a timely fashion.
Collaborate with teammates and business partners regarding new ideas, issues and projects
Perform system failure analysis and drive corrective actions. Lead root cause analysis and determine long term solutions.
Build solid relationships with product and business data and technology leaders in the organization.
Partner with key stakeholders to develop and execute operational and strategic interventions.
Work with Enterprise Architecture team to develop cutting edge solutions
Support analytics and data science teams with exploration of new data sources and procuring them appropriately. Identify and deploy appropriate analytic techniques to solve complex business problems and overcome measurement challenges
Understand and translate key client business challenges tied to value delivery into potential analytic solutions to support those business challenges
Work with leadership, finance, and operations to ensure financial viability of new data products
Initiates opportunities and influences stakeholders with ideas that drive business value
Required
10+ yrs total related experience.
5 + Hands on Snowflake experience
5+ yrs Data Management experience proposing, documenting technology solutions and development experience with a focus on database architecture and data integration.
5+ yrs of expertise working with Data in a hands-on, developer role with advanced engineering principles and analytics development
Fluent in SQL (any flavor), having served in various capacities within design, development and deployment of data warehouses or relational/analytical/multi-dimensional data marts
Familiar with Virtual Data Warehousing or use of Snowflake DWaaS
Relational Database Management Systems and expert SQL skills. Extract/Load/Transform (ELT) development skills.
Experience with cloud migrations is desirable.
Experience building data pipelines on Azure following best practices in Cloud deployments
Demonstrated ability to debug complex data issues while working on very large data sets with billions of records
Proven ability to work with business and technical audiences, on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions
Ability to take the initiative to pursue assignments, responsibilities, and intake requests to a logical and final solution and to employ System Development Life Cycle (SDLC) standards is required.
Excellent verbal and written communication skills and strong analytical and problem-solving skills.
Bachelor’s degree in Computer Science or work-related experience in lieu of degree
Benefits
InnovAge’s Program of All-inclusive Care for the Elderly (PACE) is an alternative to nursing facilities. Seniors receive customized healthcare and social support at a nearby PACE center supported by a team of medical experts dedicated to providing personalized healthcare and support to help them age at home. Our greatest assets are our team members who make a difference in the lives of those we serve every day. Elevate your future with co-workers passionate about a patient-centered care model supported by comprehensive services to improve the quality of care while reducing over-utilization of high-cost care settings.
As an equal opportunity/affirmative action employer InnovAge is committed to and values an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender, gender identity/expression, national origin, disability or protected veteran status, pregnancy or any other status prohibited by applicable law.
Salary ranges are dependent on a variety of factors, including qualifications, experience, and geographic location. Range is not inclusive of potential bonus or benefits. Comprehensive benefits include m/d/v, short and long-term disability, life insurance and add, supplemental life insurance, flexible spending accounts, 401(k) savings, paid time off, and company paid holidays.
Posted Salary Range
USD $126,500.00 - USD $144,600.00 /Yr.
Show more
Show less","Data Engineering, Machine Learning, Data Warehousing, SQL, Snowflake, Data Integration, Data Migration, Cloud Migrations, Azure, Data Pipelines, ELT, System Development Life Cycle (SDLC), Computer Science","data engineering, machine learning, data warehousing, sql, snowflake, data integration, data migration, cloud migrations, azure, data pipelines, elt, system development life cycle sdlc, computer science","azure, cloud migrations, computer science, data engineering, data integration, data migration, datapipeline, datawarehouse, elt, machine learning, snowflake, sql, system development life cycle sdlc"
"Data Engineering Lead, Pipeline (487)",Techstars,"Boulder, CO",https://www.linkedin.com/jobs/view/data-engineering-lead-pipeline-487-at-techstars-3298550046,2023-12-17,Boulder,United States,Mid senior,Onsite,"This is a fully remote role and can be located anywhere in the continental US.
As a Data Engineering Lead, you will lead a team of engineers to build innovative solutions that empower entrepreneurs worldwide. Techstars already has one of the largest portfolios in early stage venture capital, with over 2,300 portfolio companies, a combined market cap of more than $193B, and 12 unicorns. In this role, you and your team will build backend streaming data pipelines, integrations with third party SaaS applications, complex analytics features and app facing platform data APIs. You will take an active role in architecture and solution design and help optimize solution performance and reliability. As a result you will help Techstars attract 10x more founders, and contribute directly to scaling the Techstars footprint to serve more entrepreneurs than ever before.
We believe in sustainable software development using Agile Development methodologies and mature DevOps practices to quickly and consistently provide value. We believe in creating robust, performant, maintainable, observable solutions. As an organization we value innovation and collaboration.
What You Will Do
Design, build and deploy quality data pipelines, models, integrations and APIs in an agile team environment.
Lead a team to deliver solutions that allow for 10x growth of Techstars pipeline of startups and our accelerator programs
Mentor team members on standards, best practices and implementation of technology.
Work with Engineering Leadership on design and implementation of technical systems.
Support Product Owner with OKRs, Sprint planning and stakeholder meetings as a technical resource.
Maintain uptime and SLA of deployed software systems.
Foster technical growth within your team by encouraging collaboration and innovation.
What You Bring
5+ years creating secure, reliable and performant enterprise level data cleansing, statistical modeling and analytics solutions (prefer fintech experience).
5+ years working on a data platform using open source technology (Kafka, K8s, Redis) and cloud infrastructure or operating as managed services.
5+ years experience with relational and non-relational database architecture (Postgres a plus)
3+ years as a senior engineer or lead engineer, leading and mentoring other engineers
Experience developing robust APIs
Diverse experience with languages (ie. SQL, Python, Node, Scala)
Knowledge of methodologies for testing quality, release management, incident response and issue resolution
Ability to create risk mitigation strategies for system upgrades and code releases
Ability to analyze appropriate technology stacks and major infrastructural components.
Strong DevOps skills
Track record of building reusable and cohesive architecture across applications
Experience creating technical designs that fulfill product requirements
A team mentality towards accomplishing projects
Ability to breakdown, prioritize and sequence development tasks for other engineers for high team utilization
Desire to mentor and grow engineers on the team
Coding standards utilization (Unit tests, formats, use of libraries, well structured, reusable, high quality)
Strong communication skills with an ability to work with the Product Owner and stakeholders to understand and manifest software outcomes
Compensation range:
$115,000 - $155,000 + 10% Bonus
US Benefits
About Techstars
Techstars is the worldwide network that helps entrepreneurs succeed. Founded in 2006, Techstars began with three simple ideas - entrepreneurs create a better future for everyone, collaboration drives innovation and great ideas can come from anywhere. Now we are on a mission to enable every person on the planet to contribute to, and benefit from, the success of entrepreneurs. In addition to operating accelerator programs and venture capital funds, we do this by connecting startups, investors, corporations and cities to help build thriving startup communities. Techstars has invested in more than 2,300 companies with a combined market cap of more than $29B.
Techstars’ mission is to help entrepreneurs succeed wherever they are in the world and whatever their background is. Regional accelerator programs all around the world are the cornerstone of the strategy. The investment approach is fundamentally driven by the worldwide network of managing directors, who interact with startup founders daily, guiding, mentoring and cultivating them along the journey. The scale of this reach results in a diversified strategy that provides investors with a uniquely qualified deal flow.
We help Techstars founders connect with other entrepreneurs, experts, mentors, alumni, investors, community leaders, and corporations to grow their companies.
www.techstars.com
Techstars is an affirmative action, equal opportunity employer and does not discriminate on the basis of race, sex, age, national origin, religion, physical or mental handicaps or disabilities, marital status, Veteran status, sexual orientation, gender identity nor any other basis prohibited by law.
Show more
Show less","Data Pipelines, Agile Development, DevOps, APIs, Data Cleansing, Statistical Modeling, Analytics, Kafka, Kubernetes, Redis, Cloud Infrastructure, Managed Services, Relational Databases, NonRelational Databases, Postgres, SQL, Python, Node.js, Scala, Software Testing, Quality Assurance, Release Management, Incident Response, Risk Mitigation, Infrastructure Components, Reusable Architecture, Technical Design, Unit Testing, Coding Standards, Communication Skills","data pipelines, agile development, devops, apis, data cleansing, statistical modeling, analytics, kafka, kubernetes, redis, cloud infrastructure, managed services, relational databases, nonrelational databases, postgres, sql, python, nodejs, scala, software testing, quality assurance, release management, incident response, risk mitigation, infrastructure components, reusable architecture, technical design, unit testing, coding standards, communication skills","agile development, analytics, apis, cloud infrastructure, coding standards, communication skills, datacleaning, datapipeline, devops, incident response, infrastructure components, kafka, kubernetes, managed services, nodejs, nonrelational databases, postgres, python, quality assurance, redis, relational databases, release management, reusable architecture, risk mitigation, scala, software testing, sql, statistical modeling, technical design, unit testing"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Boulder, CO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709525,2023-12-17,Boulder,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Docker, Helm, Spark, PySpark, Snowflake, Kubernetes, SQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Python, Java, Bash, Git","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, docker, helm, spark, pyspark, snowflake, kubernetes, sql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, python, java, bash, git","airflow, applied machine learning, bash, data cleaning, data engineering, data mining, data normalization, datamodeling, docker, dynamodb, etl, git, helm, java, kafka, kubeflow, kubernetes, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Data Analyst III- Adobe Campaign Specialist,AAA Life Insurance Company,"Livonia, MI",https://www.linkedin.com/jobs/view/data-analyst-iii-adobe-campaign-specialist-at-aaa-life-insurance-company-3778414658,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"By joining AAA Life, you will have the opportunity to strengthen the name and reputation of the brand that millions have come to rely upon for financial piece of mind. We are company dedicated to our members and our employees. We value the unique attributes and contributions of our associates to build an inclusive, collaborative and innovative workplace where all employees are engaged and feel they belong. Delivering our company’s promise to members is what drives each of our associates every day.
We offer a dynamic work environment, excellent benefits, and competitive compensation, that will allow you will exercise your potential to innovate, finding ways to increase efficiency and enhance our business processes.
Are you a Data Analyst with experience in Adobe Campaign? Our Data Analyst III ensures that AAA Life Insurance makes effective business and operational decisions. As a Data Analyst, you will be at the forefront of transforming data into actionable insights. This role involves extensive use of Adobe Campaign and will regularly extract, analyze, and visualize data to provide valuable recommendations that drive marketing strategies and decision-making.
Use Adobe Campaign to pull together cross-channel customer data create and customize campaigns.
Set up new Adobe campaign workflows for mail and email
Lead & develop automated, easy to understand reports and ad hoc analyses to address specific marketing questions and provide insights to guide decision making.
Utilize statistical techniques and data analysis tools (i.e., Python, R, SQL) to gather, clean, analyze, and provide recommendations regarding large datasets from various sources, identifying trends, patterns, and key performance metrics.
Collaborate closely with marketing teams to interpret data and provide actionable insights to optimize marketing campaigns, customer segmentation, and overall strategy.
Develop and implement data governance and quality assurance processes.
Create and maintain Power BI reports and dashboards to translate complex data into clear visualizations that marketing staff can easily interpret and use to inform their strategies.
Lead and develop key performance indicators (KPIs), tracking marketing initiatives against established goals, and providing regular updates to stakeholders.
Conduct A/B tests and statistical analyses to evaluate the effectiveness of marketing strategies, making data-driven recommendations for improvements.
Collaborate with marketing teams to segment audiences effectively and personalize marketing approaches based on data-driven insights.
Bachelor in Statistics, Marketing, Economics, Computer Science, or related technical field. Master’s degree is a plus.
A minimum of five years’ experience working as a Data Analyst, Marketing Analyst, or similar role.
Expert in Adobe Campaign experience strongly preferred
Able to set up new Adobe campaign workflows for mail and email strongly preferred
Understands architecture needed to support very large weekly campaigns strongly needed
Extensive experience in data analysis tools and programming languages (i.e., Python, R, SQL).
Ability to create and interpret reports and dashboards using Power BI, Tableau, or similar data visualization tools.
Proficiency with marketing analytics tools and platforms (i.e., Google Analytics, Adobe Analytics).
Show more
Show less","Data Analysis, Adobe Campaign, Python, R, SQL, Statistics, Power BI, Tableau, Data Governance, Data Quality Assurance, Data Visualization, Marketing Analytics, Google Analytics, Adobe Analytics","data analysis, adobe campaign, python, r, sql, statistics, power bi, tableau, data governance, data quality assurance, data visualization, marketing analytics, google analytics, adobe analytics","adobe analytics, adobe campaign, data governance, data quality assurance, dataanalytics, google analytics, marketing analytics, powerbi, python, r, sql, statistics, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744396373,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, TDD, Pair Programming, Continuous Integration, Automated Testing, Big Data, Data Engineering, Business Intelligence, Data Science, Relational Databases, Data Modeling, Schema Design","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, etl, data warehouses, tdd, pair programming, continuous integration, automated testing, big data, data engineering, business intelligence, data science, relational databases, data modeling, schema design","airflow, automated testing, big data, business intelligence, continuous integration, data engineering, data science, data warehouses, datamodeling, docker, etl, helm, kafka, kubernetes, pair programming, python, relational databases, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Staff Data Engineer,Recruiting from Scratch,"Livonia, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394418,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Apache Airflow, Kubernetes, Docker, Helm, Apache Spark, PySpark, SQL, Testdriven Development (TDD), Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, apache airflow, kubernetes, docker, helm, apache spark, pyspark, sql, testdriven development tdd, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","apache airflow, apache spark, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, testdriven development tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397154,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention, Legal Compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Livonia, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398085,2023-12-17,Pittsfield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707770,2023-12-17,Pittsfield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, ML, NLP, Conversational AI APIs, Recommender systems, Distributed systems, Microservices","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, ml, nlp, conversational ai apis, recommender systems, distributed systems, microservices","airflow, aws, azure, bash, conversational ai apis, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, microservices, ml, nlp, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088713,2023-12-17,Pittsfield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML/DL Pipelines, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Compliance, Legal Compliance, Data Classification, Data Retention, 401K, Equity","data engineering, mldl pipelines, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data compliance, legal compliance, data classification, data retention, 401k, equity","401k, airflow, aws, azure, bash, data classification, data compliance, data engineering, data retention, docker, dynamodb, equity, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, mldl pipelines, nlp, nosql, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Principal Consultant – Data and Analytics,Genesys,"Indiana, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781014611,2023-12-17,Plainfield,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","SQL, SQL analytics, Data analysis, Data visualization, Data engineering, Business intelligence, Data governance, Data management, Snowflake, Elastic (ELK stack), Tableau, Power BI, Software development, Project management, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, Contact Center, Customer Experience, XaaS","sql, sql analytics, data analysis, data visualization, data engineering, business intelligence, data governance, data management, snowflake, elastic elk stack, tableau, power bi, software development, project management, genesys engage, genesys cloud, nice, cisco, avaya, contact center, customer experience, xaas","avaya, business intelligence, cisco, contact center, customer experience, data engineering, data governance, data management, dataanalytics, elastic elk stack, genesys cloud, genesys engage, nice, powerbi, project management, snowflake, software development, sql, sql analytics, tableau, visualization, xaas"
Data Engineer III,VRK IT Vision Inc.,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-iii-at-vrk-it-vision-inc-3677424570,2023-12-17,Stroudsburg,United States,Mid senior,Onsite,"Job Title:- Data Engineer III
Location:- Houston TX (Hybrid 3 Days/week On-Site)
Job Type:- Long Term Contract
Need Local and Senior 12+ Years
Need Oil & Gas Background
Skills
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
Required Skills
Basic Qualification :
Additional Skills
Background Check :Yes
Show more
Show less","SQL, AWS, Big Data, Data Pipelines, Data Transformation, Data Extraction, Data Loading, Analytics, Data Architecture, Data Structures, Metadata, Dependency Management, Workload Management, Message Queuing, Stream Processing, Scalable Data Stores, Project Management, CrossFunctional Teams","sql, aws, big data, data pipelines, data transformation, data extraction, data loading, analytics, data architecture, data structures, metadata, dependency management, workload management, message queuing, stream processing, scalable data stores, project management, crossfunctional teams","analytics, aws, big data, crossfunctional teams, data architecture, data extraction, data loading, data structures, data transformation, datapipeline, dependency management, message queuing, metadata, project management, scalable data stores, sql, stream processing, workload management"
Data Analyst Part Time,Toyandsons,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-toyandsons-3757203932,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393773,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data engineering, Business intelligence, Data science, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Agile engineering practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Streamprocessing systems, Data Warehouses, ETL pipelines, Legal compliance, Data classification, Data retention","python, data engineering, business intelligence, data science, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, data warehouses, etl pipelines, legal compliance, data classification, data retention","agile engineering practices, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, docker, etl pipelines, helm, kafka, kubernetes, legal compliance, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827514,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Kafka, Snowflake, Spark, ETL, Distributed Databases, Data Warehouses, TDD, Pair Programming, Continuous Integration, Automated Testing, Data Engineering, Big Data Technologies, Machine Learning, StreamProcessing Systems, Data Modeling, Data Classification, Data Management Tools","python, sql, kafka, snowflake, spark, etl, distributed databases, data warehouses, tdd, pair programming, continuous integration, automated testing, data engineering, big data technologies, machine learning, streamprocessing systems, data modeling, data classification, data management tools","automated testing, big data technologies, continuous integration, data classification, data engineering, data management tools, data warehouses, datamodeling, distributed databases, etl, kafka, machine learning, pair programming, python, snowflake, spark, sql, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398357,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Retention, Legal Compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Researcher - Data Privacy,"Huawei Technologies Canada Co., Ltd.","Waterloo, Ontario, Canada",https://ca.linkedin.com/jobs/view/researcher-data-privacy-at-huawei-technologies-canada-co-ltd-3764777201,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Our team has an immediate 12-month contract opening for a Researcher.
Responsibilities:
Conduct cutting-edge data privacy research
Participate in identifying opportunities in data privacy protection and collaborate with product teams to propose and design features that can be integrated into real Huawei products
Support software engineers to design, build and test data privacy protection features
Draft patents, papers, and design documents
What you’ll bring to the team:
Master's or Ph.D. degree in Computer Science, Electrical and Computer Engineering, or related fields
2-5 years experience in related fields such as information security, privacy, compliance, data science, audit, or risk management
Extensive knowledge in data privacy-related fields (at least 2 areas): anonymization, private information retrieval, private set intersection, cryptography, differential privacy, searchable encryption, privacy attacks, or any other privacy-preserving technologies
Good knowledge of AI deep learning and experience with deep learning frameworks, e.g. Tensorflow, Pytorch
Solid Python and C++ programming skills
Proficiency working in Linux environments
Knowledge of Computer Vision and Natural Language Processing is an asset
Knowledge of Adversarial Machine Learning/GAN is an asset
Show more
Show less","Data privacy research, Information security, Privacy, Compliance, Data science, Audit, Risk management, Anonymization, Private information retrieval, Private set intersection, Cryptography, Differential privacy, Searchable encryption, Privacy attacks, Privacypreserving technologies, AI deep learning, Deep learning frameworks, Tensorflow, Pytorch, Python, C++, Linux environments, Computer Vision, Natural Language Processing, Adversarial Machine Learning, GAN","data privacy research, information security, privacy, compliance, data science, audit, risk management, anonymization, private information retrieval, private set intersection, cryptography, differential privacy, searchable encryption, privacy attacks, privacypreserving technologies, ai deep learning, deep learning frameworks, tensorflow, pytorch, python, c, linux environments, computer vision, natural language processing, adversarial machine learning, gan","adversarial machine learning, ai deep learning, anonymization, audit, c, compliance, computer vision, cryptography, data privacy research, data science, deep learning frameworks, differential privacy, gan, information security, linux environments, natural language processing, privacy, privacy attacks, privacypreserving technologies, private information retrieval, private set intersection, python, pytorch, risk management, searchable encryption, tensorflow"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751473180,2023-12-17,Cambridge, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Mining, Statistical Analysis, Data Modeling, A/B Testing, Data Integrity, Data Manipulation, Data Visualization, SQL, R, Python, Tableau, Power BI, Hypothesis Testing, ETL, Data Management","data analysis, data mining, statistical analysis, data modeling, ab testing, data integrity, data manipulation, data visualization, sql, r, python, tableau, power bi, hypothesis testing, etl, data management","ab testing, data integrity, data management, data manipulation, data mining, dataanalytics, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Senior Technical Data Analyst,System1,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-technical-data-analyst-at-system1-3584580561,2023-12-17,Cambridge, Canada,Mid senior,Remote,"We are seeking a highly skilled and experienced
Senior Technical Data Analyst
to join our growing Data Analytics team. Needs a strong background in data analysis and to be able to effectively communicate complex analytical findings to both technical and non-technical audiences, focusing on our highly profitable Search Engine Monetization business. This role requires a strong background in exploratory data analytics, statistics, data modeling as well as excellent communication and leadership skills.
The Role You Will Have
Analyze data from search engine marketing campaigns, including keyword performance, ad spend, and conversions
Collaborate with cross-functional teams to identify data-driven opportunities and develop strategies to leverage data assets
Develop and maintain analytical dashboards and reports to track key performance indicators and provide insights to stakeholders
Analyze data trends and patterns to identify opportunities for improvement and make recommendations to leadership
Utilize statistical and machine learning techniques to extract insights from data and build predictive models
Train and mentor junior data analysts and team members on data analysis techniques and best practices
Stay up-to-date with industry trends and best practices in search engine marketing and data analysis
What You Will Bring
Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
5+ years of experience in data analysis and visualization of large data sets
Strong experience with statistical and machine learning techniques
Advanced proficiency in SQL, and at least one programming language (e.g. Python, R)
Deep familiarity with all digital marketing metrics including CPA, CPC, CPM and ROAS
Excellent communication and leadership skills
Ability to work independently and as part of a team
Experience with big data technologies (e.g. Hadoop, Spark, BigQuery)
Experience with data visualization tools (e.g. Tableau, Power BI), as well as surfacing insights with interactive notebooks (e.g Jupyter, Google Colab)
Familiarity with pulling data from third party API’s and basic ETL processing
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
Medical, Dental & Vision
RRSP w/matching
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
#Bl-Remote
#BI-Hybrid
The base salary range in Canada for this full-time position is
$102,200 - 142,000
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all U.S. and Canada locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following provinces: Ontario and British Columbia.
Show more
Show less","Data Analysis, Exploratory Data Analytics, Statistical Analysis, Data Modeling, Predictive Analysis, Machine Learning, SQL, Python, R, Digital Marketing Metrics, Data Visualization, Data Visualization Tools, Business Intelligence, ETL Processing, Hadoop, Spark, BigQuery, Tableau, Power BI, Jupyter, Google Colab, Data Communication, Leadership","data analysis, exploratory data analytics, statistical analysis, data modeling, predictive analysis, machine learning, sql, python, r, digital marketing metrics, data visualization, data visualization tools, business intelligence, etl processing, hadoop, spark, bigquery, tableau, power bi, jupyter, google colab, data communication, leadership","bigquery, business intelligence, data communication, data visualization tools, dataanalytics, datamodeling, digital marketing metrics, etl processing, exploratory data analytics, google colab, hadoop, jupyter, leadership, machine learning, powerbi, predictive analysis, python, r, spark, sql, statistical analysis, tableau, visualization"
Lead Technical Data Analyst,System1,"Guelph, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-technical-data-analyst-at-system1-3575258564,2023-12-17,Cambridge, Canada,Mid senior,Remote,"We are seeking a highly skilled and experienced
Lead Technical Data Analyst
to join our growing Data Analytics team. Needs a strong background in data analysis and to be able to effectively communicate complex analytical findings to both technical and non-technical audiences, focusing on our highly profitable Search Engine Monetization business. This role requires a strong background in exploratory data analytics, statistics, data modeling as well as excellent communication and leadership skills.
The Role You Will Have
Serve as a subject matter expert to both the Analytics team and business leaders, providing sound & data-driven strategies that help meet or exceed business goals
Develop and create models in production (classification, clustering, learning models, multivariate regression, time series, k-means, etc.) to identify the best business solution
Use Python and packages like Pandas, NumPy, etc., perform complex statistical analysis on results, and utilize APIs for data ingestion
Use SQL to self-serve data from disparate sources & manipulate multiple large data sets simultaneously for analysis
Perform complex exploratory analysis on historical search engine marketing campaign data - including keyword performance, ad spend, and conversions - and identify opportunities for new expansion & data acquisition
Collaborate with teams in Taxonomy & Performance Marketing to proactively identify keyword trends & develop data-driven optimization strategies
Use tools like Tableau, Looker, or PowerBI to visualize results and present directly to business leaders
Provide mentorship & training to junior analysts and new hires, and evangelize ways to implement better data-driven practices throughout the organization
Stay up-to-date with industry trends and best practices in search engine marketing and data analysis
What You Bring To The Team
Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
7+ years of progressive experience in data analysis/data science and handling of large data sets.
Strong experience with mathematical optimization methods, exploratory data analysis, predictive modeling and machine learning techniques
Advanced proficiency in SQL, strong proficiency in Python & its data science packages (Pandas, NumPy, MatPlotLib, Seaborn, etc.)
Familiarity with digital marketing KPIs including CPA, CPC, CPM and ROAS
Excellent communication, and experience in providing leadership within your team (code-review or pair-programming, interviewing & onboarding new hires, etc.)
Experience with big data technologies (e.g. AWS, Snowflake, Hadoop, Spark, BigQuery)
Experience with data visualization tools (e.g. Tableau, Power BI), as well as surfacing insights with interactive notebooks (e.g Jupyter, Google Colab)
Familiarity with pulling data from third party API’s and basic ETL processing
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
Medical, Dental & Vision
RRSP w/matching
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
#BI-Hybrid
The base salary range in Canada for this full-time position is
$149,200 - $207,300
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all U.S. and Canada locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following provinces: Ontario and British Columbia.
Show more
Show less","Data Analysis, Data Science, Exploratory Data Analysis, Statistics, Data Modeling, Machine Learning, Python, Pandas, NumPy, MatPlotLib, Seaborn, SQL, Tableau, Power BI, Jupyter, Google Colab, AWS, Snowflake, Hadoop, Spark, BigQuery, API, ETL","data analysis, data science, exploratory data analysis, statistics, data modeling, machine learning, python, pandas, numpy, matplotlib, seaborn, sql, tableau, power bi, jupyter, google colab, aws, snowflake, hadoop, spark, bigquery, api, etl","api, aws, bigquery, data science, dataanalytics, datamodeling, etl, exploratory data analysis, google colab, hadoop, jupyter, machine learning, matplotlib, numpy, pandas, powerbi, python, seaborn, snowflake, spark, sql, statistics, tableau"
Data Analyst - CMO Center of Excellence,TeamHealth,"Knoxville, TN",https://www.linkedin.com/jobs/view/data-analyst-cmo-center-of-excellence-at-teamhealth-3774852066,2023-12-17,Andersonville,United States,Associate,Remote,"TeamHealth is named among the ""150 Top Places to Work in Healthcare"" by Becker's Hospital Review
and has ranked three years running as ""The World's Most Admired Companies"" by Fortune Magazine as well as one of America's 100 Most Trustworthy Companies by Forbes Magazine in past years. TeamHealth, an established healthcare organization is physician-led and patient-focused. We continue to grow across the U.S. from our Clinicians to our Corporate Employees and we want you to join us.
OVERVIEW:
TeamHealth is underway bringing all its healthcare data assets, products, and intelligence together to provide scalable value to its customers. In order to accomplish this, we are looking for a rock star data enthusiast to join our CMO Center of Excellence at TeamHealth.
The Data Analyst will be responsible for deliverables related to clinical and operational analytics, reporting, and distribution. Data analysis, modeling, and reporting will encompass the areas of healthcare delivery in TeamHealth service lines. Consumers of the analyses and information will include the CMIO, Senior Executive Leadership, and organizational Operational Leadership.
The Data Analyst will use multiple languages and tools to support data discovery, analytics, modeling, reporting and visualization across large data sets. They will be expected to be comfortable with statistical inference, dashboard development, and SQL.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Under limited supervision, be responsible for multiple analyses and deliverables, while monitoring, summarizing, communicating, and reporting status
Takes analysis and reporting tasks from original concept through final implementation. Develops all aspects of these tasks including a detailed plan, schedule, and distribution methodology.
Responsible for assembling necessary external stakeholders, including business owners, database stewards and application owners to ensure that appropriate resources and needs have been identified to achieve the analyses on the necessary timeline.
Able to work closely with data engineers, database owners, application owners and executive sponsors to ensure that development of new databases, reporting marts, and data infrastructure will support ongoing scaled analytics and reporting requirements
Responsible for creating, maintaining and deploying standard reporting using enterprise tools
Works closely with system engineers, database owners, application owners and executive sponsors to ensure that analytics and reporting requirements are met
Works closely with business and operations units to understand and meet reporting and information needs
Develops and contributes ideas and opinions about collecting relevant data, analyzing data, and developing presentations to present to management
Understands underlying business operations, data, data capture processes, and system integration to ensure that analyses and reports are correctly advising the organization, and accounts for data and/or workflow limitations
Produces analytics, dashboards and reports within the framework of the CMIO | CMO data/analytics processes and the CMO Center of Excellence (CoE); including exploratory data analysis, descriptive analysis and statistics, and using statistical inference and predictive modeling techniques as appropriate
Gains understanding of TeamHealth proprietary and enterprise systems. Aids in the enhancement of current systems and implementation of new systems for the company
QUALIFICATIONS / EXPERIENCE:
Bachelor's Degree in a quantitative field such as engineering, computer science, statistics, mathematics, economics, business analytics or a relevant field of study or equivalent professional experience
Master's degree in analytics or a quantitative science is preferred
Minimum of five (5) years of analytics experience, with knowledge and experience in statistical inference, forecasting/predictive analytics, descriptive analytics, and data visualization
Experience with PowerBI or Tableau for report creation, report distribution, and administration
Database management and database structure experience
Programming experience with SQL, and R or Python
Detailed experience in data acquisition/queries for multiple database and file types, data cleaning, aggregation, and data quality assessment
Excellent computer skills (Excel, Word, PowerPoint, and Visio)
Superior communication skills - verbal, written, presentation and negotiation skills
Strong organizational and analytical skills
Patience and ability to excel under pressure, handling multiple requests often with tight deadlines
Resourcefulness, ingenuity, strong decision making and problem solving skills
Leadership, teamwork and the ability to develop others
Solution driven and flexible in interpersonal style
Willing to gain expertise in TeamHealth data and the underlying business operations that produce such data
Healthcare experience preferred
https://www.teamhealth.com/california-applicant-privacy-notice/
Show more
Show less","SQL, R, Python, PowerBI, Tableau, Dashboard development, Statistical inference, Data visualization, Data acquisition, Data cleaning, Data aggregation, Data quality assessment, Database management, Database structure, Data analysis, Data modeling, Data reporting, Statistical analysis, Predictive modeling, Communication skills, Presentation skills, Negotiation skills, Organizational skills, Analytical skills, Resourcefulness, Ingenuity, Decision making, Problem solving, Leadership, Teamwork, Solution driven, Flexible interpersonal style","sql, r, python, powerbi, tableau, dashboard development, statistical inference, data visualization, data acquisition, data cleaning, data aggregation, data quality assessment, database management, database structure, data analysis, data modeling, data reporting, statistical analysis, predictive modeling, communication skills, presentation skills, negotiation skills, organizational skills, analytical skills, resourcefulness, ingenuity, decision making, problem solving, leadership, teamwork, solution driven, flexible interpersonal style","analytical skills, communication skills, dashboard development, data acquisition, data aggregation, data cleaning, data quality assessment, data reporting, dataanalytics, database management, database structure, datamodeling, decision making, flexible interpersonal style, ingenuity, leadership, negotiation skills, organizational skills, powerbi, predictive modeling, presentation skills, problem solving, python, r, resourcefulness, solution driven, sql, statistical analysis, statistical inference, tableau, teamwork, visualization"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Knoxville, TN",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783189279,2023-12-17,Andersonville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Knoxvill-DataScientist.011
Show more
Show less","Python, JavaScript, JSON, Generative AI, Machine Learning, Data Science, Product Development, Research, Algorithms, OOP languages, Technical Communication, Project Management, Problem Solving, Innovation, Collaboration, AI Training Models, EdTech, Data Analytics, HighPerformance Coaching","python, javascript, json, generative ai, machine learning, data science, product development, research, algorithms, oop languages, technical communication, project management, problem solving, innovation, collaboration, ai training models, edtech, data analytics, highperformance coaching","ai training models, algorithms, collaboration, data science, dataanalytics, edtech, generative ai, highperformance coaching, innovation, javascript, json, machine learning, oop languages, problem solving, product development, project management, python, research, technical communication"
"Engineer III, Big Data",Pilot Company,"Knoxville, TN",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-company-3784214007,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and digital innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research, development and modification of extract, transform, load processes and jobs in support of a Big Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","SQL, Data modeling, Relational Databases (Oracle Postgres SQL Server), Apache Spark, Sparkstreaming, Message Queue technologies, Python, AWS, Cloud Big Data platform, Agile methodologies, Team leadership, Mentoring, Communication","sql, data modeling, relational databases oracle postgres sql server, apache spark, sparkstreaming, message queue technologies, python, aws, cloud big data platform, agile methodologies, team leadership, mentoring, communication","agile methodologies, apache spark, aws, cloud big data platform, communication, datamodeling, mentoring, message queue technologies, python, relational databases oracle postgres sql server, sparkstreaming, sql, team leadership"
Senior Data Engineer,Cellular Sales,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cellular-sales-3768825864,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Cellular Sales
Sr Data Engineer - ETL
About Us
At Cellular Sales, we connect our customers with Verizon, the network more people rely on. We strive to bring people together through technology and work towards great accessibility today and every day. As a trusted partner of Verizon, we share their mission: to give humans the ability to do more in this world. We create the connections that turn innovative ideas into reality.
Do you have experience working with relational databases using ETL tools? Can you design and load data patterns? Do you thrive working in a collaborative environment helping to influence company strategy?
If so, we would love to talk with you!
Summary/Objective
The Sr Data Engineer will be responsible for designing and implementing enterprise level ETL processes in order to consume data from various sources. This position will be responsible for collaboration with the rest of the ETL team to facilitate the growth of the enterprise data warehouse as well as developing ETL solutions for various applications. This position also serves to mentor other team members on ETL best practices.
What We Need From You
Experience effectively using ETL tools (Informatica or other tools).
Develop, Design, and communicate practical enterprise level ETL solutions.
Ability to translate requirements and technical design to an automated ETL solution.
Understanding of data concepts to accommodate both operational and reporting design elements.
Ability to translate requirements and technical design to an automated ETL solution.
Mentor team members in SQL/Query performance tuning as well as ETL design.
Familiarity with Cloud technologies as they relate to Data Engineering
Full knowledge of storage and consumer database best practices.
Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
What We Provide
A collaborative working environment encouraging input from team members.
Strong commitment to the success of the employees and the business.
Support from the team, tools and systems.
Ability to build strong relationships throughout the organization.
Additional Benefits
Health and Dental Benefits
401K with matching
On site gym
Great Culture and an inviting atmosphere
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
Show more
Show less","Data Engineering, ETL, Informatica, Data Warehouse, SQL, Query Performance Tuning, Cloud Technologies, Storage, Consumer Database, Agile, Scrum","data engineering, etl, informatica, data warehouse, sql, query performance tuning, cloud technologies, storage, consumer database, agile, scrum","agile, cloud technologies, consumer database, data engineering, datawarehouse, etl, informatica, query performance tuning, scrum, sql, storage"
"Engineer III, Big Data",Pilot Flying J,"Knoxville, TN",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-flying-j-3783968663,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and digital innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research, development and modification of extract, transform, load processes and jobs in support of a Big Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","Extract Transform Load (ETL), Big Data Lake, Data Analytics, SQL, Oracle, Postgres, SQL Server, Relational modeling, Triggers, Stored procedures, Constraints, Apache Spark, Sparkstreaming, Message Queue technologies, Python, Enterprise data warehouse (EDW), Data modeling, Star Schema, Amazon Web Services (AWS), Cloud Big Data","extract transform load etl, big data lake, data analytics, sql, oracle, postgres, sql server, relational modeling, triggers, stored procedures, constraints, apache spark, sparkstreaming, message queue technologies, python, enterprise data warehouse edw, data modeling, star schema, amazon web services aws, cloud big data","amazon web services aws, apache spark, big data lake, cloud big data, constraints, dataanalytics, datamodeling, enterprise data warehouse edw, extract transform load etl, message queue technologies, oracle, postgres, python, relational modeling, sparkstreaming, sql, sql server, star schema, stored procedures, triggers"
Senior Data Analyst,South College,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-analyst-at-south-college-3742878729,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Description
South College invites capable, energetic, outgoing, applicants who are focused on transforming lives of our customers/students! We are one of the nation’s fastest growing institutions of higher learning with over 10,000 students covering 7 campuses and Online learning sites. We are also one of nation’s highest producers of licensed healthcare professionals offering a myriad of undergraduate and graduate healthcare programs for our students. Our Data Analytics team is growing with a focus on supporting business needs while improving student experience! Come join us in these exciting efforts!
As Senior Data Analyst, you will be responsible for delivering value to the company through actionable insights using descriptive statistics, data science, and general data analyses. We are looking for a talented and motivated analytics professional that can solve business problems through critical and abstract thinking. The person in this role must have excellent communication skills and the ability to take complex problems in data and present simplified solutions with little oversight. This position will work out of our Knoxville - Lonas Campus.
Assist the Analytics team members with any data analysis request.
Deliver data driven insights using descriptive or predictive/prescriptive analytics depending on the needs of the business.
Perform necessary statistical analysis from pulling the data and analyzing to delivering a simplified insight.
Develop strong business acumen, connecting business problems and opportunities to data to increase company profitability and student success/experience.
Develop high quality repeatable processes and deliverables to utilize discovered insights
Requirements
Master's Degree in Analytics preferred
2+ years of experience in Analytics, Data Science, or Data Engineering in the workplace
Deep understanding of statistical concepts and modeling
Self-starter with the experience to work independently on projects with little oversight from start to finish
Desire to learn new technologies and statistical concepts and how to apply them to fit the business needs.
Excellent written and oral communication/presentation skills
Works well in a fast-paced environment and can quickly switch priorities as business needs change
Excellent attention to detail and motivated to double check work
Experience in: Excel, Word, & PowerPoint
Experience with SQL or the desire to learn
Experience in R, Python, Tableau, Power BI, JMP, Data Science, Data Analysis, Marketing Analytics, or the desire to learn.
Show more
Show less","Data Analytics, Descriptive Statistics, Data Science, Predictive Analytics, Prescriptive Analytics, Statistical Analysis, Data Visualization, Tableau, Power BI, JMP, R, Python, Business Acumen, SQL, Excel, Word, PowerPoint, Data Engineering","data analytics, descriptive statistics, data science, predictive analytics, prescriptive analytics, statistical analysis, data visualization, tableau, power bi, jmp, r, python, business acumen, sql, excel, word, powerpoint, data engineering","business acumen, data engineering, data science, dataanalytics, descriptive statistics, excel, jmp, powerbi, powerpoint, predictive analytics, prescriptive analytics, python, r, sql, statistical analysis, tableau, visualization, word"
Senior Data Analyst,South College,"Knoxville, TN",https://www.linkedin.com/jobs/view/senior-data-analyst-at-south-college-3666144183,2023-12-17,Andersonville,United States,Mid senior,Onsite,"Description
South College invites capable, energetic, outgoing, applicants who are focused on transforming lives of our customers/students! We are one of the nation’s fastest growing institutions of higher learning with over 10,000 students covering 7 campuses and Online learning sites. We are also one of nation’s highest producers of licensed healthcare professionals offering a myriad of undergraduate and graduate healthcare programs for our students. Our Data Analytics team is growing with a focus on supporting business needs while improving student experience! Come join us in these exciting efforts!
As Senior Data Analyst, you will be responsible for delivering value to the company through actionable insights using descriptive statistics, data science, and general data analyses. We are looking for a talented and motivated analytics professional that can solve business problems through critical and abstract thinking. The person in this role must have excellent communication skills and the ability to take complex problems in data and present simplified solutions with little oversight.
Assist the Analytics team members with any data analysis request.
Deliver data driven insights using descriptive or predictive/prescriptive analytics depending on the needs of the business.
Perform necessary statistical analysis from pulling the data and analyzing to delivering a simplified insight.
Develop strong business acumen, connecting business problems and opportunities to data to increase company profitability and student success/experience.
Develop high quality repeatable processes and deliverables to utilize discovered insights
Requirements
Master's Degree in Analytics preferred
2+ years of experience in Analytics, Data Science, or Data Engineering in the workplace
Deep understanding of statistical concepts and modeling
Self-starter with the experience to work independently on projects with little oversight from start to finish
Desire to learn new technologies and statistical concepts and how to apply them to fit the business needs.
Excellent written and oral communication/presentation skills
Works well in a fast-paced environment and can quickly switch priorities as business needs change
Excellent attention to detail and motivated to double check work
Experience in: Excel, Word, & PowerPoint
Experience with SQL or the desire to learn
Experience in R, Python, Tableau, Power BI, JMP, Data Science, Data Analysis, or the desire to learn.
Show more
Show less","Analytics, Data Science, Data Engineering, Statistics, Statistical modeling, Data analysis, Predictive analytics, Prescriptive analytics, Business acumen, Datadriven insights, Complex problem solving, Communication, Presentation skills, Fastpaced environment, Attention to detail, Excel, Word, PowerPoint, SQL, R, Python, Tableau, Power BI, JMP","analytics, data science, data engineering, statistics, statistical modeling, data analysis, predictive analytics, prescriptive analytics, business acumen, datadriven insights, complex problem solving, communication, presentation skills, fastpaced environment, attention to detail, excel, word, powerpoint, sql, r, python, tableau, power bi, jmp","analytics, attention to detail, business acumen, communication, complex problem solving, data engineering, data science, dataanalytics, datadriven insights, excel, fastpaced environment, jmp, powerbi, powerpoint, predictive analytics, prescriptive analytics, presentation skills, python, r, sql, statistical modeling, statistics, tableau, word"
"Healthcare Data Analyst or Senior Healthcare Data Analyst, Analytics Hub",ECG Management Consultants,"St Louis, MO",https://www.linkedin.com/jobs/view/healthcare-data-analyst-or-senior-healthcare-data-analyst-analytics-hub-at-ecg-management-consultants-3785921708,2023-12-17,Belleville,United States,Associate,Hybrid,"Overview
ECG is a national management consulting firm working exclusively in the healthcare industry. At ECG, our primary emphasis is on quality—in our people as well as our services—and we’re seeking others who appreciate our high standards of excellence.
What’s in It for You: Consult with Purpose
At ECG, you can have a bigger impact than you ever imagined. The work you do will help health systems deliver care more effectively and efficiently—and that’s just the beginning. We’re looking for innovators, problem-solvers, and self-starters to collaborate across our five divisions, take on challenging projects, and find new ways to improve patient care. ECG is committed to ensuring a friendly work environment that rewards high performance and welcomes, values, and supports all people.
Join Our Analytics Hub
Our growing Analytics Hub works with our consulting teams to deliver new insights, streamlined analysis, and world-class intelligence to the healthcare organizations we serve as clients. Combined with our consulting teams, you will be helping physician and executive leadership make informed decisions that can alter the course of their enterprises in areas such as physician alignment, mergers and acquisitions, service line planning, provider compensation, financial performance, transforming care models, patient access, managed care portfolio optimization, digital health expansion, and more. ECG uses a Microsoft Azure and Microsoft 365 environment, with Microsoft Power BI as ECG’s current analytics visualization platform. The goal of the position is to further capitalize on the existing ECG infrastructure, as well as implement new features and technologies. The ideal candidate is open to new challenges, exceptional at multitasking, and proficient at implementing new solutions.
Our detailed plans incorporate qualitative findings with data-driven strategic, operational, and financial considerations that enable organizations to pursue realistic change. Our consultants bridge the gap between strategic thinking and operational implementation with sensible action plans and tactical recommendations. This approach allows us to stretch our clients’ thinking while ensuring that initiatives with the broadest and deepest impact are prioritized and implementable. Our wide-ranging engagements often focus on helping our clients:
Position themselves for value-based care delivery.
Align hospitals and physician organizations.
Strengthen financial performance.
Enhance clinical programs.
Develop strategic partnerships and/or mergers.
Your Opportunity with ECG: Data Analyst or Senior Data Analyst
As a data analyst or senior data analyst, you will support the firm’s client delivery and business development efforts, working with consultants and senior leaders to manage large data sets, aggregate internal and external data, develop scalable models and other analyses, and support data visualization and presentation sets. In addition, the analyst will work directly with our internal support teams (IT, business development, and L&D) and associated vendors for data management and IT solutions and will assist with a range of innovative analytic opportunities. Here, no two days or projects are alike, which means you’ll have a lot to learn and plenty of support to help you succeed.
Your Responsibilities May Include:
Gathering and synthesizing data from various sources (e.g., national surveys, proprietary research, client interviews, industries, markets)
Preparing analyses related to hospital inpatient databases and provider claims data (Stratasan)
Building flexible, dynamic, and scalable financial and operational models to forecast trends, preparing scenarios for consulting efforts related to delivery and business development (e.g., strategic planning, operational improvement, market assessments)
Enhancing existing models and capabilities built using SQL, primarily, with some use of Python or R in certain models or scenarios
Performing complex analyses on big data, including cleaning, preparing, and interpreting
Identifying and executing analyses in response to consultant business intelligence inquiries
Building relational databases using a Microsoft SQL Server or Fabric
Communicating market insights to pursuit teams, and supporting business development efforts
Configuring, maintaining, and building flexible reports, queries, and visuals within the Power BI environment and the production and development environments of related databases
Researching, testing, and implementing solutions in Power BI
Partnering with various consulting units (strategy, academic health, provider financial services, performance transformation, etc.) to gather requirements related to data analytics
Developing, configuring, and maintaining interfaces to exchange data among Power BI, SQL databases, and other cloud-based applications
Creating and maintaining analytical support documentation and operating procedures
Developing new and innovative methodologies and approaches
Contributing to internal education and learning opportunities for consultants and operations members for the Analytics Hub and analytics in general, and supporting education about ECG’s use cases related to analytics in work
Assisting the director of data analytics in developing analytics and data strategy to support data analytics capabilities at the firm
Our Expectations of You
A bachelor’s degree in either computer science, computer engineering, mathematics, statistics, health information management, health administration, business, or a related degree that relies heavily on critical thinking, logic, and math
Prior data analytics work experience using databases, SQL, and Power BI in a healthcare or enterprise environment (five+ years of experience for senior analyst level; one to three years for analyst)
Experience using Python, R, Fabric, and Tableau a plus
Proficiency in Microsoft Word, Excel, and PowerPoint
Comfort with manipulating and synthesizing large data sets
Strong written and verbal communication skills
Excellent organizational skills
Job Locations
St. Louis and Washington, DC, offices are preferred. Other offices may include Atlanta, Boston, Chicago, Minneapolis, San Diego, or Seattle. Able to work a hybrid schedule with days in the office and remote.
Schedule
Full time/exempt
What You Can Expect Of Us
To reward our driven, innovative, and passionate employees, we’ve built a company culture that’s centered on performance. We offer an attractive compensation package, challenging work, and an entrepreneurial environment where you can take ownership of your career—and get out as much as you put in.
About ECG
ECG is a strategic consulting firm leading healthcare forward using knowledge and expertise built over the course of five decades to help clients see clearly where the industry is going and navigate toward success. We work as trusted, professional partners with hospitals, health systems, medical groups, and academic medical centers across the country. We thrive on delivering smart counsel and pragmatic solutions to the critical challenges facing healthcare providers. Client success is our primary objective. ECG’s national presence includes offices in Atlanta, Boston, Chicago, Dallas, Minneapolis, San Diego, Seattle, St. Louis, and Washington, DC.
Apply now and make an impact for years to come.
To begin the recruitment process, please submit your resume via our career site at https://careers.ecgmc.com.
ECG provides equal employment opportunities to all employees and applicants for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, disability, pregnancy, medical condition (cancer and genetic characteristics), genetic information, gender, gender identity or expression, sexual orientation, marital status, military or veteran status, or any other legally protected characteristic. People of color are encouraged to apply. We participate in E-Verify as part of our onboarding process. Having the permanent legal right to work in the United States is a condition of employment. ECG is not currently able to provide assistance to candidates requiring sponsorship or a visa.
Residents of the states of California or Washington may receive salary information for this job through this link or by contacting the recruiter directly at schavez@ecgmc.com.
Show more
Show less","Power BI, Microsoft Azure, Microsoft 365, SQL, Python, R, Fabric, Tableau, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Data analytics, Statistical analysis, Financial modeling, Market analysis, Data visualization, Data interpretation, Data management, Data mining, Business intelligence, Data warehousing, Data mining","power bi, microsoft azure, microsoft 365, sql, python, r, fabric, tableau, microsoft word, microsoft excel, microsoft powerpoint, data analytics, statistical analysis, financial modeling, market analysis, data visualization, data interpretation, data management, data mining, business intelligence, data warehousing, data mining","business intelligence, data interpretation, data management, data mining, dataanalytics, datawarehouse, fabric, financial modeling, market analysis, microsoft 365, microsoft azure, microsoft excel, microsoft powerpoint, microsoft word, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Sr. Data Analyst,SteadyMD,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-analyst-at-steadymd-3697498895,2023-12-17,Belleville,United States,Mid senior,Onsite,"Be part of a team enabling access to better healthcare at SteadyMD!
SteadyMD is a technology company and healthcare provider that powers high-quality telehealth experiences for its partners, including fast-growing digital healthcare companies, labs, pharmacies, large employers, and Fortune 100 companies like Amazon, AmerisourceBergen, and Abbott. SteadyMD initially launched in two states: California and Missouri. By 2018, the company was licensed, operating, and providing care in all 50 states. We’ve raised over $60 million in funding from top tier investors including Lux Capital, Pelion Ventures and AB Health Ventures.
We are currently seeking a talented
Sr. Data Analyst
to join our Product team. As a Sr. Data Analyst you will play a pivotal role in transforming raw data into meaningful insights. You will collaborate with cross-functional teams to analyze data, identify trends, and provide actionable recommendations. Reporting to the VP of Product Management, this position offers an exciting opportunity to work with different datasets, contribute to data-driven initiatives, and impact the success of our company. This is a hybrid role in which you will be located in the St. Louis, MO area.
At SteadyMD, we value what diverse teams can accomplish together, and we honor each of our unique lived experiences. We look for a diverse pool of applicants, including those from historically marginalized groups, and we are committed to ensuring a safe work environment that is distinctly anti-discriminatory against any person. This is one of the reasons we are ranked #81 on Forbes’ America’s Best Startup Employers List. We know the value of building a team that encompasses a variety of backgrounds, experiences, and skills.
Works with other teams to provide key analytics support in identifying process, software, and data improvements
Define, maintain, and communicate the data “source of truth” for cross-functional teams to use in assessing performance and quality
Provide support and documentation for associated business units to understand how to use data and insights
Creates and owns ETLs: identifies data sources, writes queries, validates, and makes modifications as needed
Write clean code that can be maintained and extended by other technical stakeholders
Ensure safe and secure data handling by partnering with company security leaders
Sets up and manages usage, reliability, quality, and performance of products, services, solutions or processes and proposes improvements
Guides business leaders with data-driven reports, dashboards, and visualizations
Interprets data, analyzes results, and provides insights to support data-driven decision-making for ad hoc and on-going reports
Stay up-to-date with industry technologies and frameworks to identify trends to maintain best and cutting edge practices in data analysis
Participate in special projects and initiatives as needed
Requirements
4+ years of related experience in a business analytics or data management experience
Relevant Industry Experience or Bachelor’s Degree with emphasis in: Information Technology, Mathematics, Management Information System (MIS), Statistics, Engineering, Computer Science, or related
Strong proficiency in:
SQL, Python, writing SQL queries
Advanced SQL aggregation functions
database performance concepts and query optimization techniques
Experience with Looker and Tableau
Can provide helpful insights from dense datasets
Creative self-starter capable of first principles thinking
Strong interpersonal skills that can work across a highly cross-functional environment
Excellent oral and written communication skills required
Detail oriented and strong organizational skills
Previous experience in a B2B SaaS start-up preferred
Experience in healthcare is a bonus
Benefits
Competitive Compensation. The annual salary range for this role is $85,000 - $110,000 depending on experience, and participation in the company bonus program
Fast-paced Startup Environment. An environment that is focused on disrupting the status quo and challenging conventional professional norms. We are focused on the results you can achieve, not how many hours you spend at a desk
Complimentary Lemonaid Primary Care Membership. So that you can experience what we have to offer and be able to speak first-hand about what the future of medicine will look like
Company-paid health, dental, and vision insurance. Also includes Basic Life and ADD offerings
401k with Match & Parental Leave Benefits offered to all full-time employees
Unlimited PTO. We trust our employees to make the right decisions for the business, and we also recognize that means taking time to take care of yourself
Show more
Show less","SQL, Python, Looker, Tableau, ETL, Data analysis, Data management, Datadriven decisionmaking, Database performance concepts, Query optimization, Industry technologies, Frameworks, Healthcare, B2B SaaS","sql, python, looker, tableau, etl, data analysis, data management, datadriven decisionmaking, database performance concepts, query optimization, industry technologies, frameworks, healthcare, b2b saas","b2b saas, data management, dataanalytics, database performance concepts, datadriven decisionmaking, etl, frameworks, healthcare, industry technologies, looker, python, query optimization, sql, tableau"
Lead Data Engineer,Kforce Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-at-kforce-inc-3782232154,2023-12-17,Belleville,United States,Mid senior,Onsite,"Responsibilities
Kforce has a client that is seeking a Lead Data Engineer in Saint Louis, MO. Your role here: From Day 1, you will collaborate closely with the data analytics, Power BI reporting, and functional teams to deliver top notch analytics. Key Tasks:
Lead Data Engineer provides technical leadership in Microsoft Synapse, spanning numerous services (Pipelines, Notebooks, Azure Storage, Logic Apps)
Interacts with various business functions to define requirements and build a strategy to implement within our modern data framework
Interacts with engineering team to implement a high quality, efficient, dependable, and secure data platform while maintaining consistent data standards, patterns, and practices across the organization (Data warehouse, Data Lakehouse)
As a Lead Data Engineer, you will perform code review and verifies compliance with code standards
Owns Data Analytics team delivery outcomes
What You Can Expect From Us
Our salaries are competitive
Comprehensive benefits in medical, dental, and vision insurance
401(k) plan with employer match
Paid time off plus holidays
Tuition reimbursement, and much more
Requirements
To be considered for this position, candidates must have experience in a similar role, or they must possess significant knowledge, experience, and abilities to successfully perform the responsibilities listed
Relevant education and/or training will be considered a plus
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $128,000 - $140,000 per year
Show more
Show less","Data Engineering, Microsoft Synapse, Pipelines, Notebooks, Azure Storage, Logic Apps, Power BI, Data Analytics, Data Framework, Data Warehouse, Data Lakehouse, Code Review, Data Compliance, Medical Insurance, Dental Insurance, Vision Insurance, 401(k) Plan, Paid Time Off, Holidays, Tuition Reimbursement","data engineering, microsoft synapse, pipelines, notebooks, azure storage, logic apps, power bi, data analytics, data framework, data warehouse, data lakehouse, code review, data compliance, medical insurance, dental insurance, vision insurance, 401k plan, paid time off, holidays, tuition reimbursement","401k plan, azure storage, code review, data compliance, data engineering, data framework, data lakehouse, dataanalytics, datawarehouse, dental insurance, holidays, logic apps, medical insurance, microsoft synapse, notebooks, paid time off, pipelines, powerbi, tuition reimbursement, vision insurance"
Senior Software Engineer (Python/Data Pipelines),Diversity Resource Staffing Inc.,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-software-engineer-python-data-pipelines-at-diversity-resource-staffing-inc-3548643839,2023-12-17,Belleville,United States,Mid senior,Onsite,"As Senior Software Engineer specializing in Python, you will be responsible for designing, developing, maintaining, and operationalizing quantitative models and analytic environments supporting NISA’s core line of business. You will work closely with business counterparts and our quantitative research groups to understand their needs, formulate requirements, design creative and extensible solutions for data distribution, analysis, and modeling for consumption by internal business groups.
As a senior member of the IT Solutions team, you will also participate in design and code reviews, will collaborate with other team members, and mentor junior teammate.
Required Qualifications
Bachelor's degree or equivalent experience in a field requiring strong analytical and quantitative skills, such as Computer Science, Engineering, Mathematics, Finance, or Information Systems
Significant experience developing using Python (expertise in other high-level languages considered)
Extensive experience designing queries and data structures in SQL Server or another relational database platform
Experience in data science and data analysis
Preferred Qualifications
Experience building solutions using public cloud platforms (AWS, Azure, GCP)
Experience designing & implementing data pipelines to support quantitative research
Familiarity with MATLAB or R
Experience with unit testing frameworks
Working knowledge of modern application frameworks
Previous professional experience in financial services sector is a plus
Show more
Show less","Python, SQL, Data Science, Data Analysis, Cloud Platforms (AWS Azure GCP), MATLAB, R, Unit Testing Frameworks, Application Frameworks","python, sql, data science, data analysis, cloud platforms aws azure gcp, matlab, r, unit testing frameworks, application frameworks","application frameworks, cloud platforms aws azure gcp, data science, dataanalytics, matlab, python, r, sql, unit testing frameworks"
Data Automation Engineer,The Wise Seeker,"Arnold, MO",https://www.linkedin.com/jobs/view/data-automation-engineer-at-the-wise-seeker-3776917565,2023-12-17,Belleville,United States,Mid senior,Onsite,"As the Data Automation Engineer, you'll be exposed and contribute to the development and application of innovative technologies such as machine learning, artificial intelligence, and advanced data analytics. Our work depends on a Data Engineer joining our team to support our intelligence customer in Springfield, VA or St. Louis, MO.
What you will be working on:
Support the Geospatial Services and Solutions business area to provide high-quality, cost-effective solutions to the customer
Design and implement automation solutions to enhance data capture, data refinement, and processes. Coding examples include:
Interfacing with device APIs in order to collect operational metrics
Providing automated VoIP phone setup
Administering and automating data pipelines between different environments
Produce and deploy code via GitLab projects in collaboration with other team members
Utilize best practices for source control, testing, and deployment of software changes
Work in close collaboration with other automation engineers, infrastructure administrators, and data scientists
Diagnose, isolate, and expediently resolve complex problems pertaining to data structures
Develop methods of ensuring data incompatibilities among systems are systematically eliminated
Develop and recommend data management policies, standards, practices, and security measures to ensure effective and consistent data management operations
Participate in continuous improvement efforts to increase data availability, data quality, and speed of access
Maintain up-to-date documentation of designs/configurations, ensuring team members have continuity of recurring tasks
In office work requirement > 80%
Travel requirement 0%
What you will bring to us:
Bachelor's Degree in Computer Science or related technical discipline, or the equivalent combination of education, technical certifications or training, and work experience
8+ years of related systems engineering experience
Scripting, coding, or software development experience
Comfort with Linux/Windows command-line
Automation mindset
System administration and/or DevOps environment experience
Active TS/SCI clearance and eligibility to obtain a CI poly
Would be nice if you bring the following:
Python experience
Shell scripting experience such as Bash or PowerShell
Experience with Database technologies such as Postgres, SQL Server, Oracle, or MySQL
Experience writing and working with SQL commands
Version control experience with Git
Experience with Gitlab and Git workflows
Familiarity with Agile Scrum methodologies
Time management skills and the drive to work with limited supervision within a small team
Bonus Skills:
Web App development experience such as Flask, Django, React, etc.
UI/UX experience
Experience with Analytics tools such as Tableau
Infrastructure as Code experience
Experience in technical operations at DoD/IC agencies
Cloud experience such as AWS, Azure, GCP, etc.
Show more
Show less","Machine Learning, Artificial Intelligence, Data Analytics, Data Engineering, Geospatial Services, Automation Solutions, Data Capture, Data Refinement, Device APIs, VoIP, Data Pipelines, GitLab, Source Control, Software Deployment, Data Structures, Data Management, Data Security, Data Availability, Data Quality, Data Access, Technical Documentation, Computer Science, Systems Engineering, Scripting, Coding, Software Development, Linux, Windows, Automation, System Administration, DevOps, Clearance, Python, Shell Scripting, Bash, PowerShell, Database Technologies, Postgres, SQL Server, Oracle, MySQL, SQL Commands, Version Control, Git, Gitlab, Agile Scrum, Time Management, Flask, Django, React, UI/UX, Tableau, Infrastructure as Code, DoD/IC agencies, Cloud, AWS, Azure, GCP","machine learning, artificial intelligence, data analytics, data engineering, geospatial services, automation solutions, data capture, data refinement, device apis, voip, data pipelines, gitlab, source control, software deployment, data structures, data management, data security, data availability, data quality, data access, technical documentation, computer science, systems engineering, scripting, coding, software development, linux, windows, automation, system administration, devops, clearance, python, shell scripting, bash, powershell, database technologies, postgres, sql server, oracle, mysql, sql commands, version control, git, gitlab, agile scrum, time management, flask, django, react, uiux, tableau, infrastructure as code, dodic agencies, cloud, aws, azure, gcp","agile scrum, artificial intelligence, automation, automation solutions, aws, azure, bash, clearance, cloud, coding, computer science, data access, data availability, data capture, data engineering, data management, data quality, data refinement, data security, data structures, dataanalytics, database technologies, datapipeline, device apis, devops, django, dodic agencies, flask, gcp, geospatial services, git, gitlab, infrastructure as code, linux, machine learning, mysql, oracle, postgres, powershell, python, react, scripting, shell scripting, software deployment, software development, source control, sql commands, sql server, system administration, systems engineering, tableau, technical documentation, time management, uiux, version control, voip, windows"
Sr Data Analyst,Cushman & Wakefield,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-analyst-at-cushman-wakefield-3779636196,2023-12-17,Belleville,United States,Mid senior,Onsite,"Job Title
Sr Data Analyst
Job Description Summary
The role is for a Senior Data Analyst as part of a team collecting and analyzing data supporting ad hoc and strategic client projects primarily involving architectural building design.
The candidate will provide advanced expertise in data analysis, collaborate with key client partners, support the data team with identifying project requirements, refining project work, and providing recommendations on optimizing data management and workflows.
Job Description
Core Responsibilities
Collect, clean, study, transform, load, and visualize data for ad hoc and strategic projects
Identify trends and provide insights from data that contribute to solving business problems
Code programs, as needed, to help capture and organize relevant data
Collaborate directly with internal and external partners to satisfy project needs
Clearly communicate useful information to business partners derived from data analysis
Assist in managing completion of team data analysis tasks
Lead problem-solving and refinement of project activities and tasks
Lead project identifying requirements from analysis of current state versus desired future state
Identify opportunities for workflow optimization
Qualifications
Three or more years of experience in data analytics, data management, or related roles
Advanced knowledge of data analytics, cleaning, preparation, and visualization techniques
Advanced experience with data analytics tools and programs (Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau)
Advanced understanding of best practices in data management and visualization
Strong critical thinking and problem solving skills
Strong focus on solutions serving client/end user
Ability to write and speak clearly to both technical and non-technical audiences
Keen attention to both technical detail and quality of work acceptable to client/end user
Demonstrated ability to collaborate effectively with partners across multiple teams
Strong ability to prioritize work tasks in alignment with changing project and team needs
Preferred candidate will have experience managing/analyzing architectural design data
Cushman & Wakefield provides equal employment opportunity. Discrimination of any type will not be tolerated. Cushman & Wakefield is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other characteristic protected by state, federal, or local law.
In compliance with the Americans with Disabilities Act Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position at Cushman & Wakefield, please call the ADA line at
1-888-365-5406
or email
HRServices@cushwake.com
. Please refer to the job title and job location when you contact us.
Show more
Show less","Data Analytics, Data Management, Data Analysis, Data Visualization, Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau, ProblemSolving, Data Cleaning, Data Preparation, Data Loading, Critical Thinking, Attention to Detail, Communication, Collaboration, Prioritization, Workflow Optimization, Business Intelligence, Architecture Design Data","data analytics, data management, data analysis, data visualization, microsoft excel, microsoft power bi, python, sql, tableau, problemsolving, data cleaning, data preparation, data loading, critical thinking, attention to detail, communication, collaboration, prioritization, workflow optimization, business intelligence, architecture design data","architecture design data, attention to detail, business intelligence, collaboration, communication, critical thinking, data cleaning, data loading, data management, data preparation, dataanalytics, microsoft excel, microsoft power bi, prioritization, problemsolving, python, sql, tableau, visualization, workflow optimization"
Lead Data Engineer,TricorBraun,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-at-tricorbraun-3733281824,2023-12-17,Belleville,United States,Mid senior,Onsite,"Make our IT Team your best move ever
We’re looking for a highly detailed Lead Data Engineer to join our IT team to deliver reliable and scalable analytics.
Why here
?
TricorBraun is a global packaging leader, with team members working in locations throughout the Americas, Europe, Asia, Australia and New Zealand. As North America’s largest distributor of primary packaging, we provide innovative solutions to customers from a wide variety of industries. Our customers range from cutting-edge start-ups to the world’s most iconic brands. We put people first and live by that every day. Join us and you will be welcomed by our friendly, motivated and supportive team. Many of the products we distribute are already sitting in your home.
Your role here
From Day 1, you will collaborate closely with the data analytics, Power BI reporting, and functional teams to deliver top notch analytics.
Your Experience And Background
Provides technical leadership in Microsoft Synapse, spanning numerous services (Pipelines, Notebooks, Azure Storage, Logic Apps)
Interacts with various business functions to define requirements and build a strategy to implement within our modern data framework.
Interacts with engineering team to implement a high quality, efficient, dependable, and secure data platform while maintaining consistent data standards, patterns, and practices across the organization (Data warehouse, Data Lakehouse)
Performs code review and verifies compliance with code standards.
Owns Data Analytics team delivery outcomes.
What You Can Expect From Us
Because we’re a well-known and respected leader in packaging, we have many opportunities here. We’ll get you started with an exceptional training program providing classroom, online and hands on work with colleagues. There’s always someone to answer any questions and ensure you’re getting the right information you need to excel. And the compensation and benefits are what you can expect from a people-first company.
Our salaries are competitive
Comprehensive benefits in medical, dental, and vision insurance
401(k) plan with employer match
Paid time off plus holidays
Tuition reimbursement, and much more
We are proudly an equal-opportunity employer and will consider all applications.
Show more
Show less","Data Engineering, Microsoft Synapse, Azure Storage, Logic Apps, Power BI, Data Analytics, Data Warehouse, Data Lakehouse, Code Review, Data Standards, Data Patterns, Data Practices","data engineering, microsoft synapse, azure storage, logic apps, power bi, data analytics, data warehouse, data lakehouse, code review, data standards, data patterns, data practices","azure storage, code review, data engineering, data lakehouse, data patterns, data practices, data standards, dataanalytics, datawarehouse, logic apps, microsoft synapse, powerbi"
Security Engineer III - Data Security,Stifel Financial Corp.,"St Louis, MO",https://www.linkedin.com/jobs/view/security-engineer-iii-data-security-at-stifel-financial-corp-3739533819,2023-12-17,Belleville,United States,Mid senior,Onsite,"Summary
Under minimal supervision, the Data Security Engineer III is a front-line member of the Data Security team that has responsibility for protecting corporate information assets. The Data Security Engineer III will be responsible for configuring and improving DLP policies on multiple tools, working towards increasing DLP Program coverage, crafting and maintaining DLP Program process documentation, defining new processes and controls to further mature the DLP Program, and addressing gaps that impact the DLP process.
Essential Duties & Responsibilities
Deploys and manages technology and process solutions to reduce the potential of data compromise
Develops technical requirements, evaluating vendor solutions, and testing of data security solutions
Utilizes security tools to enhance data loss prevention capabilities across the Enterprise
Tune DLP policies on a continuous basis to maintain a mature set of policies within the scope of the DLP Program.
Implement security policies to comply with data privacy, governance and regulatory requirements
Performs data protection monitoring and reporting, analyzes security alerts and escalates security alerts to local support teams.
Proposes improvements and assists in the implementation of enterprise wide security policies, procedures and standards to meet compliance responsibilities.
Prepares status reports to develop security risk analysis scenarios.
Assist in documenting standard operating procedures and protocols for the Data Security Pillar
Assist in the development of technical solutions and processes to help mitigate security vulnerabilities and automate repeatable tasks.
Partner with teams as needed to enhance DLP monitoring / response processes on an ongoing basis.
Qualifications
Strong understand of how to identify and prioritize security incidents and/or escalate to management or other team members.
Experience translating data security questions into data analytical approaches
Hands-on experience in security systems, including data loss prevention, data classification, etc.
Understanding of data classification frameworks and processes
Knowledge of information security standards including CIS Critical controls and the NIST Cybersecurity Framework.
Ability to systematically assess a problem or situation to accurately identify probable causes and solutions.
Understanding of a broad range of IT disciplines that would impact overall security posture.
Proficiency in relating complex technical situations to non-technical customers.
Ability to prioritize workload and consistently meet deadlines
Education & Experience
Bachelor's degree in Computer Science, Information Systems, Cybersecurity, or related field; or related combination of education and experience
4+ years experience in an information technology or information security role
CISSP, CISM, or Security+ certifications preferred
About Stifel
Stifel is a more than 130 years old and still thinking like a start-up. We are a global wealth management and investment banking firm serious about innovation and fresh ideas. Built on a simple premise of safeguarding our clients’ money as if it were our own, coined by our namesake, Herman Stifel, our success is intimately tied to our commitment to helping families, companies, and municipalities find their own success.
While our headquarters is in St. Louis, we have offices in New York, San Francisco, Baltimore, London, Frankfurt, Toronto, and more than 400 other locations. Stifel is home to approximately 9,000 individuals who are currently building their careers as financial advisors, research analysts, project managers, marketing specialists, developers, bankers, operations associates, among hundreds more. Let’s talk about how you can find your place here at Stifel, where success meets success.
At Stifel we offer an entrepreneurial environment, comprehensive benefits package to include health, dental and vision care, 401k, wellness initiatives, life insurance, and paid time off.
Stifel is an Equal Opportunity Employer.
Show more
Show less","Data Security, DLP Tools, Data Loss Prevention, Security Policies, Security Alerts, Security Standards, Risk Analysis, Data Classification Frameworks, NIST Cybersecurity Framework, CIS Critical Controls, IT Disciplines, Technical Communication, Workload Prioritization, Computer Science, Information Systems, Cybersecurity, CISSP, CISM, Security+","data security, dlp tools, data loss prevention, security policies, security alerts, security standards, risk analysis, data classification frameworks, nist cybersecurity framework, cis critical controls, it disciplines, technical communication, workload prioritization, computer science, information systems, cybersecurity, cissp, cism, security","cis critical controls, cism, cissp, computer science, cybersecurity, data classification frameworks, data loss prevention, data security, dlp tools, information systems, it disciplines, nist cybersecurity framework, risk analysis, security, security alerts, security policies, security standards, technical communication, workload prioritization"
Staff Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395481,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Data Analyst,Kforce Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-analyst-at-kforce-inc-3782228566,2023-12-17,Belleville,United States,Mid senior,Onsite,"Responsibilities
Kforce has a client in Saint Louis, MO that is seeking a Senior Data Analyst. Responsibilities:
Provides guidance on the most complex and unique matters related to data reporting and analytic techniques that have a significant impact to the region and ministry; May lead work of others within business operations
Ensures collaboration with key decision makers to identify and solve a variety of business or operational problems and objectives
Interprets complex data, identifies trends, establishes/utilizes benchmark data, creates data visualization, and applies information to the business to determine impact, significance, and opportunities
Responsible for developing specifications of analytic and reporting needs through participation in project teams or meetings with internal/external customers for projects and/or ad hoc requests
Utilizes a variety of databases and advanced query tools to gather data and information
Ensures all requirements are well defined from the customer to meet reporting and analysis specifications
Ensures data integrity and data quality; Supplies business and leadership with deep insights into the daily operations and overall business operations; Viewed as a subject matter expert for the team
Partners closely with technical and build team members to establish reports and dashboards within applications that will support operations
Partners with customers, peers and cross functional teams to discover new business needs; Proactively offer options and solutions
Oversees the evaluation, analysis, and development of reporting and analytical tools to analyze department and division performance indicators
Leverages advanced analytic techniques necessary to meet needs of business users supported by the analytic team
Supports both new and ongoing operational reporting needs, business process redesign, process improvement initiatives by providing data insights
Requirements
Bachelor's degree or equivalent combination of education and experience
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $65 - $75 per hour
Show more
Show less","Data analysis, Data reporting, Data visualization, Business operations, Decision making, Data interpretation, Trend analysis, Benchmarking, Data integrity, Data quality, Deep insights, Reporting needs, Analytical tools, Business insights, Data mining, Process redesign, Process improvement, Datadriven decision making, Advanced analytic techniques, Compensation, Benefits, Equal Opportunity/Affirmative Action","data analysis, data reporting, data visualization, business operations, decision making, data interpretation, trend analysis, benchmarking, data integrity, data quality, deep insights, reporting needs, analytical tools, business insights, data mining, process redesign, process improvement, datadriven decision making, advanced analytic techniques, compensation, benefits, equal opportunityaffirmative action","advanced analytic techniques, analytical tools, benchmarking, benefits, business insights, business operations, compensation, data integrity, data interpretation, data mining, data quality, data reporting, dataanalytics, datadriven decision making, decision making, deep insights, equal opportunityaffirmative action, process improvement, process redesign, reporting needs, trend analysis, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827522,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, ETL, Dimensional Data Modeling, Schema Design, Legal Compliance, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, etl, dimensional data modeling, schema design, legal compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744399380,2023-12-17,Belleville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Data Management Tools, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data management tools, data classification, data retention","agile engineering practices, airflow, automation, continuous delivery, data classification, data engineering, data management tools, data retention, data warehouses, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Junior Data Scientist,Concero,Greater St. Louis,https://www.linkedin.com/jobs/view/junior-data-scientist-at-concero-3771636398,2023-12-17,Belleville,United States,Mid senior,Onsite,"Job Description
Onsite St. Louis, MO
Data Scientist
- Design and analyze lift experiments to drive product improvements with cross-functional teams.
- Conduct research and analysis to improve the statistical/ML model lift using experimental design and causal inference methods
- Write SQL queries to clean, aggregate, and/or impute data from multiple tables and/or across jump servers
- Manage IT tickets for simultaneous projects and tasks
- Maintain proper project documentation (Project scope, business requirements, workflow charts, data mapping, etc.)
- Build cross-functional relationships with SQL developers, business analysts, product marketing, Customer Success Managers (CSM), and other key stakeholders to identify opportunities to improve products, drive product launches, and influence product roadmaps
Qualifications
- Bachelor's degree in a quantitative discipline (e.g., Statistics, Engineering, Biostatistics, Economics, Computer Science, Mathematics, Physics) and/or 2+ years of equivalent practical experience
- knowledge and experience in statistical methodologies, especially probability, hypothesis testing, experimental design, and causal inference
- Familiarity with machine learning algorithms and A.I. methodologies
- Understanding of project management principles
- Basic understanding of A/B testing
- Practical experience querying and analyzing large datasets using SQL
- Proficient in scripting languages like Java, JavaScript, Python, and/or R
- Able to work in an innovative and fast-paced environment
#5737
Show more
Show less","Data Analysis, Hypothesis Testing, Experimental Design, AI, Project Management, A/B Testing, Machine Learning, Causal Inference, SQL, Statistical Methods, Python, Java, JavaScript, R","data analysis, hypothesis testing, experimental design, ai, project management, ab testing, machine learning, causal inference, sql, statistical methods, python, java, javascript, r","ab testing, ai, causal inference, dataanalytics, experimental design, hypothesis testing, java, javascript, machine learning, project management, python, r, sql, statistical methods"
Senior Statistical Data Analyst - Biostatistics,Washington University in St. Louis,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-statistical-data-analyst-biostatistics-at-washington-university-in-st-louis-3727312628,2023-12-17,Belleville,United States,Mid senior,Onsite,"Scheduled Hours
40
Position Summary
This position is in the Xiong Lab in the Center for Biostatistics. The Center for Biostatistics is part of the Institute for Informatics, Data Science and Biostatistics and the Office of Health Information and Data Science (OHIDS). At OHIDS we have a people-centric approach where our team members are our number one asset. We are committed to providing safe and inclusive working conditions and take great care to support employees’ health and well-being. We support a flexible work environment in order to support a range of work schedules and better work-life guardrails. Ensuring the well-being of our team members is a top priority.
Performs database management, and data harmonization across different studies, and data sharing, as well as data analysis expertly using statistical packages. Assists investigators in the design of experiments, clinical trials, and epidemiological studies. May be required to assume major responsibility on large project.
Job Description
Primary Duties & Responsibilities
Performs management duties as part of several large NIH-funded longitudinal data repository containing clinical, psychological, imaging, neuropathologic and biomarker data.
Harmonizes databases across multiple independent studies by working with database team of each study, standardizing database dictionary and creating standard metadata template for data sharing with both internal and external investigators.
Transfers QC’d data to national and local data repository as well as to individual investigators who requested the data, and track the data sharing.
Monitors and evaluates data integrity.
Performs quality control on all forms of data (paper, electronic, etc.);
Assists with research coordination of projects.
Assists with research management (administration, scheduling, tracking, recruitment, etc.).
Assists in development of data capture forms and screens.
Utilizes REDCap and other EDC systems as required for generation of such forms and reports.
Generate routine reports on statistics and figures and tables summarizing the current status of databases, using standard statistical packages, SAS, R.
Performs other duties as assigned (e.g. may assist in manuscript preparation).
Preferred Qualifications
Master’s degree in Biostatistics, Statistics, Informatics, Computer Science or related field.
Expert knowledge base and experience with database design, management and analysis.
PC/UNIX knowledge.
Skills and demonstrated success in designing and executing appropriate statistical data analysis in SAS, R Python, or other packages.
Experience in REDCap.
Capable of assuming a leadership role.
Demonstrated effective verbal, written and interpersonal communication skills.
Demonstrated organization skills.
Required Qualifications
Bachelor’s degree and three years of related experience; a combination of college education and/or relevant experience equaling seven years may substitute for this requirement.
Grade
G13
Salary Range
$64,700.00 - $110,500.00 / Annually
The salary range reflects base salaries paid for positions in a given job grade across the University. Individual rates within the range will be determined by factors including one's qualifications and performance, equity with others in the department, market rates for positions within the same grade and department budget.
Accommodation
If you are unable to use our online application system and would like an accommodation, please email CandidateQuestions@wustl.edu or call the dedicated accommodation inquiry number at 314-935-1149 and leave a voicemail with the nature of your request.
Pre-Employment Screening
All external candidates receiving an offer for employment will be required to submit to pre-employment screening for this position. The screenings will include criminal background check and, as applicable for the position, other background checks, drug screen, an employment and education or licensure/certification verification, physical examination, certain vaccinations and/or governmental registry checks. All offers are contingent upon successful completion of required screening.
Benefits Statement
Personal
Up to 22 days of vacation, 10 recognized holidays, and sick time.
Competitive health insurance packages with priority appointments and lower copays/coinsurance.
Want to Live Near Your Work and/or improve your commute? Take advantage of our free Metro transit U-Pass for eligible employees. We also offer a forgivable home loan of up to $12,500 for closing costs and a down payment for homes in eligible neighborhoods.
WashU provides eligible employees with a defined contribution (403(b)) Retirement Savings Plan, which combines employee contributions and university contributions starting at 7%.
Wellness
Wellness challenges, annual health screenings, mental health resources, mindfulness programs and courses, employee assistance program (EAP), financial resources, access to dietitians, and more!
Family
We offer 4 weeks of caregiver leave to bond with your new child. Family care resources are also available for your continued childcare needs. Need adult care? We’ve got you covered.
WashU covers the cost of tuition for you and your family, including dependent undergraduate-level college tuition up to 100% at WashU and 40% elsewhere after seven years with us.
For policies, detailed benefits, and eligibility, please visit: https://hr.wustl.edu/benefits/
EEO/AA Statement
Washington University in St. Louis is committed to the principles and practices of equal employment opportunity and especially encourages applications by those from underrepresented groups. It is the University’s policy to provide equal opportunity and access to persons in all job titles without regard to race, ethnicity, color, national origin, age, religion, sex, sexual orientation, gender identity or expression, disability, protected veteran status, or genetic information.
Diversity Statement
Washington University is dedicated to building a diverse community of individuals who are committed to contributing to an inclusive environment – fostering respect for all and welcoming individuals from diverse backgrounds, experiences and perspectives. Individuals with a commitment to these values are encouraged to apply.
Show more
Show less","Data management, Data harmonization, Data analysis, Statistical packages, SAS, R, Python, REDCap, Database design, Database management, PC/UNIX, Statistical data analysis, Verbal communication, Written communication, Interpersonal communication, Organization skills","data management, data harmonization, data analysis, statistical packages, sas, r, python, redcap, database design, database management, pcunix, statistical data analysis, verbal communication, written communication, interpersonal communication, organization skills","data harmonization, data management, dataanalytics, database design, database management, interpersonal communication, organization skills, pcunix, python, r, redcap, sas, statistical data analysis, statistical packages, verbal communication, written communication"
Healthcare Data Analyst (Remote),Conexess Group,"St Louis, MO",https://www.linkedin.com/jobs/view/healthcare-data-analyst-remote-at-conexess-group-3784395054,2023-12-17,Belleville,United States,Mid senior,Remote,"Our History:
From our start in 2009, Conexess has established itself in 3 markets, employing nearly 200+ individuals nation-wide. Operating in over 15 states, our client base ranges from Fortune 500/1000 companies to mid-small range companies. For the majority of the mid-small range companies, we are exclusively used due to our outstanding staffing track record.
Who We Are:
Conexess is a full-service staffing firm offering contract, contract-to hire, and direct placements. We have a wide range of recruiting capabilities extending from help desk technicians to CIOs. We are also capable of offering project-based work.
Conexess Group is aiding a large healthcare client in their search for a Healthcare Data Analyst in a remote capacity. This is a long-term opportunity with a competitive compensation package.
Responsibilities:
Provides advanced professional input to complex Business Analytics assignments/projects.
Works in collaboration with business partners to provide analyses and reports to aide decision making based on data, facts, and analytical findings across different parts of the organization.
Analyzes and interprets collected data, spots trends, writes reports and recommendations and completes data modeling.
Supports and provides direction to more junior professionals.
Works autonomously, only requiring “expert” level technical support from others.
Exercises judgment in the evaluation, selection, and adaptation of both standard and complex techniques and procedures.
Utilizes in-depth professional knowledge and acumen to develop models and procedures, and monitor trends, within Business Analytics.
Qualifications:
Bachelor's Degree in related field required. Advanced degree preferred.
Experience with SQL, Relational Models, Hadoop, Tableau, Alteryx
Experience with Databricks Platform.
Ability to work effectively in a dynamic, rapidly changing environment; operates well with ambiguity.
2+ years relevant work experience in healthcare advanced analytics.
Show more
Show less","SQL, Relational Models, Hadoop, Tableau, Alteryx, Databricks Platform, Business Analytics, Data Analytics, Data Modeling, Reporting, Data Interpretation, Trend Analysis, Data Visualization, Decision Making, Project Management, Communication, Collaboration, Problem Solving, Analytical Thinking, Critical Thinking, Attention to Detail, Time Management, Microsoft Office Suite","sql, relational models, hadoop, tableau, alteryx, databricks platform, business analytics, data analytics, data modeling, reporting, data interpretation, trend analysis, data visualization, decision making, project management, communication, collaboration, problem solving, analytical thinking, critical thinking, attention to detail, time management, microsoft office suite","alteryx, analytical thinking, attention to detail, business analytics, collaboration, communication, critical thinking, data interpretation, dataanalytics, databricks platform, datamodeling, decision making, hadoop, microsoft office suite, problem solving, project management, relational models, reporting, sql, tableau, time management, trend analysis, visualization"
Senior Database Engineer - SQL,Enterprise Mobility,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-database-engineer-sql-at-enterprise-mobility-3769040147,2023-12-17,Belleville,United States,Mid senior,Remote,"Description
Enterprise Mobility is the world’s largest car rental operator and an industry leader in mobility and technology. We’re one of the top global travel companies, ranking ahead of many airlines and most cruise lines and hotels. And no matter what transportation challenges our customers face, we have an innovative solution.
We operate the Enterprise Rent-A-Car National Car Rental and Alamo Rent A Car brands via more than 10,000 fully staffed neighborhood and airport offices, including franchisee branches, in over 90 countries and territories.
Through this robust global network, we operate a fleet of over 2.3 million vehicles and provide a comprehensive portfolio of transportation solutions, including car rental, carsharing, vanpooling, car sales, truck rental, vehicle-subscription and affiliated fleet management services. As a total mobility provider, we serve the needs of a wide variety of customers, businesses, government agencies and organizations every day.
At the center of it all, our dedicated IT teams innovate, design and develop the technology that is redefining how customers rent, buy and share vehicles from our family of brands. Here, you will be part of a diverse and talented team that creates and delivers powerful technology solutions for our customers and employees across the world with the resources and support to develop in a variety of career paths.
As an Enterprise employee, we offer an excellent package with market-competitive pay, comprehensive healthcare packages, 401k matching & profit sharing, schedule flexibility, work from home opportunities, paid time off, and organizational growth potential.
This position is open to candidates who wish to work from home (WFH). Employees who choose virtual / remote work should have an adequate space to serve as their home office.
Responsibilties
As an
Senior Database Engineer
, you will be responsible for using your technical knowledge of professional concepts to solve business problems. We are looking for a talented individual that can work on a diverse work tasks and work with others in the department on complex assignments. You will be responsible for evaluating elements of technology's effectiveness through requirements gathering, testing, research and investigation and make recommendations for improvements that result in increased quality and effectiveness.
In this role, you will play a significant role in the implementation, migration, and maintenance of SQL Server On Prem and Azure SQL production databases. In addition, you will monitor daily database activities and overall performance, growth, and tuning configurations as well as the security patching of the databases. You will assist with process improvements and change activities, as well as participate in the teams on-call. We are looking for a self-starter that will maintain established service level agreements to meet customer expectations and quality standards.
Equal Opportunity Employer/Disability/Veterans
Qualifications
Required:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3+ years of related experience
3+ years of SQL Database engineering experience
3+ years of experience with Microsoft and SQL scripting to automate tasks such as monitoring, patching, and generating reports
2+ years of Azure Cloud Database experience
Strong detail-oriented skills combined with sound problem solving and time management
Strong verbal and written communication skills
Proven experience working in a fast-paced production environment
Must be able to effectively collaborate and work with others in a remote work environment
Must be able to work effectively in a change management environment
Preferred
Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferred
Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance
Knowledge of JIRA and Confluence a plus
Knowledge of BitBucket and Git a plus
Knowledge of Office 365 Windows environment
Knowledge of Database Client software
Knowledge of ServiceNow a plus
Knowledge of SQL to Azure migration processes
#IT
Show more
Show less","SQL Server, Azure SQL, Microsoft SQL, JIRA, Confluence, BitBucket, Git, Office 365, Database Client software, ServiceNow, Azure Cloud Database, SQL to Azure migration processes","sql server, azure sql, microsoft sql, jira, confluence, bitbucket, git, office 365, database client software, servicenow, azure cloud database, sql to azure migration processes","azure cloud database, azure sql, bitbucket, confluence, database client software, git, jira, microsoft sql, office 365, servicenow, sql server, sql to azure migration processes"
Data Engineer - Scala(U.S. remote),Railroad19,"St Louis, MO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782294502,2023-12-17,Belleville,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, RESTful APIs, AWS, EMR clusters, S3, Relational databases, Nonrelational databases, Hadoop, Java","scala 212, spark 24, restful apis, aws, emr clusters, s3, relational databases, nonrelational databases, hadoop, java","aws, emr clusters, hadoop, java, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
CRM Data Analyst,Netskope,"St Louis, MO",https://www.linkedin.com/jobs/view/crm-data-analyst-at-netskope-3756678621,2023-12-17,Belleville,United States,Mid senior,Remote,"About Netskope
Today, there's more data and users outside the enterprise than inside, causing the network perimeter as we know it to dissolve. We realized a new perimeter was needed, one that is built in the cloud and follows and protects data wherever it goes, so we started Netskope to redefine Cloud, Network and Data Security.
Since 2012, we have built the market-leading cloud security company and an award-winning culture powered by hundreds of employees spread across offices in Santa Clara, St. Louis, Bangalore, London, Melbourne, and Tokyo. Our core values are openness, honesty, and transparency, and we purposely developed our open desk layouts and large meeting spaces to support and promote partnerships, collaboration, and teamwork. From catered lunches and office celebrations to employee recognition events (pre and hopefully post-Covid) and social professional groups such as the Awesome Women of Netskope (AWON), we strive to keep work fun, supportive and interactive. Visit us at Netskope Careers. Please follow us on LinkedIn and Twitter@Netskope.
About the position:
The Netskope Data Operations team is looking for a CRM (Customer Relationship Management) Data Analyst, who will help drive data operational excellence. As a member of the Data Operations team, this candidate will be responsible for the integrity, completeness and quality of data within the CRM. This individual will develop and implement standard procedures for capturing, cleansing, and updating data in the CRM and document data governance policies.
Netskope’s Go-to-Market (GTM) Data Operations team exists to maintain accurate, actionable, and non-duplicative Account, Lead and Contact data that accurately reflects corporate hierarchies and key data points enabling Sales to better manage Customers and target Prospects. This role requires subject matter expertise on enrichment, normalization and cleansing of data in Salesforce and an understanding of how changes can impact other systems and processes.
Responsibilities:
Regularly perform data cleansing and validation processes to ensure data accuracy, completeness, and consistency
Proactively identify data quality issues and work with relevant teams to implement improvements
Perform tasks related to data acquisition, validation and enrichment as they relate to Accounts, Leads and Contacts
Work cross-functionally within the organization to ensure data from other systems (such as marketing automation tools, sales and customer service platforms) are properly integrated with Salesforce to ensure data integrity is maintained
Propose and implement data governance solutions for the proper management of CRM data
Identify opportunities to streamline data-related processes and workflows to drive greater efficiency and accuracy.
Provide support to cross functional teams such as: training on data entry best practices and troubleshooting issues related to data quality
Requirements:
5+ years of work experience as a technical data steward with a strong background in Data Governance Policies, Processes and Tools
Experience in Salesforce, Excel, 3rd Party Data Tools (e.g. Zoominfo, D&B) and Data Cleansing Tools (e.g. Ringlead, Demandtools, Openprise)
Strong attention to detail with a focus on building standardized and consistent best practices
Understanding of marketing and privacy regulations (e.g. Opt In/Out, GDPR)
Proven ability to collaborate well with others and meet deadlines
Persistent in finding accurate information leveraging all available resources
Experience documenting business processes and developing training materials for data integrity and governance
Preferred Qualifications:
High level of integrity and transparency in working with peers and stakeholders
Comfort with ambiguity and proven ability to work independently
Strong written and verbal communication skills
Ability to multitask and meet Service Level Agreements (SLAs), specific to work efforts
Proven ability to meet timelines and deliver business results
A self-starter who is detail-oriented and can engage across stakeholders with confidence
Education:
Bachelor’s degree
Netskope is committed to implementing equal employment opportunities for all employees and applicants for employment. Netskope does not discriminate in employment opportunities or practices based on religion, race, color, sex, marital or veteran statues, age, national origin, ancestry, physical or mental disability, medical condition, sexual orientation, gender identity/expression, genetic information, pregnancy (including childbirth, lactation and related medical conditions), or any other characteristic protected by the laws or regulations of any jurisdiction in which we operate.
Netskope respects your privacy and is committed to protecting the personal information you share with us, please refer to Netskope's Privacy Policy for more details.
Show more
Show less","Salesforce, Data Governance, Data Cleansing, Data Integrity, Data Quality, Data Enrichment, Data Validation, Data Acquisition, Data Stewardship, Data Governance Policies, Data Governance Processes, Data Governance Tools, Excel, Zoominfo, D&B, Ringlead, Demandtools, Openprise, GDPR, Service Level Agreements","salesforce, data governance, data cleansing, data integrity, data quality, data enrichment, data validation, data acquisition, data stewardship, data governance policies, data governance processes, data governance tools, excel, zoominfo, db, ringlead, demandtools, openprise, gdpr, service level agreements","data acquisition, data enrichment, data governance, data governance policies, data governance processes, data governance tools, data integrity, data quality, data stewardship, data validation, datacleaning, db, demandtools, excel, gdpr, openprise, ringlead, salesforce, service level agreements, zoominfo"
Big Data Developer,Sigmaways Inc,"St Louis, MO",https://www.linkedin.com/jobs/view/big-data-developer-at-sigmaways-inc-3778728506,2023-12-17,Belleville,United States,Mid senior,Hybrid,"We are looking for a
Big data Developer
who will contribute to high-quality technology solutions that address business needs by developing data applications for the customer business lines. You will contribute to the development and ongoing maintenance of several strategic data initiatives and data and analytic applications.
Responsibilities:
Hands-on development role focused on creating big data and analytics solutions.
Coding of mission-critical components.
A strong Big data developer/coder who can write database queries and also tune those queries to perform optimally.
Analyze business and functional requirements and contribute to the overall solution.
Participate in design reviews, and provide input to the design recommendations.
Participate in project planning sessions with project managers, business analysts, and team members.
Required skills:
BS or MS in Computer Science or equivalent experience.
Software development experience with solid working experience in Big Data technologies.
Knowledge of the architecture and internals of technologies in the Hadoop ecosystem
Experience designing and implementing large, scalable distributed systems.
Must have solid expertise and hands-on experience in
Spark,
Scala and SQL
S
olid experience of the Hadoop Ecosystem ( HDFS, Yarn, MapReduce, Spark, Hive, Impala )
and should be able to mentor and lead junior team members.
Good understanding of database technologies, including SQL and NoSQL databases
Ability to debug and promptly resolve production issues.
Proficiency with advanced object-oriented programming.
Excellent problem-solving and analytical skills.
Excellent written and oral communication skills.
Show more
Show less","Big Data, Data Analytics, Hadoop Ecosystem, NoSQL Databases, Spark, Scala, SQL, HDFS, Yarn, MapReduce, Hive, Impala, Database Management Systems, ObjectOriented Programming, ProblemSolving, Analytical Skills, Communication Skills","big data, data analytics, hadoop ecosystem, nosql databases, spark, scala, sql, hdfs, yarn, mapreduce, hive, impala, database management systems, objectoriented programming, problemsolving, analytical skills, communication skills","analytical skills, big data, communication skills, dataanalytics, database management systems, hadoop ecosystem, hdfs, hive, impala, mapreduce, nosql databases, objectoriented programming, problemsolving, scala, spark, sql, yarn"
Looking for Azure Database Engineer -St. Louis MO (Hybrid) - Contract,Extend Information Systems Inc.,"St Louis, MO",https://www.linkedin.com/jobs/view/looking-for-azure-database-engineer-st-louis-mo-hybrid-contract-at-extend-information-systems-inc-3704057850,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Hi,
I hope you are doing well!
We have an opportunity for
Azure Database Engineer
with one of our clients for
St. Louis MO (Hybrid).
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Azure Database Engineer
Location: St. Louis MO (Hybrid)
Terms:
Contract
Job Details
Job Responsibility:
Collaborate with data architects and other stakeholders to design scalable and optimized data warehouse solutions on the Azure platform.
Develop and maintain data warehouse architectural documentation, including schema designs, data flow diagrams, and integration patterns.
Move SSIS / ETL Processes and execute SSIS within Azure Data Factory.
Design the NextGen Data Warehouse using Azure.
Expertise in using Master Data Management strategies, design Data Warehouse structure and schema with Azure Data services..
Execute SSIS packages in Azure from SSDT to assess the cloud compatibility of SSIS packages and run them on Azure-SSIS IR within ADF.
Fix any issues found in IR testing and can migrate all SSIS packages to run in Azure.
Re-write all SSIS packages into ADF and expand analytics using Azure Synapse and Power BI
Investigate use of Azure Fabric for end to end data processes from Data Science to Analytics
Write efficient T-SQL queries for data transformation, extraction, loading, and reporting purposes.
Implement data partitioning, indexing, and distribution strategies to enhance query performance.
Integrate data from various sources, both on-premises and in the cloud, into the Azure Data Warehouse using appropriate ETL/ELT techniques.
Work with data integration tools such as Azure Data Factory or SSIS to automate data pipelines.
Monitor and analyze query performance and data warehouse health, identifying and resolving bottlenecks and performance issues.
Optimize query execution plans, indexing strategies, and data distribution to achieve optimal performance.
Implement security measures to protect sensitive data within the Azure Data Warehouse, following best practices for data encryption, access controls, and data masking.
Ensure compliance with relevant data privacy and security regulations, such as GDPR or HIPAA.
Provide training and guidance to junior team members on data warehouse best practices and technologies.
Qualifications
Bachelor's or higher degree in Computer Science, Information Technology, or a related field.
Proven experience in designing, implementing, and optimizing data warehousing solutions using Azure services (Azure Synapse Analytics, Azure Data Factory, Azure SQL Data Warehouse).
Plan the ETL process for the data warehouse design.
Choose appropriate Azure data services (Synapse, ADF, ?)
Create data warehouse operations runbook
Manage Data Quality ongoing
.
--
Thanks & Regards
Monika Singh
Extend Information System Inc
Phone: (571)-622-3980
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","Azure Data Factory, Azure Data Warehouse, Azure SQL Data Warehouse, Azure Synapse Analytics, AzureSSIS IR, ADF, Data Science, Data Warehouse, ETL, GDPR, HIPAA, Master Data Management, Power BI, SSIS, SSDT, TSQL","azure data factory, azure data warehouse, azure sql data warehouse, azure synapse analytics, azuressis ir, adf, data science, data warehouse, etl, gdpr, hipaa, master data management, power bi, ssis, ssdt, tsql","adf, azure data factory, azure data warehouse, azure sql data warehouse, azure synapse analytics, azuressis ir, data science, datawarehouse, etl, gdpr, hipaa, master data management, powerbi, ssdt, ssis, tsql"
Sr. Power BI / Data Engineer-expert,neteffects,"St. Louis City County, MO",https://www.linkedin.com/jobs/view/sr-power-bi-data-engineer-expert-at-neteffects-3779391637,2023-12-17,Belleville,United States,Mid senior,Hybrid,"At neteffects, we are looking for a
Sr. Power BI/ Data Engineer-expert
for our direct client in
St. Louis MO.
Contract -No c2c.
Hybrid in St. Louis MO
Education: Bachelor's degree or Master's degree with six years of experience
Top Technical Skills/Tools/Experience Needed for the Role:
Technical Skills:
•
Programming Languages:
Python, Scala, Go, SQL (required), R (preferred)
Business Intelligence: Power BI, Power Apps
Power BI and Data Visualizations
Power Apps
•
Cloud Platforms
: AWS, GCP
•
Data Warehousing
: BigQuery, Redshift, Snowflake-
expertise on any one
•
Streaming Technologies
: Kafka
•
Data Pipelines
: Spark, AWS SQS, Lambda, Step Functions, ECS, Fargate, Athena, BigQuery, GCP PubSub, Cloud Functions, Cloud Run, Kubernetes
• I
nfrastructure as Code
: Terraform, AWS CloudFormation
•
DevOps:
Continuous Integration/Continuous Delivery (CI/CD)
•
Version Control:
Git
•
Documentation:
Haystack, SharePoint
•
• Other:
Statistical and/or mathematical programming packages, machine learning (ML) (optional)
Experience:
• Building data models
• Engineering data-intensive software
• Implementing data pipelines
• Working with cloud platforms
• Collaborating with cross-functional teams
• Leading and participating in design sessions
• Providing technical recommendations
• Estimating project timelines
• Communicating technical information effectively
Additional Desired Skills:
• Business Acumen: Understanding of business processes and needs
• Leadership: Leading and motivating teams
• Communication: Excellent written and oral communication skills
• Project Management: Successfully managing projects from start to finish
• Organizational Skills: Prioritizing tasks and meeting deadlines
• Interpersonal Skills: Building and maintaining positive relationships
• Global Experience: Working with culturally diverse teams
• Learning: Continuously learning and staying up-to-date with new technologies
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Python, Scala, Go, SQL, R, Power BI, Power Apps, AWS, GCP, BigQuery, Redshift, Snowflake, Kafka, Spark, AWS SQS, Lambda, Step Functions, ECS, Fargate, Athena, GCP PubSub, Cloud Functions, Cloud Run, Kubernetes, Terraform, AWS CloudFormation, Git, Haystack, SharePoint, Machine Learning, Statistical Programming, Mathematical Programming Packages, Data Modeling, DataIntensive Software Engineering, Data Pipeline Implementation, Cloud Platform Experience, CrossFunctional Team Collaboration, Design Session Participation, Technical Recommendation, Project Timeline Estimation, Technical Information Communication, Business Acumen, Leadership, Communication, Project Management, Organizational Skills, Interpersonal Skills, Global Experience, Learning","python, scala, go, sql, r, power bi, power apps, aws, gcp, bigquery, redshift, snowflake, kafka, spark, aws sqs, lambda, step functions, ecs, fargate, athena, gcp pubsub, cloud functions, cloud run, kubernetes, terraform, aws cloudformation, git, haystack, sharepoint, machine learning, statistical programming, mathematical programming packages, data modeling, dataintensive software engineering, data pipeline implementation, cloud platform experience, crossfunctional team collaboration, design session participation, technical recommendation, project timeline estimation, technical information communication, business acumen, leadership, communication, project management, organizational skills, interpersonal skills, global experience, learning","athena, aws, aws cloudformation, aws sqs, bigquery, business acumen, cloud functions, cloud platform experience, cloud run, communication, crossfunctional team collaboration, data pipeline implementation, dataintensive software engineering, datamodeling, design session participation, ecs, fargate, gcp, gcp pubsub, git, global experience, go, haystack, interpersonal skills, kafka, kubernetes, lambda, leadership, learning, machine learning, mathematical programming packages, organizational skills, power apps, powerbi, project management, project timeline estimation, python, r, redshift, scala, sharepoint, snowflake, spark, sql, statistical programming, step functions, technical information communication, technical recommendation, terraform"
"Data DevOps Engineer // St. Louis, MO",Motion Recruitment,"St Louis, MO",https://www.linkedin.com/jobs/view/data-devops-engineer-st-louis-mo-at-motion-recruitment-3785829212,2023-12-17,Belleville,United States,Mid senior,Hybrid,"A well-known brewing and beverage company is looking for a DevOps Engineer to join their team on a contract-to-hire basis. This will be a hybrid role working onsite in St. Louis, MO 2-3 days per week.
As a DevOps Engineer, you will be partnering up with Data Engineering and Data Science teams to architect data solutions. You will be using a DevOps approach to move data across their platforms and environments, so they are looking for someone who is knowledgeable on a variety of data systems.
Skills & Experience
4+ years of experience in DevOps/Cloud Engineering
2+ years of experience in an Azure cloud environment
Hands-on experience with Databricks, Snowflake, and/or Apache Airflow
CI/CD experience using GitHub Actions is a plus
Posted By:
Carolyn Regimbal
Show more
Show less","DevOps, Cloud Engineering, Azure, Databricks, Snowflake, Apache Airflow, GitHub Actions, CI/CD","devops, cloud engineering, azure, databricks, snowflake, apache airflow, github actions, cicd","apache airflow, azure, cicd, cloud engineering, databricks, devops, github actions, snowflake"
Senior Cloud Data Engineer,BDO USA,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467837,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, SQL, Data Warehousing, Data Modeling, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch, Streaming, AI Algorithms, Machine Learning, Automation, Computer Vision, UiPath, Alteryx, Tableau, Qlik, PySpark, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS, Snowflake, Athena, Glue, RedShift, QuickSight, SageMaker, S3, Databricks","data analytics, business intelligence, artificial intelligence, sql, data warehousing, data modeling, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch, streaming, ai algorithms, machine learning, automation, computer vision, uipath, alteryx, tableau, qlik, pyspark, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs, snowflake, athena, glue, redshift, quicksight, sagemaker, s3, databricks","ai algorithms, alteryx, artificial intelligence, athena, automation, aws, azure, azure analysis services, batch, bicep, business intelligence, c, computer vision, data lake medallion architecture, data ops, dataanalytics, databricks, datamodeling, datawarehouse, delta, devops, git, glue, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, streaming, tableau, terraform, uipath"
"Lead Data Engineer java sparc sqlLocation: St Louis, MO (Onsite)",Executive Staff Recruiters / ESR Healthcare,"St Louis, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-java-sparc-sqllocation-st-louis-mo-onsite-at-executive-staff-recruiters-esr-healthcare-3645958534,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Company Profile
esrhealthcare.com.mysmartjobboard.com
Lead Data Engineer
>
> Location: St Louis, MO (Onsite)
>
> Duration: 12 month with possible extension
>
> Required Work Experience:
>
>
5+ years of experience in an engineering role using Python,
> Java, Spark, and SQL.
>
>
GCP Cloud Experience Required
>
>
Strong working experience in HealthCare domain is a must
>
>
Strong Linux/Unix background and hands on knowledge.
>
>
Experience with Shell scripting and bash.
>
>
Experience with version control platform github
>
>
Experience with development ecosystem including Jenkins,
> Artifactory, CI/CD, and Terraform.
>
>
Pipeline creation and automation for Data Acquisition
>
>
Metadata extraction pipeline design and creation between
> raw and finally transformed datasets
>
>
Past experience with big data technologies including HDFS,
> Spark, Impala, Hive
>
>
Able to collaborate with scrum team including scrum master,
> product owner, data analysts, Quality Assurance, business owners, and
> data architecture to produce the best possible end products
>
>
Works on problems of diverse scope and complexity ranging
> from moderate to substantial
>
>
Assists senior professionals in determining methods and
> procedures for new tasks
>
>
Leads basic or moderately complex projects/activities on
> semi-regular basis
>
>
Must possess excellent written and verbal communication
> skills
>
>
Ability to understand and analyze complex data sets
>
>
Exercises independent judgment on basic or moderately
> complex issues regarding job and related tasks
>
>
Makes recommendations to management on new processes, tools
> and techniques, or development of new products and services
>
>
Works under minimal supervision, uses independent judgment
> requiring analysis of variable factors
>
>
Collaborates with senior professionals in the development
> of methods, techniques and analytical approach
>
>
Able to effectively communicate highly technical
> information to numerous audiences, including management, the user
> community, and less-experienced staff.
>
>
Consistently communicate on status of project deliverables
>
>
Consistently provide work effort estimates to management to
> assist in setting priorities
>
>
Deliver timely work in accordance with estimates
>
>
Solve problems as they arise and communicate potential
> roadblocks to manage expectations
>
>
Adhere strictly to all security policies
>
> Regards...!!!!
>
> Aravind
Powered by Webbtree
Show more
Show less","Python, Java, Spark, SQL, GCP Cloud, Linux/Unix, Shell scripting, Bash, GitHub, Jenkins, Artifactory, CI/CD, Terraform, Data Acquisition Pipeline, Metadata extraction pipeline, HDFS, Impala, Hive, Scrum, Data analysis, Quality Assurance, Data Architecture, Communication, Data analysis, Independent judgment, Recommendations, Supervision, Collaboration, Technical communication, Status updates, Work effort estimates, problem solving, Security policies","python, java, spark, sql, gcp cloud, linuxunix, shell scripting, bash, github, jenkins, artifactory, cicd, terraform, data acquisition pipeline, metadata extraction pipeline, hdfs, impala, hive, scrum, data analysis, quality assurance, data architecture, communication, data analysis, independent judgment, recommendations, supervision, collaboration, technical communication, status updates, work effort estimates, problem solving, security policies","artifactory, bash, cicd, collaboration, communication, data acquisition pipeline, data architecture, dataanalytics, gcp cloud, github, hdfs, hive, impala, independent judgment, java, jenkins, linuxunix, metadata extraction pipeline, problem solving, python, quality assurance, recommendations, scrum, security policies, shell scripting, spark, sql, status updates, supervision, technical communication, terraform, work effort estimates"
Senior Data Analyst,Equifax,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-data-analyst-at-equifax-3781960084,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
Equifax Workforce Solutions (EWS), headquartered in St. Louis, MO
is EFX’s fastest growing and most profitable business unit. Learn more about EWS and our workforce solutions here: https://workforce.equifax.com/
Equifax (EFX)
,
HQ Atlanta, GA is a global data, analytics, and technology company. We believe knowledge drives progress. We blend unique data, analytics, and technology with a passion for serving customers globally, to create insights that power decisions to move people forward.
The senior data analyst will understand the needs of different audiences, deliver content both technically and non-technically, and have a knack for visually telling the story with data.
What You’ll Do
This role will be responsible for driving analytics that support sales enablement, marketing, and product development in a state of the art analytics cloud environment. Must be self motivated, collaborative, and results oriented.
Develop innovative analytical solutions that drive actionable insights to enrich a variety of revenue driving functions such as sales enablement, customer lifetime value, and market penetration
Consultative approach to data projects, with the ability to artfully articulate the analysis to non-technical audiences
Take ownership of delivering quality projects to the business, applying rigor to business requirements, design, and aftercare of the solution when deployed
Utilizes Equifax’s diverse data sources to craft solutions using various modeling techniques through the most appropriate open source and/or proprietary tooling
What Experience You'll Need
Bachelor’s degree in Computer Science, Information Systems, Economics, Engineering, or a comparable discipline
5+ years of work experience in identifying, gathering, transforming and analyzing data within and across database platforms (SQL, Python)
Strong development skills in visualization software such as Tableau, Data Studio, or other business intelligence tools
Strong analytical skills including the ability to rapidly learn new business, data assets and how to apply them to drive business impacts
You possess excellent written and verbal communication skills with the ability to communicate with team members at various levels, including business leaders
What Could Set You Apart
Advanced degree in Statistics, Engineering, Business Analytics or a comparable discipline or experience
Work experience in the SaaS industry driving sales or marketing functions
Work experience building, loading, transforming, and analyzing data within Google Cloud Platform (GCP) or other similar cloud environment
Work experience with regulated data
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
Are you ready to power your possible? Apply today, and get started on a path toward an exciting new career at Equifax, where you can make a difference!
Equifax is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","SQL, Python, Tableau, Data Studio, Statistics, Engineering, Business Analytics, Google Cloud Platform, GCP","sql, python, tableau, data studio, statistics, engineering, business analytics, google cloud platform, gcp","business analytics, data studio, engineering, gcp, google cloud platform, python, sql, statistics, tableau"
Data Scientist-Deep Learning-Bioinformatics,neteffects,"St Louis, MO",https://www.linkedin.com/jobs/view/data-scientist-deep-learning-bioinformatics-at-neteffects-3775479912,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Our direct client in St. Louis MO is looking for a DataScientist with Deep Learning & Bioinformatics/ Biomolecular focus.
Contract
Hybrid / Remote for exceptional candidates
No c2c
Basic requirements:
The candidate will join a cross divisional, global team to generate innovative machine learning solutions to better understand biomolecules.
The successful candidate will be responsible for
implementing machine learning models and work collaboratively with data scientists and research scientists leveraging available datasets and influencing new dataset generation.
Must Haves:
·
PhD degree (or M.Sc. with 4 years of working experience) in computer sciences, computational chemistry, computational biology, physics
or related fields.
· Profound experience with state-of-the-art
advanced mathematical models, machine learning methods and model selection concepts; previous experience with deep learning
would be of advantage.
· Real interest in
biology and the life sciences with knowledge of proteins
. Proficient in applying deep learning algorithms to solve biomolecular problems.
· E
xcellent programmin
g and software engineering skills in
Python are essential.
· Expertise in Python libraries like
Biopython and NumPy
would be beneficial.
· The ability to write clean, efficient, and well-documented Python code is crucial.
· Proficiency in writing code and experience in cloud computing.
· Highly creative, independent, fast-learning person with outstanding problem-solving ability and the willingness to undertake challenging analysis tasks autonomously and in a timely fashion.
· Strong interpersonal skills, excellent written and verbal communication, and the ability to work effectively both independently and in cross-functional teams.
· Willingness to travel between research sites domestically and globally.
· Fluency in English, both written and spoken.
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Deep Learning, Bioinformatics, Biomolecular, Machine Learning, Python, Biopython, NumPy, Cloud Computing","deep learning, bioinformatics, biomolecular, machine learning, python, biopython, numpy, cloud computing","bioinformatics, biomolecular, biopython, cloud computing, deep learning, machine learning, numpy, python"
Sr. Data Governance Analyst,Accounting Career Consultants & HR Career Consultants,"St Louis, MO",https://www.linkedin.com/jobs/view/sr-data-governance-analyst-at-accounting-career-consultants-hr-career-consultants-3768713926,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Why is This a Great Opportunity?
-great boss
-autonomy in role
-tremendous flexibility
-high-level exposure role
-tons of growth
-top company
-strong comp package
Job Description:
-Expertise in Data Management: Demonstrate a profound understanding of master data management, data standards, data quality, and data lineage, ensuring a comprehensive approach to Data Governance.
-Partnership and Documentation: Collaborate closely with functional stakeholders to develop and document Data Governance best practices, standards, principles, and policies that align with organizational objectives.
-Data Quality Reporting: Contribute to the development of data quality reports and dashboards, providing valuable insights to stakeholders and ensuring the health of data aligns with business goals.
-Continuous Improvement: Embrace a continuous improvement mindset, identifying opportunities to enhance processes and systems that align with Data Governance policies and standards.
-Promotion of Data Governance Tools: Actively facilitate and promote the adoption of data governance tools, processes, and procedures among functional business users, ensuring widespread understanding and compliance.
Qualifications:
-2+ years of data governance experience
-self-starter
-strong
Show more
Show less","Data Management, Data Governance, Master Data Management, Data Standards, Data Quality, Data Lineage, Partnership, Documentation, Data Quality Reporting, Dashboards, Continuous Improvement, Data Governance Tools, Data Governance Policies, Data Governance Standards","data management, data governance, master data management, data standards, data quality, data lineage, partnership, documentation, data quality reporting, dashboards, continuous improvement, data governance tools, data governance policies, data governance standards","continuous improvement, dashboard, data governance, data governance policies, data governance standards, data governance tools, data lineage, data management, data quality, data quality reporting, data standards, documentation, master data management, partnership"
Senior Staff AI Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087762,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, ML, NLP, LLM, Data engineering, Data mining, Data cleaning, Data normalizing, Data modeling, Data platforms, Data frameworks, Data processing, Data pipelines, Data governance, Data risk, Data compliance, Data infrastructure, Statistical analysis, Data visualization, Pandas, R, Conversational AI, Recommender systems, Distributed systems, Microservices, Docker images, Streamprocessing systems","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, ml, nlp, llm, data engineering, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, data processing, data pipelines, data governance, data risk, data compliance, data infrastructure, statistical analysis, data visualization, pandas, r, conversational ai, recommender systems, distributed systems, microservices, docker images, streamprocessing systems","airflow, aws, azure, bash, conversational ai, data cleaning, data compliance, data engineering, data frameworks, data governance, data infrastructure, data mining, data normalizing, data platforms, data processing, data risk, datamodeling, datapipeline, distributed systems, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, llm, microservices, ml, nlp, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, streamprocessing systems, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"St Louis, MO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709551,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML Data Engine, ML Data Ops, Data pre/post processing, ML models, Data mining, Data cleaning, Data normalizing, Data modeling, Data platforms, Data frameworks, Big data, Data governance, Data risk, Data compliance, Data infrastructure, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, SQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","data engineering, ml data engine, ml data ops, data prepost processing, ml models, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, big data, data governance, data risk, data compliance, data infrastructure, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, sql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, aws, azure, bash, big data, data classification, data cleaning, data compliance, data engineering, data frameworks, data governance, data infrastructure, data management, data mining, data normalizing, data platforms, data prepost processing, data retention, data risk, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, ml data engine, ml data ops, ml models, python, snowflake, spark, sparkstreaming, sql, storm"
Data Migration Analyst - Hybrid,"Swank Motion Pictures, Inc.","St Louis, MO",https://www.linkedin.com/jobs/view/data-migration-analyst-hybrid-at-swank-motion-pictures-inc-3766477789,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Swank Motion Pictures is looking for a technical business analyst to join an existing and growing team working on innovative and leading-edge technologies. We are looking for someone with strong data analysis skills necessary to work on multiple projects simultaneously with minimal oversight in an agile/iterative IT environment. These projects are primarily custom software development and business process improvements. This position is a liaison between non-technical business stakeholders and technology teams. This position is in the IT department and will interact with multiple levels of management within the company. This position is intended to be a year-long position with the opportunity of becoming a permanent role within the group. At Swank Motion Pictures, we are not just looking for employees; we are investing in future leaders. For the Data Migration Analyst role, we offer a pathway for professional growth and development. As our company evolves, there will be abundant opportunities for you to expand your skill set, take on new challenges, and advance your career. We are committed to fostering a culture of learning and innovation where your contributions will be recognized and your professional aspirations supported.
Responsibilities
This position has the primary responsibility to analyze corporate data and assist in its migration from one system to another. Corporate data includes but is not limited to accounts, contacts, agreements, orders, and invoices. The data migration analyst will elicit, verify, and document business-defined data migration rules; analyze data based on those rules for consistency, cleanliness, and outliers; perform quality control checks against data transfer files; and share findings from those checks with the data migration team. The Data Migration Analyst will be expected to develop and maintain comprehensive documentation, including data mappings, transformation rules, and workflow diagrams, to ensure clarity and consistency throughout the data migration process. This individual will be an integral part of a team consisting of data architects, system specialists, and project managers. The Data Migration Analyst will collaborate closely with data governance teams to uphold meaningful data quality and compliance standards across all stages of the migration process. This position is a hybrid position – the individual must be willing to be in the office at least 2 days per week.
Requirements
Skills Needed
Interpersonal skills to negotiate priorities and collaborate with both business and IT peers
Interview skills to ask the proper questions for gathering essential requirements and listen attentively to their feedback
Analytical skills to critically evaluate data of different types, discern data patterns, and identify data outliers using a high level of attention to detail
Communication skills to effectively share ideas and requirements with both technical and non-technical audiences through meetings, working group sessions, whiteboard sessions using data visualization skills and too
Creativity skills to be flexible and think outside the box when solving problems
Organizational skills to meet deadlines, ensure quality deliverables, and cope with rapidly changing information in a hybrid work environment
Experience Needed
3 to 5 years of experience executing business analysis with a focus on data analysis and migration
Experience working with and analyzing large amounts of data via Excel – SQL experience is nice to have but not required
An understanding of best practices for eliciting, analyzing, documenting, validating, and managing requirements, along with knowing when to apply them
Experience facilitating meetings with both business and IT stakeholders from any level within the organization
Experience eliciting requirements via one-on-one interviews, group meetings, brainstorming sessions, and other methods as needed
Experience collaborating with Project Management, Development, and Quality Assurance personnel on software development projects
Detailed expertise using Microsoft Word, PowerPoint, and Excel a must; familiarity with Atlassian Jira and Confluence preferred but not required
Educational Requirements
Bachelor’s degree in technology related field required
Benefits
We are pleased to offer:
Comprehensive compensation and healthcare packages, including medical, dental, vision, and life insurance products
401(K) plan with employer match
Competitive paid time off: vacation, personal time, holidays and winter break
Company sponsored volunteer & community outreach opportunities
Organizational growth potential through our company sponsored online learning platform
EOE, including disability/vets
Show more
Show less","Data Analysis, Agile/Iterative IT, Custom Software Development, Business Process Improvement, Data Migration, Data Mapping, Transformation Rules, Workflow Diagrams, Data Governance, Interpersonal Skills, Negotiation, Interview Skills, Analytical Skills, Communication Skills, Creativity, Organizational Skills, Business Analysis, Excel, SQL, Requirements Gathering, Microsoft Word, PowerPoint, Jira, Confluence, Bachelor's in Technology","data analysis, agileiterative it, custom software development, business process improvement, data migration, data mapping, transformation rules, workflow diagrams, data governance, interpersonal skills, negotiation, interview skills, analytical skills, communication skills, creativity, organizational skills, business analysis, excel, sql, requirements gathering, microsoft word, powerpoint, jira, confluence, bachelors in technology","agileiterative it, analytical skills, bachelors in technology, business analysis, business process improvement, communication skills, confluence, creativity, custom software development, data governance, data mapping, data migration, dataanalytics, excel, interpersonal skills, interview skills, jira, microsoft word, negotiation, organizational skills, powerpoint, requirements gathering, sql, transformation rules, workflow diagrams"
Voice Data Communications Engineer III,"All Native Group, The Federal Services Division of Ho-Chunk Inc.","Scott AFB, IL",https://www.linkedin.com/jobs/view/voice-data-communications-engineer-iii-at-all-native-group-the-federal-services-division-of-ho-chunk-inc-3724381768,2023-12-17,Belleville,United States,Mid senior,Hybrid,"Summary
All Native Group is looking for a Voice/Data Communications Engineer I provide technical direction and engineering knowledge for communications activities including planning, designing, developing, testing, installing, and maintaining large communications networks.
Essential Functions
Installs and maintains voice, wireless, video, and data communications systems.
Coordinates with users to determine requirements.
Applies fundamental telco and data network installation and maintenance concepts, processes, practices, and procedures on technical assignments.
Supervisory Responsibility
None required for this position
Competencies
Network Installation (Cable) Experience (Inside and/or Outside Plant)
Network (Cable) Termination and Splicing Experience (Fiber and/or Copper)
Network (Cable) Maintenance & Inspection Experience (Inside and/or Outside Plant)
Work Environment
Outside environment and inside environment with some work in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines. Work may also require the use of tools and machinery required to install, splice, and terminate network infrastructure (e.g., cables, etc.).
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Must be able to remain in a stationary position 75% of the time.
Occasionally moves about inside the office to access
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
Expresses or exchanges ideas by means of the spoken word. Those activities in which they must convey detailed or important spoken instructions to other workers accurately, loudly, or quickly.
Frequently moves standard office equipment up to 25 pounds.
Must be able to work indoor conditions 90% of the time.
While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand; walk; use hands to finger, handle or feel; and reach with hands and arms.
Position Type/Expected Hours of Work
This is a full-time position. Typical days and hours of work are Monday through Friday, 8:00 a.m. to 5:00 p.m. Evening and weekend hours required, as required by business need.
Travel
No travel is required for this position.
Experience
A minimum of 5-7 years of relevant experience is required.
Education
Associates Degree, Bachelors or higher desired.
One or more of the following certifications are required:
DURALINE
CTNS
iNARTE
IPEP
NCTI
RCDD
Security Clearance
Active Secret Security Clearance is required
Executive Order 14043 COVID-19 Vaccination Requirement
Pursuant to Executive Order 14043, this position requires full vaccination against COVID-19 regardless of work location.
AAP/EEO Statement
All Native Group is an equal opportunity employer. All applicants are considered without regard to age, sex, race, national origin, religion, marital status, or physical disability. However, preference may be extended to persons of Indian descent in accordance with applicable laws.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with
or without notice.
Apply Now
Show more
Show less","Telecommunication, Network Installation, Network Maintenance, Network Inspection, Network Termination, Network Splicing, Fiber Optics, Copper Cabling, DURALINE, CTNS, iNARTE, IPEP, NCTI, RCDD","telecommunication, network installation, network maintenance, network inspection, network termination, network splicing, fiber optics, copper cabling, duraline, ctns, inarte, ipep, ncti, rcdd","copper cabling, ctns, duraline, fiber optics, inarte, ipep, ncti, network inspection, network installation, network maintenance, network splicing, network termination, rcdd, telecommunication"
Data Scientist- Journeyman,ECS,"St Louis, MO",https://www.linkedin.com/jobs/view/data-scientist-journeyman-at-ecs-3784918694,2023-12-17,Belleville,United States,Mid senior,Hybrid,"ECS is seeking a
Data Scientist- Journeyman
to work in our
St Louis, MO
office .
Job Description:
ECS is seeking a Data Scientist- Journeyman to work in our St Louis, MO office. This offer is contingent upon acceptance by the government customer, validation of appropriate clearances and approval from a cognizant government contracting officer. Candidate will be submitted for Counterintelligence Polygraph after customer indoctrination. The primary role for these data analysts are to provide support to data management and automation activities as part of SFP’s technical operations team. The scope of work aligns to SFP’s efforts to modernize and advance requirements management and production feedback capabilities. The contractor shall support the office in modernizing the Community’s requirements management process, to include automation, data management, statistical analysis, data visualization, training, and process documentation. The primary result of this effort will be the implementation of enhanced requirements management capabilities in order to support Community stakeholders and SF leadership.
The contractor shall:
Work in a team environment and interact with production offices, analysts, and data owners as required.
Advise and consult SF stakeholders on data management best practices.
Assist SF analysts and external partners on data management procedures and best practices.
Implement enterprise database management processes.
Perform data management tasks, to include data manipulation.
Perform data analysis to assess integrity, identify patterns, and determine and correct shortfalls.
Develop and document data management workflows.
Transfer data between multiple formats.
Automate processes through the use of Python scripting and model building.
Perform statistical analysis of data to identify trends and patterns and to build reports for SF leadership and IC community (or NSG).
Provide advanced data visualization to convey findings and communicate insights utilizing capabilities such as IC Portal and other dynamic viewers.
Independently identify, document, and address technical challenges.
Deliver schema mapping and entity resolution across disparate geospatial data sources through data scientist applied expertise in the subject.
Understand and communicate the interrelationships across many disparate datasets.
Document and visualize data both temporally and spatially to assist in data integrity checks, ask the next question, and display analytical assessments.
Develop and apply methods to identify, collect, process, and analyze large volumes of data to build and enhance GEOINT processes, and systems.
Provide support to produce GEOINT web services publication to develop application-ready content, support GEOINT content metadata-tagging to enable content discovery; and publish GEOINT web services.
Implement ESRI Story Maps in a classified environment.
Work with ESRI Portal technology to develop and maintain map services.
Support and manage geodatabase versioning workflows.
Support, manage, and automate geodatabase synchronization workflows.
Required Skills:
Active TS/SCI CI-Poly Clearance.
Minimum of a Bachelor’s Degree or 4+ years of experience.
Demonstrated advanced proficiency with the following programs:
Python o ArcGIS, ArcSDE, ArcPro, ArcGIS Online.
PostgreSQL, Postgis, PgAdmin.
Application programming interfaces (APIs).
Demonstrated advanced proficiency of database principles and technology.
Demonstrated advanced data transformation skills.
Demonstrated advanced data processing and analytic skills.
Demonstrated advanced experience with enterprise geodatabase management.
Demonstrated advanced experience geodatabase versioning workflows.
Demonstrated advanced experience with geodatabase synchronization and integration.
Demonstrated advanced experience with Python (i.e. numpy, pandas, matplotlib libraries).
Demonstrated advanced experience in SQL and NoSQL technologies.
Demonstrated advanced experience text parsing.
Demonstrated advanced experience in metadata tagging (data storage, processing, and & analyzing).
Demonstrated advanced experience in work with a variety of geospatial data formats.
Desired Skills:
Demonstrated experience with JAVA programming language.
Demonstrated experience with cloud-based database solutions.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Python, ArcGIS, ArcSDE, ArcPro, ArcGIS Online, PostgreSQL, PostGIS, PgAdmin, Application programming interfaces (APIs), SQL, NoSQL, Geodatabase, Geospatial data formats, JAVA, Cloudbased database solutions, Data management, Data analysis, Data visualization, Data manipulation, Statistical analysis, Data integration, Data transformation, Metadata tagging, Geospatial data","python, arcgis, arcsde, arcpro, arcgis online, postgresql, postgis, pgadmin, application programming interfaces apis, sql, nosql, geodatabase, geospatial data formats, java, cloudbased database solutions, data management, data analysis, data visualization, data manipulation, statistical analysis, data integration, data transformation, metadata tagging, geospatial data","application programming interfaces apis, arcgis, arcgis online, arcpro, arcsde, cloudbased database solutions, data integration, data management, data manipulation, data transformation, dataanalytics, geodatabase, geospatial data, geospatial data formats, java, metadata tagging, nosql, pgadmin, postgis, postgresql, python, sql, statistical analysis, visualization"
Senior Data Engineer,Kforce Inc,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3775449352,2023-12-17,Utah,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is looking for a Senior Data Engineer in Salt Lake City, UT. Job Summary: We are currently seeking an experienced Senior Data Engineer to join our dynamic team. The ideal candidate will possess a strong technical skill set in data engineering. A proven track record in designing, developing, and deploying data-intensive solutions in cloud environments is essential. This role involves utilizing languages such as Python and SQL to construct data pipelines, prioritizing data quality and resiliency. The Senior Data Engineer will play a pivotal role in ensuring the reliability, efficiency, and maintainability of data pipeline infrastructure, contributing significantly to the evolution of our data capabilities.
Requirements
Degree in a required field
5+ years of experience as a Data Engineer
Hands-on experience with orchestration and pipeline tools like Apache Airflow, Azure Data Factory, Prefect, Fivetran, or Mage
Experience with cloud platforms
Expertise in Snowflake
Proficiency in data engineering languages such as Python, SQL, Java, or Scala
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $140,000 - $150,000 per year
Show more
Show less","Data Engineering, Python, SQL, Cloud Platforms, Snowflake, Apache Airflow, Azure Data Factory, Prefect, Fivetran, Mage, Java, Scala","data engineering, python, sql, cloud platforms, snowflake, apache airflow, azure data factory, prefect, fivetran, mage, java, scala","apache airflow, azure data factory, cloud platforms, data engineering, fivetran, java, mage, prefect, python, scala, snowflake, sql"
Lead Data Analyst,Serenity Healthcare,"Lehi, UT",https://www.linkedin.com/jobs/view/lead-data-analyst-at-serenity-healthcare-3728004829,2023-12-17,Utah,United States,Associate,Onsite,"Serenity Healthcare is looking for a seasoned data analyst for our Lehi, UT headquarters. Title will depend on prior experience. Preference given to those with full time experience in SQL, building/managing data pipelines, and Exploratory Data Analysis.
This is an onsite position located in Lehi, UT
Desired Skillsets
SQL – the most important skillset (we use SQL SERVER)
SSIS – previous experience making and managing packages.
PowerBI
Day-to-day Work Description
Using automated tools to extract data from primary and secondary sources
Removing corrupted data and fixing coding errors and related problems
Developing and maintaining databases, data systems – reorganizing data in a readable format
Performing analysis to assess quality and meaning of data
Filter Data by reviewing reports and performance indicators to identify and correct code problems
Using statistical tools to identify, analyze, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction
Assigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.
Preparing reports for management stating trends, patterns, and predictions using relevant data
Job Fit
Capable of “Deep work”
Problem Solver
Reliable, consistent, and thorough
What We Offer To You
Competitive pay, including additional target compensation
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.
Show more
Show less","SQL, SSIS, PowerBI, Data extraction, Data cleaning, Database development, Data analysis, Data interpretation, Statistical analysis, Reporting, Problem solving, Reliability, Consistency, Thoroughness","sql, ssis, powerbi, data extraction, data cleaning, database development, data analysis, data interpretation, statistical analysis, reporting, problem solving, reliability, consistency, thoroughness","consistency, data cleaning, data extraction, data interpretation, dataanalytics, database development, powerbi, problem solving, reliability, reporting, sql, ssis, statistical analysis, thoroughness"
Senior Data Engineer,Jobot,"Lehi, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobot-3786066516,2023-12-17,Utah,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Join Jobot as a Senior Database Engineer and revolutionize auto financing! Work with a talented team in Silicon Slopes, enjoy a 4-day work week, and help scale our data infrastructure to the next level. Apply now!
This Jobot Job is hosted by Jeff Sorensen
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $85,000 - $110,000 per year
A Bit About Us
Join our dynamic and thriving FinTech startup based in the vibrant Silicon Slopes region, where innovation meets opportunity! We're on the lookout for a seasoned Senior Database Engineer to play a pivotal role in shaping the future of auto financing without mentioning Lease End.
As a key member of our talented team, you'll have the unique opportunity to contribute to the evolution of our data infrastructure and performance. In this role, you'll be the driving force behind database design, performance optimization, and the establishment of industry-leading standards. Your expertise will extend to designing, developing, and enhancing various back-end services.
Tech Stack
React
Node.js
TypeScript
PostgreSQL
GraphQL
AWS (CloudFront, ECS, Aurora, SQS, Lambda)
Responsibilities
Lead the architectural, performance, scalability, and security aspects of our databases
Take charge as a senior engineer, focusing on the development and deployment of back-end data-related services
Fine-tune and establish best practices for queries, indexing, partitioning, and read/write efficiency
Guide and actively participate in the database design and architecture for new product initiatives
Troubleshoot and perform root cause analysis on database and application issues
Establish deployment procedures in collaboration with the infrastructure team
Maintain monitoring and alerting for databases, contributing to a rotating on-call schedule
Ensure adherence to best practices for AWS database infrastructure and services
Oversee data backup processes and contribute to disaster recovery planning
Cultivate a positive culture of innovation and problem-solving
Why join us?
Collaborate with a talented team in a dynamic startup environment
Work with cutting-edge technologies and innovate in the AI and DeFi space
Enjoy a 4-day work week for improved work-life balance
Opportunity to make a significant impact and contribute to the company's growth
Competitive salary and benefits package
Job Details
Qualifications
Must live in Utah (NO REMOTE CANDIDATES - Only Utah Based)
Must be a US Citizen
Graduate degree or equivalent real-world experience in Software Engineering
8+ years of professional experience specializing in relational databases, back-end data-related services, and SQL tuning
Proficiency in Node.js, TypeScript, PostgreSQL, and GraphQL
Solid experience managing relational databases on AWS infrastructure (RDS, Aurora, etc.)
Advanced communication skills, both verbal and written
Detail-oriented with critical thinking skills
Experience with AI/ML technologies is a plus
Ability to thrive in a fast-paced startup environment
If you're passionate about cutting-edge technology, problem-solving, and making a meaningful impact in the FinTech space, we invite you to be part of our innovative journey. Join us and be at the forefront of revolutionizing auto financing!
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","React, Node.js, TypeScript, PostgreSQL, GraphQL, AWS (CloudFront ECS Aurora SQS Lambda), SQL, AWS (RDS Aurora), Relational databases, Software Engineering, SQL tuning, Critical thinking, AI/ML, FinTech","react, nodejs, typescript, postgresql, graphql, aws cloudfront ecs aurora sqs lambda, sql, aws rds aurora, relational databases, software engineering, sql tuning, critical thinking, aiml, fintech","aiml, aws cloudfront ecs aurora sqs lambda, aws rds aurora, critical thinking, fintech, graphql, nodejs, postgresql, react, relational databases, software engineering, sql, sql tuning, typescript"
Senior Data Engineer,Strider Technologies,"South Jordan, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-strider-technologies-3725127890,2023-12-17,Utah,United States,Mid senior,Onsite,"Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to nation-state directed IP theft and supply chain vulnerabilities.
The purpose of Strider is to protect the ideals and innovations of the free world. If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Job Description
Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to foreign government directed IP theft and supply chain vulnerabilities. We are a data company. We have billions of documents online and are collecting millions of new documents each day. The Data Engineering team is responsible for building structured data assets from these raw documents to build new products and enhance existing ones.
If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Responsibilities For This Role
Develop large scale data processing pipelines
Create data assets from unstructured data
Build and support internal and external facing APIs
Author design documents
Mentor teammates
Drive software engineering best practices
Participate in code reviews
Collaborate with a talented cross-functional team of engineers, data scientists and subject-matter experts
You are a good fit for this role if you:
Have 4-5+ years of software engineering experience
Previous experience working in a data heavy role
Understand how to use cloud infrastructure effectively
Are a natural problem solver with an affinity for data
Are opinionated about how software is built
Are proficient at breaking down large, sometimes ambiguous, problems into well-defined tasks
Value shipping code early and often
Have a well-honed mental model for how software systems execute and interact
Technologies We Use
Python
Elasticsearch
Memgraph
Flyte
AWS (EC2, S3, SQS, DynamoDB, Lambda, EKS)
Benefits
Competitive Compensation
Company Equity Options
Flexible PTO
Wellness Reimbursement
US Holidays (Office Closed)
Paid Parental Leave
Comprehensive Medical, Dental, and Vision Insurance
401(k) Plan
Strider provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Strider complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Learn more about us by visiting https://www.striderintel.com/
Show more
Show less","Software Engineering, Data Engineering, Problem Solving, Data Processing, Cloud Infrastructure, APIs, Design Documentation, Mentorship, Code Reviews, CrossFunctional Collaboration, Python, Elasticsearch, Memgraph, Flyte, AWS, EC2, S3, SQS, DynamoDB, Lambda, EKS","software engineering, data engineering, problem solving, data processing, cloud infrastructure, apis, design documentation, mentorship, code reviews, crossfunctional collaboration, python, elasticsearch, memgraph, flyte, aws, ec2, s3, sqs, dynamodb, lambda, eks","apis, aws, cloud infrastructure, code reviews, crossfunctional collaboration, data engineering, data processing, design documentation, dynamodb, ec2, eks, elasticsearch, flyte, lambda, memgraph, mentorship, problem solving, python, s3, software engineering, sqs"
Staff Data Engineer,Strider Technologies,"South Jordan, UT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-strider-technologies-3725130477,2023-12-17,Utah,United States,Mid senior,Onsite,"Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to nation-state directed IP theft and supply chain vulnerabilities.
The purpose of Strider is to protect the ideals and innovations of the free world. If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Job Description
Strider is revolutionizing the way companies, universities, research institutions, and government agencies protect their innovation and compete in a new era of global strategic competition. Our trailblazing technology and intelligence solutions enable clients to proactively identify, manage, and respond to foreign government directed IP theft and supply chain vulnerabilities. We are a data company. We have billions of documents online and are collecting millions of new documents each day. The Data Engineering team is responsible for building structured data assets from these raw documents to build new products and enhance existing ones.
If you’re ready to be part of an elite team tackling some of the most pressing security and technology challenges, let’s talk.
Responsibilities For This Role
Develop large scale data processing pipelines
Create data assets from unstructured data
Build and support internal and external facing APIs
Author design documents
Mentor teammates
Drive software engineering best practices
Participate in code reviews
Collaborate with a talented cross-functional team of engineers, data scientists and subject-matter experts
You are a good fit for this role if you:
Have 8+ years of software engineering experience
Previous experience working in a data heavy role
Understand how to use cloud infrastructure effectively
Are a natural problem solver with an affinity for data
Are opinionated about how software is built
Are proficient at breaking down large, sometimes ambiguous, problems into well-defined tasks
Value shipping code early and often
Have a well-honed mental model for how software systems execute and interact
Technologies We Use
Python
Elasticsearch
Memgraph
Flyte
AWS (EC2, S3, SQS, DynamoDB, Lambda, EKS)
Benefits
Competitive Compensation
Company Equity Options
Flexible PTO
Wellness Reimbursement
US Holidays (Office Closed)
Paid Parental Leave
Comprehensive Medical, Dental, and Vision Insurance
401(k) Plan
Strider provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Strider complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Learn more about us by visiting https://www.striderintel.com/
Show more
Show less","Python, Elasticsearch, Memgraph, Flyte, AWS (EC2 S3 SQS DynamoDB Lambda EKS), Software engineering, Data engineering, Data science, Cloud infrastructure, Problem solving, Software design, Collaboration","python, elasticsearch, memgraph, flyte, aws ec2 s3 sqs dynamodb lambda eks, software engineering, data engineering, data science, cloud infrastructure, problem solving, software design, collaboration","aws ec2 s3 sqs dynamodb lambda eks, cloud infrastructure, collaboration, data engineering, data science, elasticsearch, flyte, memgraph, problem solving, python, software design, software engineering"
Data - Staff Software Engineer,Backcountry,"Park City, UT",https://www.linkedin.com/jobs/view/data-staff-software-engineer-at-backcountry-3755347577,2023-12-17,Utah,United States,Mid senior,Onsite,"EMPLOYER: Backcountry.com, LLC
POSITION: Staff Software Engineer - Data
DUTIES: Provide analytic and strategic technical leadership and support to Backcountry’s Data Team which is responsible for maintaining the data platform and maturing the analytical capabilities of the organization. Act as a steward of our core platform and data pipelines that power analytical capabilities. Evolve data models in several components of the data stack and help architect, build, and launch scalable data pipelines to support BC's growing data processing and analytics needs. Collaborate with functional and business leaders and teams and work closely with the Data Engineering team and managers to enable decision support and key insights. Work with a team of high-performing analytics, data science, data engineering professionals, and cross-functional teams to identify business opportunities, monitor data platform performance and optimize analytical capabilities. Build and maintain the infrastructure required for optimal transformation and integration from a wide variety of data sources using appropriate data integration technologies. Deploy pipelines using scheduling and orchestration frameworks. Build data expertise, manage complex data systems for the Data team. Take ownership of core data pipelines that power analytical metrics. Evolve data models in several components of the data stack and help architect, build, and launch scalable data pipelines to support growing data processing and analytics needs. Implement data governance practices in partnership with business stakeholders and peers. Initiate and drive projects to completion with minimal guidance. Create proof of concepts as per business requirements. Contribute to data and engineering innovations that fuel the company’s vision and mission.
REQUIREMENTS: Bachelor’s degree or foreign equivalent in Computer Science, Electrical Engineering, Computer Engineering, Engineering, or a related field of study , and Four (4) years of experience building data intensive applications, tackling challenging architectural, scalability and reliability problems as a Software Engineer or similar role . Work experience or academic coursework must have included: Knowledge of different phases of SDLC including Analysis, High Level and Detailed Design, Development, Testing, Implementation and Production Support activities; experience building highly scalable ETL pipelines; experience with on-prem and relational database platforms including Oracle, SQL Server, PostGreSQL and Snowflake; experience with one of the cloud platforms: GCP, Azure or AWS; experience with Python; experience maintaining data quality frameworks, data observability and monitoring frameworks; knowledge of application monitoring, handling user tickets and analyzing data issues; and experience manipulating and analyzing large datasets.
JOB SITE: 1678 W. Redstone Ctr., Dr., Suite 210, Park City, UT 84098 and various unanticipated work locations. Telecommuting permitted from anywhere within the U.S.
Apply Now
Show more
Show less","Data Engineering, Analytics, Data Pipelines, Data Modeling, Data Integration, Data Governance, Software Development, Python, SQL, Oracle, SQL Server, PostgreSQL, Snowflake, GCP, Azure, AWS, ETL, Data Observability, Monitoring, Application Monitoring, Data Analysis, Data Processing","data engineering, analytics, data pipelines, data modeling, data integration, data governance, software development, python, sql, oracle, sql server, postgresql, snowflake, gcp, azure, aws, etl, data observability, monitoring, application monitoring, data analysis, data processing","analytics, application monitoring, aws, azure, data engineering, data governance, data integration, data observability, data processing, dataanalytics, datamodeling, datapipeline, etl, gcp, monitoring, oracle, postgresql, python, snowflake, software development, sql, sql server"
Senior Data Engineer,"Medifast, Inc","Lehi, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-medifast-inc-3741968685,2023-12-17,Utah,United States,Mid senior,Onsite,"About The Opportunity
At Medifast, our team members are relentless in our mission of driving Lifelong Transformation, One Healthy Habit at a Time®. When you join Medifast, you become part of a dynamic, fast-growing community of highly motivated, like-hearted people who share a passion for promoting health and wellness. Just as OPTAVIA Coaches inspire Clients to reach their personal wellness goals, at Medifast, we inspire each other to bring our best to work each day to further our shared mission. If you want to build a rewarding career that makes lives better on a daily basis, Medifast may be the perfect place for you.
Medifast is looking for a talented Senior Data Engineer to join our team in our exciting pursuit to transform and leverage a data-driven enterprise to impact every customer along their health journey. If you are creative and love building data products, we are innovating with new technologies and seeking curious, passionate, and diverse team members to drive change and impact our business. In this role, our Senior Data Engineer will collaborate across all of Medifast. While this position will exist in our data enterprise, you will partner with many amazing technologists, product managers, and internal and external business partners.
Overview Of Position
A Senior Data Engineer is responsible for developing and maintaining data processing software like databases, data structures, or algorithms to process data. Their duties include coordinating with company Executives and other professionals to create unique data infrastructure, running tests on their designs to isolate errors and updating systems to accommodate changes in company needs. They should be comfortable as both a player and a coach in this dynamic work environment, be naturally curious, and have a proven track record of mentoring and influencing senior engineers.
II. Job Responsibilities
Assembling large, complex sets of data that meet non-functional and functional business requirements
Leads the creation of software and data architecture and designs. Identifies and implements complex data products with best practices focused on greater scalability, optimizing data delivery, and automating manual processes
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
Acts as “Player and Coach” to team of software and data engineers – Mentoring, Coaching, and Collaborating
Lead projects and work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
III. Scope
Data engineering needs across business functions. Managing geographically dispersed internal/external team members.
IV. Knowledge, Education, Skills & Abilities
Bachelor’s Degree in CS, IT, or Engineering
Expert proficiency writing software to build data structures and algorithms
In-depth knowledge and experience creating high performance, fault-tolerant data and software architectures
Proven track record identifying and delivering relational and distributed database performance tuning best practices
Practitioner experience of the full data engineering lifecycle through maintenance and monitoring
Works independently and serves as aIV subject matter expert in data engineer solutions
Mentors, coaches, and acts as a resource for colleagues with less experience
5+ years of experience programming in Python
5+ years of experience programming in SQL
7+ years in data engineering focused on data enrichment, data integration and data warehouse projects
5+ years building data analytic pipelines to enable artificial intelligence or machine learning efforts
5+ years writing artificial intelligence software in an advanced analytic enterprise
5+ years writing software to build complex data pipelines between relational databases, NoSQL databases, API’s, flat files, and external sources
4+ years of AWS experience in services like RedShift, Amazon Redshift Spectrum, AWS Glue, AWS DMS, Kafka, etc.
1-3 years of Linux experience
5+ years of experience with Relational DB / NoSQL experience (PostgreSQL and Amazon Redshift or similar) with specific implementation on AWS cloud
5+ yrs experience with different DBMS like Oracle, SQL Server, MySQL, etc
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow for complex problem solving
Experience with data serialization languages such as JSON, XML, YAML
Hands-on experience with Transactional and Dimensional data modeling methodologies (eg. Normalization, Star Schema, Snowflake Schema)
5+ yrs experience with data mapping, data transformation and handling different data formats – CSV, XML, JSON, JMS
At Medifast, Relationships Are At The Center Of What We Do!
We thrive by elevating our connections with one another as well as with our Coaches & Clients. We believe that everyone has the potential to be
OUTSTANDING
. The Medifast culture is built on seven core values:
integrity, courage, teaming, accountability, empowerment, partnership and diversity
. These values aren’t just words on a page – they are celebrated as a core part of the company’s philosophy.
We Lead By…
Mastering Relationships
: We build trust, promote collaboration and we are reliable.
Being Innovative
: We strive to improve things in our areas of influence; test, refine and expand within the business strategy; and reach beyond real and perceived boundaries.
Simplifying
: We are committed to making things measurable, repeatable and scalable; focusing on outcomes not activities; and eliminating complexity to increase focus.
Anticipating
: We predict long-term business and organizational needs; challenge assumptions; and expect and prepare for the unexpected.
More About Medifast
About Medifast®:
Medifast (NYSE: MED) is the health and wellness company known for its habit-based and coach-guided lifestyle solution
OPTA
VIA®, which provides people with a simple, yet comprehensive approach to help them achieve lasting optimal health and wellbeing.
OPTA
VIA's lifestyle plans deliver clinically proven health benefits as well as evidence-based tools, including scientifically developed products and a framework for habit creation reinforced by independent Coaches and Community support. As a physician-founded company with a 40+ year history, Medifast is a leader in the U.S. weight management industry. Through a collaboration with the national virtual primary care provider LifeMD,
OPTA
VIA customers have access to board-certified affiliated clinicians and medications, such as GLP-1s, that support treatment plans for obesity and other health conditions. The company continues to innovate and build upon its scientific and clinical heritage to fulfill its mission of offering the world Lifelong Transformation, One Healthy Habit at a Time®. Medifast was recognized in 2023 by Financial Times as one of The Americas' Fastest Growing Companies and in 2022 as one of America's Best Mid-Sized Companies by Forbes. For more information, visit MedifastInc.com and
OPTAVIA.com
and follow @Medifast on X.
Thank you for taking the time to learn more about Medifast.
Show more
Show less","Python, SQL, AWS, Linux, PostgreSQL, NoSQL, JSON, XML, YAML, Airflow, Glue, Dataflow, Data mining, Data transformation, Data visualization, Data analytics, Data modeling, Database management, Software development, Data pipelines, Data architecture, Data engineering, Artificial intelligence, Machine learning, Agile methodology, Scrum, Jira, Data structures, Algorithms, Problemsolving, Communication, Teamwork, Leadership, Mentoring","python, sql, aws, linux, postgresql, nosql, json, xml, yaml, airflow, glue, dataflow, data mining, data transformation, data visualization, data analytics, data modeling, database management, software development, data pipelines, data architecture, data engineering, artificial intelligence, machine learning, agile methodology, scrum, jira, data structures, algorithms, problemsolving, communication, teamwork, leadership, mentoring","agile methodology, airflow, algorithms, artificial intelligence, aws, communication, data architecture, data engineering, data mining, data structures, data transformation, dataanalytics, database management, dataflow, datamodeling, datapipeline, glue, jira, json, leadership, linux, machine learning, mentoring, nosql, postgresql, problemsolving, python, scrum, software development, sql, teamwork, visualization, xml, yaml"
Senior Cloud Database Engineer,NICE,"Sandy, UT",https://www.linkedin.com/jobs/view/senior-cloud-database-engineer-at-nice-3757568363,2023-12-17,Utah,United States,Mid senior,Onsite,"At NICE, we don’t limit our challenges. We challenge our limits. Constantly. We’re relentless. We’re ambitious. And we make an impact. Our NICErs bring their A game and spend each day turning it into an A+. And if you’re like us, we can offer you the kind of challenge that will light a fire within you.
Sr. Cloud Database Engineer
Location: Salt Lake City, UT
The Sr. Cloud Database Engineer will ensure SQL Server database performance and availability in a 24x7 environment.
Typical Day Might Include the Following:
Install, configure, upgrade, monitor, maintain and manage multiple SQL Server databases.
Perform database and application tuning.
Establish and maintain sound backup and recovery policies and procedures.
Implement and maintain database security
Create, maintain and monitor SQL Server replication.
Perform code review and provide consultation to development teams.
Provide 24x7 support in an on call rotation with other staff members.
Setup and maintain documentation and standards.
Review, approve and create database logical and physical designs.
Integrate third party software and databases with corporate databases.
Train other staff as necessary.
Attend meetings and training as required.
To Land This Gig You'll Need:
Bachelor’s degree in Computer Science, Business Information Systems, or related field or equivalent work experience required.
5+ years maintaining SQL Server mission critical databases
5+ years TSQL programming
2+ years implementing and maintaining database replication
Ability to work with technical and non-technical people
Problem solving skills are a must
Ability to meet deadlines
Bonus Experience:
Newer data/streaming platforms: Kafka, MSK, Snowflake, Aurora DB
HP Blade and EVA SAN optimization
Veritas Net Backup
MCSE/MCDBA/OCP
NET technologies
PowerShell
C#
About NICE
NICE Ltd. (NASDAQ: NICE) software products are used by 25,000+ global businesses, including 85 of the Fortune 100 corporations, to deliver extraordinary customer experiences, fight financial crime and ensure public safety. Every day, NICE software manages more than 120 million customer interactions and monitors 3+ billion financial transactions.
Known as an innovation powerhouse that excels in AI, cloud and digital, NICE is consistently recognized as the market leader in its domains, with over 8,500 employees across 30+ countries.
NICE is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, age, sex, marital status, ancestry, neurotype, physical or mental disability, veteran status, gender identity, sexual orientation or any other category protected by law.
Show more
Show less","SQL Server, TSQL, Database replication, Veritas Net Backup, MCSE/MCDBA/OCP, NET technologies, PowerShell, C#, Kafka, MSK, Snowflake, Aurora DB, HP Blade, EVA SAN","sql server, tsql, database replication, veritas net backup, mcsemcdbaocp, net technologies, powershell, c, kafka, msk, snowflake, aurora db, hp blade, eva san","aurora db, c, database replication, eva san, hp blade, kafka, mcsemcdbaocp, msk, net technologies, powershell, snowflake, sql server, tsql, veritas net backup"
Data Engineer (1700566),Tier4 Group,"Lehi, UT",https://www.linkedin.com/jobs/view/data-engineer-1700566-at-tier4-group-3766307070,2023-12-17,Utah,United States,Mid senior,Onsite,"Job Title: Data Engineer
Location: Lehi, UT
Employment Type: Full-time, Hybrid 3 days onsite
Our client is seeking a skilled Data Engineer to join their dynamic and growing team, contributing to the forefront of technological innovation. This role involves leveraging cutting-edge AWS tools and technologies to handle large volumes of data, design and manage datasets, implement machine learning models, and drive insightful data-driven solutions.
Key Responsibilities:
Utilize AWS tools such as Kinesis, Glue, and Data Pipeline to collect, store, and process extensive data volumes efficiently.
Design, manage, and analyze large datasets, identifying patterns, anomalies, and ensuring data quality using services like Amazon S3, RDS, and Redshift.
Develop, train, and deploy machine learning models, employing ML Ops methodologies (classification, regression, clustering, NLP, CNN) through AWS SageMaker or related services.
Implement analytical algorithms focusing on predictive and prescriptive modeling.
Create visualizations and dashboards, either using tools like Amazon QuickSight or integrating with platforms such as Tableau and PowerBI.
Develop and manage DB objects like tables, procedures, triggers, and functions.
Proficiently write complex SQL queries, optimizing performance and tuning queries as necessary.
Extract data from SQL/NoSQL databases for generating ad hoc reports and analysis.
Script and program using Python and R.
Document procedures, workflows, and best practices related to data processing and modeling.
Job Specifications:
5+ years of experience as a Data Engineer, demonstrating proficiency with large datasets and data science methodologies.
3+ years of experience with SQL-based database technologies (Oracle, SQL, MySQL, Postgres), including store procedures, functions, triggers, etc.
3+ years of experience with data visualization tools such as Power BI, AWS QuickSight, etc.
3+ years of hands-on experience with cloud services, preferably AWS (SageMaker, S3, EC2, EMR, RDS, Airflow, Redshift).
Familiarity with big data tools like Hadoop, Spark, Kafka, etc.
Strong programming skills in languages like Python, Java, or Scala.
Experience with machine learning frameworks like TensorFlow or PyTorch and ETL tools is a plus.
Exceptional communication and teamwork skills.
This role offers an exciting opportunity to work in a collaborative environment, leveraging advanced technologies, and contributing to impactful data-driven solutions. If you are passionate about data engineering and thrive in a dynamic, cutting-edge tech environment, we encourage you to apply!
Show more
Show less","AWS, Kinesis, Glue, Data Pipeline, Amazon S3, Amazon RDS, Redshift, ML Ops, SageMaker, Classification, Regression, Clustering, NLP, CNN, Amazon QuickSight, Tableau, PowerBI, SQL, Pythtforms, frameworks, big data, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data compliance, data classification, data retention","airflow, aws, azure, bash, big data, data classification, data cleaning, data compliance, data engineering, data mining, data normalization, data platforms, data retention, datapipeline, docker, dynamodb, etl, frameworks, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, ml data ops, nlp, nosql databases, pandas, python, r, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization",
Lead Data Engineer,ADLIB Recruitment | B Corp™,"Exeter, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-adlib-recruitment-b-corp%E2%84%A2-3752316684,2023-12-17,Exeter, United Kingdom,Mid senior,Onsite,"Lead the creation of an impactful data platform, shaping the future of the business.
Work with a mix of advanced cloud, data and machine learning technologies.
Work remotely with an employee-owned company that encourages continuous learning with substantial training time and support for skill enhancement.
A wonderful opportunity for a Lead Data Engineer to join an industry-leading company that’s committed to making a difference. This role is perfect for those who want to make their mark in a well-established yet forward-thinking business.
In this role, you’ll leverage your strategic vision and proficiency in data manipulation to lead a dynamic team into the new frontier of cloud-based analytics and machine learning. This position is more than a job; it’s an opportunity to be part of a collective that values every voice and an enterprise that makes a real difference.
Their goal is to provide the best service to their customers and improve their lives through their product offering. In this role, you will be vital to that mission, building the data foundation for their new customer platform, and enabling data-driven decisions across the organisation.
What You’ll Be Doing
Orchestrate the migration to a state-of-the-art cloud data environment from the get go, amplifying the company’s analytics and machine learning capabilities.
Use your adeptness in data programming languages to mentor and grow your team’s technical acumen.
Design and implement strategic data initiatives that empower user-friendly analytics across the enterprise.
Contribute your voice to high-level IT strategy sessions, making pivotal decisions that affect the company’s technological path.
Instill best practices and foster an ethos of continuous learning within your department.
What Experience You’ll Need To Apply
A proven track record in managing cloud-based data warehouse projects.
Experience leading teams with a strategic and forward-thinking approach.
Strong command of data manipulation languages and tools.
Clear communicator of complex data principles to diverse audiences.
History of nurturing learning and mastery in technology or analytical fields.
What You’ll Get In Return For Your Experience
An attractive salary of £90,000, profit sharing and an array of unique benefits that truly set them apart.
Expect a plethora of resources aimed at supporting your well-being, be empowered with funded learning opportunities and embrace flexible work arrangements that honour your work-life harmony.
This is a remote first position with the requirement from the Technology Director of being available 3 days per month in the office (Exeter), if required with expenses covered. This amount won’t be the norm, but when needed for key architectural and strategy meetings with the senior leadership team, you will be available as the SME for Data.
What next?
Are you ready to shape the data landscape of a leading ethical enterprise? Apply with and up-to-date CV today.
Show more
Show less","Cloud computing, Data engineering, Machine learning, Data programming languages, Data manipulation, Data analytics, Data visualization, Data warehouse, Data architecture, IT strategy, Software development, Team leadership, Communication, Training and development, Cloudbased data warehouse projects, Data manipulation languages and tools","cloud computing, data engineering, machine learning, data programming languages, data manipulation, data analytics, data visualization, data warehouse, data architecture, it strategy, software development, team leadership, communication, training and development, cloudbased data warehouse projects, data manipulation languages and tools","cloud computing, cloudbased data warehouse projects, communication, data architecture, data engineering, data manipulation, data manipulation languages and tools, data programming languages, dataanalytics, datawarehouse, it strategy, machine learning, software development, team leadership, training and development, visualization"
Data Engineer,BJSS,"Exeter, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-2742691085,2023-12-17,Exeter, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software Engineering, Data Engineering, AWS, Azure, GCP, DataOps, Python, CI/CD, Relational Databases, NonRelational Databases, Parallel Computing, Workflow Management, SQL, NoSQL, Data Warehousing, Big Data, Cloud Computing, Microservices, Agile Development, Data Pipelines, Data Modelling, Data Mining, Data Analysis, Data Visualization, Machine Learning, Artificial Intelligence","software engineering, data engineering, aws, azure, gcp, dataops, python, cicd, relational databases, nonrelational databases, parallel computing, workflow management, sql, nosql, data warehousing, big data, cloud computing, microservices, agile development, data pipelines, data modelling, data mining, data analysis, data visualization, machine learning, artificial intelligence","agile development, artificial intelligence, aws, azure, big data, cicd, cloud computing, data engineering, data mining, data modelling, dataanalytics, dataops, datapipeline, datawarehouse, gcp, machine learning, microservices, nonrelational databases, nosql, parallel computing, python, relational databases, software engineering, sql, visualization, workflow management"
Senior Oracle Database Engineer (Requires ACTIVE Secret Security Clearance),TEKsystems,"Kansas City, MO",https://www.linkedin.com/jobs/view/senior-oracle-database-engineer-requires-active-secret-security-clearance-at-teksystems-3780295870,2023-12-17,Leavenworth,United States,Mid senior,Onsite,"Description
TEKsystems is seeking an experienced Oracle Database Administrator for a long-term contract supporting a data center local to the Kansas City area. As a Senior Oracle Database Administrator you will be responsible for planning, engineering, operating, maintaining, and problem resolution of the Oracle Database Platform as a Service capabilities. You will also work with associated hybrid cloud datacenter infrastructure and teams: virtualization; backup/storage; operating systems management; networking; and management, monitoring and automation tools. You will support system operations, hardening, migration capabilities, customer interaction and support, and patching. You will also be involved in planning and executing cloud migration and operations activities.
The program operates and provides cybersecurity services to support the client application hosting environment. The team also provides engineering and implementation services to deploy and operate the on-prem, hybrid-cloud and cloud datacenter environments. This is an opportunity to shape the way the client achieves its goals to provide enterprise application hosting while ensuring daily support and secure operations for a world-wide warfighter community.
Responsibilities
As a Senior Oracle Database Administrator you will be responsible for planning, engineering, operating, maintaining, and problem resolution of the Oracle Database Platform as a Service capabilities
You will also work with associated hybrid cloud datacenter infrastructure and teams: virtualization; backup/storage; operating systems management; networking; and management, monitoring and automation tools
You will support system operations, hardening, migration capabilities, customer interaction and support, and patching
You will also be involved in planning and executing cloud migration and operations activities
Administer Oracle Platform as a Service hosting environment (Oracle 19c on RHEL operating as virtualized databases)
Monitor, triage, respond to events, incidents, tasks, changes including working ticket queues and executing approved Data Center projects and activities across the Oracle service area
Monitor, execute and troubleshoot RMAN backup/restore
Implement and manage oracle networking with SSL/TLS
Develop, update, and troubleshoot scripts used to manage the Oracle environment
Current scripts include bash, python, and ansible
Provides cluster and datacenter database administration, operational support, and problem resolution
Collaborates with other datacenter engineers on incident, event, and problem resolution; expertise in Red Hat Enterprise Linux 7 and 8 desired; experience with VMWare virtualization and cloud platforms desired
Works with various vendors to install, upgrade, configure, administer, automate, and optimize Oracle virtualization architecture and associated software, providing a secure, reliable, and highly available data base platform to Data Center customers and services
Performs software installation and upgrade automation including scripting in bash and python as well as automation of data center efforts using VMWare tools (VMWare Ops Manager, vRealize Automation/Orchestrator)
Develops and maintains a comprehensive system event, performance, and capacity monitoring plan
Troubleshoots problems, takes appropriate corrective action and/or interacts with IT staff or vendors in performing complex testing, support and troubleshooting functions
Coordinates troubleshooting and collaborates with customers to resolve customer database operation incidents
Provides 24x7x365 on-call support in rotation with Technical Lead nights/weekends with after hours response required in support of some incidents
Supports monthly and quarterly security/application patching process
Qualifications
BS+ 8-10 MS + 5-7 years experience, will consider HS+12 years experience
Experiencing implementing DISA STIGs and security controls
Experience implementing/configuring and troubleshooting RMAN backup/restore
Experience with Oracle networking with SSL/TLS
Experience with Data Guard implementations
Senior level experience in VMWare vRealize Operations Suite (Operations Manager), HP Blade System (or similar hardware platform), Log Management tool (SysLog/Log Insight), NetBackup, Red Hat Enterprise Linux; 7.x/8.x, DISA STIGs/SRG
IAT Level II Certification (DOD 8570/8140), e.g. CompTIA Security+, CASP, CISSP, CISM, CIS,
Oracle Certification required: Oracle Database Certified Master
Active DoD SECRET Security Clearance
Must be legally eligible for employment in the US.
Skills
Database administrator, Oracle database, Oracle
Top Skills Details
Database administrator, Oracle database, Oracle
Experience Level
Expert Level
#CJ
About TEKsystems
We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.
The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.
Show more
Show less","Oracle Database Administration, Oracle Database Platform as a Service, Hybrid cloud datacenter infrastructure, Virtualization, Backup/storage, Operating systems management, Networking, Management monitoring and automation tools, System operations, Hardening, Migration capabilities, Patching, Cloud migration, Cloud operations, Cybersecurity services, Application hosting, Data Center projects and activities, RMAN backup/restore, Oracle networking with SSL/TLS, Bash scripting, Python scripting, Ansible scripting, Cluster and datacenter database administration, Red Hat Enterprise Linux 7 and 8, VMWare virtualization, Cloud platforms, Oracle virtualization architecture, VMWare Ops Manager, vRealize Automation/Orchestrator, System event performance and capacity monitoring, Troubleshooting, IT staff or vendors, Data Guard implementations, VMWare vRealize Operations Suite (Operations Manager), HP Blade System, Log Management tool (SysLog/Log Insight), NetBackup, IAT Level II Certification (DOD 8570/8140), CompTIA Security+, CASP, CISSP, CISM, CIS, Oracle Certification required: Oracle Database Certified Master, DoD SECRET Security Clearance","oracle database administration, oracle database platform as a service, hybrid cloud datacenter infrastructure, virtualization, backupstorage, operating systems management, networking, management monitoring and automation tools, system operations, hardening, migration capabilities, patching, cloud migration, cloud operations, cybersecurity services, application hosting, data center projects and activities, rman backuprestore, oracle networking with ssltls, bash scripting, python scripting, ansible scripting, cluster and datacenter database administration, red hat enterprise linux 7 and 8, vmware virtualization, cloud platforms, oracle virtualization architecture, vmware ops manager, vrealize automationorchestrator, system event performance and capacity monitoring, troubleshooting, it staff or vendors, data guard implementations, vmware vrealize operations suite operations manager, hp blade system, log management tool sysloglog insight, netbackup, iat level ii certification dod 85708140, comptia security, casp, cissp, cism, cis, oracle certification required oracle database certified master, dod secret security clearance","ansible scripting, application hosting, backupstorage, bash scripting, casp, cis, cism, cissp, cloud migration, cloud operations, cloud platforms, cluster and datacenter database administration, comptia security, cybersecurity services, data center projects and activities, data guard implementations, dod secret security clearance, hardening, hp blade system, hybrid cloud datacenter infrastructure, iat level ii certification dod 85708140, it staff or vendors, log management tool sysloglog insight, management monitoring and automation tools, migration capabilities, netbackup, networking, operating systems management, oracle certification required oracle database certified master, oracle database administration, oracle database platform as a service, oracle networking with ssltls, oracle virtualization architecture, patching, python scripting, red hat enterprise linux 7 and 8, rman backuprestore, system event performance and capacity monitoring, system operations, troubleshooting, virtualization, vmware ops manager, vmware virtualization, vmware vrealize operations suite operations manager, vrealize automationorchestrator"
Commissioning Agent - Data Center,Olsson,"Kansas City, MO",https://www.linkedin.com/jobs/view/commissioning-agent-data-center-at-olsson-3737832313,2023-12-17,Leavenworth,United States,Mid senior,Onsite,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson has a job opportunity available to work directly with the world’s largest technology companies. As a Commissioning Agent on the Data Center On-Site Services Team, you will serve as project manager on projects, performing various commissioning functions within the data center. You will also coordinate with other Olsson teams, professional staff, technical staff, and clients. You may travel to job sites for observation and attend client meetings.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor’s Degree in engineering preferred
5 or more years of engineering experience in Data Centers
Registered as a professional engineer is preferred
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","engineering, data center, project management, communication skills, teamwork, bachelor's degree, engineering experience, professional engineer, planning and design, field services, environmental, technology","engineering, data center, project management, communication skills, teamwork, bachelors degree, engineering experience, professional engineer, planning and design, field services, environmental, technology","bachelors degree, communication skills, data center, engineering, engineering experience, environmental, field services, planning and design, professional engineer, project management, teamwork, technology"
Senior Oracle Database Engineer (Requires ACTIVE Secret Security Clearance),Dice,"Kansas City, MO",https://www.linkedin.com/jobs/view/senior-oracle-database-engineer-requires-active-secret-security-clearance-at-dice-3783005090,2023-12-17,Leavenworth,United States,Mid senior,Hybrid,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, TEKsystems c/o Allegis Group, is seeking the following. Apply via Dice today!
Description:
TEKsystems is seeking an experienced Oracle Database Administrator for a long-term contract supporting a data center local to the Kansas City area. As a Senior Oracle Database Administrator you will be responsible for planning, engineering, operating, maintaining, and problem resolution of the Oracle Database Platform as a Service capabilities. You will also work with associated hybrid cloud datacenter infrastructure and teams: virtualization; backup/storage; operating systems management; networking; and management, monitoring and automation tools. You will support system operations, hardening, migration capabilities, customer interaction and support, and patching. You will also be involved in planning and executing cloud migration and operations activities.
The program operates and provides cybersecurity services to support the client application hosting environment. The team also provides engineering and implementation services to deploy and operate the on-prem, hybrid-cloud and cloud datacenter environments. This is an opportunity to shape the way the client achieves its goals to provide enterprise application hosting while ensuring daily support and secure operations for a world-wide warfighter community.
Responsibilities
As aSeniorOracle Database Administrator you will be responsible for planning, engineering, operating, maintaining, and problem resolution of the Oracle Database Platform as a Service capabilities
You will also work with associated hybrid cloud datacenter infrastructure and teams: virtualization; backup/storage; operating systems management; networking; and management, monitoring and automation tools
You will support system operations, hardening, migration capabilities, customer interaction and support, and patching
You will also be involved in planning and executing cloud migration and operations activities
Administer Oracle Platform as a Service hosting environment (Oracle 19c on RHEL operating as virtualized databases)
Monitor, triage, respond to events, incidents, tasks, changes including working ticket queues and executing approved Data Center projects and activities across the Oracle service area
Monitor, execute and troubleshoot RMAN backup/restore
Implement and manage oracle networking with SSL/TLS
Develop, update, and troubleshoot scripts used to manage the Oracle environment
Current scripts include bash, python, and ansible
Provides cluster and datacenter database administration, operational support, and problem resolution
Collaborates with other datacenter engineers on incident, event, and problem resolution; expertise in Red Hat Enterprise Linux 7 and 8 desired; experience with VMWare virtualization and cloud platforms desired
Works with various vendors to install, upgrade, configure, administer, automate, and optimize Oracle virtualization architecture and associated software, providing a secure, reliable, and highly available data base platform to Data Center customers and services
Performs software installation and upgrade automation including scripting in bash and python as well as automation of data center efforts using VMWare tools (VMWare Ops Manager, vRealize Automation/Orchestrator)
Develops and maintains a comprehensive system event, performance, and capacity monitoring plan
Troubleshoots problems, takes appropriate corrective action and/or interacts with IT staff or vendors in performing complex testing, support and troubleshooting functions
Coordinates troubleshooting and collaborates with customers to resolve customer database operation incidents
Provides 24x7x365 on-call support in rotation with Technical Lead nights/weekends with after hours response required in support of some incidents
Supports monthly and quarterly security/application patching process
Qualifications:
BS+ 8-10 MS + 5-7 years experience, will consider HS+12 years experience
Experiencing implementing DISA STIGs and security controls
Experience implementing/configuring and troubleshooting RMAN backup/restore
Experience with Oracle networking with SSL/TLS
Experience with Data Guard implementations
Seniorlevel experience in VMWare vRealize Operations Suite (Operations Manager), HP Blade System (or similar hardware platform), Log Management tool (SysLog/Log Insight), NetBackup, Red Hat Enterprise Linux; 7.x/8.x, DISA STIGs/SRG
IAT Level II Certification (DOD 8570/8140), e.g. CompTIA Security+, CASP, CISSP, CISM, CIS,
Oracle Certification required: Oracle Database Certified Master
Active DoD SECRET Security Clearance
Must be legally eligible for employment in the US.
Skills:
Database administrator, Oracle database, Oracle
Top Skills Details:
Database administrator, Oracle database, Oracle
Experience Level:
Expert Level
#CJ
About TEKsystems:
We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.
The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. Senior Oracle Database Engineer (Requires ACTIVE Secret Security Clearance)
Show more
Show less","Oracle, SQL, RMAN, SSL/TLS, Data Guard, Virtualization, Red Hat Enterprise Linux, Bash, Python, Ansible, VMWare Ops Manager, VMWare vRealize Automation/Orchestrator, NetBackup, Log Management tool, SysLog/Log Insight, Oracle Database Certified Master, DISA STIGs, SRG, IAT Level II Certification, DOD 8570/8140, CompTIA Security+, CASP, CISSP, CISM, CIS","oracle, sql, rman, ssltls, data guard, virtualization, red hat enterprise linux, bash, python, ansible, vmware ops manager, vmware vrealize automationorchestrator, netbackup, log management tool, sysloglog insight, oracle database certified master, disa stigs, srg, iat level ii certification, dod 85708140, comptia security, casp, cissp, cism, cis","ansible, bash, casp, cis, cism, cissp, comptia security, data guard, disa stigs, dod 85708140, iat level ii certification, log management tool, netbackup, oracle, oracle database certified master, python, red hat enterprise linux, rman, sql, srg, ssltls, sysloglog insight, virtualization, vmware ops manager, vmware vrealize automationorchestrator"
Senior Electrical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Kansas City, MO",https://www.linkedin.com/jobs/view/senior-electrical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3775563149,2023-12-17,Leavenworth,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a WSP company, is looking for a
Senior Electrical Engineer
for our
kW Tempe, Arizona office.
As an Electrical Engineer with us, you will design complex power and other building systems including generator plants, medium voltage distribution, uninterruptible power systems, lighting, fire alarm, and grounding.
Your Impact
Design electrical systems for buildings including lighting, receptable, general, essential, and critical electrical infrastructure
Work in a team or independently, planning and executing engineering tasks within projects
Demonstrate significant understanding of the range of services provided by the kW MCE engineering teams & related practices
Independently, support the team during design and construction stages of projects
Lead the electrical design of complex projects
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Lead and mentor entry-level and junior engineers
Collaborate and coordinate with internal project discipline team members, equipment vendors, and manufacturers
Communicate complex electrical engineering concepts and decisions to clients and stakeholders
Integrate complex electrical engineering requirements into facility designs
Interact regularly with clients to maintain current relationships and develop new relationships
Lead meetings with internal and external stakeholders
Research & recommend fundamental components identified in electrical designs
Develop equipment rooms layouts, floor plan, and one-line diagrams
Schedule equipment
Prepare short circuit, coordination and arc flash calculations
Perform construction administration tasks
Survey and evaluate existing conditions
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client and construction team members.
Required Qualifications
Bachelor’s degree in Electrical Engineering or Architectural Engineering with electrical building systems emphasis
5+ years of experience in designing electrical systems for the high performing, commercial, industrial or mission critical/data center buildings.
Excellent interpersonal skills, teamwork, and communication skills, both written and verbal
Proficiency with applicable software packages including AutoCAD, Revit, SKM, eTap or Cyme.
Knowledge of building, electrical and energy codes.
Ability to organize and present design information to project staff
Attention to detail, highly organized, self-starter
Ability to travel to project sites
Preferred Qualifications:
EIT or Registered Professional Engineer (PE), if applicable
Experience with the analysis and modeling of short circuit, coordination, and arc flash analysis
Experience with the design of highly reliable, robust and concurrently maintainable Medium and Low Voltage infrastructure
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 20%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-MO-Kansas City
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, Building Information Modeling, AutoCAD, SKM, eTap, Cyme, Short circuit analysis, Coordination analysis, Arc flash analysis, Electrical engineering, Electrical design, Power distribution, Generator plants, Uninterruptible power systems, Lighting, Fire alarm, Grounding, Mission critical environments, Data centers, Electrical codes, Energy codes","revit, building information modeling, autocad, skm, etap, cyme, short circuit analysis, coordination analysis, arc flash analysis, electrical engineering, electrical design, power distribution, generator plants, uninterruptible power systems, lighting, fire alarm, grounding, mission critical environments, data centers, electrical codes, energy codes","arc flash analysis, autocad, building information modeling, coordination analysis, cyme, data centers, electrical codes, electrical design, electrical engineering, energy codes, etap, fire alarm, generator plants, grounding, lighting, mission critical environments, power distribution, revit, short circuit analysis, skm, uninterruptible power systems"
Senior Data Insights Analyst,Cambia Health Solutions,"Utah, United States",https://www.linkedin.com/jobs/view/senior-data-insights-analyst-at-cambia-health-solutions-3770777957,2023-12-17,Manti,United States,Mid senior,Hybrid,"Remote Within WA, OR, UT or ID
Primary Job Purpose
The Senior Data Insights Analyst is responsible for developing and providing analytics. These roles provide regular reporting and analytics on the performance and value of apps, web and mobile platforms and perform ad hoc analyses that inform site optimization and user experience design. These roles are also responsible for working with our product management and client service teams to perform analyses of impact of platforms which entails analysis method design, consulting, execution of the statistical analysis, mathematical modelling and reporting.
General Functions And Outcomes
Works cross functionally to understand business objectives, and provide analyses on routine and ad hoc projects that are directly relatable to business stakeholders.
Designs and executes analytics projects (conceptualization, method, data sourcing, analysis structure, and report/data visualization).
Ensures the timely delivery of accurate project reporting and analyses.
Provides insights to clients on tracking deliverables and ad hoc analysis projects which leverage insights from consumer digital site behavior with campaign, survey and consumer activity data, and claims data.
Performs analyses that move beyond an explanation of ‘what happened’ to ‘why it happened’ and ‘what should be done’. The successful analyst turns data observations into insights, insights into recommended actions, and ultimately developing strategy and site optimization.
Provide reporting to meet stakeholder agreements.
Creates, manages and drives analyses, KPI definition/measurement and analytics in support of corporate initiatives.
Collaborates with team members to define goals and KPI’s and identifies the best way to track metrics within digital platforms to assess success of new and existing site features and functionality in order to shape the strategy of future plans.
Collaborates with various internal and external teams to identify and prioritize recommendations based on insights uncovered from data analysis.
Collaborates with team members to develop analytical and statistical methods, best practices and systemic processes that enable standardization and automation of tracking and reporting and client self-service for routine reports.
Drive the continued development of customer understanding via quantitative and qualitative methodologies as it pertains to both online behavior as well as the intersection of online behavior and all other customer touch points.
Builds models that represent the consumer digital data in a manner that promotes understanding of influences on digital behavior.
Able to provide recommendations to client on appropriate methodology.
Provide guidance to level I analysts.
Provides work team leadership for cross functional projects.
Provides review of data sourcing and manipulation strategy, analytical methods and computational integrity.
Provide mentoring for developing Digital Insights Analysts, assisting in project design, method selection and review of analyses and quality.
Able to work with clients and/or senior leadership to make recommendations which allow clients and/or leadership to gain greater understanding regarding impacts of features/ functionality.
Able to guide company wide data strategy including recommendations for enhanced data capture and how to leverage data and analytics for greater impact to company
Minimum Requirements
Whole-brain thinking: Possessing the ability to think creatively and demonstrate analytical skills, analyzing complex situations both alone and as part of a team, learning quickly and synthesizing solutions, options and action plans.
Experience with tools for data mining, statistics, analysis, and scripting (e.g., Python, R, SAS, Scala, MATLAB, Ruby).
Demonstrated experience creating complex SQL queries for standard as well as ad hoc data mining purposes, analyze large amounts of data, interpret qualitative data (research, feedback) and incorporate into analyses.
Experience with data reporting tools- Tableau, Micro-strategy, SPSS, SQL Server Reporting Service)
Demonstrated understanding of research and ability to develop methodologies in data mining and other innovative statistical/mathematical approaches.
Ability to collaborate with team members and business partners to define, develop, and deliver web and digital analytics that meet the needs of the business to manage routine operations, identify tactical decisions, and inform strategic direction.
Ability to present complex data, analysis or findings to teams in a way that is clear and understandable and supports the overall business decisions and program efforts.
Demonstrated experience analyzing consumer web, app and mobile behavior.
Experience with web analytics software- Web-trends, Google Analytics, Adobe Analytics
Ability to work independently, managing deliverable timelines and clients.
Ability to develop and produce accurate, error free final deliverables and to participate in and support a peer review culture.
Experience with statistical modelling of results to drive enhanced decision-making.
Advanced skills in data extraction and manipulation.
Experience leading work teams both internal to the department and comprised of multi-disciplinary staff.
Expert in data extraction design and manipulation, coaching junior level analysts in best practices.
Normally to be proficient in the competencies listed above
Digital Insights Analyst Senior would have a Master’s degree in Statistics, Mathematics, Computer Science, Information Technology, or Economics or related field and 7+ years of related work experience or equivalent combination of education and experience.
The expected hiring range for a Senior Data Insights Analyst is $97,000 - $132,000 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for this position is 15% . The current full salary range for this role is $91,500 - $149,000
Benefits
Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
medical, dental, and vision coverage for employees and their eligible family members
annual employer contribution to a health savings account ($1,200 or $2,500 depending on medical coverage, prorated based on hire date)
paid time off varying by role and tenure in addition to 10 company holidays
up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
one-time furniture and equipment allowance for employees working from home
up to $225 in Amazon gift cards for participating in various well-being activities. for a complete list see our External Total Rewards page.
We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
If you need accommodation for any part of the application process because of a medical condition or disability, please email CambiaCareers@cambiahealth.com. Information about how Cambia Health Solutions collects, uses, and discloses information is available in our Privacy Policy. As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our Careers site.
Show more
Show less","Data analysis, Data mining, Statistical modelling, Machine learning, Business intelligence, Data visualization, SQL, Python, R, SAS, Scala, MATLAB, Ruby, Tableau, Microstrategy, SPSS, SQL Server Reporting Service, Webtrends, Google Analytics, Adobe Analytics, Data extraction, Data manipulation, Mathematical modelling, Research methodologies, Innovation, Data integrity, Data security, Data governance, Data management, Business strategy, Site optimization, User experience design, KPI definition, Measurement, Analytics, Digital marketing","data analysis, data mining, statistical modelling, machine learning, business intelligence, data visualization, sql, python, r, sas, scala, matlab, ruby, tableau, microstrategy, spss, sql server reporting service, webtrends, google analytics, adobe analytics, data extraction, data manipulation, mathematical modelling, research methodologies, innovation, data integrity, data security, data governance, data management, business strategy, site optimization, user experience design, kpi definition, measurement, analytics, digital marketing","adobe analytics, analytics, business intelligence, business strategy, data extraction, data governance, data integrity, data management, data manipulation, data mining, data security, dataanalytics, digital marketing, google analytics, innovation, kpi definition, machine learning, mathematical modelling, matlab, measurement, microstrategy, python, r, research methodologies, ruby, sas, scala, site optimization, spss, sql, sql server reporting service, statistical modelling, tableau, user experience design, visualization, webtrends"
Data Analyst,Fives DyAG Corp.,"Maumee, OH",https://www.linkedin.com/jobs/view/data-analyst-at-fives-dyag-corp-3768022908,2023-12-17,Ohio,United States,Associate,Onsite,"Fives Group is seeking a Data Analyst
About Fives Group
Fives Group, headquartered in Paris, France, is an industrial engineering group founded in 1812 that designs and supplies machines, process equipment and production lines for the world's largest industrial groups including the aluminum, steel, tube and pipe, glass, automotive, aerospace, logistics, cement, and energy sectors. In 2018, Fives achieved sales of $2 billion with a team of close to 8,500 employees and a network of over 100 operational units in over 25 countries.
Summary
We are looking for a passionate Data Analyst. The successful candidate will turn data into information, information into insight and insight into business decisions. The Company processes 100,000's data points monthly across its customer, sales, and operating functions. The analyst should be able to readily assemble and organize data into prescribed weekly and monthly templates and reports. In addition, this person would assist management in identifying trends, issues, and opportunities and communicate these results effectively.
Job Responsibilities
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining data sets
Interpret data, analyze results using statistical techniques and provide ongoing reports
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
Acquire data from primary or secondary data sources and maintain databases/data systems
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and ""clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
Interpret data, analyze results using statistical techniques and provide ongoing reports
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
Acquire data from primary or secondary data sources and maintain databases/data systems
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and ""clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
Work with management to prioritize business and information needs
Locate and define new process improvement opportunities
Experience/Education/Skills
Proven working experience as a Data Analyst or Business Data Analyst
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Knowledge of statistics and experience using statistical packages for analyzing datasets. Strong Excel skills are required.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Prefer a BS Degree in Mathematics, Economics, Business, Computer Science, Information Management or Statistics OR no degree with 4-7+ years of being a Data Analyst
Shift 1st
Benefits
Hired employees will receive great benefits along with an extremely competitive compensation package. Benefits include Health, Dental, Vision, Life, Disability, Holidays, Personal Time Off and 401K with company match and more.
Show more
Show less","Data Analysis, Statistics, Data Mining, Data Segmentation, Data Acquisition, Data Modeling, Data Collection, Data Reporting, Data Visualization, ProblemSolving, Business Intelligence, Microsoft Excel, Data Analysis Software, Business Analytics, Data Interpretation, Data Dissemination, Data Quality, Data Cleaning, Data Processing, Data Interpretation","data analysis, statistics, data mining, data segmentation, data acquisition, data modeling, data collection, data reporting, data visualization, problemsolving, business intelligence, microsoft excel, data analysis software, business analytics, data interpretation, data dissemination, data quality, data cleaning, data processing, data interpretation","business analytics, business intelligence, data acquisition, data analysis software, data cleaning, data collection, data dissemination, data interpretation, data mining, data processing, data quality, data reporting, data segmentation, dataanalytics, datamodeling, microsoft excel, problemsolving, statistics, visualization"
Logistics Data Analyst,Delivered,"Middleburg Heights, OH",https://www.linkedin.com/jobs/view/logistics-data-analyst-at-delivered-3779214223,2023-12-17,Ohio,United States,Associate,Onsite,"Company Overview:
Delivered is a technology-driven company that unlocks capacity by stitching together currently available space for nationwide parcel delivery. Delivered provides high-volume e-retail brands with the ability to connect their customers with their products faster and more cost-efficiently. Additionally, the Delivered visibility layer provides customers customized delivery options that significantly reduce loss, damage, and theft, the things that can make online shopping a real pain.
Delivered’s customer experience is a world of change. When the parcel leaves a shipper’s dock, Delivered’s technology takes over. Through our sort centers, we put packages on the most efficient path to their destination through our network of middle and last-mile partners resulting in an incredible experience for the shipper and recipient. As a bonus, Delivered’s parcel orchestration process saves up to 70% on greenhouse as we utilize currently available capacity.
Delivered started as a side project for the founder, Andrew Hurst. Spending nearly 20 years in the logistics industry, Andrew saw firsthand how the industry has changed along with people's purchasing habits. He saw his current customers facing capacity issues, transit delays, and substantial price increases from a few select incumbent carriers. The real problem was that this not only affected the e-retailers, but it also affected the consumer, the buyer… people like him. As a proven entrepreneur, he set out to fix the problem e-retailers face when connecting their customers with their physical products.
Delivered is currently in the process of building our product for commercialization. With a successfully proven leadership team, Delivered is poised for high growth over the next two years as we rapidly expand our footprint across the US. With nearly 1m packages ready to move on Delivered, we are excited to start delivering for our customers.
Job Summary:
Do you have Samurai Spirit, Trailblazer Passion, and Indiana Jones Attitude? If so, come join our team! As a logistics data analyst, you will improve the efficiency of Delivered’s logistics through data collection and analysis. In this role, you will use analytical methods and tools to understand, predict, and/or control logistics operations and processes. This position requires excellent attention to detail, in-depth understanding of the supply chain process and exceptional analytics skills. You will use interpersonal and problem-solving skills to gather information from the Sales, Operations and Carrier Engagement teams to recommend ways of improving our supply chain.
Essential Job Duties:
· Collects, interprets, and analyzes various types of logistics data including availability of product, reliability of transport and delivery, and other data related to product supply chain management, sourcing, and distribution.
· Recommends and implements improvements to maximize delivery efficiency while minimizing costs.
· Monitors and collects data on current operations, including scheduling, sourcing, warehousing, production, and delivery.
· Maintains database(s) of logistics information.
· Investigates pain points within the process and develops solutions to resolve conflicts and inefficiencies.
· Creates performance reports for management to review.
· Communicates with other departments within the organization to identify efficiency issues, develop solutions, and implement changes.
· Plans and carries out large-scale projects.
· Tracks product flow from origin to delivery via web-based logistics systems and other appropriate methods of monitoring inventory and shipment vehicles.
· Performs other related duties, as assigned.
Experience/Education Requirements:
Bachelor’s degree in business management, Finance, Supply Chain Management, Logistics, or related field required.
At least 3 years of previous experience in logistics, supply chain, or transportation data analysis required.
Project management experience required.
Extensive knowledge of shipping and distribution channels.
Excellent analytical and problem-solving skills.
Excellent written and verbal communication skills.
Extensive experience in data visual software tools.
Extensive experience in working with database tooling.
Experience in writing database queries (MySQL and Postgres).
Expert in Excel.
Proficient with Microsoft Office Suite or similar software.
We are an equal opportunity employer. We maintain a policy of non-discrimination in providing equal employment to all qualified individuals regardless of race, sex, sexual orientation, gender identity, age, color, religion, national origin, disability, genetic information, protected veteran’s status, or other legally protected classification in accordance with applicable federal, state, and local law.
Show more
Show less","Data analysis, Supply chain management, Logistics, SQL, MySQL, PostgreSQL, Excel, Microsoft Office","data analysis, supply chain management, logistics, sql, mysql, postgresql, excel, microsoft office","dataanalytics, excel, logistics, microsoft office, mysql, postgresql, sql, supply chain management"
Data Scientist,Avery Dennison,"Mentor, OH",https://www.linkedin.com/jobs/view/data-scientist-at-avery-dennison-3784210727,2023-12-17,Ohio,United States,Associate,Onsite,"Company Description
Avery Dennison Corporation (NYSE: AVY) is a global materials science and digital identification solutions company that provides branding and information labeling solutions, including pressure-sensitive materials, radio-frequency identification (RFID) inlays and tags, and a variety of converted products and solutions. The company designs and manufactures a wide range of labeling and functional materials that enhance branded packaging, carry or display information that connects the physical and the digital, and improve customers’ product performance. The company serves an array of industries worldwide, including home and personal care, apparel, e-commerce, logistics, food and grocery, pharmaceuticals and automotive. The company employs approximately 36,000 employees in more than 50 countries. Reported sales in 2022 were $9.0 billion. Learn more at www.averydennison.com.
At Avery Dennison, some of the great benefits we provide are:
Health & Wellness benefits starting on day 1 of employment
Paid parental leave
401K eligibility
Tuition reimbursement
Flexible work arrangements
Employee Assistance Program eligibility / Health Advocate
Paid vacation and Paid holidays
Job Description
You will be joining Avery Dennison’s newly stood up Data Science Center of Excellence (CoE), a centralized team responsible for planning and driving the digital roadmap within Avery Dennison’s Materials Group North America division. The Data Science CoE will be filled with digital talent that will be working on the most important initiatives within Materials Group to increase operational efficiency and unlock value for our customers, in close collaboration with internal resources and external partners.
You will apply your skills and experience to systematically answer questions using data to provide actionable recommendations. You will commonly use advanced statistical analysis and machine learning techniques. You will be responsible for modeling complex problems, discovering insights and finding opportunities through the use of statistical, algorithmic, mining and visualization techniques. A successful candidate will have demonstrated experience applying advanced analytics to a variety of business situations, such that they can efficiently and effectively develop solutions to a broad array of use cases.
Your responsibilities will include:
Experimenting against data points, providing information based on experiment results and providing previously undiscovered solutions to command data challenges
Developing methodology and processes for prioritization and scheduling of projects
Formulating and leading guided, multifaceted analytic studies against large volumes of data
Analyzing problems and determining root causes
Able to distill complex results into simple, clear visualizations
Able to explain sophisticated data science concepts in an understandable manner
Interpreting and analyzing data using exploratory mathematic and statistical techniques based on the scientific method
Defining company data assets, data models, and jobs to populate data models
Coordinating research and analytic activities using various data points unstructured and structured and employ programming to clean, message, and organize the data
Coordinating with Data Engineers to build data environments providing data identified by Data Analysts, Data Integrators, Knowledge Managers, and Analysts
Working closely with all business units and engineering teams to develop strategy for long term data platform architecture
Qualifications
WHAT WE WILL BE LOOKING FOR IN YOU:
Bachelor’s, Master’s degree (preferred), or other advanced degree required in computer science, business analytics, applied mathematics, statistics, machine learning, or other data centric field
8+ years of relevant industry work experience providing advanced analytics solutions, with a focus on Forecasting based models.
Proficiency with one or more programming languages: Java, NodeJS, Python, R
Knowledgeable in Cloud and Big Data technologies: Microsoft Azure, Azure Data Lake, Databricks, Google Cloud Platform, Apache Spark
Analytics skills: descriptive statistical modeling & algorithms, machine learning algorithms, modeling dataset building, optimization, data visualization, pattern/cluster/segmentation analysis, predictive analytics, Analytics tools and Technologies: QlikView, Cognos, Visual Basic, Apache, Oracle, SQL, GraphQL, NoSQL
Outstanding record of academic achievement, professional accomplishment, project management, and leadership
Passion for building and comfort working with modern development tools and writing code collaboratively (bonus points if you have a software development or DevOps experience)
Advanced analytical knowledge of data, ability to conduct big data analysis, data conditioning experience, programming advanced computing skill
Additional experience in core analytic methods such as predictive modeling, customer segmentation/clustering, network analysis, supply chain optimization, etc.
Business sense: Good knowledge of industry perspective and ability to translate business needs into data
Project lead experience is strongly preferred
The salary range for this position is $95,325-$142,352 / year.
The hiring base salary range above represents what Avery Dennison reasonably expects to pay for this position as of the date of this posting. Actual salaries will vary within the range, and in some circumstances may be above or below the range, based on various factors including but not limited to a candidate’s relevant skills, experience, education and training, and location, as well as the job scope and complexity, responsibilities, and regular and/or necessary travel required for the position, which may change depending on the candidate pool. Avery Dennison reserves the right to modify this information at any time, subject to applicable law.
Additional Information
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or other protected status. EEOE/M/F/Vet/Disabled. All your information will be kept confidential according to EEO guidelines.
If you require accommodations to view or apply for a job, alternative methods are available to submit an application. Please contact (440)534-6000 or NA.TA.Operations@averydennison.com to discuss reasonable accommodations.
Show more
Show less","Python, Java, NodeJS, R, Microsoft Azure, Azure Data Lake, Databricks, Google Cloud Platform, Apache Spark, Data analytics, Descriptive statistical modeling, Machine learning algorithms, Modeling dataset building, Optimization, Data visualization, Pattern/cluster/segmentation analysis, Predictive analytics, QlikView, Cognos, Visual Basic, Apache, Oracle, SQL, GraphQL, NoSQL","python, java, nodejs, r, microsoft azure, azure data lake, databricks, google cloud platform, apache spark, data analytics, descriptive statistical modeling, machine learning algorithms, modeling dataset building, optimization, data visualization, patternclustersegmentation analysis, predictive analytics, qlikview, cognos, visual basic, apache, oracle, sql, graphql, nosql","apache, apache spark, azure data lake, cognos, dataanalytics, databricks, descriptive statistical modeling, google cloud platform, graphql, java, machine learning algorithms, microsoft azure, modeling dataset building, nodejs, nosql, optimization, oracle, patternclustersegmentation analysis, predictive analytics, python, qlikview, r, sql, visual basic, visualization"
Sr FinTech Analyst (Data Analytics),Associated Materials,"Cuyahoga Falls, OH",https://www.linkedin.com/jobs/view/sr-fintech-analyst-data-analytics-at-associated-materials-3786435848,2023-12-17,Ohio,United States,Associate,Remote,"Company Description
With new ownership and supportive leadership at the helm since early 2022, Associated Materials is embarking on a transformation to maximize our potential through investments in people, operations, and brands.
This role is for a skilled professional with data analytics (FinTech) experience across a spectrum of platforms ideally Power BI and programming experience in ""R"" and Python. You will be asked in the interviewing process to demonstrate your skills and proficiency in these applications.
If you want to be part of a company where your ideas and input are more than just encouraged—they’re valued—this is the place for you. At Associated Materials, you will be part of the change, and your contributions will provide an immediate and lasting impact, helping us achieve what’s possible.
Job Description
OBJECTIVES
Develop strategic relationships across various parts and levels of the business, leveraging core responsibilities of the position to create business value.
Develop and deliver timely and accurate forecast, budgets, reports and analysis for assigned business units.
Measurably improve assigned business units operational and financial performance through industry accepted financial practices.
Key Responsibilities
Design, develop, and maintain business intelligence reports and dashboards using Power BI and Foundry (Palantir), emphasizing financial data.
Regularly update AM dashboards to ensure accurate and timely information representation.
Collaborate with stakeholders to comprehend their financial data needs, transforming these requirements into interactive reports.
Address data inconsistencies and ensure data quality across all financial reports and dashboards.
Derive insights from complex financial datasets using various analytical methods.
Engage with cross-functional teams, especially finance, to align on data projects and requirements.
Spearhead data collection systems to optimize statistical efficiency and data integrity.
Analyze and interpret financial trends, presenting findings to management through comprehensive visualizations.
Oversee data integrity, security, and adherence to data protection standards.
Qualifications
Bachelor’s degree in finance/accounting is required with an MBA preferred.
5 plus years of progressive experience in FP&A role in a corporate setting.
Experience in financial reporting and variance analysis
Proficiency with delivering analysis and SG&A experience.
Experience in Oracle financials and AS 400 desirable
At least 3 years of hands-on experience with data analysis tool like Tableau, Power BI and Palantir including DAX, Power Query, with familiarity in Python or R.
Proficiency in advanced Excel functionalities, including pivot tables, VLOOKUPs, Macros and complex formulas, to efficiently manage and analyze large datasets.
Familiarity with statistical analysis methods and tools.
Proven experience in ensuring data quality, consistency, and protection.
Strong problem solving, technical and interpersonal skills.
This role is open to being remote with periodic travel - supporting both our locations in Canada (Burlington) and HQ in Cuyahoga Falls, OH.
Additional Information
A brand of Associated Materials, Alside has been a leading manufacturer of vinyl siding and accessories and vinyl windows and patio doors for the residential and commercial remodeling and new construction markets for more than seven decades.
With our strong vertical integration of manufacturing, distribution and installation, we provide a one-stop resource for high-quality building materials and streamlined operations. With headquarters in Cuyahoga Falls, Ohio, Alside owns and operates more than 100 supply centers and has seven manufacturing facilities across the U.S.
Associated Materials is an equal opportunity employer.
Show more
Show less","Power BI, Foundry (Palantir), Tableau, DAX, Power Query, Python, R, Oracle financials, AS 400, Excel, Pivot tables, VLOOKUPs, Macros, Statistical analysis, Data quality, Data consistency, Data protection, Problem solving, Technical skills, Interpersonal skills","power bi, foundry palantir, tableau, dax, power query, python, r, oracle financials, as 400, excel, pivot tables, vlookups, macros, statistical analysis, data quality, data consistency, data protection, problem solving, technical skills, interpersonal skills","as 400, data consistency, data protection, data quality, dax, excel, foundry palantir, interpersonal skills, macros, oracle financials, pivot tables, power query, powerbi, problem solving, python, r, statistical analysis, tableau, technical skills, vlookups"
Master Data Analyst,Applied Industrial Technologies,"Cleveland, OH",https://www.linkedin.com/jobs/view/master-data-analyst-at-applied-industrial-technologies-3751751325,2023-12-17,Ohio,United States,Associate,Hybrid,"The Master Data Analyst is responsible for working with Service Centers, Field Management, Finance, Supply Chain, and IT to analyze data to maximize revenue and operating efficiency as it pertains to all vendor and customer related master data. Maintain data integrity. Assist with creating requirements for changes to vendor and customer data models. The Master Data Analyst will be involved in designing and enforcing Data Governance rules and standards and provide consulting expertise to Service Centers. Problem solving and resolution for an array of data related issues. Work in a highly collaborative manner with several departments across the company.
This is a permanent full-time position at our corporate headquarters campus in the Midtown Corridor of Cleveland, OH.
Job Duties
Manage the daily operations by creating, reviewing, validating, and maintaining vendor and customer master data in SAP
Organizes and determines course of action necessary to complete master data requests within the established Service Level Agreements
Perform root-cause analysis on master data integration problems. Recommend and execute corrective action
Routinely audit data against established rule sets to ensure consistency and accuracy of information, and update data as necessary.
Assist with the design, testing, and implementation of new process and/or enhancements to the systems that support master data
Leads process standardization, data governance, cleansing activities, maintenance, and data quality improvement efforts in alignment with data strategy goals
Supports SAP deployments across the company by ensuring the data cleansing meets the global and business specific standards
Coach and advise the business through data quality procedures to ensure data is complete, accurate, and timely
Communicate timely and professionally to users, management, and senior leadership
Develop standard procedures, user guides, and other necessary training and job aid materials. Provide training / coaching as needed
Requirements
Bachelor's degree or equivalent of work experience
Up to 1 year of relevant experience
Solid analytical& problem-solving skills
Process orientation, detail focus, ability to multi-task. Sense of humor.
Results oriented. Self-Motivated. Ability to handle stress& deadlines.
Applied Industrial Technologies (NYSE: AIT)
is a leading value-added distributor and technical solutions provider of industrial motion, fluid power, flow control, automation technologies, and related maintenance supplies. Our leading brands, specialized services, and comprehensive knowledge serve MRO and OEM end users in virtually all industrial markets through our multi-channel capabilities that provide choice, convenience, and expertise.
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, gender, sexual orientation, gender identity, age, disability, protected veteran status, marital status, medical condition or any other characteristic protected by law.
If you need accommodation for any part of the employment process because of a disability, please send an email to hiring@applied.com or call 216-426-4389 to let us know the nature of your request.
Show more
Show less","SAP, Master data, Data governance, Data analysis, Data integrity, Data cleansing, Data quality, Problem solving, Analytical skills, Detail focus, Multitasking, Communication skills, Training skills, Process orientation, Sense of humor, Results oriented, Selfmotivated, Ability to handle stress & deadlines","sap, master data, data governance, data analysis, data integrity, data cleansing, data quality, problem solving, analytical skills, detail focus, multitasking, communication skills, training skills, process orientation, sense of humor, results oriented, selfmotivated, ability to handle stress deadlines","ability to handle stress deadlines, analytical skills, communication skills, data governance, data integrity, data quality, dataanalytics, datacleaning, detail focus, master data, multitasking, problem solving, process orientation, results oriented, sap, selfmotivated, sense of humor, training skills"
Senior GCP Data Engineer,IVIDTEK INC,"Westlake, OH",https://www.linkedin.com/jobs/view/senior-gcp-data-engineer-at-ividtek-inc-3767897018,2023-12-17,Ohio,United States,Mid senior,Onsite,"Sr. GCP Data Engineer
Primary Skills:
GCP big query, Python programming, Strong SQL, Teradata to GCP migration.
Required Skills
Strong experience in Spark, need to be able to tune the spark jobs.
Strong Python programmer.
Very good experience Bigdata( In GCP it is Data Proc), programming and Hadoop admin level.
Google cloud platform experience like BQ (Big Query), Dataproc, Serverless, cloud functions, GCR (Google Container Registry) etc.
Good at orchestration tolls like cloud composer
Good experience Kubernetes (GKE or EKS )
Experience in Docker images and corresponding repositories.
CI/CD experience like Jenkins.
Need good experience in how machine learning models work , what are different algorithms, how each model performance will be measured etc. Entire life cycle of machine learning model.
Show more
Show less","GCP Big Query, Python Programming, SQL, Spark, Hadoop Administration, Google Cloud Platform, Big Query, Dataproc, Serverless, Cloud Functions, GCR, Cloud Composer, Kubernetes (GKE or EKS), Docker, Jenkins, Machine Learning, Machine Learning Models, Machine Learning Algorithms, Machine Learning Performance Measurement","gcp big query, python programming, sql, spark, hadoop administration, google cloud platform, big query, dataproc, serverless, cloud functions, gcr, cloud composer, kubernetes gke or eks, docker, jenkins, machine learning, machine learning models, machine learning algorithms, machine learning performance measurement","big query, cloud composer, cloud functions, dataproc, docker, gcp big query, gcr, google cloud platform, hadoop administration, jenkins, kubernetes gke or eks, machine learning, machine learning algorithms, machine learning models, machine learning performance measurement, python programming, serverless, spark, sql"
Principal Data Engineer,Dealer Tire,"Cleveland, OH",https://www.linkedin.com/jobs/view/principal-data-engineer-at-dealer-tire-3771469518,2023-12-17,Ohio,United States,Mid senior,Remote,"Who We Are
We’re Dealer Tire, a family-owned, international distributor of tires and parts established in 1918 in Cleveland, OH. We’re laser focused on helping the world’s largest and most trusted auto manufacturers grow their tire business—in fact, we’ve sold more than 60 million tires to date. We’re a thriving company, and we’re looking for driven individuals to join our team. That’s where you come in!
The Principal Data Engineer is a thought leader, coach, and mentor on a highly skilled team of innovative data professionals who are responsible for designing and implementing our enterprise data lake, data model, and the data pipelines that feed our business and analytical systems. As a passionate learner and master of their craft, they have a deep understanding of best practices and approaches and leverage that knowledge to make their team better and empower their customers to solve business problems and gain powerful insights using high-performance datasets, enterprise analytics systems, and self-service tools such as Alteryx, R, Snowflake, Python, and Power BI.
As a Principal Data Engineer with Dealer Tire, your essential job functions will include the following:
Engineering
Contribute as member of a data engineering team, delivering data solutions, and responding to operational needs.
Effectively balance engineering with other Principal responsibilities while clearly communicating value, risks, feasibility, and capacity when making commitments with the team.
Build robust, scalable, and high-performing ETL/ELT solutions involving structured and unstructured data.
Articulate ideas and architectural concepts clearly and concisely through verbal and written communication.
Monitor and measure system and process performance and make corrections and enhancements where needed as part of a focus on continual improvement.
Collaborate with business and IT teams to implement a comprehensive and easily expandable Enterprise Data Model based on business need.
Provide on-call/after-hours support for processes and systems owned by the team on a rotating basis.
System Design
Create new design patterns to ensure maintainability, performance, and observability for data systems; evaluate proposed design patterns; and deprecate outdated patterns.
Leverage appropriate design patterns for the problem being solved, such as near-real-time/change data capture, batch processing, streaming data, etc.
Determine minimum technical acceptance criteria to ensure desired design characteristics.
Recommend priority of technical acceptance criteria to ensure the integrity of data systems and maximize business value.
Technology Strategy
Collaborate with internal and external customers, our affiliates, and other IT teams to define and implement a roadmap for the enterprise data model and the systems that support it, including the maintenance of consistent data entity and element definitions across multiple environments.
Communicate business impact of engineering needs to stakeholders.
Be a thought leader on future data usage and a catalyst for converting data into a knowledge base.
Develop budget recommendations for architectural improvements and managing technical debt.
Ensure teams understand technical debt in data systems and ETL processes and recommend approaches for remediation.
Lead estimation and forecasting of technical debt and architectural improvements to ensure a clear understanding of value, costs, and business risks.
Evangelize and democratize our data assets and empower users with self-service tools and training to enable them to leverage those assets effectively.
Team performance and growth
Advise teams on effective use of design patterns.
Coach data engineers on the effective refinement of work to ensure successful delivery.
Provide clear and direct feedback on team adherence to technical acceptance criteria.
Provide feedback on individual growth and development opportunities for other engineers.
Participate in design reviews to foster team accountability and maintain a high standard of quality.
Other Duties as Assigned.
Position Requirements:
Bachelor's degree in Computer Science, Engineering, or related field, preferred.
10+ years of experience in data engineering
Expert-level skills in querying databases (t-SQL, PL-SQL, etc.), data modeling (including data warehouse design concepts such as star schema design, etc.), data engineering, and data reverse engineering skills are required.
Strong verbal and written communication skills are required.
Ability to work cross-functionally in a fast-paced, high growth environment and manage multiple concurrent workstreams and priorities is a must.
Expertise in OLTP performance optimization via stored procedures and other techniques is a plus.
Experience with any of the following technologies is a plus: SAP Data Integrator, Syniti Data Replication, Microsoft SQL Server, SQL Server Integration Services (SSIS), Oracle Business Intelligence/Oracle Analytics Server, JD Edwards, DB2, Python, Power Shell, R, Scala, Go, Amazon AWS (EC2 instances, Redshift, lambda functions, S3, etc.), Google Analytics, Google Big Query, Snowflake, Alteryx, Power BI, and Windows and Linux servers.
General expertise in enterprise IT architecture (databases, ERP, middleware, UI, networking, infrastructure) is a plus.
Experience implementing high availability (HA) and disaster recovery (DR) solutions is a plus.
Competencies Required:
Results Orientation
Agility
Initiative
Influence
Customer Focus
Business Acumen
Learn Quickly
Recognize Implications
Apply Knowledge
Physical Job Requirements:
Continuous viewing from and inputting data into a computer screen
Sitting for long periods of time
Travel as necessary (<10%)
Drug Policy:
Dealer Tire is a drug-free environment. All applicants being considered for employment must pass a pre-employment drug screen before beginning work.
Base Pay Range:
$130,000 - 150,000
EOE Statement: Dealer Tire is an Equal Employment Opportunity (EEO) employer and does not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability (in compliance with the Americans with Disabilities Act*), or any other legally protected status, with respect to employment opportunities.
Show more
Show less","Data engineering, SQL, ETL/ELT, Data modeling, Data warehouse design, Data reverse engineering, OLTP performance optimization, Stored procedures, SAP Data Integrator, Syniti Data Replication, Microsoft SQL Server, SQL Server Integration Services (SSIS), Oracle Business Intelligence/Oracle Analytics Server, JD Edwards, DB2, Python, Power Shell, R, Scala, Go, Amazon AWS, Google Analytics, Google Big Query, Snowflake, Alteryx, Power BI, Windows servers, Linux servers, Enterprise IT architecture, High availability (HA), Disaster recovery (DR) solutions","data engineering, sql, etlelt, data modeling, data warehouse design, data reverse engineering, oltp performance optimization, stored procedures, sap data integrator, syniti data replication, microsoft sql server, sql server integration services ssis, oracle business intelligenceoracle analytics server, jd edwards, db2, python, power shell, r, scala, go, amazon aws, google analytics, google big query, snowflake, alteryx, power bi, windows servers, linux servers, enterprise it architecture, high availability ha, disaster recovery dr solutions","alteryx, amazon aws, data engineering, data reverse engineering, data warehouse design, datamodeling, db2, disaster recovery dr solutions, enterprise it architecture, etlelt, go, google analytics, google big query, high availability ha, jd edwards, linux servers, microsoft sql server, oltp performance optimization, oracle business intelligenceoracle analytics server, power shell, powerbi, python, r, sap data integrator, scala, snowflake, sql, sql server integration services ssis, stored procedures, syniti data replication, windows servers"
Data Engineer - Scala(U.S. remote),Railroad19,Greater Cleveland,https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782291788,2023-12-17,Ohio,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, S3, EMR, Hadoop, Data Engineering, Restful APIs, Linux, Python, Bash, Unix, SQL, NoSQL, Software Development, Software Architecture, Agile, Scrum","scala, spark, aws, s3, emr, hadoop, data engineering, restful apis, linux, python, bash, unix, sql, nosql, software development, software architecture, agile, scrum","agile, aws, bash, data engineering, emr, hadoop, linux, nosql, python, restful apis, s3, scala, scrum, software architecture, software development, spark, sql, unix"
Senior Cloud Data Engineer,BDO USA,"Cleveland, OH",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765473104,2023-12-17,Ohio,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, Data Warehousing, Data Modeling, SQL, Python, Java, Scala, C#, Azure, AWS, Power BI, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch and/or streaming data ingestion into a data lake, AI Algorithms/Machine Learning, Automation tools, Computer Vision based AI technologies, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored procedures, Performance tuning, Semantic Model Definition, Star Schema Construction, Tabular modeling, Fabric, Analysis Services, UiPath, Alteryx, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Qlik, Athena, Data Pipeline, Glue, SSIS, SSAS, SSRS, PySpark, Dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, machine learning, data warehousing, data modeling, sql, python, java, scala, c, azure, aws, power bi, git, devops, linux, data lake medallion architecture, batch andor streaming data ingestion into a data lake, ai algorithmsmachine learning, automation tools, computer vision based ai technologies, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, semantic model definition, star schema construction, tabular modeling, fabric, analysis services, uipath, alteryx, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, qlik, athena, data pipeline, glue, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithmsmachine learning, alteryx, analysis services, athena, automation tools, aws, aws lake formation, azure, batch andor streaming data ingestion into a data lake, bicep, business intelligence, c, computer vision based ai technologies, data definition language ddl, data lake medallion architecture, data manipulation language dml, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, fabric, functions, git, glue, java, kinesis, linux, machine learning, pandas, performance tuning, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, tabular modeling, terraform, uipath, views"
Senior Integrations Data Engineer,Sherwin-Williams,"Cleveland, OH",https://www.linkedin.com/jobs/view/senior-integrations-data-engineer-at-sherwin-williams-3761933916,2023-12-17,Ohio,United States,Mid senior,Hybrid,"Job Description
The Senior Integrations Data Engineer is responsible for the design, development, testing, and support of enterprise ETL Feeds in support of various modern architecture projects. In-depth hands-on knowledge of modern Integrations Platform solutions, file and database ETL, XML transformation, web services, data security, integrity, validity, and reliability are required at the enterprise level. Incumbents will provide technical consulting to peers, management, business analysts and technical associates and will collaborate with the architecture team and larger enterprise Integrations Community of Practice. The incumbent is expected be a reliable technical professional that can handle job responsibilities autonomously.
Responsibilities
Strategy & Planning
Ability to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Partner with the Enterprise, Domain, Solution, and System Architecture teams to interpret, modify and/or produce documented integration application frameworks or architectures for global deployments.
Submit and review technical and process improvement recommendations as a Subject Matter Expert of the technical team.
Acquisition & Deployment
Deliver integration solutions leveraging the capability of SOA, Microservice, and ETL expertise for clients across application processes while adhering to the enterprise architecture, data security and data management standards.
Take a lead role in implementing integration solutions leveraging tools within Dell Boomi, Oracle Fusion Middleware suite, and Talend Open Data Studio.
Extend core service functionality through deep Java development knowledge, including J2EE design patterns.
Assist business users develop functional requirements for integration.
Write integration test scripts and provide support during user acceptance testing.
Prepare documentation for code changes.
Mitigate significant risks associated with projects, which have a high technical complexity and/or involve significant challenges to the business.
Operational Management
Support ongoing daily use of ETL environments and Integration platforms.
Enable citizen integration patterns where practical through guide-rail development, data governance tools, and hands-on guidance.
Resolve incidents in a timely fashion with full accountability for root-cause resolution.
Incidental Functions
Research into new technologies, including tools, components, and frameworks.
Mentoring and coaching developers and/or Professional Apprentices.
Project and task management and reporting as necessary.
Schedule and lead project and communication meetings with clients.
Provide on-call support for critical web application problems and issues.
Act as a Technical Expert in the discovery and investigation of critical production problems as required.
Analyze performance of programs and takes action to correct deficiencies based on consultation with clients and approval of supervisor.
Assist with projects as may be required to contribute to efficiency and effectiveness of the work.
Participate in hiring activities and fulfilling affirmative action obligations and ensuring compliance with the equal employee opportunity policy.
Incidental travel as needed to attend conferences and training.
Work outside the standard office 7.5-hour workday may be required.
Qualifications
Formal Education & Certification
Bachelor’s degree (or foreign equivalent) in a Computer Science, Computer Engineering, or Information Technology field of study (e.g., Information Technology, Electronics and Instrumentation Engineering, Computer Systems Management, Mathematics) or equivalent experience.
Knowledge & Experience
5+ years IT experience.
5+ years experience with the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment and post-production support.
3+ years experience building file-sourced ETL feeds on Talend Open Data Studio or a similar platform.
2+ years developing ETL between at least two of the following formats: CSV, XML, JSON, Database.
2+ years developing or administering solutions in an enterprise application platform such as Java EE.
Excellent understanding of XML, XSD, XSLT, WSDL, and WSDL versioning best practices.
Excellent understanding of Enterprise RDBMS, ER Diagrams, and developing SQL programs.
Strong ability in designing reusable and highly performant solutions.
Preferred Qualifications
Experience with Oracle Fusion Middleware, including Oracle Service Bus and Oracle SOA Suite.
Experience with Dell Boomi Integration Platform-as-a-Service.
Excellent understanding of development / documentation of RESTful APIs, including documented experience developing Swagger/OpenAPI Documentation.
Experience developing, securing, and optimizing data feeds using public cloud services such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform.
Experience in creating Proxy and Business Services on an Enterprise Service Bus to handle Virtualization, Throttling and Protocol Translations.
Strong understanding of Service Oriented Architecture concepts.
Strong understanding of Microservices concepts.
Fundamental understanding of API Gateway / API Manager concepts.
Knowledge of authentication utilizing LDAP compliant technologies and authorization through SAML tokens.
Experience in creating/maintaining deployments using ANT or MAVEN scripts.
Experience in working in an Agile development environment.
Personal Attributes
Advanced analytical, conceptual, and problem-solving abilities.
Strong written and oral communication skills.
Proven ability and initiative to learn and research new concepts, ideas, and technologies quickly.
Ability to work in a team-oriented, collaborative environment.
Ability to translate an idea or need into a completed solution or work product.
Strong commitment to inclusion and diversity.
About Us
Here, we believe there’s not one path to success, we believe in careers that grow with you. Whoever you are or wherever you come from in the world, there’s a place for you at Sherwin-Williams. We provide you with the opportunity to explore your curiosity and drive us forward. Sherwin-Williams values the unique talents and abilities from all backgrounds and characteristics. All qualified individuals are encouraged to apply, including individuals with disabilities and Protected Veterans. We’ll give you the space to share your strengths and we want you show us what you can do. You can innovate, grow and discover in a place where you can thrive and Let Your Colors Show!
At Sherwin-Williams, part of our mission is to help our employees and their families live healthier, save smarter and feel better. This starts with a wide range of world-class benefits designed for you. From retirement to health care, from total well-being to your daily commute—it matters to us. A general description of benefits offered can be found at http://www.myswbenefits.com/ . Click on “Candidates” to view benefit offerings that you may be eligible for if you are hired as a Sherwin-Williams employee.
Compensation decisions are dependent on the facts and circumstances of each case and will impact where actual compensation may fall within the stated wage range. The wage range listed for this role takes into account the wide range of factors considered in making compensation decisions including skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.
The wage range, other compensation, and benefits information listed is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, with or without notice, subject to applicable law.
Sherwin-Williams is proud to be an Equal Employment Opportunity/Affirmative Action employer committed to an inclusive and diverse workplace. All qualified candidates will receive consideration for employment and will not be discriminated against based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, pregnancy, genetic information, creed, marital status or any other consideration prohibited by law or by contract.
As a VEVRAA Federal Contractor, Sherwin-Williams requests state and local employment services delivery systems to provide priority referral of Protected Veterans.
Show more
Show less","ETL, Data Security, Data Integrity, Data Validity, SOA, Microservice, Java, Talend Open Data Studio, Dell Boomi Integration PlatformasaService, RESTful APIs, Swagger/OpenAPI Documentation, Amazon Web Services, Microsoft Azure, Google Cloud Platform, LDAP, SAML tokens, ANT, MAVEN, XML, XSD, XSLT, WSDL, RDBMS, ER Diagrams, SQL","etl, data security, data integrity, data validity, soa, microservice, java, talend open data studio, dell boomi integration platformasaservice, restful apis, swaggeropenapi documentation, amazon web services, microsoft azure, google cloud platform, ldap, saml tokens, ant, maven, xml, xsd, xslt, wsdl, rdbms, er diagrams, sql","amazon web services, ant, data integrity, data security, data validity, dell boomi integration platformasaservice, er diagrams, etl, google cloud platform, java, ldap, maven, microservice, microsoft azure, rdbms, restful apis, saml tokens, soa, sql, swaggeropenapi documentation, talend open data studio, wsdl, xml, xsd, xslt"
Data Engineer,SiXworks Limited,"Herefordshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-sixworks-limited-3775973988,2023-12-17,Gloucester, United Kingdom,Associate,Hybrid,"Join the
SiXworks
Revolution as an Data Engineer
!
🌟
Welcome to SiXworks
, where innovation thrives and excitement knows no bounds!
Established in 2017, we're not your ordinary company. We're a dynamic team specialising in fail-safe, fail-fast, Agile design that empowers organizations to rapidly refine their requirements and seize IT opportunities.
Are you ready to be part of something extraordinary? We're on the lookout for Data Engineers to join our team and dive headfirst into exciting, game-changing projects!
We are looking to recruit an experienced Data Engineer to join us on fast moving, innovative and influential work that is underway in the Defence and Security Sector.
We are looking for a full-time employee but are open to candidates who wish to work as a contractor.
Flexible Working – 2 days a week in office based in Herefordshire area, induction training to be completed at Farnborough (3 days max).
Essential Skills and Qualities
Great communicator
Team player
Self-starter with drive, keen to work in a small team
Thirst for new skills
Either holds or has recently held government security clearance already, or is suitable for and willing to go through the vetting process
Experience (Essential)
Knowledge of functionalities surrounding Apache NiFi. Development of custom processors and end to end data flow management.
Programming and scripting languages, e.g. SQL, Python, Linux, Java
Experience working closely in a team/squad/tribe using agile methodologies (Scrum and/or Kanban,), practicing DevOps and Continuous Integration / Delivery.
Experience in using Elastic stack products to ingest, manage and visualise data.
Data modelling and understanding of different data structures (structured and unstructured).
Experience in using IaC tooling (e.g. Terraform, ARM, Pulumi) to deploy to Cloud environments.
Experience (Nice to have)
Experience with Hadoop Platform & Ecosystem at Enterprise scale would be nice to have.
Knowledge in administration of Big Data systems in a production / live environment
Experience deploying solutions on top of Microsoft Azure. (e.g Azure Event Hubs, Azure Data Factory, Azure DataBricks)
Knowledge of Apache Airflow
Understanding of Apache Kafka / Event-based architecture and design patterns
Benefits:
• Exciting and interesting work.
• Bonus scheme.
• Casual dress.
• Free hot and cold drinks in our own, modern kitchen/break-out.
• Secure, on-site parking, cafe, restaurant, and gym facilities.
• Company events & social hours.
• Professional Development opportunities.
• Private Healthcare.
• Contributory pension scheme.
• Maternity / Paternity pay.
• Friendly and supportive team.
Our SiXworks team is a fusion of brilliance, featuring senior operational, technical, and business leaders from various industries and the armed forces. We're also powered by a league of extraordinary IT engineers, architects, developers, and project managers. Together, we're an unstoppable force of innovation!
SiXworks’ expertise includes secure-by-design, cloud computing and services, advanced network, platform and infrastructure design, rapid application development, cross-security domain systems, multi-source data platforms, cyber vulnerability mitigation and intelligence systems. SiXworks provides supplier-agnostic, technical and business consultancy to customers, and champions opensource and best of breed technologies.
To date we have worked exclusively in the Defence and Security Sector, providing IT innovation projects and programmes, analysis and output-based consultancy.
*Direct applicants only - no agency submissions*
Show more
Show less","Apache NiFi, SQL, Python, Linux, Java, Elastic Stack, Data Structures, Terraform, ARM, Pulumi, Data Modelling, Hadoop, Azure, Apache Airflow, Apache Kafka, Eventbased Architecture","apache nifi, sql, python, linux, java, elastic stack, data structures, terraform, arm, pulumi, data modelling, hadoop, azure, apache airflow, apache kafka, eventbased architecture","apache airflow, apache kafka, apache nifi, arm, azure, data modelling, data structures, elastic stack, eventbased architecture, hadoop, java, linux, pulumi, python, sql, terraform"
Business Data Analyst,Techniche Global Ltd,"Worcester, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-techniche-global-ltd-3766811279,2023-12-17,Gloucester, United Kingdom,Mid senior,Onsite,"Techniche Global are supporting an engineering defence contractor to appoint a Business / Data Analyst supporting People Analytics team with the gathering, analysis, summarizing and circulating of people metrics and data to HR and the business. You will perform a critical role in transforming people data analytics capability across the business, gaining a unique insight into the workforce and engaging key stakeholders.Rate: £27.75p/hr UMB or £21.64p/hr PAYE + holiday + pension (inside IR 35);Location: home based with very occasional visits to company offices for meetings (various office hubs across the UK)Duration: 12 months initiallyRequirements: due to the nature of the role we are looking for sole UK nationals living and working in the UK for the past 5 yearsYour proactive information and reporting will impact future business decisions; driving the transition of the people analytics capability from ad-hoc reporting into a predictive and trend-focused approach.Key Responsibilities:* Engage with stakeholders at various levels in the business across multiple functions to support the strategic workforce planning for the organisation* Support the interpretation and delivery of the customers requirement from concept to solution.* Supporting the gathering, analysis, summarizing and circulating of people metrics and data to HR and the business.* Identify gaps in data provision and reporting mechanisms and work with team members to address gaps.* Cleaning and dissecting data to get rid of irrelevant information* Assist with the data quality assurance processes that support analytics and reporting* Design and implement automated tools using coding languages such as SQL & DAX for use in data processing and for ad hoc reporting requirements.* Utilise reporting tools such as Power BI to deliver meaningful BI dashboards that add interpretative value to raw data.* Designing, creating and maintaining databases and data systems* Analysing and interpreting results using statistical tools and techniques* Pinpointing trends and patterns in data sets* Providing data reports for management* Identifying new opportunities for process improvement* Fixing code problems and data-related issues* Undertake project work or special tasks in accordance with business requirements* Uphold adherence to the Health, Safety & Environmental Polices.Required Knowledge, Skills, Experience and CompetencesEssential:* Advanced excel skills* Good analytical and problem-solving skills* Relevant experience in BI Reporting* Broad understanding of software development life cycle and IT delivery approaches* Experience of data manipulation, management and reporting;* Good communication and influencing skills* Strong interpersonal skills* Good attention to detailDesirable:* Proven skills using Power BI * Project management experience* Knowledge of Success factors* Knowledge of SharePointQualifications* Degree level qualification in IT, science, maths or related subject
Show more
Show less","Data Analysis, Data Visualization, Business Intelligence, SQL, DAX, Power BI, Database Management, Data Interpretation, Statistical Analysis, Trend Analysis, Data Reporting, Process Improvement, Excel, Analytical Skills, Problem Solving, Communication Skills, Interpersonal Skills, Attention to Detail, Project Management, SuccessFactors, SharePoint, Software Development Life Cycle, IT Delivery Approaches","data analysis, data visualization, business intelligence, sql, dax, power bi, database management, data interpretation, statistical analysis, trend analysis, data reporting, process improvement, excel, analytical skills, problem solving, communication skills, interpersonal skills, attention to detail, project management, successfactors, sharepoint, software development life cycle, it delivery approaches","analytical skills, attention to detail, business intelligence, communication skills, data interpretation, data reporting, dataanalytics, database management, dax, excel, interpersonal skills, it delivery approaches, powerbi, problem solving, process improvement, project management, sharepoint, software development life cycle, sql, statistical analysis, successfactors, trend analysis, visualization"
Senior Datacentre Engineer,Energy Jobline,"Abbots Salford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-datacentre-engineer-at-energy-jobline-3773345159,2023-12-17,Gloucester, United Kingdom,Mid senior,Onsite,"Title: Senior Datacentre Engineer
Salary: £40,000 DOE
Location: Salford
We are looking for an experienced Data Centre Engineer who wants to join a medium sized company with a great community culture and lots of progression opportunities onto management roles. They have been providing their clients with fully-managed dedicated server hosting, colocation, cloud hosting and data backup services to the benefit of their business and their client relations.
With paid certification and training, a generous holiday and benefits package, and free parking next to the office our client offers a comfortable space for you to grow your team lead skills and develop your existing knowledge with new technologies and exciting projects.
You will assisting a small team of dedicated technical support engineers responsible for providing top-notch Windows/Linux and cloud infrastructure support. You will be instrumental in ensuring our clients' success by delivering timely and effective solutions to their technical challenges. If you are a dynamic and motivated engineer with a passion for Linux and cloud technologies, this is an exciting opportunity to lead a team that makes a real impact on our clients' businesses. Ideally, you will already have experience of being in a similar role at a data centre or hosting company within the UK.
The Ideal Candidate Will Have The Following Experience
Client Support: Manage provision of technical support to clients, helping to resolve complex Linux and cloud infrastructure issues promptly and effectively.
Incident Management: Manage and prioritise client incidents and service requests, ensuring timely resolution and adherence to service level agreements (SLAs).
Managing client escalations and conflicts to resolve issues, rebuild client trust and identify root cause to prevent reoccurrence.
Technical Guidance: Offer guidance and expertise to team members, assisting them in troubleshooting and resolving technical challenges.
Pre-sales: Provide technical pre-sales assistance to Sales staff by capturing customer requirements and proposing appropriate solutions.
Process Improvement: Continuously improve support processes and procedures to enhance service quality and efficiency.
Client Communication: Maintain clear and effective communication with clients, keeping them informed about incident status and resolution progress.
Monitoring progress, tracking team KPIs, managing budgets, developing and delivering new client technologies.
Manage occasionally challenging scenarios and elevate the team performance.
Documentation: Contribute to the development and maintenance of knowledge base articles and documentation to empower clients and team members.Benefits:
Casual dress
Company events
On-site parking
Bonuses for achieving key metrics
Reward programme
Shift allowance
8 hour shift
Monday to Friday
Contributory pension scheme
Over-time allowance
Bonuses for achieving key metrics
Holidays: 26 days, plus all bank holidays. As well as one day extra for every year of serviceIf you believe you are suitable for this role and like the sound of the opportunity, please reply with a copy of your up to date CV. If you know of anyone who you believe would be suitable for this position then please get in contact with us with their details.
In Technology Group Ltd is acting as an Employment Agency in relation to this vacancy
Show more
Show less","Linux, Cloud computing, Client support, Incident management, Presales, Process improvement, Client communication, Monitoring, Documentation, Data center, Server hosting, Colocation, Cloud hosting, Data backup, Windows, Service level agreements (SLAs), Key performance indicators (KPIs), Knowledge base articles","linux, cloud computing, client support, incident management, presales, process improvement, client communication, monitoring, documentation, data center, server hosting, colocation, cloud hosting, data backup, windows, service level agreements slas, key performance indicators kpis, knowledge base articles","client communication, client support, cloud computing, cloud hosting, colocation, data backup, data center, documentation, incident management, key performance indicators kpis, knowledge base articles, linux, monitoring, presales, process improvement, server hosting, service level agreements slas, windows"
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time,Townepaucekltd,"Abbots Salford, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-townepaucekltd-3742324181,2023-12-17,Gloucester, United Kingdom,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Data mining, Data interpretation, Statistical modeling, A/B testing, Data visualization, SQL, R, Python, Tableau, Power BI, ETL, Data management, Data integrity, Data cleansing, Data manipulation","data analysis, data mining, data interpretation, statistical modeling, ab testing, data visualization, sql, r, python, tableau, power bi, etl, data management, data integrity, data cleansing, data manipulation","ab testing, data integrity, data interpretation, data management, data manipulation, data mining, dataanalytics, datacleaning, etl, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Engineer,Tata Consultancy Services,"México, Mexico",https://mx.linkedin.com/jobs/view/data-engineer-at-tata-consultancy-services-3748266842,2023-12-17,Hollywood,United States,Associate,Hybrid,"Role: Data Engineer
Location: Mexico City -Queretaro-Guadalajara-Monterrey
Roles & Responsibilities:
Production Support for BI applications using Control-M, DataStage, AWS S3, Athena, Pyspark, Snowflake, Oracle
Analyzing data ingestion and integration process
Handling Incident and Service Request
Follow best practices, development standard and security standard of customer environment.
Coordinate with offshore team
Skill Requirements:
Bachelor's degree in computer science, information technology, or a relevant field preferred
Knowledge of systems development, including system development life cycle, project management approaches and requirements, design, and testing techniques
Working knowledge in DataStage is essential.
A good understanding of Data Warehousing design and SQL is essential.
Knowledge of AWS services e.g. S3, Athena, EMR is required.
Knowledge of
Pyspark
is preferred.
Should Have Good communication skill in English.
Mostrar más
Mostrar menos","ControlM, DataStage, AWS S3, Athena, Pyspark, Snowflake, Oracle, Data Warehousing, SQL, EMR, System Development Life Cycle, Project Management, Requirements Gathering, Design, Testing, English Communication","controlm, datastage, aws s3, athena, pyspark, snowflake, oracle, data warehousing, sql, emr, system development life cycle, project management, requirements gathering, design, testing, english communication","athena, aws s3, controlm, datastage, datawarehouse, design, emr, english communication, oracle, project management, requirements gathering, snowflake, spark, sql, system development life cycle, testing"
Senior Data Engineer,AgileThought,Mexico City Metropolitan Area,https://mx.linkedin.com/jobs/view/senior-data-engineer-at-agilethought-3778207022,2023-12-17,Hollywood,United States,Associate,Hybrid,"This is ahyb rid position:
employees
may come in 1 or 2 times a week
, but it's not consistent; the client informs them based on needs.
The offices are located in Toreo and Azcapotzalco
.
Requirements:
7+ years of experience in ETL, ELT for data treatment (ingestion, transformations, modeling, consumption)
Experience with big data technologies for data processing
Extensive and tuned SQL querying for data transformation and business rule implementation
Data Profiling knowledge
Strong experience in python language
Experience developing resilient data pipelines in public clouds (Azure or GCP)
Experience working with object distributed storage on cloud environments
Knowledge of the API usage
English speaking
Experience working with Agile/Scrum Framework
Ability to guide the data engineering teams
Ability to understand the final requirements and analyze the end to end solutions
Mostrar más
Mostrar menos","ETL, ELT, Data treatment, Data processing, SQL, Data transformation, Python, Data pipelines, Agile/Scrum, Data engineering, Cloud computing, Azure, GCP, Object distributed storage, API, English language","etl, elt, data treatment, data processing, sql, data transformation, python, data pipelines, agilescrum, data engineering, cloud computing, azure, gcp, object distributed storage, api, english language","agilescrum, api, azure, cloud computing, data engineering, data processing, data transformation, data treatment, datapipeline, elt, english language, etl, gcp, object distributed storage, python, sql"
Data Engineer  AWS,Adecco,"Miguel Hidalgo, Mexico City, Mexico",https://mx.linkedin.com/jobs/view/data-engineer-aws-at-adecco-3779286391,2023-12-17,Hollywood,United States,Associate,Hybrid,"On Adecco we are seeking our next:
""Data Engineer""
The specialized profile will be responsible for designing, building, and managing the information management architecture in a cloud environment. This includes setting up and maintaining large-scale data processing systems, creating ETL pipelines, and ensuring the data's security, scalability, and accessibility.
• Design, construct, install, test, and maintain highly scalable data management systems in the cloud.
• Build high-performance algorithms, prototypes, predictive models, and proof of concepts using ETL techniques.
• Develop and implement data collection systems and other strategies that optimize statistical efficiency and data quality.
• Build automated data validation and error tracking systems to ensure data integrity and accuracy.
• Design cloud-based data storage infrastructure and ensure it meets the needs of our organization.
• Migrate data from legacy systems to cloud storage solutions.
• Implement effective metrics and monitoring processes
We Offer:
-Salary around 70-90 k gross
-Law Benefits
-Life Insurance
-Remote Job
Adecco has your place!
Mostrar más
Mostrar menos","Data Engineering, Cloud Computing, Big Data Management, ETL Pipelines, Data Security, Data Scalability, Data Accessibility, Data Validation, Error Tracking, Cloudbased Data Storage, Data Migration, Metrics and Monitoring","data engineering, cloud computing, big data management, etl pipelines, data security, data scalability, data accessibility, data validation, error tracking, cloudbased data storage, data migration, metrics and monitoring","big data management, cloud computing, cloudbased data storage, data accessibility, data engineering, data migration, data scalability, data security, data validation, error tracking, etl pipelines, metrics and monitoring"
Lead Data Statistician,Jackson Lewis P.C.,"Monmouth County, NJ",https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3774342473,2023-12-17,Asbury Park,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Data Analytics, Litigation, Exposure Estimates, Microsoft Excel, VBA, R, SQL, Python, Applied Statistics, Mathematics, Econometrics","data analytics, litigation, exposure estimates, microsoft excel, vba, r, sql, python, applied statistics, mathematics, econometrics","applied statistics, dataanalytics, econometrics, exposure estimates, litigation, mathematics, microsoft excel, python, r, sql, vba"
ITS Application Analyst II / IS Data Governance,Hartford HealthCare,"Farmington, CT",https://www.linkedin.com/jobs/view/its-application-analyst-ii-is-data-governance-at-hartford-healthcare-3759326168,2023-12-17,Litchfield,United States,Mid senior,Onsite,"Description
Job Schedule:
Full Time
Standard Hours
40
Job Shift
Shift 1
Shift Details
Monday - Friday
Work where every moment matters.
Every day, over 30,000 Hartford HealthCare colleagues come to work with one thing in common: Pride in what we do, knowing every moment matters here. We invite you to become part of Connecticut’s most comprehensive healthcare network.
The creation of the HHC System Support Office recognizes the work of a large and growing group of employees whose responsibilities are continually evolving so that we and our departments now work on behalf of the system as a whole, rather than a single member organization.
With the creation of our new umbrella organization we now have our own identity with a unique payroll, benefits, performance management system, service recognition programs and other common practices across the system.
Position Summary
Reports to the ITS Director Analytics.
Provides application and technical information system support for all systems related to the HHC data governance program
Facilitate issue resolution, providing alternative solutions to problems and proactively enhancing application design and implementation
Provide administrative support to the Data Governance Program, staffing assigned committees and workstreams, generating standard and ad hoc reports, logging data governance activity and disposition in supporting applications.
Position Responsibilities
Key Areas of Responsibility
Maintains assigned applications and interfaces.
Conducts testing of assigned applications.
Participates in data governance project management, including, but not limited to implementing data domains; training application users; designing, implementing and modifying application workflows; creating logs activities and decisions, supporting information governors, data stewards, application stewards and analytic stewards on all data governance matters; documenting and assigning requests to applicable committees, sub-committees and workstreams
Participates in training and development of system users and super users.
Provides customer response and updates according to current policies and procedures.
Performs other duties as assigned.
Participates in close collaboration with business and technical colleagues in the implementation and modification of data governance applications
Serves as a link between technical resources and business users/participants
Attends all mandatory education and training programs (including department specific) and can describe his/her responsibilities related to general safety and regulatory compliance as well as department safety and specific job-related hazards.
Follows the hospital exposure control plans/bloodborne and airborne pathogens.
Attire is professional, neat, clean, and appropriate for the work environment, according to the HHC and/or Department Appearance Policy and a high standard of personal cleanliness is maintained.
Reports to work on time and meets acceptable standard for attendance, based upon the department attendance policy.
Communicates messages clearly and concisely
Qualifications
Education
Bachelor's degree in Computer Science or related field
Experience
Minimum: Seven - nine (7 - 9) years of related experience.
Preferred: Healthcare IT experience, especially data policy development and data governance is preferred.
Knowledge, Skills And Ability Requirements
Work experience may substitute for minimum education requirement.
Working knowledge of state and federal health care information technology laws and regulations preferred
Working knowledge of general information technology policies and procedures.
Ability to multi-task.
Ability to operate various office machines including printers, computers and various software packages.
Ability to problem-solve.
Strong analytical skills.
Project management skills.
Conflict resolution skills.
Presentation skills.
Prioritization skills.
Process improvement and quality skills.
Interpersonal skills.
Organizational skills.
Ability to train others.
Ability to work as part of a team.
Knowledge of Microsoft Office products.
Written and verbal communication skills.
Ability to maintain confidentiality, exercise discretion and sound judgment.
Must be willing to learn new technologies and techniques.
Must be willing and able to come to work in adverse weather conditions.
Travel to Hartford Healthcare sites within 60 miles of Hartford will be required occasionally.
We take great care of careers.
With locations around the state, Hartford HealthCare offers exciting opportunities for career development and growth. Here, you are part of an organization on the cutting edge – helping to bring new technologies, breakthrough treatments and community education to countless men, women and children. We know that a thriving organization starts with thriving employees-- we provide a competitive benefits program designed to ensure work/life balance. Every moment matters. And this is
your moment.
Show more
Show less","Data Governance, Data Management, Information Technology, Healthcare IT, Microsoft Office, Project Management, Conflict Resolution, Presentation Skills, Prioritization, Problem Solving, Analytical Skills, Organizational Skills, Interpersonal Skills, Team Collaboration, Written and Verbal Communication, Confidentiality, Microsoft Office, Healthcare Information Technology Laws and Regulations","data governance, data management, information technology, healthcare it, microsoft office, project management, conflict resolution, presentation skills, prioritization, problem solving, analytical skills, organizational skills, interpersonal skills, team collaboration, written and verbal communication, confidentiality, microsoft office, healthcare information technology laws and regulations","analytical skills, confidentiality, conflict resolution, data governance, data management, healthcare information technology laws and regulations, healthcare it, information technology, interpersonal skills, microsoft office, organizational skills, presentation skills, prioritization, problem solving, project management, team collaboration, written and verbal communication"
Data and Analytics Lead,Capgemini,"Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/data-and-analytics-lead-at-capgemini-3786895765,2023-12-17,Perth, Australia,Mid senior,Onsite,"As a leading strategic partner to companies around the world, we have leveraged technology to enable business transformation for more than 50 years. We address the entire breadth of business needs, from strategy and design to managing operations. To do this, we draw on deep industry expertise and a command of the fast-evolving fields of cloud, data artificial intelligence, connectivity, software, digital engineering, and platforms. At Capgemini, we are driven by a shared purpose: unleashing human energy through technology for an inclusive and sustainable future. Capgemini. Get the future you want.
About the Service line:
Our Insights and Data team helps our clients make better business decisions by transforming an ocean of data into streams of insight. Our clients are among Australia's top performing companies, and they choose to partner with Capgemini for a very good reason - our exceptional people. Due to continued growth within Capgemini's Insights & Data practice we intend to recruit a Data and Analytics Lead with relevant consulting and communication skills. If you are already working in a consultancy role, or have excellent client-facing skills gained within large organizations, we would like to discuss our consultant opportunities with you.
Role Description:
1. SAP Analytics Solution Architect and Project Delivery Expert specializing in SAP BW/4HANA, SAP Datasphere, SAC and PaPM
2. Minimum 20 years Experience in Analytics BW/4HANA & SAP DataSphere and other Analytical Products
3. Capable of Managing team 50 Associates
5. Good Communication and Collaboration Skills
Let's talk about what's in it for you!
We believe technology is about more than business transformation - it’s also about building a more sustainable, inclusive future. This starts with our people, working with you to get the future you want. We invite everyone to bring their whole selves to work, contributing to our inclusive culture and a safe place for our people to be themselves and truly belong. Through championing our staff-led community groups, we are committed to being active allies for everyone. We also aim to build an environment where you can thrive at work and at home. We champion remote working and are in the process of making ‘truly flexible’ working arrangements the day-to-day reality for our people. In addition, we strive to ensure your growth and learning experience is unique with access to leading learning platforms, external certifications, and opportunities to grow and learn the way you want. Finally, we offer opportunities to use your skills to help society address the impact of the digital and automation revolution, and we’re ruthlessly focused on sustainability for our clients and our business - committing to becoming a net zero business by 2030.
Don’t meet every single requirement? Studies have shown that women and people from diverse minority backgrounds are less likely to apply to jobs unless they meet every single requirement. At Capgemini, we are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly, we encourage you to apply and tell us what you can bring to the role.
Capgemini Australia has developed and manages, process orientated Management Systems compliant to the requirements of ISO9001, ISO27001 and ISO14001. We are committed to delivering secure solutions and services through state-of-the-art processes and continual improvement frameworks in line with industry best practices as well as contractual, statutory, and regulatory requirements.
#GetTheFutureYouWant - Come join us, bring your whole self to work, create new possibilities for you, your customers and your community and help us to be Architects of Positive Futures.
Show more
Show less","SAP Analytics Solution Architect, SAP BW/4HANA, SAP Datasphere, SAC, PaPM, Analytics BW/4HANA, SAP DataSphere, ISO9001, ISO27001, ISO14001","sap analytics solution architect, sap bw4hana, sap datasphere, sac, papm, analytics bw4hana, sap datasphere, iso9001, iso27001, iso14001","analytics bw4hana, iso14001, iso27001, iso9001, papm, sac, sap analytics solution architect, sap bw4hana, sap datasphere"
Data and Integrations Analyst/Lead,Juniper Aged Care,"Balcatta, Western Australia, Australia",https://au.linkedin.com/jobs/view/data-and-integrations-analyst-lead-at-juniper-aged-care-3768761351,2023-12-17,Perth, Australia,Mid senior,Onsite,"WHAT'S ON OFFER:
Competitive rate of pay.
Access to salary sacrifice options up to $18,550 per annum.
Juniperks: an online platform for peer recognition, wellness, and retail discounts.
Health insurance discounts with HBF or Medibank
We are seeking to recruit a
Data
and Integrations Analyst/Lead
to join the team at our
Juniper
Central
site, located in
Balcatta
.
As a
Data
and Integrations Analyst/Lead
at Juniper, you will play a crucial role in overseeing and enhancing our data management and integration processes across all group assets. You will have the opportunity to deliver flexible, customer-focused solutions through collaboration with internal and external teams, including close coordination with the client support team and managed service provider to promptly escalate changes as required.
The successful candidate will be offered a
Permanent Full-Time
contract working
75 hours per fortnight.
KEY RESPONSIBILITIES:
Lead reporting and data analytics across group assets by integrating systems and master data sets.
Designs and develops robust data models, supporting documentation and processes to meet BI requirements.
Build dashboards and data visualisations using Tableau (or Power BI).
Provides support across various functions, including annual budgeting, forecasts, variance analysis and data modelling.
Develop data integration pipelines across different sources, systems and platforms by using various tools and technologies.
Develop and enforce effective data management policies and procedures.
JOB REQUIREMENTS:
Degree or post graduate qualification in Computer Science, Data Science, Software Engineering, Mathematics, or related discipline.
Minimum 3 years’ experience in data analyst, data integrations, database administration, or related roles.
High level of proficiency working in data analysis tools including SQL and visualisation tools such as Tableau or Power BI.
Practical experience with ETL tools, integration platforms and middleware such as KNIME or Talend.
Extensive experience in data modelling, cleansing, and warehousing tools with expertise in statistical and data mining techniques.
Proficient in conducting complex requirements gathering, design workshops, and driving data literacy initiatives.
Excellent customer service attitude and enthusiasm for solving problems and creating technical solutions.
To be eligible for this role, please complete your application by submitting cover letter addressing the key responsibilities and a current CV. Please contact Deeqa with any enquiries about this position
6119 5905
or
deeqa.ali@juniper.org.au
To work at Juniper all staff must have received at least two COVID-19 vaccinations and have had, and be willing to obtain, influenza vaccination annually.
Juniper is a leading provider of care, accommodation, and support services for older Western Australians. We are a value driven, community benefit organisation that excels in social enterprise for the benefit of all people. Our response to increasing need encompasses a strong presence throughout metropolitan Perth and in the Kimberley, Mid-West, Wheatbelt and Great Southern regions, giving us one of the largest care footprints in Australia. We believe in celebrating life and strive to add value to the communities in which we operate, where our staff and volunteers work to deliver our Vision and Mission.
All ages and cultures welcome to apply. Recruitment may start before the closing date.
Show more
Show less","Data analytics, Tableau, Power BI, SQL, KNIME, Talend, ETL tools, Data modelling, Data cleansing, Data warehousing, Data mining, Statistical analysis, Data integration, Data management, Computer science, Data science, Software engineering, Mathematics","data analytics, tableau, power bi, sql, knime, talend, etl tools, data modelling, data cleansing, data warehousing, data mining, statistical analysis, data integration, data management, computer science, data science, software engineering, mathematics","computer science, data integration, data management, data mining, data modelling, data science, dataanalytics, datacleaning, datawarehouse, etl tools, knime, mathematics, powerbi, software engineering, sql, statistical analysis, tableau, talend"
Senior Database Engineer,Syrinx Consulting,"Burlington, MA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-syrinx-consulting-3120114797,2023-12-17,Haverhill,United States,Mid senior,Onsite,"Must be local to Burlington, MA (For now this is a mostly remote contract role with a Syrinx Biotech Partner)
U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time.
You will need to have hands-on experience with production databases. You should be able to:
Create databases with efficient structures
Write optimized queries, views and triggers for integration with other applications
Maintain high standards of data quality and integrity
Understand issues related to network performance and security
You Should Also Have a Background In
Reporting and business intelligence tools like Microsoft SQL Server Reporting Services
(SSRS) and SAP Crystal Reports
SQL database systems
Big data technologies like Hadoop and Spark
Amazon AWS Cloud Services
The Software Development Life Cycle (SDLC), specifically Scrum and Agile
methodology
Tasks & Responsibilities
Develop and support production MySQL databases.
Develop and support MySQL database replication and clustering
Monitor and optimize MySQL database performance and capacity planning
Troubleshoot database issues, maintain database systems availability
Plan and execute for database scalability
Develop and optimize database design for new applications
Preferred Qualifications
10+ years of experience in MySQL Administration and PL/SQL programming
Demonstrated experience with MySQL database design, implementation, performance tuning, and
capacity planning. Managing large scale database system is a PLUS
Demonstrated experience with Open Source/Linux development and production environments
Solid knowledge in PL/SQL programming and its optimization
Proficient at using the MySQL admin tools for issue inspection
Experienced with performance analysis (IO, CPU, etc) and optimization (indexes, partitions, schema
redesign, etc)
Solid knowledge in server monitoring for detection of emerging issues (performance, locking, bad
usage)
Possessing a solid understanding of Linux - shell scripting but also ability to inspect hardware
performance and load indicators
Good understanding of master-master replication
Show more
Show less","SQL, MySQL, PL/SQL, Hadoop, Spark, Amazon AWS Cloud Services, Scrum, Agile, SSRS, SAP Crystal Reports, Linux, Shell scripting","sql, mysql, plsql, hadoop, spark, amazon aws cloud services, scrum, agile, ssrs, sap crystal reports, linux, shell scripting","agile, amazon aws cloud services, hadoop, linux, mysql, plsql, sap crystal reports, scrum, shell scripting, spark, sql, ssrs"
Associate Staff- Data Engineer,MIT Lincoln Laboratory,"Lexington, MA",https://www.linkedin.com/jobs/view/associate-staff-data-engineer-at-mit-lincoln-laboratory-3685795295,2023-12-17,Haverhill,United States,Mid senior,Onsite,"The Biological and Chemical Technologies Group addresses significant current and emerging national security challenges using advanced biological and chemical technologies. Capabilities within the Group include molecular, cellular, and synthetic biology; microbiome engineering; rapid DNA sequencing; bioinformatics; microfluidics; mass spectrometry; laser spectroscopy; and organic chemistry. These capabilities are applied to missions spanning chemical, biological, and explosives (CBE) detection and attribution; medical diagnostics and countermeasures; pathogen identification; point-of-care diagnostics; molecular intelligence; biosecurity; biomanufacturing, DNA forensics, and additive manufacturing. See: https://www.ll.mit.edu/r-d/biotechnology-and-human-systems/biological-and-chemical-technologies .
Job Description
The Biological & Chemical Technologies Group is in search of an innovative and highly motivated data architect to execute on our efforts in developing platforms and key enabling technologies for large-scale data. As an associate staff in this area, you will work hand in hand with software engineers, data scientists, and domain experts in biology, chemistry, and signal processing to address key national stakeholder needs and further the Group’s mission by developing and delivering the next generation software and information architectures to enable biomedical solutions. You will drive architecture solutions and innovations to support data processing pipelines and delivering national large-scale curated and harmonized data resources.
Requirements
Masters in Computer Science, Data Science, or related scientific field or equivalent experience in technical analysis. In lieu of a Master’s degree, a bachelor’s degree with 5+ years of directly related experience will be considered.
Champion of data technologies in areas of data integration, data curation and harmonization
Experience with data science and machine learning techniques to help with automation
Functional expertise in building microservice architectures and methodologies
Data modeling and database design experience, both SQL and NoSQL
Knowledge of DevSecOps and agile software development methodologies
Demonstrable experience in deploying software prototypes using languages such as R, Python, Javascript
Ability to work collaboratively and lead small to medium team of software developers and data scientists to execute your vision and software architecture
Demonstrated skills in scientific writing, as well as development and clear presentation of technical materials to a wide spectrum of technical and non-technical audiences.
Occasional travel (<10%) with optional opportunities for travel.
At MIT Lincoln Laboratory, our exceptional career opportunities include many outstanding benefits to help you stay healthy, feel supported, and enjoy a fulfilling work-life balance. Benefits offered to employees include:
Comprehensive health, dental, and vision plans
MIT-funded pension
Matching 401K
Paid leave (including vacation, sick, parental, military, etc.)
Tuition reimbursement and continuing education programs
Mentorship programs
A range of work-life balance options
... and much more!
Please visit our Benefits page for more information. As an employee of MIT, you can also take advantage of other voluntary benefits, discounts and perks .
Selected candidate will be subject to a pre-employment background investigation and must be able to obtain and maintain a Secret level DoD security clearance.
MIT Lincoln Laboratory is an Equal Employment Opportunity (EEO) employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, veteran status, disability status, or genetic information; U.S. citizenship is required.
Requisition ID:
40375
Show more
Show less","Data Architecture, Software Engineering, Data Science, Biology, Chemistry, Signal Processing, Data Integration, Data Curation, Data Harmonization, Machine Learning, Microservices Architecture, Data Modeling, Database Design, SQL, NoSQL, DevSecOps, Agile Software Development, R, Python, Javascript, Scientific Writing, Technical Presentation","data architecture, software engineering, data science, biology, chemistry, signal processing, data integration, data curation, data harmonization, machine learning, microservices architecture, data modeling, database design, sql, nosql, devsecops, agile software development, r, python, javascript, scientific writing, technical presentation","agile software development, biology, chemistry, data architecture, data curation, data harmonization, data integration, data science, database design, datamodeling, devsecops, javascript, machine learning, microservices architecture, nosql, python, r, scientific writing, signal processing, software engineering, sql, technical presentation"
Staff Data Engineer,Recruiting from Scratch,"Medford, MA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395241,2023-12-17,Haverhill,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, Spark Streaming, Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, spark streaming, data modeling, schema design, data warehouses, etl, data management tools, data classification, retention","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, datamodeling, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, retention, schema design, snowflake, spark, spark streaming, sql, storm, tdd"
Senior Data Engineer with Security Clearance,ClearanceJobs,"Lexington, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-with-security-clearance-at-clearancejobs-3753493128,2023-12-17,Haverhill,United States,Mid senior,Onsite,"Job Description
Performs data engineer tasks, planning, technical leadership, and management for the team. Provides clients with technical expertise for a project or program. Able to manage technology projects or lead technology solutioning. Provides highly technical and specialized guidance and solutions to complex IT problems; performs elaborate analyses and studies. Evaluates, recommends, and executes new technologies and updates existing infrastructure to ensure optimal performance and efficiency. Develops IT strategies to ensure the systems meet existing and future requirements based on needs and regulations.
Works to design, implement, and maintain a data pipeline and analytical environment using reporting tools, modeling metadata, and building reports and dashboards. Works in a variety of environments and has excellent verbal and non-verbal communication skills. Works effectively and independently
Requirements
8 or more years’ experience working in the IT industry 4 or more years’ experience implementing event/data streaming services such as Kafka
Must have Kubernetes and docker experience.
Experience leading teams working with big data technologies
At least 2 years of experience developing in languages commonly used for data analysis such as Python, R, Julia, or SAS
Experience working with multiple database types such as SQL, Redis and MongoDB
Experience building and integrating the at the application and database level
Experience developing REST/SOAP APIs and messaging protocols and formats At least 2 years of theoretical and practical background in statistical analysis, machine learning, predictive modeling, and/or optimization
Experience prototyping front-end visualizations utilizing data visualization suites such as Kibana or Splunk
Experience implementing and building event driven architectures
Familiarity with event driven finite state machines
High proficiency in SQL to include schema design, data definition, and advanced queries
Experience with MPP data warehouses Experience with scripting languages for automating repetitive tasks
Experience with creating automated data pipelines for complex systems
Extensive Linux server management background
Can prototype visualizations with lightweight data visualization suite Excellent verbal and written communications skills along with the ability to present technical data and approaches to both technical and non-technical audiences
Current U.S. security clearance or ability to obtain a U.S. security clearance
About OneGlobe: OneGlobe LLC was founded in 2005 to provide quality Information Technology solutions that exceed expectations. We focus on IT System Modernization using agile software development practices and DevSecOps to deliver intuitive and maintainable systems that we help our customers improve their processes and capabilities. We provide full service IT solutions and have the skill to identify, plan and perform cost-saving steps throughout the system lifecycle to enhance system efficiency, while optimizing the value that we deliver to our customers. Our team has the drive and right mindset to feel ownership on the projects they work on. They partner with our customers to give the extra effort sometimes required to deliver success. We provide a highly competitive benefits package to include: extensive medical/dental/vision, 7% of your annual salary toward 401(k), Paid Time Off (PTO), $5K annually toward ongoing education and training, and more. We also have monthly social and tech events! See additional positions at: http://www.oneglobeit.com/#careers OneGlobe is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation or veteran status.
Show more
Show less","Data engineering, Kubernetes, Docker, Python, R, Julia, SAS, SQL, Redis, MongoDB, REST/SOAP APIs, Statistical analysis, Machine learning, Predictive modeling, Optimization, Kibana, Splunk, Event driven architectures, Event driven finite state machines, Schema design, Data definition, MPP data warehouses, Scripting languages, Data pipelines, Linux server management, Data visualization","data engineering, kubernetes, docker, python, r, julia, sas, sql, redis, mongodb, restsoap apis, statistical analysis, machine learning, predictive modeling, optimization, kibana, splunk, event driven architectures, event driven finite state machines, schema design, data definition, mpp data warehouses, scripting languages, data pipelines, linux server management, data visualization","data definition, data engineering, datapipeline, docker, event driven architectures, event driven finite state machines, julia, kibana, kubernetes, linux server management, machine learning, mongodb, mpp data warehouses, optimization, predictive modeling, python, r, redis, restsoap apis, sas, schema design, scripting languages, splunk, sql, statistical analysis, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Medford, MA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091276,2023-12-17,Haverhill,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, SQL, Pandas, Airflow, Snowflake, Kubernetes, Docker, Spark, NoSQL, Kafka, Git, Bash, AWS, GCP, Azure, Conversational AI APIs, Recommender Systems, Microservices, Data Classification, Data Retention, ETL Pipelines","data engineering, machine learning, python, java, sql, pandas, airflow, snowflake, kubernetes, docker, spark, nosql, kafka, git, bash, aws, gcp, azure, conversational ai apis, recommender systems, microservices, data classification, data retention, etl pipelines","airflow, aws, azure, bash, conversational ai apis, data classification, data engineering, data retention, docker, etl pipelines, gcp, git, java, kafka, kubernetes, machine learning, microservices, nosql, pandas, python, recommender systems, snowflake, spark, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Medford, MA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711065,2023-12-17,Haverhill,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, Data Mining, Normalization, Modeling, Airflow, KubeFlow, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Machine Learning, Data Management, ETL, LGBTQ","data engineer, data mining, normalization, modeling, airflow, kubeflow, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, machine learning, data management, etl, lgbtq","airflow, aws, azure, bash, data management, data mining, dataengineering, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, lgbtq, machine learning, modeling, normalization, python, snowflake, spark, sparkstreaming, sql, storm"
Data Engineer,Insight Global,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-at-insight-global-3780029725,2023-12-17,Glendale,United States,Mid senior,Hybrid,"Title:
Data Engineer (Phoenix Local)
Location:
Phoenix, AZ 85054 USA (Hybrid Schedule)
Duration:
Exciting 12-month journey with potential for extension or permanent hire
Compensation
: Rewarding rate of $60/hour
Visa Requirement:
Green Card
What We’re Looking For:
A seasoned Data Engineer with 5+ years of experience in managing, designing, and performance tuning relational and non-relational databases.
Proficiency in the Hadoop ecosystem, including HBase, HDFS, Map-reduce programming model, Hive, PIG, etc.
Expertise in data-warehousing, requirement-driven data modeling, and scalable database programming.
Experience with GCP Bigtable, Bigquery, and public cloud environments like GCP or AWS.
Proficiency in ETL methodology with large data sets and corporate-wide solution design using DataStage.
Comfortable with creating structured data solutions using various programming languages such as Python or Java.
Exposure to streaming frameworks like Kafka and large-scale data analytics cluster computing frameworks like Spark or Flink.
Experience working with big data file formats like Parquet, Avro, etc.
Good interpersonal skills and a positive attitude.
Nice to Have:
Any Big Data certification
Public Cloud certification
Join a large financial enterprise in Phoenix, AZ as a Data Engineer. You’ll be part of our Database Management team, helping with a large finance initiative centered around our Big Data Platforms, relational and non-relational databases, and working within our Hadoop ecosystem. Your responsibilities will include scaling large data analytics clusters, creating data solutions that are structured and scalable, and working with data sets in a public cloud environment. If you’re looking for a challenging role that offers growth and learning opportunities, we’d love to hear from you!
Why Join Us:
Work with cutting-edge technology at our prestigious Corporate Headquarters in Phoenix, AZ.
Be part of a Fortune 500 company, contributing to critical initiatives within the payments industry.
Collaborate with internal and external stakeholders, building lasting relationships and making a real impact.
Show more
Show less","SQL, Database Management, Hadoop Ecosystem, HBase, HDFS, MapReduce Programming Model, Hive, PIG, Data Warehousing, Data Modeling, Database Programming, GCP Bigtable, Bigquery, ETL Methodology, DataStage, Python, Java, Kafka, Spark, Flink, Parquet, Avro","sql, database management, hadoop ecosystem, hbase, hdfs, mapreduce programming model, hive, pig, data warehousing, data modeling, database programming, gcp bigtable, bigquery, etl methodology, datastage, python, java, kafka, spark, flink, parquet, avro","avro, bigquery, database management, database programming, datamodeling, datastage, datawarehouse, etl methodology, flink, gcp bigtable, hadoop ecosystem, hbase, hdfs, hive, java, kafka, mapreduce programming model, parquet, pig, python, spark, sql"
GCP Data Engineer (Bigdata),LTIMindtree,"Phoenix, AZ",https://www.linkedin.com/jobs/view/gcp-data-engineer-bigdata-at-ltimindtree-3772901381,2023-12-17,Glendale,United States,Mid senior,Hybrid,"Skill Sets :
GCP Data Engineer with Bigdata skills: GCP experience. Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion,
8+ years of application development experience required;
In-depth knowledge about SQL databases and ability to execute queries quickly.
Knowledge of data warehousing and data modeling.
Build and maintain data structures and databases. Design data processing systems. Analyze data and enable machine learning.
Developing comprehensive abilities for Google data structures and matrix for upcoming product development
Good to have: Amex Experience and understanding of Cornerstone DAM = Data access manage, Automated Case workflow mgmt (Case Management tools) , ISP (Intuitive Servicing Platform). Inventory Management tools
Show more
Show less","GCP, BigQuery, BigTable, Google Cloud Storage, PubSub, Data Fusion, SQL, Data warehousing, Data modeling, Data processing, Data analysis, Machine learning, Data structures, Matrix, Amex, Cornerstone DAM, Data access management, Automated case workflow management, Case management tools, ISP, Inventory management","gcp, bigquery, bigtable, google cloud storage, pubsub, data fusion, sql, data warehousing, data modeling, data processing, data analysis, machine learning, data structures, matrix, amex, cornerstone dam, data access management, automated case workflow management, case management tools, isp, inventory management","amex, automated case workflow management, bigquery, bigtable, case management tools, cornerstone dam, data access management, data fusion, data processing, data structures, dataanalytics, datamodeling, datawarehouse, gcp, google cloud storage, inventory management, isp, machine learning, matrix, pubsub, sql"
Big Data Developer,NR Consulting,"Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-developer-at-nr-consulting-3762821176,2023-12-17,Glendale,United States,Mid senior,Hybrid,"Job Description
ETL - Big Data / Data Warehousing; Java
Objectives of this Role
Develop and implement data pipelines that extracts, transforms and loads data into an information product that helps to inform the organization in reaching strategic goals
Work on ingesting, storing, processing and analyzing large data sets
Create scalable and high-performance web services for tracking data
Translate complex technical and functional requirements into detailed designs
Investigate and analyze alternative solutions to data storing, processing etc. to ensure most streamlined approaches are implemented
Serve as a mentor to junior staff by conducting technical training sessions and reviewing project outputs
Daily And Monthly Responsibilities
Develop and maintain data pipelines implementing ETL processes
Take responsibility for Hadoop development and implementation
Work closely with a data science team implementing data analytic pipelines
Help define data governance policies and support data versioning processes
Maintain security and data privacy working closely with Data Protection Officer internally
Analyze a vast number of data stores and uncover insights
Skills And Qualifications
Experience in Python, Spark and Hive
Understanding of data warehousing and data modeling techniques
Knowledge of industry-wide analytical and visualization tools (Tableau and R)
Strong data engineering skills on the Azure Cloud Platform is essential
Streaming frameworks like Kafka
Knowledge of core Java, Linux, SQL, and any scripting language
Good interpersonal skills and positive attitude
Show more
Show less","ETL, Big Data, Data Warehousing, Java, Python, Spark, Hive, Data modeling, Tableau, R, Azure Cloud Platform, Kafka, Core Java, Linux, SQL, Scripting language","etl, big data, data warehousing, java, python, spark, hive, data modeling, tableau, r, azure cloud platform, kafka, core java, linux, sql, scripting language","azure cloud platform, big data, core java, datamodeling, datawarehouse, etl, hive, java, kafka, linux, python, r, scripting language, spark, sql, tableau"
Data Analyst,OrthoArizona,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-analyst-at-orthoarizona-3668506255,2023-12-17,Glendale,United States,Mid senior,Hybrid,"The Data analyst will be responsible for using their knowledge of data processing and business development strategies to partner with business executives on key decisions within OrthoArizona. This role will collaborate with other data professionals to extra data figures, create reports, and assist with new process improvements.
Duties And Responsibilities
Interpret data, analyze results using statistical techniques and provide ongoing reports for internal and/pr external clients
Develop, implement and maintain leading-edge analytics systems, taking complicated problems and building simple frameworks
Proactively analyze data to answer key questions for stakeholders or yourself, with an eye on what drives business performance, and investigate and communicate which areas need improvement in efficiency and productivity
Acquire data from primary and secondary data sources, maintain data systems, provide executive dashboards with data
Collaborate with team members to Identify, analyze, and interpret trends or patterns in complex data sets
Blend data from multiple systems to create reports, dashboards, and presentations based on recommendations and findings
Filter and clean data by reviewing reports, and performance indicators to locate and correct problems
Work with executives and other business leaders to identify and prioritize business and information needs
Locate and define new process improvement opportunities
Qualifications
Must have a minimum of two (2) years of progressively responsible healthcare data management and analysis experience using revenue cycle and hospital financial software applications
3-5 years experience as a data analyst or business data analyst
Bachelors degree in Computer science, information management, or related field
Technical expertise regarding data models, database design development, data mining, and segmentation techniques
Experience with EMR Databases
Rev Cycle background a huge plus
SQL/Pivot table a must
Power BI or similar visualization experience
Experience with Microsoft Access, Excel, and other business intelligence tools to extract, analyze, and report data
Ability to querying SQL
General knowledge of the healthcare industry
Strong knowledge of and experience with reporting packages, databases, and programming
Knowledge of statistics and experience using statistical packages for analyzing data sheets
Show more
Show less","Data processing, Business development, Statistical analysis, Reporting, Analytics, Data mining, Segmentation, EMR databases, SQL, Pivot tables, Power BI, Microsoft Access, Excel, Business intelligence tools, Healthcare industry, Reporting packages, Databases, Programming, Statistics, Statistical packages","data processing, business development, statistical analysis, reporting, analytics, data mining, segmentation, emr databases, sql, pivot tables, power bi, microsoft access, excel, business intelligence tools, healthcare industry, reporting packages, databases, programming, statistics, statistical packages","analytics, business development, business intelligence tools, data mining, data processing, databases, emr databases, excel, healthcare industry, microsoft access, pivot tables, powerbi, programming, reporting, reporting packages, segmentation, sql, statistical analysis, statistical packages, statistics"
Data Analyst,Spectrum Health & Human Services,"Orchard Park, NY",https://www.linkedin.com/jobs/view/data-analyst-at-spectrum-health-human-services-3660592674,2023-12-17,West Seneca,United States,Mid senior,Onsite,"Agency Profile:
Spectrum Health & Human Services respectfully partners with adults, children, and families as they recover from behavioral, emotional, mental health and/or substance-related disorders by offering individualized and meaningful opportunities of hope, empowerment, and support to achieve self-defined improvements in their quality of life.
Full-time: 227 Thorn Avenue, Orchard Park, NY
Summary Of Position Function
The Data Analyst position is responsible for working closely with the Population Health Data Manager in identifying relevant sources of data and assisting with the collection, analyzing, and the development of interactive dashboards and reports that will be used to prove recommendations for improving the operations and outcomes as well as meeting internal and external reporting requirements. The Data Analyst works closely with the departmental leads including, program, quality, and the finance leadership to develop and maintain the data infrastructure. This person reports the Population Health/Data Analytics Manager.
Major Duties And Responsibilities
Assist with developing, implementing , and maintaining leading-edge analytic systems, taking complicated problems and building simple frameworks
Work closely with departmental leads to assist in developing strategic data and information management to assist in reaching established outcomes and measures
Identify, collect and analyze data, specifically as it relates to the agreed upon data points.
Assist in identifying trends and opportunities for growth through analysis of data sets
Assist with evaluating internal systems for efficiency, problems, and inaccuracies, developing and maintaining protocols for handling, processing, and cleaning data
Create best-practice reports based on data mining, analysis, and visualization
Work directly with management and users to gather requirements, provide status updates, and build relationships
Skills/Competencies
Proven analytic skills, including mining, evaluation, analysis, and visualization
Technical writing experience in relevant areas, including queries, reports, and presentations
Strong SQL or Excel skills with the ability to learn other analytic tools
Prior experience with database and model design and segmentation techniques
Familiar with EMR and EHR data platforms and a practical understanding of data
Strong programming experience with frameworks including XML, JavaScript, SQL, and ETL
Practical experience in dashboard development through the use of technology tools such as Microsoft Power BI, Tableau, and
Proven success in a collaborative, team-oriented environment
Service delivery motivated with a service-oriented attitude
Education Requirements
Bachelor’s degree in Business, Computer Science, Statistics or equivalent experience to meet the needs of the position.
Experience
3+ years of experience mining data as a Data Analyst or Data Specialist
3+ years of experience with building analytical in enterprise environment and dashboards
Must possess a valid Driver’s License with a satisfactory driving record
COMPENSATION:
$50,000 - $61,000/yr
Show more
Show less","Data Analysis, Data Mining, Data Visualization, SQL, Excel, Database Design, Model Design, Segmentation Techniques, EMR, EHR, XML, JavaScript, ETL, Dashboard Development, Microsoft Power BI, Tableau, Statistics, Data Specialist","data analysis, data mining, data visualization, sql, excel, database design, model design, segmentation techniques, emr, ehr, xml, javascript, etl, dashboard development, microsoft power bi, tableau, statistics, data specialist","dashboard development, data mining, data specialist, dataanalytics, database design, ehr, emr, etl, excel, javascript, microsoft power bi, model design, segmentation techniques, sql, statistics, tableau, visualization, xml"
Senior Technical Data Analyst,System1,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-technical-data-analyst-at-system1-3584577846,2023-12-17,Pasadena,United States,Mid senior,Hybrid,"We are seeking a highly skilled and experienced
Senior Technical Data Analyst
to join our growing Data Analytics team. Needs a strong background in data analysis and to be able to effectively communicate complex analytical findings to both technical and non-technical audiences, focusing on our highly profitable Search Engine Monetization business. This role requires a strong background in exploratory data analytics, statistics, data modeling as well as excellent communication and leadership skills.
The Role You Will Have
Analyze data from search engine marketing campaigns, including keyword performance, ad spend, and conversions
Collaborate with cross-functional teams to identify data-driven opportunities and develop strategies to leverage data assets
Develop and maintain analytical dashboards and reports to track key performance indicators and provide insights to stakeholders
Analyze data trends and patterns to identify opportunities for improvement and make recommendations to leadership
Utilize statistical and machine learning techniques to extract insights from data and build predictive models
Train and mentor junior data analysts and team members on data analysis techniques and best practices
Stay up-to-date with industry trends and best practices in search engine marketing and data analysis
What You Bring To The Team
Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
5+ years of experience in data analysis and visualization of large data sets
Strong experience with statistical and machine learning techniques
Advanced proficiency in SQL, and at least one programming language (e.g. Python, R)
Deep familiarity with all digital marketing metrics including CPA, CPC, CPM and ROAS
Excellent communication and leadership skills
Ability to work independently and as part of a team
Experience with big data technologies (e.g. Hadoop, Spark, BigQuery)
Experience with data visualization tools (e.g. Tableau, Power BI), as well as surfacing insights with interactive notebooks (e.g Jupyter, Google Colab)
Familiarity with pulling data from third party API’s and basic ETL processing
What We Have To Offer
Competitive salary + bonus + equity
Generous PTO + 11 company holidays
Open sick time
100% covered Medical, Dental, Vision for employees
401k with match
Health & Dependent Care Flex Spending Account
Paid professional development
Leadership & growth opportunities
Virtual company and team building events
#BI-Hybrid
#Bl-Remote
The U.S. base salary range for this full-time position is
$116,500 - $164,600
+ bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in U.S. role postings reflect the base salary only, and do not include bonus, equity, or benefits.
System1 offers flexible work arrangements for most employees (unless they hold positions which are identified as having to be 100% onsite in Marina del Rey, CA, Bellevue, WA or Guelph, ON Canada). Most System1 full-time employees choose to work in a hybrid environment, splitting their time between working in our offices and working remotely. System1 allows fully-remote work in the following approved locations: Arizona, Colorado, Georgia, Hawaii, Minnesota, Missouri, New Jersey, New York, North Carolina, Oklahoma, Oregon, Pennsylvania, Tennessee, Texas and Virginia. Prospective U.S. employees who live outside of any of these states will need to establish residency in one of the approved states prior to employment.
Show more
Show less","Data analysis, Statistics, Data modeling, Communication, Leadership, Search engine marketing, Keyword performance, Ad spend, Conversions, Datadriven opportunities, Data assets, Analytical dashboards, Reports, Key performance indicators, Insights, Data trends, Patterns, Improvement, Recommendations, Machine learning, Predictive models, Data visualization, SQL, Python, R, Digital marketing metrics, CPA, CPC, CPM, ROAS, Big data technologies, Hadoop, Spark, BigQuery, Data visualization tools, Tableau, Power BI, Interactive notebooks, Jupyter, Google Colab, ETL processing","data analysis, statistics, data modeling, communication, leadership, search engine marketing, keyword performance, ad spend, conversions, datadriven opportunities, data assets, analytical dashboards, reports, key performance indicators, insights, data trends, patterns, improvement, recommendations, machine learning, predictive models, data visualization, sql, python, r, digital marketing metrics, cpa, cpc, cpm, roas, big data technologies, hadoop, spark, bigquery, data visualization tools, tableau, power bi, interactive notebooks, jupyter, google colab, etl processing","ad spend, analytical dashboards, big data technologies, bigquery, communication, conversions, cpa, cpc, cpm, data assets, data trends, data visualization tools, dataanalytics, datadriven opportunities, datamodeling, digital marketing metrics, etl processing, google colab, hadoop, improvement, insights, interactive notebooks, jupyter, key performance indicators, keyword performance, leadership, machine learning, patterns, powerbi, predictive models, python, r, recommendations, reports, roas, search engine marketing, spark, sql, statistics, tableau, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Culver City, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090498,2023-12-17,Pasadena,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data OPs, Machine Learning, Data Pre/Post Processing, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Platforms, Data Frameworks, Big Data Technologies, Cloud Computing, SQL, Python, Java, Bash, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, ETL, Distributed Systems, Microservices, Kafka, Storm, SparkStreaming, Applied Machine Learning, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, ml data ops, machine learning, data prepost processing, data mining, data cleaning, data normalization, data modeling, data platforms, data frameworks, big data technologies, cloud computing, sql, python, java, bash, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, distributed systems, microservices, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, big data technologies, cloud computing, data classification, data cleaning, data engineering, data frameworks, data management tools, data mining, data normalization, data platforms, data prepost processing, data retention, datamodeling, distributed systems, docker, etl, gcp, git, helm, java, kafka, kubernetes, legal compliance, machine learning, microservices, ml data ops, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Data Analyst,Global Accounting Network,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-analyst-at-global-accounting-network-3784844239,2023-12-17,Atlanta,United States,Associate,Hybrid,"We are hiring for a leading global consumer services company dedicated to delivering exceptional experiences to our customers. With a diverse portfolio of services, we strive to understand and meet the evolving needs of our clients around the world. As we continue to expand our operations, we are seeking a talented and motivated Data Analyst to join our dynamic team.
Position Summary:
We are looking for a skilled and detail-oriented Data Analyst to play a crucial role in our data-driven decision-making processes. The ideal candidate will possess strong technical skills in SQL and Python, along with expertise in data visualization using either Tableau or PowerBI. The candidate should be a strategic thinker with problem-solving abilities, critical thinking skills, and a strong willingness to learn and adapt to new challenges.
Responsibilities:
Execute complex data queries using SQL to extract and analyze relevant information from diverse datasets.
Utilize Python for data manipulation, cleansing, and transformation tasks to ensure data accuracy and integrity.
Create compelling and insightful visualizations using Tableau or PowerBI to communicate key findings to stakeholders.
Design and develop interactive dashboards to facilitate data-driven decision-making processes.
Collaborate with cross-functional teams to understand business requirements and contribute to the development of data-driven strategies.
Provide analytical insights to support strategic initiatives and help drive business growth.
Identify data-related challenges and proactively propose solutions to enhance data quality and usability.
Conduct root cause analysis for data discrepancies and implement corrective measures.
Stay abreast of industry trends, emerging technologies, and best practices in data analytics.
Actively participate in professional development opportunities to enhance skills and knowledge.
Qualifications:
Bachelor’s degree in a related field such as Computer Science, Statistics, or Mathematics.
Proven experience as a Data Analyst with a focus on consumer services or a similar industry.
Strong proficiency in SQL and Python for data analysis and manipulation.
Experience in data visualization using Tableau or PowerBI.
Excellent problem-solving and critical thinking skills.
Strong communication skills with the ability to convey complex findings to non-technical stakeholders.
Strategic mindset with the ability to contribute to data-driven decision-making processes.
Show more
Show less","SQL, Python, Tableau, PowerBI, Data Visualization, Data Analytics, Data Manipulation, Data Cleansing, Data Transformation, Data Quality, Data Usability, DataDriven Decision Making, Dashboard Design, Dashboard Development, Interactive Dashboards, CrossFunctional Collaboration, Business Requirements, DataDriven Strategies, Analytical Insights, Strategic Initiatives, Business Growth, DataRelated Challenges, Corrective Measures, Industry Trends, Emerging Technologies, Best Practices, Professional Development","sql, python, tableau, powerbi, data visualization, data analytics, data manipulation, data cleansing, data transformation, data quality, data usability, datadriven decision making, dashboard design, dashboard development, interactive dashboards, crossfunctional collaboration, business requirements, datadriven strategies, analytical insights, strategic initiatives, business growth, datarelated challenges, corrective measures, industry trends, emerging technologies, best practices, professional development","analytical insights, best practices, business growth, business requirements, corrective measures, crossfunctional collaboration, dashboard design, dashboard development, data manipulation, data quality, data transformation, data usability, dataanalytics, datacleaning, datadriven decision making, datadriven strategies, datarelated challenges, emerging technologies, industry trends, interactive dashboards, powerbi, professional development, python, sql, strategic initiatives, tableau, visualization"
Data Software Engineer,PlayOn! Sports,"Alpharetta, GA",https://www.linkedin.com/jobs/view/data-software-engineer-at-playon%21-sports-3787111039,2023-12-17,Atlanta,United States,Associate,Hybrid,"THE ROLE
PlayOn! Sports is looking for a Data Software Engineer skilled in Python programming and SQL expertise. You will be responsible for ensuring data quality of our Payments product. You will play a pivotal role in developing robust data-driven solutions while ensuring seamless data validation processes.
Additional responsibilities and accountabilities for this role include:
• You should be hands-on and comfortable in an Agile environment.
• Writing code to test and validate the business logic encoded in the database SQL code and other code repositories.
• Test back-end API interfaces for the financial platform.
• Build tools that help developers be more productive, write better code and test it themselves.
• Provide a risk assessment on the defects identified and set the correct priority and severity.
• Define the scope and procedures for quality engineering of the application, including the development and maintenance of automated scripts.
• Understanding of how API-driven applications work and interact with other API or microservices environments.
• Participate in Scrum meetings and ensure accurate and timely status updates to application team.
THE PERSON
• 3-5 years of experience in Data Engineering (SQL & Python)
• Strong Data Validity, Data Integrity & Data format testing experience.
• The candidate should be able to validate the database code to some degree to make sure the business logic has been coded as per the requirements and stage data to test them.
• Work in an Agile Software Delivery methodology highly focused on developing automated tests.
• Proven development ability with Microservices, Web & Cloud platforms.
• Understanding of Web frontend & backend technologies, such as JavaScript, Java, APIs, Microservices.
• Proficient with agile tools like Jira, CI/CD, Bitbucket or similar.
Show more
Show less","Python, SQL, Agile, API, Microservices, JavaScript, Java, Jira, CI/CD, Bitbucket, Data Engineering, Data Validation, Data Integrity, Data Quality, DataDriven Solutions","python, sql, agile, api, microservices, javascript, java, jira, cicd, bitbucket, data engineering, data validation, data integrity, data quality, datadriven solutions","agile, api, bitbucket, cicd, data engineering, data integrity, data quality, data validation, datadriven solutions, java, javascript, jira, microservices, python, sql"
Procurement Data Analyst,"Accountable Search Group, LLC","Alpharetta, GA",https://www.linkedin.com/jobs/view/procurement-data-analyst-at-accountable-search-group-llc-3773574404,2023-12-17,Atlanta,United States,Associate,Hybrid,"Temp to Perm, Hybrid
Must currently be located in Metro Atlanta
The Procurement Data Analyst role provides analytical expertise for Procurement initiatives, sourcing strategies, savings reporting, and other activities requiring analytics support. This role is responsible for developing, supporting, and reporting key Procurement performance indicators as well as supporting the development and execution of sourcing strategies, RFPs, projects, and initiatives. Core to the function is analytics and modeling, including building financial models,
forecasting models, scenario building, sensitivity analysis, and total cost of ownership modeling.
Show more
Show less","Financial modeling, Forecasting models, Scenario building, Sensitivity analysis, Total cost of ownership modeling","financial modeling, forecasting models, scenario building, sensitivity analysis, total cost of ownership modeling","financial modeling, forecasting models, scenario building, sensitivity analysis, total cost of ownership modeling"
HR Data & Process Analyst,Avanos Medical,"Alpharetta, GA",https://www.linkedin.com/jobs/view/hr-data-process-analyst-at-avanos-medical-3787307049,2023-12-17,Atlanta,United States,Associate,Hybrid,"Here at Avanos Medical, we passionately believe in three things:
Making a difference in our products, services and offers, never ceasing to fight for groundbreaking solutions in everything we do;
Making a difference in how we work and collaborate, constantly nurturing our nimble culture of innovation;
Having an impact on the healthcare challenges we all face, and the lives of people and communities around the world.
At Avanos you will find an environment that strives to be independent and different, one that supports and inspires you to excel and to help change what medical devices can deliver, now and in the future.
Avanos is a medical device company focused on delivering clinically superior breakthrough solutions that will help patients get back to the things that matter. We are committed to creating the next generation of innovative healthcare solutions which will address our most important healthcare needs, such as reducing the use of opioids while helping patients move from surgery to recovery.
Headquartered in Alpharetta, Georgia, we develop, manufacture and market recognized brands in more than 90 countries. Avanos Medical is traded on the New York Stock Exchange under the ticker symbol AVNS. For more information, visit www.avanos.com.
The Role
The HR Data and Process Analyst role will support the HR organization by providing second tier support for HR data transactions, process troubleshooting and system analysis. This role requires a strong detail-oriented individual with knowledge across HR Centers of Excellence (COEs) who can analyze, troubleshoot and recommend the best solutions with a focus on process efficiency. This role will partner with our Global Human Resources team, working directly with the Centers of Excellence, the Global Human Resources Business Partners, and HR leadership to improve HR data, processes, and systems by proactively recommending improvements to make work easier.
This is a hybrid role located in Alpharetta, GA.
Job Roles and Responsibilities
Manage data requests from HR & managers for promotions, demotions, internal transfers, organizational restructures, manager changes, and other changes submitted through the HR Change Request form.
Maintain SuccessFactors Position data for the organization.
Analyze Change Requests coming from the HR Service Center to determine process improvements that could be made.
Recommend process efficiencies across the HR space.
Recommend processes to add to SuccessFactors or the HR Portal for employee/manager self-service submissions to further automate processes.
Manage data requests for annual reporting, census data, etc.
Regularly process errors from the HR Data Loader and Audit & Compliance Dashboard
Recommend system updates/audits and/or new business rules to implement in the dashboards based on data issues being uncovered.
Analyze issues with the position management process & recommend possible improvements to data, process, systems or behaviors.
Create and/or update standard operating procedures (SOPs) on a regular basis.
Publish SOPs to the appropriate location for easy access by the appropriate audience.
Qualifications
Required
Years of experience: 4+ years of HR data & transaction experience
Education: bachelor’s degree
Strong problem-solving skills, excellent verbal and written communication skills
Curious, flexible to change, self-starter with a high level of accountability
Detail oriented, process minded individual who partners well with others to process data, follow standard operating procedures, and provide improvement recommendations
Experience writing standard operating procedures
Understands the importance & impact of quality data & processes
Able to connect the impact of data to the process, people & technology
Familiarity with SuccessFactors functionality
Proficient with Excel and Microsoft Office suite
Able to quickly identify data anomalies and provide solutions for permanent fixes
Preferred
Experience working in a regulated industry environment
Previous experience writing Standard Operating Procedures
The statements above are intended to describe the general nature and level of work performed by employees assigned to this classification. Statements are not intended to be construed as an exhaustive list of all duties, responsibilities and skills required for this position.
Avanos Medical is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity or any other characteristic protected by law. If you are a current employee of Avanos, please apply here .
Join us at Avanos
Join us and you can make a difference in our products, solutions and our culture. Most of all, you can make a difference in the lives, people, and communities around the world.
Make your career count
Our commitment to improving the health and wellbeing of others begins with our employees – through a comprehensive and competitive range of benefits. We provide more than just a salary – our Total Rewards package encompasses everything you receive as an employee; your pay, health care benefits, retirement plans and work/life benefits.
Avanos offers a generous 401(k) employer match of 100% of each pretax dollar you contribute on the first 4% and 50% of the next 2% of pay contributed with immediate vesting.
free onsite gym | benefits on day 1 | HQ region voted ‘best place to live’ by USA Today
Show more
Show less","HR data, HR transactions, Process troubleshooting, System analysis, SuccessFactors, Data requests, Data management, Data analysis, Process improvement, Process automation, Data quality, Data governance, Data integrity, Data standards, Data security, Data privacy, Data reporting, Data analytics, Data visualization, Microsoft Office suite, Excel, Standard operating procedures, Problemsolving","hr data, hr transactions, process troubleshooting, system analysis, successfactors, data requests, data management, data analysis, process improvement, process automation, data quality, data governance, data integrity, data standards, data security, data privacy, data reporting, data analytics, data visualization, microsoft office suite, excel, standard operating procedures, problemsolving","data governance, data integrity, data management, data privacy, data quality, data reporting, data requests, data security, data standards, dataanalytics, excel, hr data, hr transactions, microsoft office suite, problemsolving, process automation, process improvement, process troubleshooting, standard operating procedures, successfactors, system analysis, visualization"
Sybase Database Engineer,ASK Consulting,"Alpharetta, GA",https://www.linkedin.com/jobs/view/sybase-database-engineer-at-ask-consulting-3776137959,2023-12-17,Atlanta,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
job Title: Sybase Database Engineer
Location: Alpharetta, GA
Duration: 12 months
Pay rate: $69
Job Description:
Required Skills:
Sybase
Oracle or PostGres, Nosql
Shell or Python Scripting
Roles And Responsibilities:
Looking for Sybase SME with some automation expertise with Oracle or PostGres, Nosql
Expertise in Installation, Upgrade & Patching of different components in Oracle, Postgres, Sybase and experience with the other platforms we manage listed in the Duties and Responsibilities.
Experience with Replication including setup, troubleshooting issues and failovers
Monitor all systems (work processes, users, system logs, short dumps, locks, developer traces, system traces, disk space, etc.).
Ensure production uptime service levels are maintained and made available per requirements that include backup, recovery, refresh, performance tuning, and security
Provide data cleansing services, verify data integrity, and implement access controls
Provide routine maintenance including problem defect analysis and resolution, space, storage, object management, physical database layout, rollback segment management, and temporary space management
Experience with Schema creation, deployment and maintenance
Experience with Setting up and configuration of Auditing, audit trails
Carry out recovery of databases in an event of log full, system crash
Troubleshoot issues related to the installation for our supported platforms
Should have knowledge of Disaster Recovery setup and maintenance
Automation using shell-script, python, go, etc.
Performance & Tuning Related Expertise
Assist Developers and business analysts with their queries and tune them, if required
Experience with Performance analysis, monitoring of SQL queries and platform specific tuning.
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Sybase, Oracle, PostgreSQL, NoSQL, Unix Shell, Python, Linux, Unix, Data Cleansing, Data Integrity, Access Control, Performance Tuning, Database Schema, Auditing, Disaster Recovery, Automation","sybase, oracle, postgresql, nosql, unix shell, python, linux, unix, data cleansing, data integrity, access control, performance tuning, database schema, auditing, disaster recovery, automation","access control, auditing, automation, data integrity, database schema, datacleaning, disaster recovery, linux, nosql, oracle, performance tuning, postgresql, python, sybase, unix, unix shell"
Senior Data Analyst,NorthPoint Search Group,"Roswell, GA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-northpoint-search-group-3787791690,2023-12-17,Atlanta,United States,Mid senior,Onsite,"What You’ll Do
Collaborate with multiple internal stakeholders in Google Cloud Learning organization to fulfill their reporting and analytic requests and convert them into BI solutions
Create ETL packages and scripts to extract and load data into a data warehouse using Google internal workflow engines to meet the business data needs
Migrate legacy workflows to the new infrastructure adhering to the new design standards
Implement data security and access policy standards
Requirements
Minimum Qualifications:
5-10 years of hands-on experience in Data Warehousing, ETL & Reporting
Extensive experience of executing multiple data warehouse projects in variety of business domains
Strong SQL and data manipulation skills, proficient in query development
Fluent in data visualizations techniques using tools such as Google Dashboards (PLX Dashboards), Data Studio, Looker, Tableau or similar technologies
Preferred Qualifications:
Prior experience with Google internal BI stack (Dremel/F1, BigQuery, PLX, Looker)
Experience in statistical modeling using tools such as R, SciPy, SAS
Proficient in building analytical models in Google Sheets or Excel
Excellent problem-solving, project management, team collaboration and organizational skills
Nice, but not required: Familiar with HTML, Apps Script, Javascript, Python, Unix Shell Scripting
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Powered by JazzHR
iNoCEKS5QS
Show more
Show less","Data Warehousing, ETL, Reporting, Data Visualization, SQL, Data Manipulation, Google Dashboards, Data Studio, Looker, Tableau, Dremel/F1, BigQuery, PLX, R, SciPy, SAS, Google Sheets, Excel, ProblemSolving, Project Management, Team Collaboration, Organizational Skills, HTML, Apps Script, Javascript, Python, Unix Shell Scripting","data warehousing, etl, reporting, data visualization, sql, data manipulation, google dashboards, data studio, looker, tableau, dremelf1, bigquery, plx, r, scipy, sas, google sheets, excel, problemsolving, project management, team collaboration, organizational skills, html, apps script, javascript, python, unix shell scripting","apps script, bigquery, data manipulation, data studio, datawarehouse, dremelf1, etl, excel, google dashboards, google sheets, html, javascript, looker, organizational skills, plx, problemsolving, project management, python, r, reporting, sas, scipy, sql, tableau, team collaboration, unix shell scripting, visualization"
HR Data Analyst,MetroPower,"Tucker, GA",https://www.linkedin.com/jobs/view/hr-data-analyst-at-metropower-3785900260,2023-12-17,Atlanta,United States,Mid senior,Onsite,"PPC Partners, Inc. is looking for an HR Data Analyst to join our team!
Under the direction of the HR Shared Services Manager, the HR Data Analyst will collaborate with members of the HR team across multiple states to complete various assignments supporting the success of the Human Resources Department
Essential Duties And Responsibilities
Works as the primary point of contact for report requests coming from both HR team members as well as PPC Operational groups. These reports may include Affirmative Action data, benefits census data, and salary surveys
Work as the primary point of contact for managing employee changes within our payroll and ERP platforms
Work closely with the payroll departments to report any required payroll changes such as retro-payments, benefit deduction corrections, and paid time off payouts
Confirm that all systems are staying up to date for costs associated with benefit elections and other employee changes initiated within ADP or other internal processes
Coordinate drug testing programs across all PPC companies including post accident, random, and reasonable suspicion requests
Support the termination processes, reporting to appropriate internal and external teams as required
Answer general employee inquiries from across the PPC enterprise
Assist employees and manager navigate our online platforms including ADP, IFS, Cornerstone, and Macorva
Maintain accurate employee records, filing documentation electronically in a timely manner
Show more
Show less","Report creation, Data analysis, Payroll management, ERP systems, ADP, IFS, Cornerstone, Macorva, Employee records management","report creation, data analysis, payroll management, erp systems, adp, ifs, cornerstone, macorva, employee records management","adp, cornerstone, dataanalytics, employee records management, erp systems, ifs, macorva, payroll management, report creation"
"Engineer III, Big Data",Pilot Company,"Roswell, GA",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-company-3784209680,2023-12-17,Atlanta,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and
digital
innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research,
development
and modification of extract, transform, load processes and jobs in support of a
Big
Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","Data Extraction, Data Transformation, Data Loading, Big Data Infrastructure, Data Architecture, Data Modeling, SQL, Oracle, Postgres, SQL Server, Relational Databases, Triggers, Stored Procedures, Constraints, Apache Spark, Spark Streaming, Message Queues, Python, Enterprise Data Warehouse (EDW), Star Schema Data Modeling, Amazon Web Services (AWS), Cloud Big Data Platform, Analytical Skills, Data Problem Solving, Customer Service, Written Communication, Verbal Communication, New Technology Learning, Mentoring, Motivation, Prioritization, Multitasking, Project Management, Organizational Skills, Attention to Detail, Communication, Negotiation, Problem Solving, Decision Making, SelfMotivation, Teamwork","data extraction, data transformation, data loading, big data infrastructure, data architecture, data modeling, sql, oracle, postgres, sql server, relational databases, triggers, stored procedures, constraints, apache spark, spark streaming, message queues, python, enterprise data warehouse edw, star schema data modeling, amazon web services aws, cloud big data platform, analytical skills, data problem solving, customer service, written communication, verbal communication, new technology learning, mentoring, motivation, prioritization, multitasking, project management, organizational skills, attention to detail, communication, negotiation, problem solving, decision making, selfmotivation, teamwork","amazon web services aws, analytical skills, apache spark, attention to detail, big data infrastructure, cloud big data platform, communication, constraints, customer service, data architecture, data extraction, data loading, data problem solving, data transformation, datamodeling, decision making, enterprise data warehouse edw, mentoring, message queues, motivation, multitasking, negotiation, new technology learning, oracle, organizational skills, postgres, prioritization, problem solving, project management, python, relational databases, selfmotivation, spark streaming, sql, sql server, star schema data modeling, stored procedures, teamwork, triggers, verbal communication, written communication"
Master Data Analyst I,WEG Electric Corp. (USA),"Duluth, GA",https://www.linkedin.com/jobs/view/master-data-analyst-i-at-weg-electric-corp-usa-3747767691,2023-12-17,Atlanta,United States,Mid senior,Onsite,"About WEG Electric Corp.
WEG is the largest industrial electric motor manufacturer in the Americas and one of the largest manufacturers of electric motors in the world producing more than 21 million units annually. Founded in 1961, WEG operates mainly in the sector of capital goods, having five main businesses: Motors, Energy, Automation, Coatings Transmission & Distribution, providing global solutions for electric motors, variable frequency drives, soft starters, controls, panels, transformers, and generators. The US corporate office is located in Duluth, GA with manufacturing units in Minnesota, Indiana and Missouri with over 30 thousand employees worldwide. Visit our website to learn more: www.weg.net
About The Role
WEG Electric Corp. has a great opportunity for a Master Data Analyst I to join our team in Duluth, GA. As a key addition to the team who will have in-depth knowledge of material Master Data in SAP, the role will primarily support Low Voltage Motors Department, Engineering and Product Management teams, throughout the entire product life cycle.
Assist Engineering and Product Management with general support requests: answering questions about processes and procedures; assist with historical data information in legacy systems; material validation exceptions.
Assist Product Managers coordinating Low Voltage Motors and Parts Product Launch: creating/expanding standard and optional materials in SAP configuration and via class; communicating with factories in Brazil, Mexico and others; filling out Product Management Template and submitting claims with information for workflow completion.
Collaborate with cross-functional teams to ensure Low Voltage Motors data is correct through all work streams; understand how data management process impact sales and supply chain processes.
Validation of Low Voltage Motors Master Data on a monthly schedule to find inconsistencies and fix them; collaborate with Global Master Data teams at the factory levels to ensure integrated processes consider local branch requirements.
Update in-house Motor Modification Tool with new and updated product and/or pricing features
Contribute new topics to SENS (in-house virtual assistant): topics include products, engineering questions, commercial questions or systems used in our Sales Department.
Support Master Data team members by completing new material workflows, pricing requests or collaborating on group projects as needed.
Collaborate with other Product Management or Master Data teams to ensure best practices are developed and shared throughout the organization.
Communicate printed catalog updates to our Marketing teams in Headquarters and local office.
Other duties and tasks as assigned.
Education
High School diploma or GED.
Knowledge / Training
SAP working knowledge.
Excel.
Experience working with large data sets.
Experience working with database management or data life cycle management processes.
Electrical/Mechanical/Technical background a plus but not required.
Experience
2 years of working experience or Bachelors Degree in related field.
We offer a pleasant work environment, competitive compensation and full benefits package. To Apply: All candidates must submit an updated resume and complete our application in order to be considered for this position.
We are proud to be an Equal Opportunity/Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, sex, disability, veteran, or other protected status.
Show more
Show less","SAP, Excel, Data Management, Database Management, Data Life Cycle Management, Product Management, Low Voltage Motors, Master Data","sap, excel, data management, database management, data life cycle management, product management, low voltage motors, master data","data life cycle management, data management, database management, excel, low voltage motors, master data, product management, sap"
"Data Engineer Tech Lead - Atlanta, GA",CRH,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-tech-lead-atlanta-ga-at-crh-3763922765,2023-12-17,Atlanta,United States,Mid senior,Onsite,"Job ID: 489781
CRH's Americas Materials, a division of CRH, is the leading vertically integrated supplier of aggregates, asphalt, ready mixed concrete and paving and construction services in North America. Our operations span North America with over 28,000 employees at close to 1,600 locations in 43 US States and 2 Canadian provinces.
Position Overview
The Data tech lead will bring substantial experience in data solutions to help unlock the power of our data for our stakeholders. As a data tech lead you will handle the increasing complexity and demand for data from various sources, enabling business to make informed decisions. You will provide technical leadership to team of ETL developers in designing and implementing a cutting-edge data platform and pipelines. Data tech lead will also work closely with business analysts and stakeholders in other departments to identify, recommend, develop, implement, and support data solutions.
Key Responsibilities (Essential Duties And Functions)
Effectively communicate complex technical concepts to business stakeholders, helping them understand the development process and set realistic expectations.
Offer guidance and recommendations to the development team, stakeholders based on best practices and industry trends and maximize the return on the investment.
Clarify/translate requirements, data strategy into Epics/Stories & remove ambiguity and provide accurate estimates of the time and resources needed to complete tasks.
Anticipate potential challenges or edge cases, addressing them before they escalate into significant issues.
Collaborate between business stakeholders and development teams, ensuring everyone’s input is considered, and the project stays on track.
Lead the design and implementation of the data & analytics architecture ensuring compliance, quality and a sustainable growth of the platform.
Mentor and train other data team members and business users as needed.
Build scalable data pipelines to integrate and model datasets from different sources that meet functional and non-functional requirements.
Provide the technology, the tools and data to support enterprise-wide reporting, analytics, data science and self-service need.
Delegating tasks and work with the technical team to achieve established timeline.
Ensure the data quality by implementing and monitoring a data quality framework together with the data source teams.
Formulate and complete end-to-end analysis that includes data gathering, analysis, ongoing scaled deliverables, and presentations.
Help to grow our data team with right talents.
Qualifications
Education/Experience
Master or Bachelor’s Degree in Engineering/Computer related discipline, or equivalent experience required
10+ years of ETL / ELT Design, development and implementation experience using standard ETL/ ELT tool
10+ years of SQL Development Experience
7+ years of experience defining and managing ETL / ELT using tools and industry standard practices.
3+ years of Cloud (Azure) experience
3+ years of Snowflake experience
3+ years of SQL Server Reporting Services
2+ years of experience leading, coaching, and providing mentorship to ETL developers in an agile environment.
Hands-on and deep experience using Star/Snowflake for schema design, data modeling, data pipelines.
Work Requirements
Must be 18 years in age or older.
Must pass pre-employment drug screen and criminal background check.
Strict adherence to safety requirements and procedures as outlined in the Employee Handbook.
Willingness to work independently within in a team environment and assist the team with other duties as required.
Knowledge/Skill Requirements
Advanced knowledge of SQL databases and queries.
Advanced knowledge of CDC (Change Data Capture) & CT (Change Tracking) inner workings.
Experience in implementing and supporting several different ETL / ELT solutions.
Experience in working with many different data sets, large and small.
Experience working with data in multiple ERP and data movement technologies.
Experience with normalizing and de-normalizing data and creating different fit-for-purpose data model.
Knowledge of ETL best practices and standards.
Ability to quickly resolve issues.
Experience with Cloud (Azure), Snowflake, SQL Server 2014 – SQL Server 2019 , SQL Server Reporting Services , SQL Server Integration Services.
Ability to present ideas in concise, business-friendly and user-friendly language.
Excellent written, interpersonal and verbal communication skills.
Experience with managing and collaborating with multidisciplinary teams.
Experience with design and development of EDW (Enterprise Data warehouse) is preferred.
Experience with Data Lakehouse, Data Fabric architecture framework is preferred.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Able to communicate with others by telephone and in person.
Able to utilize a computer for word processing, email communication, and preparation of documents and presentations.
Sitting for extended periods of time.
Ability to safely lift up to 50 pounds.
The statements included in this job description are not intended to be all-inclusive. They represent typical elements and criteria necessary to successfully perform the job. Other duties may be assigned as required. Requirements for this job are subject to modification by the Company and its designees, and reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
What CRH Offers You
Highly competitive base pay
Comprehensive medical, dental and disability benefits programs
Group retirement savings program
Health and wellness programs
A diverse and inclusive culture that values opportunity for growth, development, and internal promotion
About CRH
CRH has a long and proud heritage. We are a collection of hundreds of family businesses, regional companies and large enterprises that together form the CRH family. CRH operates in a decentralized, diversified structure that allows you to work in a small company environment while having the career opportunities of large international organization.
If you’re up for a rewarding challenge, we invite you to take the first step and apply today! Once you click apply now, you will be brought to our official employment application. Please complete your online profile and it will be sent to the hiring manager. Our system allows you to view and track your status 24 hours a day. Thank you for your interest!
CRH Americas Materials Inc. is an Affirmative Action and Equal Opportunity Employer.
EOE/Vet/Disability--If you want to know more, please click on this link.
Show more
Show less","SQL, Cloud (Azure), Snowflake, SQL Server Reporting Services, SQL Server Integration Services, ETL/ELT, CDC (Change Data Capture), CT (Change Tracking), Data modeling, Data pipelines, Data quality framework, Star/Snowflake schema design, EDW (Enterprise Data Warehouse), Data Lakehouse, Data Fabric architecture framework","sql, cloud azure, snowflake, sql server reporting services, sql server integration services, etlelt, cdc change data capture, ct change tracking, data modeling, data pipelines, data quality framework, starsnowflake schema design, edw enterprise data warehouse, data lakehouse, data fabric architecture framework","cdc change data capture, cloud azure, ct change tracking, data fabric architecture framework, data lakehouse, data quality framework, datamodeling, datapipeline, edw enterprise data warehouse, etlelt, snowflake, sql, sql server integration services, sql server reporting services, starsnowflake schema design"
"Engineer III, Big Data",Pilot Flying J,"Roswell, GA",https://www.linkedin.com/jobs/view/engineer-iii-big-data-at-pilot-flying-j-3783968651,2023-12-17,Atlanta,United States,Mid senior,Onsite,"Company Description
Pilot Company is an industry-leading network of travel centers with more than 30,000 team members and over 750 retail and fueling locations in 44 states and six Canadian provinces. Our energy and logistics division serves as a top supplier of fuel, employing one of the largest tanker fleets and providing critical services to oil operations in our nation's busiest basins. Pilot Company supports a growing portfolio of brands with expertise in supply chain and retail operations, logistics and transportation, technology and
digital
innovation, construction, maintenance, human resources, finance, sales and marketing.
Founded in 1958 by Jim A. Haslam II and currently led by CEO Adam Wright, our founding values, people-first culture and commitment to giving back remains true to us today. Whether we are serving guests, a fellow team member, or a trucking company, we are dedicated to fueling people and keeping North America moving.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other characteristic protected under applicable federal, state or local law.
Job Description
The purpose of this job is to provide technical expertise for research,
development
and modification of extract, transform, load processes and jobs in support of a
Big
Data infrastructure for Pilot Flying J (PFJ).
Research, develop, document, and modify Big Data Lake processes and jobs per data architecture and modeling requirements; collaborate with Data and Analytics data strategists and data scientists
Collaborate with business stakeholders to understand data needs including data velocity, veracity, and access patterns
Provide technical expertise to implement Data and Analytics specifications
Serve on cross-functional project teams and provide the data and big data perspective on executing key deliverables
Troubleshoot complex, escalated issues including connection, failed jobs, application errors, server alerts and space thresholds within predefined service level agreements (SLAs)
Proactively maintain and tune all code according to Big Data and EDW best practices to prevent issues
Review and ensure appropriate documentation for all new development and modifications of the Big Data Lake processes and jobs
Perform code and process reviews and oversee testing for solutions developed, and ensure integrity and security of institutional data
Educate business stakeholders on the usage and benefits of the EDW, Big Data Lake and related technologies.
Mentor and guide less experienced team members and provide feedback on project work
Model behaviors that support the company’s common purpose; ensure guests and team members are supported at the highest level
Ensure all activities are in compliance with rules, regulations, policies, and procedures
Complete other duties as assigned
Qualifications
Bachelor’s degree in computer science, engineering, information technology, or related field, required.
Minimum 7 years of technology operations experience required.
Strong SQL knowledge and skills required
Strong knowledge of Relational Databases like Oracle, Postgres or SQL Server required
Strong knowledge of relational modeling and features including triggers, stored procedures, and constraints required
Experience with Apache Spark or Spark-streaming, Message Queue technologies and Python required
Strong knowledge of enterprise data warehouse (EDW) data models with a focus on Star Schema data modeling
techniques required
Strong knowledge of Amazon Web Services (AWS) or similar Cloud Big Data platform preferred
Excellent analytical skills and the ability to identify solutions to complex data problems
Ability to provide excellent customer service
Excellent written and verbal communication skills
Willingness to learn and embrace new technologies
Ability to mentor and motivate a diverse team; ensure team and individual accountability and performance standards are met
Ability to prioritize, multitask and manage multiple projects successfully in a fast-paced and dynamic environment
Strong organizational skills with attention to detail
Ability to communicate and interact effectively with different levels of the organization to negotiate, problem solve, complete projects and influence decision making
Self-motivated with ability to work both independently and within teams in order to establish and meet deadlines, goals, and objectives
Additional Information
Travel required less than 10%
General office work requiring sitting or standing for long periods of time
Able to lift up to 30 lbs.
Able to work evenings, weekends and odd hours as needed
Show more
Show less","Apache Spark, Sparkstreaming, Message Queue technologies, Python, AWS, Cloud Big Data platform, EDW, Star Schema data modeling, Relational Databases, Oracle, Postgres, SQL, SQL Server, Software Development, Data Warehouse, Big Data, Data Lake, Serverless Architectures","apache spark, sparkstreaming, message queue technologies, python, aws, cloud big data platform, edw, star schema data modeling, relational databases, oracle, postgres, sql, sql server, software development, data warehouse, big data, data lake, serverless architectures","apache spark, aws, big data, cloud big data platform, data lake, datawarehouse, edw, message queue technologies, oracle, postgres, python, relational databases, serverless architectures, software development, sparkstreaming, sql, sql server, star schema data modeling"
AWS Lead Data Engineer,Extend Information Systems Inc.,"Atlanta, GA",https://www.linkedin.com/jobs/view/aws-lead-data-engineer-at-extend-information-systems-inc-3758286294,2023-12-17,Atlanta,United States,Mid senior,Onsite,"Hi,
Naveen, this side I have a new an excellent opportunity for you. This opportunity is a Contract position as
AWS Lead Data Engineer
. Please have a look at the job description below and let me know if you or someone you know is interested in this role. You can mail me at naveen@extendinfosys.com.
Job Title AWS Lead Data Engineer
Location Atlanta, GA (once in a month)
Job Type Contract
Job Description
Bachelors in a data and analytics field such as Information Systems, Computer Science, Business Administration etc.
5+ years of hands-on experience implementing projects on AWS workloads in a solutions delivery role
5+ years' experience with solutions delivery using well-architected cloud frameworks, services, and technologies
5+ years of experience in Python scripting, Shell Scripting, R, SQL Server, UNIX, and Linux.
5+ years of experience in data compilation, reporting and analytics and data analysis
10+ years total IT, development, and architecture experience.
Experience designing processes around migration and assessments of applications moving to the cloud.
Experience data processing, data normalization, data ingestion, and automation using web-based RESTFUL APIs
Experience with open-source technology, software development and systems engineering
Knowledge of Microsoft Office products and analytic tools such as Tableau, SAS etc.
Knowledge of Amazon Web Services Cloud and technical services such as EC2, Lambda, S3, RDS and Redshift
Ability to strategize, define and deliver cloud migration plan per customer needs
Demonstrated experience overseeing the migration of multiple applications to the cloud.
Ability to discuss highly technical cloud concepts and topics to non-technical audiences
Strong organizational and project management skills.
AWS certifications would be a plus.
Thanks and Regards,
Naveen Shukla |Technical Recruiter| Extend Information Systems
Cell:
(571) 547-2799
Email: naveen@extendinfosys.com
Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA - 20166
Web: www.extendinfosys.com
Show more
Show less","AWS, Python, Shell Scripting, R, SQL Server, UNIX, Linux, Tableau, SAS, EC2, Lambda, S3, RDS, Redshift, RESTFUL APIs, Data migration, Data assessment, Data processing, Data normalization, Data ingestion, Automation, Opensource technology, Software development, Systems engineering, Microsoft Office products, Analytic tools, Cloud migration, Cloud computing","aws, python, shell scripting, r, sql server, unix, linux, tableau, sas, ec2, lambda, s3, rds, redshift, restful apis, data migration, data assessment, data processing, data normalization, data ingestion, automation, opensource technology, software development, systems engineering, microsoft office products, analytic tools, cloud migration, cloud computing","analytic tools, automation, aws, cloud computing, cloud migration, data assessment, data ingestion, data migration, data normalization, data processing, ec2, lambda, linux, microsoft office products, opensource technology, python, r, rds, redshift, restful apis, s3, sas, shell scripting, software development, sql server, systems engineering, tableau, unix"
Big Data Engineer,The Intersect Group,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/big-data-engineer-at-the-intersect-group-3775444254,2023-12-17,Atlanta,United States,Mid senior,Onsite,"Big Data Engineer
On-site in ATL, GA
12 Month Contract
Job Summary:
You will participate in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions and perform data ingestion development and support duties.
Requirements:
6+ years of overall IT experience
3+ years of experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming
Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka
Deep knowledge of troubleshooting and tuning Spark applications
3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV
3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, HDFS, or
3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets
2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark
3+ years of experience with Cloud platforms e.g. AWS, GCP, etc.
3+ years of experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or BigQuery
2+ years of experience with NoSQL databases, including HBASE and/or Cassandra
Experience in successfully building and deploying a new data platform on Azure/ AWS
Experience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and Glue
Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series
Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog
Knowledge of Unix/Linux platform and shell scripting is a must
Strong analytical and problem-solving skills
Show more
Show less","Apache Kafka, Spark Streaming, Spark Structured Streaming, Troubleshooting, Spark applications, Message Queues, Hadoop, Spark, Spark SQL, Sqoop, Hive, S3, HDFS, Python, PySpark, Scala, ScalaSpark, AWS, GCP, Cloud platforms, Kudu, Impala, Delta Lake, Snowflake, BigQuery, NoSQL databases, HBASE, Cassandra, Azure, Azure / AWS Serverless technologies, S3, Kinesis/MSK, lambda, Glue, Messaging Platforms, Kafka, Amazon MSK, TIBCO EMS, IBM MQ Series, Databricks UI, Databricks Notebooks, Delta Lake, Unix/Linux platform, Shell scripting","apache kafka, spark streaming, spark structured streaming, troubleshooting, spark applications, message queues, hadoop, spark, spark sql, sqoop, hive, s3, hdfs, python, pyspark, scala, scalaspark, aws, gcp, cloud platforms, kudu, impala, delta lake, snowflake, bigquery, nosql databases, hbase, cassandra, azure, azure aws serverless technologies, s3, kinesismsk, lambda, glue, messaging platforms, kafka, amazon msk, tibco ems, ibm mq series, databricks ui, databricks notebooks, delta lake, unixlinux platform, shell scripting","amazon msk, apache kafka, aws, azure, azure aws serverless technologies, bigquery, cassandra, cloud platforms, databricks notebooks, databricks ui, delta lake, gcp, glue, hadoop, hbase, hdfs, hive, ibm mq series, impala, kafka, kinesismsk, kudu, lambda, message queues, messaging platforms, nosql databases, python, s3, scala, scalaspark, shell scripting, snowflake, spark, spark applications, spark sql, spark streaming, spark structured streaming, sqoop, tibco ems, troubleshooting, unixlinux platform"
"Hadoop/Python Database Engineer, Remote",Syrinx Consulting,"Boston, MA",https://www.linkedin.com/jobs/view/hadoop-python-database-engineer-remote-at-syrinx-consulting-3589720444,2023-12-17,Cambridge,United States,Mid senior,Remote,"U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. No Corp to Corp. W2 only.
Description
Top Tier Boston/National based client is looking for a new (long term/perm) contractor to assist the data build deployments of a special services department ongoing project. Financial systems and subject matter experts is a plus. Some analysis is also expect as a regular part of the analytics support of the Big Data work. Specific Hadoop, and Python skills demonstration will be required.
Responsibilities
Creating data repository schema for databases and data warehouses (i.e., specifying the overall database/data warehouse field and data structure based on functional and technical requirements).
Ideally, a candidate has experience in developing logical and physical data models. Developing strategies for data acquisition, archive recovery, and database implementation. Managing data migrations/conversions and troubleshooting data processing issues. Provide functional level data analytics in supporting of reporting needs. Bachelor’s degrees in any of these fields: mathematics, statistics, computer science, data science/analytics or similar quantitative field (maters preferred)
Qualifications
3-5+ years’ experience with data modeling, managing data transfers, ETL Development, and data warehousing
Primary tools/techniques – Python, R, Hadoop, clustering, high availability environments
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Visualization techniques and tools using Tableau, Power BI, or other BI packages.
AWS or Azure cloud certified. Having both would be a dream.
Experience with machine learning and natural language (NLP) processing techniques is a big plus.
U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. No Corp to Corp. W2 only.
The ideal candidate is in the Boston Area or EST US time zone, and can go permanent long term. The role has a measure of security interest, so only certain authorizations can be considered.
If you are interested, please apply to learn more.
Show more
Show less","Python, R, Hadoop, Clustering, Data Modeling, Data Warehousing, Data Analytics, SQL, Tableau, Power BI, Machine Learning, NLP, AWS, Azure","python, r, hadoop, clustering, data modeling, data warehousing, data analytics, sql, tableau, power bi, machine learning, nlp, aws, azure","aws, azure, clustering, dataanalytics, datamodeling, datawarehouse, hadoop, machine learning, nlp, powerbi, python, r, sql, tableau"
Staff Data Engineer,Recruiting from Scratch,"Portland, OR",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392329,2023-12-17,Bethany,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Stream Processing, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management, Data Classification, Data Retention, Legal Compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering, tdd, pair programming, continuous integration, automated testing, deployment, stream processing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data management, data classification, data retention, legal compliance","agile engineering, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, stream processing, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Portland, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744390731,2023-12-17,Bethany,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification, data retention","airflow, data classification, data management tools, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm"
Principal Data Engineer / Data Architect,HP,"Vancouver, WA",https://www.linkedin.com/jobs/view/principal-data-engineer-data-architect-at-hp-3769169927,2023-12-17,Bethany,United States,Mid senior,Hybrid,"HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations, and dreams. We apply new thinking and ideas to create simpler, more valuable, and trusted experiences with technology, continuously improving the way our customers live and work.
In the GTM advanced analytics COE, our mission is to deliver impact by building machine learning (ML) products to optimize pricing, marketing investments and provide guidance to sales and other HP teams.
We're looking for a principal data engineer / data architect to join our data engineering team.
Qualifications
Typically 5+ years of experience in software or data engineering.
Extensive experience in data modeling, data integration and processing of structured and unstructured data.
Highly proficient in one or more programming languages (Python preferred).
Strong SQL proficiency (experience with NoSQL – advantage).
Highly experienced with Apache Spark / pyspark
Excellent communication skills; mastery in English and local language.
Ability to effectively communicate product architectures, design and change proposals.
Familiar with best practices of the data and software engineering lifecycle and/or best practices of the above platforms and tools.
Additional Preferred Qualifications / Advantages
Databricks
Pandas
Dataiku DSS
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Python, SQL, NoSQL, Apache Spark, Pyspark, Data Modeling, Data Integration, Data Processing, Dataiku DSS, Databricks, Pandas","python, sql, nosql, apache spark, pyspark, data modeling, data integration, data processing, dataiku dss, databricks, pandas","apache spark, data integration, data processing, databricks, dataiku dss, datamodeling, nosql, pandas, python, spark, sql"
Data Analyst,"Kelly Science, Engineering, Technology & Telecom",Greensboro--Winston-Salem--High Point Area,https://www.linkedin.com/jobs/view/data-analyst-at-kelly-science-engineering-technology-telecom-3777099742,2023-12-17,North Carolina,United States,Associate,Onsite,"Kelly® Engineering is seeking a Cost / Data Analyst to join one of our leading automotive manufacturing clients at their Greensboro, NC location.
Duration: Contract to Hire
Shift: 1st - 100% on-site in Greensboro, NC
Compensation: Negotiable between $35-45/hr - W2
IDEAL CANDIDATE
:
2+ years industry experience in data analytics, cost analytics, materials management, and/or inside sales.
Automotive, Materials, or Manufacturing experience preferred
Great communication skills
Job Summary
:
Product cost and material cost forecasting for new products, ensuring accurate content and information.
Work with Engineering to create & update bills of materials Identify & capture product costs, risks, and opportunities from cross functional teams.
Analyze data for insights, trends, and reporting.
Product cost optimization and control/monitor product cost activities within projects.
Work in close cooperation with product development and purchasing.
Support other PFEs, Finance Project Managers, CPMs, etc. in analysis.
Preferred skills/competencies
2+ year professional experience
Business, Engineering, Purchasing, or Cost Accounting background
Previous automotive experience preferred
Teamwork oriented - able to manage and structure large amount of data
MS excel (large amounts of data, pivot, etc.)
Able to compile information and present in a structured way.
Show more
Show less","Data Analytics, Cost Analytics, Materials Management, Inside Sales, Automotive Experience, Materials Experience, Manufacturing Experience, Communication Skills, Product Cost Forecasting, Bill of Materials, Product Cost Analysis, Data Analysis, Reporting, Product Cost Optimization, Cost Control, Product Development, Purchasing, Teamwork, Data Management, Microsoft Excel, Presentation Skills","data analytics, cost analytics, materials management, inside sales, automotive experience, materials experience, manufacturing experience, communication skills, product cost forecasting, bill of materials, product cost analysis, data analysis, reporting, product cost optimization, cost control, product development, purchasing, teamwork, data management, microsoft excel, presentation skills","automotive experience, bill of materials, communication skills, cost analytics, cost control, data management, dataanalytics, inside sales, manufacturing experience, materials experience, materials management, microsoft excel, presentation skills, product cost analysis, product cost forecasting, product cost optimization, product development, purchasing, reporting, teamwork"
Marketing Data Analyst,Starhired,"Reidsville, NC",https://www.linkedin.com/jobs/view/marketing-data-analyst-at-starhired-3755359494,2023-12-17,North Carolina,United States,Associate,Onsite,"We are looking for an experienced Marketing Data Analyst to work in our marketing department. The ideal Data Analytics professional will have a strong Data driven marketing background, Analytics, Competitive Analysis, Data Collection, and data management experience. Monitor and report competitor activities, Loyalty Pricing & Promo, Digital Product Development, Consumer Insights. Actively look for branding opportunities on channel level, Develop Sales Incentives programs, and implement data management systems.
What you will do as a Marketing Data Analyst role:
Marketing performance metrics and use statistical modeling to interpret content consumption patterns and behavioral signals to provide recommendations for Marketing optimization.
Analytics & Behavioral Change, Retail Analytics, Loyalty Pricing & Promo, Digital Product Development, Consumer Insights and Retail Marketing
Implement data analysis process into sales initiatives.
Analyze customer data, generate insights, and provide actionable recommendations to optimize our marketing efforts.
Generate reports, dashboards, and visualizations to communicate key findings and insights to stakeholders in a clear and concise manner.
Define KPI's and other metric sales goals data.
Other tasks as assigned.
Requirements For The Marketing Data Analyst Role
3 or more years’ experience as a Data Analyst
A bachelor's degree in computer science, statistics, or a related data-focused field
Mathematical/statistical/applied analytics background
Excel expert level experience required.
Microsoft Business Intelligence
Machine learning/AI for marketing experience a plus
If you are a Marketing Data Analyst with experience, please apply today!
Show more
Show less","Data Analytics, Competitive Analysis, Data Collection, Data Management, Data driven marketing, Statistical Modeling, Data Visualization, Sales Incentives, Data Analysis, Customer Data Analysis, Actionable Recommendations, Reporting, Dashboards, Key Performance Indicators, Machine Learning, Artificial Intelligence, Excel, Microsoft Business Intelligence","data analytics, competitive analysis, data collection, data management, data driven marketing, statistical modeling, data visualization, sales incentives, data analysis, customer data analysis, actionable recommendations, reporting, dashboards, key performance indicators, machine learning, artificial intelligence, excel, microsoft business intelligence","actionable recommendations, artificial intelligence, competitive analysis, customer data analysis, dashboard, data collection, data driven marketing, data management, dataanalytics, excel, key performance indicators, machine learning, microsoft business intelligence, reporting, sales incentives, statistical modeling, visualization"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783189119,2023-12-17,North Carolina,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Charlott-DataScientist.017
Show more
Show less","Python, JavaScript, JSON, Generative AI, Data Science, R, OOP, Algorithms, Coaching, Problem Solving, Collaboration, Communication, Machine Learning, Research, Software Development, Product Development, AI Training Models, Finetune, Natural Language Processing","python, javascript, json, generative ai, data science, r, oop, algorithms, coaching, problem solving, collaboration, communication, machine learning, research, software development, product development, ai training models, finetune, natural language processing","ai training models, algorithms, coaching, collaboration, communication, data science, finetune, generative ai, javascript, json, machine learning, natural language processing, oop, problem solving, product development, python, r, research, software development"
Data Analyst,Apex Systems,Raleigh-Durham-Chapel Hill Area,https://www.linkedin.com/jobs/view/data-analyst-at-apex-systems-3783985388,2023-12-17,North Carolina,United States,Associate,Hybrid,"Our client is seeking a Data Analyst to work hybrid in Raleigh, NC.
This position will support the marketing department that leverages customer data to determine communications and campaigns. Creates and manages outreach and lead programs. Provides reporting and analysis on outreach programs to product and line of business owners. Provides consultation to team members and other stakeholders on tactics that leverage data.
Qualifications:
3-5 years where your primary job function was Data Analysis
Strong MS Excel skills – think pivot tables, macros, etc.
Strong experience with either Alteryx, Power BI or Tableau
Know how to extract and query data using SAS
Experience working with spreadsheets that contain millions of rows of data
Banking/financial industry experience is preferred
Bachelors Degree is preferred
Responsibilities:
Data Visualization – Develops intuitive and insightful dashboards and reports to solve business problems.
Data Acquisition – Collects, refines, and prepares data for analytics and visualization. Optimizes query and dashboard performance.
Data Strategy – Identify opportunities for the Bank to better leverage data and technology to improve profitability and make recommendations to management. Facilitates initiatives that develop the Bank’s data management capabilities. Assists management in creating and implementing business plans to achieve planned objectives. Maintains a strong knowledge of current and emerging technologies or trends impacting the financial service industry.
Data Enhancement - Identifies and implements enhancements for the quality, management, and storage of enterprise data. Executes initiatives that improve profiling, exception reporting, and issue resolution as well as general fixes for the Enterprise Data Warehouse.
Show more
Show less","Data Analysis, MS Excel, Alteryx, Power BI, Tableau, SAS, Data Visualization, Data Acquisition, Data Strategy, Data Enhancement, Profiling, Exception Reporting, Issue Resolution, Enterprise Data Warehouse","data analysis, ms excel, alteryx, power bi, tableau, sas, data visualization, data acquisition, data strategy, data enhancement, profiling, exception reporting, issue resolution, enterprise data warehouse","alteryx, data acquisition, data enhancement, data strategy, dataanalytics, enterprise data warehouse, exception reporting, issue resolution, ms excel, powerbi, profiling, sas, tableau, visualization"
jr Data Engineer,"Princeton IT Services, Inc","Raleigh, NC",https://www.linkedin.com/jobs/view/jr-data-engineer-at-princeton-it-services-inc-3695655087,2023-12-17,North Carolina,United States,Mid senior,Onsite,"Position
: Jr Data Engineer
Location
: Raleigh, NC or Boston, MA
Job Length
: Long term
Position Type
: C2C/W2
Qualifications
4 years of Experience in Java , Python , Spark
4 years Experience in Snowflake, Data Pipelines , SQL
Stays current with technology trends in order to provide best options for solutions Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently
Responsibilities
Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required
: Snowflake, Data Pipelines , SQL
Show more
Show less","Java, Python, Spark, Snowflake, Data Pipelines, SQL, Data Processing, Data Analysis, Data Warehousing, Data Transformation, Data Optimization, Business Intelligence, Reporting, Analytics","java, python, spark, snowflake, data pipelines, sql, data processing, data analysis, data warehousing, data transformation, data optimization, business intelligence, reporting, analytics","analytics, business intelligence, data optimization, data processing, data transformation, dataanalytics, datapipeline, datawarehouse, java, python, reporting, snowflake, spark, sql"
Data Engineer / Analyst,"Locus Biosciences, Inc.","Triangle, NC",https://www.linkedin.com/jobs/view/data-engineer-analyst-at-locus-biosciences-inc-3686682988,2023-12-17,North Carolina,United States,Mid senior,Onsite,"Locus Biosciences is seeking a highly motivated and entrepreneurial applicant to join our team. In this role, you will be responsible for developing, maintaining, and troubleshooting workflows and associated architecture to process and distribute data. You will play a pivotal role establishing automation that integrates our Electronic Lab Notebook with data generated via high-throughput robotics, ensuring that results of these workflows are delivered in a timely manner to support data-driven decision making. Additionally, you will be responsible for performing bioinformatic and general data analysis tasks across our clinical, manufacturing, and R&D departments, making this an ideal position for candidates that desire a broad array of experiences. Your contributions will be instrumental in streamlining our lab operations, enabling insightful data analysis, and accelerating the clinical deployment of Locus’ engineered phage therapies.
Why Locus?
Change the future of medicine and address serious and unmet patient needs by developing engineered bacteriophage as a novel therapeutic modality and generalizable drug development platform
Fast-track your career by tackling large & meaningful projects on the cutting edge of science
Work with a team of exceptionally talented and hardworking individuals while striving to achieve a common mission
Partner with some the most highly respected companies and organizations in the world
Free access to fitness center and wellness classes
Responsibilities may include:
Automated analysis pipeline creation, deployment using cloud infrastructure, maintenance, and testing.
Collaborate with bioinformaticians, biologists, and clinicians to develop and implement analysis workflows to address specific research questions and company objectives
Develop and implement methodologies to capture, store, and visualize data from Electronic Lab Notebook, automated instruments, and high-throughput experiments.
Integration of pipelines with cloud storage, databases, and visualization software.
Implement quality control checks and data pre-processing steps to ensure the integrity of data
Dashboard development to rapidly return results to researchers and decision makers
Continuous and frequent communication and presentations across company
Required Qualifications
:
The ideal candidate is available for on-site work; however, hybrid and remote candidates will be considered.
A MS in Bioinformatics, Genomics, Biomedical Informatics, or related quantitative field, OR a BS with an equivalent level (2+ years) of demonstrable professional experience
Familiarity with bioinformatics tools, genomics data, and resources
Strong proficiency in R, Julia, and/or Python and proven experience in developing data processing pipelines
Familiarity with Linux/Unix, Bash, and SQL
Experience developing software collaboratively using version control systems (e.g. Git)
Willingness to develop solutions to difficult objectives, communicate findings, and deliver results independently or as part of a team
Desire to build automated feedback systems to improve operational excellence over time
Desired Q
ualifications
:
Hands-on experience with (bio)statistics
Working knowledge of genetics and/or microbiology
Hands on experience with at least one cloud compute provider (AWS, GCP, Azure, etc.)
Experience working with lab automation or high-throughput screening technologies
Working knowledge of microbiology and/or clinical trials
Experience deploying CI/CD architectures
Experience or understanding of solving common IT issues related to the listed responsibilities, such as internet routing, networking, firewalls, and access control
Knowledge of machine learning techniques and their applications in bioinformatics
Job Type:
Full-Time
Job Location:
Research Triangle Park, North Carolina
Direct applicants only. No agencies please.
Show more
Show less","Bioinformatics, Genomics, Biomedical Informatics, R, Julia, Python, Bash, SQL, Linux/Unix, Git, Cloud storage, Databases, Visualization software, AWS, GCP, Azure, Lab automation, Highthroughput screening, Machine learning, Statistics, Genetics, Microbiology, CI/CD","bioinformatics, genomics, biomedical informatics, r, julia, python, bash, sql, linuxunix, git, cloud storage, databases, visualization software, aws, gcp, azure, lab automation, highthroughput screening, machine learning, statistics, genetics, microbiology, cicd","aws, azure, bash, bioinformatics, biomedical informatics, cicd, cloud storage, databases, gcp, genetics, genomics, git, highthroughput screening, julia, lab automation, linuxunix, machine learning, microbiology, python, r, sql, statistics, visualization software"
Sr. Big Data / Software Developer,QATOS,"Raleigh, NC",https://www.linkedin.com/jobs/view/sr-big-data-software-developer-at-qatos-3644973600,2023-12-17,North Carolina,United States,Mid senior,Onsite,"Job Description
8+ years of IT experience with in-depth big data technology
Conceptualize and design big data system involving Hadoop, PySpark, Hive.
Extensively worked on Python Programming
Knowledge on Spark and PySpark language
Knowledge on Cluster management and storage mechanism in Big Data Cloud
Design functional and technical architectures
Develop applications on the big data platform using open source programming languages
Hands-on experience with fundamental Hadoop tools and technologies
Work closely with Administrator, Architect and Developers
Show more
Show less","Hadoop, PySpark, Hive, Python, Spark, Cluster management, Storage mechanism, Big Data Cloud, Functional architecture, Technical architecture, Open source programming languages, Hadoop tools and technologies","hadoop, pyspark, hive, python, spark, cluster management, storage mechanism, big data cloud, functional architecture, technical architecture, open source programming languages, hadoop tools and technologies","big data cloud, cluster management, functional architecture, hadoop, hadoop tools and technologies, hive, open source programming languages, python, spark, storage mechanism, technical architecture"
Senior Data Engineer,Epic Games,"Cary, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-epic-games-3636012066,2023-12-17,North Carolina,United States,Mid senior,Onsite,"WHAT MAKES US EPIC?
At the core of Epic’s success are talented, passionate people. Epic prides itself on creating a collaborative, welcoming, and creative environment. Whether it’s building award-winning games or crafting engine technology that enables others to make visually stunning interactive experiences, we’re always innovating.
Being Epic means being a part of a team that continually strives to do right by our community and users. We’re constantly innovating to raise the bar of engine and game development.
ECOSEC
What We Do
Our ECOSEC team provides a safer experience for Epic's users. We work across multiple products and services to improve technology and craft transparent policies so our players and users can have positive experiences on our platforms.
What You'll Do
You will be responsible for designing, building, and maintaining our data infrastructure to ensure the reliability and efficiency of our data and systems used by our Ecosystem Security team. Your role will include building and maintaining data pipelines which transform and load data from various products and managing the AWS infrastructure for our machine learning platform. Additionally, you will work with engineers, product managers, and data scientists to design and implement robust and scalable data services that support the Ecosystem Security mission while ensuring our user’s privacy. Your work will directly combat bad actors and keep our platform safe for all users.
In this role, you will
Interact with product teams to understand how our safety systems interact with their data systems
Design and implement automated end-to-end ETL process to prepare data for machine learning and ad-hoc analysis, including data anonymization
Manage and scale the tools and technologies that we use to label data which run on AWS
Devise database structure and technology for storing and efficiently accessing large data sets (millions of records) of different types (text, images, videos, etc.)
Use and implement data extraction APIs
Support data versioning strategies using automated tools, such as DVC
Support devising strategies for labeling of new data by humans
What We're Looking For
Strong analytical background: BSc or MSc in Computer Science/Software Engineering or related subject - candidates without a degree are welcome as long as they have proven extensive hands-on experience
Experience of ETL technical design, automated data quality testing, QA and documentation, data warehousing, and data modeling
Experience with Python for interaction with Web Services (e.g. Rest and Postman)
Experience with using and developing data APIs
Experience with using AWS, Snowflake, or other comparable large scale analytics platforms
Experience monitoring and managing databases (we use Elasticsearch/MongoDB/PostgreSQL)
Experience with SQL
Experience with data versioning tools
Experience developing and maintaining data infrastructure for ETL pipelines, such as Apache Airflow
This role is open to multiple locations in North America and Europe (excluding CA, NY, and WA).
EPIC JOB + EPIC BENEFITS = EPIC LIFE
Our intent is to cover all things that are medically necessary and improve the quality of life. We pay 100% of the premiums for both you and your dependents. Our coverage includes Medical, Dental, a Vision HRA, Long Term Disability, Life Insurance & a 401k with competitive match. We also offer a robust mental well-being program through Modern Health, which provides free therapy and coaching for employees & dependents.
Throughout the year we celebrate our employees with events and company-wide paid breaks. We offer unlimited PTO and sick time and recognize individuals for 7 years of employment with a paid sabbatical.
About Us
Epic Games spans across 19 countries with 55 studios and 4,500+ employees globally. For over 25 years, we’ve been making award-winning games and engine technology that empowers others to make visually stunning games and 3D content that bring environments to life like never before. Epic’s award-winning Unreal Engine technology not only provides game developers the ability to build high-fidelity, interactive experiences for PC, console, mobile, and VR, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. As we continue to build our Engine technology and develop remarkable games, we strive to build teams of world-class talent.
Like what you hear? Come be a part of something Epic!
Epic Games deeply values diverse teams and an inclusive work culture, and we are proud to be an Equal Opportunity employer. Learn more about our Equal Employment Opportunity (EEO) Policy here.
Note to Recruitment Agencies:
Epic does not accept any unsolicited resumes or approaches from any unauthorized third party (including recruitment or placement agencies) (i.e., a third party with whom we do not have a negotiated and validly executed agreement). We will not pay any fees to any unauthorized third party. Further details on these matters can be found here.
Show more
Show less","Python, ETL, Data Quality Testing, QA, Data Warehousing, Data Modeling, Web services (REST Postman), AWS, Snowflake, Elasticsearch, MongoDB, PostgreSQL, SQL, Data Versioning Tools, Apache Airflow, Hadoop, Hive, Spark, Informatica, Tableau","python, etl, data quality testing, qa, data warehousing, data modeling, web services rest postman, aws, snowflake, elasticsearch, mongodb, postgresql, sql, data versioning tools, apache airflow, hadoop, hive, spark, informatica, tableau","apache airflow, aws, data quality testing, data versioning tools, datamodeling, datawarehouse, elasticsearch, etl, hadoop, hive, informatica, mongodb, postgresql, python, qa, snowflake, spark, sql, tableau, web services rest postman"
"Data Engineer 2 (RTP, NC Hybrid role) #3328",GRAIL,"Durham, NC",https://www.linkedin.com/jobs/view/data-engineer-2-rtp-nc-hybrid-role-%233328-at-grail-3787023748,2023-12-17,North Carolina,United States,Mid senior,Onsite,"GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. GRAIL, LLC is a wholly-owned subsidiary of Illumina, Inc. (NASDAQ:ILMN). For more information, please visit www.grail.com .
Operational Intelligence is seeking an experienced Data Engineer to help grow our ability to organize, transform, analyze and monitor operational data streams to derive significant insights while maintaining high standards of data integrity and reproducibility. In this role, you will play a critical role in amplifying the signal present in the noise of information generated by day-to-day operations. The varied nature of this role requires excellent interpersonal skills and the ability to easily translate information between functional groups within the organization. Working closely with different partners, you will collaborate with various teams, from Operations to Software Engineering to build and maintain access to data. The insights and processes you generate will help drive the informed decision-making process and ensure GRAIL’s robust products continue to detect cancer early!
You Will:
Build and maintain end-to-end data analytics pipelines that include: design, data gathering, processing, analysis, iteration with partners, and presentation of results
Define and develop data workflows from data pre-processing and automation through data dissemination
Perform critical data analyses to support operations such as real-time process monitoring or troubleshooting
Process, analyze, and present a diverse data set including customer service, sample processing, engineering, and many other sources
Assist in developing and handling interactive data visualization and analytics tools for reporting and trending
Work closely with the Lab Operations teams to generate useful metrics for monitoring efficacy and preventing failures before they occur
Assist in the preparation of presentation materials for Process Monitoring meetings where analyses and graphs are presented to various partners
Interact cross-functionally with other teams including software, operations, research, and product development
Your Background Will Include:
BS/MS in a quantitative scientific field (computer science, engineering, mathematics, statistics, bioinformatics, etc.) with 3 - 5 years of experience
Experience with R or Python programming and at least one system-level programming language like Go or C++
Experience with SQL development and data warehousing concepts (e.g. ETL/ELT)
Effective written and verbal communication skills
Excellent teammate with a proven track record of success in a multi-functional team environment
Technical leadership, self-direction, and willingness to both teach others and learn new techniques is highly desirable
This is a hybrid role in the RTP area which includes two days onsite at our RTP location.
Based on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time.
In addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings.
GRAIL is an Equal Employment Office and Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.
Show more
Show less","Data Engineering, Data Analytics, Data Pipelines, Data Gathering, Data Processing, Data Analysis, Data Visualization, Data Warehousing, ETL/ELT, SQL, R, Python, Go, C++, Effective Communication, Teamwork, Technical Leadership, SelfDirection, Teaching, Learning","data engineering, data analytics, data pipelines, data gathering, data processing, data analysis, data visualization, data warehousing, etlelt, sql, r, python, go, c, effective communication, teamwork, technical leadership, selfdirection, teaching, learning","c, data engineering, data gathering, data processing, dataanalytics, datapipeline, datawarehouse, effective communication, etlelt, go, learning, python, r, selfdirection, sql, teaching, teamwork, technical leadership, visualization"
Senior Cloud Database Engineer,Eliassen Group,"Durham, NC",https://www.linkedin.com/jobs/view/senior-cloud-database-engineer-at-eliassen-group-3766036264,2023-12-17,North Carolina,United States,Mid senior,Onsite,"Job Description
Our client, a financial industry leader, needs a Senior Cloud Database Engineer to server as a subject matter expert on automating the deployment and migration of PostgreSQL and distributed SQL databases to both AWS and Azure.
Due to client requirement, applicants must be willing and able to work on a w2 basis. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.
Responsibilities Of The Senior Cloud Database Engineer
Work with database team to develop database-as-a-service automation
Collaborate to create a common deployment pipeline across all cloud service providers
Ensure applications are efficient and appropriately use cloud databases in line with organizational operational and security policies
Requirements Of The Senior Cloud Database Engineer
Bachelor’s degree in computer science or similar discipline
Hands-on experience and expertise as principal database automation engineer
Knowledge and hands-on experience with CloudFormation, YML, Vault, GitHub, Bitbucket, Artifactory
Please be advised- If anyone reaches out to you about an open position connected with Eliassen Group, please confirm that they have an Eliassen.com email address and never provide personal or financial information to anyone who is not clearly associated with Eliassen Group. If you have any indication of fraudulent activity, please contact InfoSec@eliassen.com.
Job ID: 380937
Show more
Show less","PostgreSQL, AWS, Azure, YML, CloudFormation, Vault, GitHub, Bitbucket, Artifactory","postgresql, aws, azure, yml, cloudformation, vault, github, bitbucket, artifactory","artifactory, aws, azure, bitbucket, cloudformation, github, postgresql, vault, yml"
Principal Data Engineer,Epic Games,"Cary, NC",https://www.linkedin.com/jobs/view/principal-data-engineer-at-epic-games-3732688713,2023-12-17,North Carolina,United States,Mid senior,Onsite,"WHAT MAKES US EPIC?
At the core of Epic’s success are talented, passionate people. Epic prides itself on creating a collaborative, welcoming, and creative environment. Whether it’s building award-winning games or crafting engine technology that enables others to make visually stunning interactive experiences, we’re always innovating.
Being Epic means being a part of a team that continually strives to do right by our community and users. We’re constantly innovating to raise the bar of engine and game development.
ANALYTICS
What We Do
The Analytics team is responsible for elevating the player’s voice and delivering the insights that inform Epic’s strategy and shape the games and products that players and developers will love.
What You'll Do
As the Principal Analytics Data Engineer, you will be a technical leader responsible for architecting our next-generation data warehouse that will scale Epic Analytics. You will identify business needs and serve as a bridge between data producers, consumers, and platform teams to ensure data quality and enable actionable insights.
In this role, you will
Build process and tooling to improve our data capabilities to answer business questions in a way that is efficient, repeatable, and scalable
Shape the roadmap of the data platform team to make sure we have the right technology in place to meet future demand
Partner with our data infrastructure teams to understand the pain points of the business and propose the right solutions to address them
Understand the requirements of the business with an analytics mindset and partner with data analyst teams to structure the data in a way that is both good for the end user and underlying systems
Influence cross-functional teams, including engineers, product managers, and data scientists to identify data opportunities, and represent them in a meaningful way
Elevate the team whether, through mentorship, process, or tools, this technical leader will be responsible for elevating the work of their team
What We're Looking For
Experience identifying data needs for business and shaping a data strategy in partnership with cross-functional partners
Strong analytical background with extensive hands-on experience shaping data models and data warehouses to answer business questions efficiently
Proven process builder within a data organization with a track record of influencing cross-functional teams and facilitating collaboration
Expertise in building and optimizing ETL and adjacent technologies (SQL, Python, schedulers)
Experience working with petabyte-scale data warehouses
Knowledge of the games industry is a plus but not required
EPIC JOB + EPIC BENEFITS = EPIC LIFE
Our intent is to cover all things that are medically necessary and improve the quality of life. We pay 100% of the premiums for both you and your dependents. Our coverage includes Medical, Dental, a Vision HRA, Long Term Disability, Life Insurance & a 401k with competitive match. We also offer a robust mental well-being program through Modern Health, which provides free therapy and coaching for employees & dependents. Throughout the year we celebrate our employees with events and company-wide paid breaks. We offer unlimited PTO and sick time and recognize individuals for 7 years of employment with a paid sabbatical.
About Us
Epic Games spans across 19 countries with 55 studios and 4,500+ employees globally. For over 25 years, we’ve been making award-winning games and engine technology that empowers others to make visually stunning games and 3D content that bring environments to life like never before. Epic’s award-winning Unreal Engine technology not only provides game developers the ability to build high-fidelity, interactive experiences for PC, console, mobile, and VR, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. As we continue to build our Engine technology and develop remarkable games, we strive to build teams of world-class talent.
Like what you hear? Come be a part of something Epic!
Epic Games deeply values diverse teams and an inclusive work culture, and we are proud to be an Equal Opportunity employer. Learn more about our Equal Employment Opportunity (EEO) Policy here.
Note to Recruitment Agencies:
Epic does not accept any unsolicited resumes or approaches from any unauthorized third party (including recruitment or placement agencies) (i.e., a third party with whom we do not have a negotiated and validly executed agreement). We will not pay any fees to any unauthorized third party. Further details on these matters can be found here.
Show more
Show less","Data Engineering, Data Warehousing, Data Modeling, ETL, Data Analysis, SQL, Python, Hadoop, Spark, Hive, Pig, Oozie, Airflow, Big Data, Petabytescale Data Warehouses, Machine Learning, Data Mining, Data Visualization, Cloud Computing, AWS, Azure, Google Cloud Platform","data engineering, data warehousing, data modeling, etl, data analysis, sql, python, hadoop, spark, hive, pig, oozie, airflow, big data, petabytescale data warehouses, machine learning, data mining, data visualization, cloud computing, aws, azure, google cloud platform","airflow, aws, azure, big data, cloud computing, data engineering, data mining, dataanalytics, datamodeling, datawarehouse, etl, google cloud platform, hadoop, hive, machine learning, oozie, petabytescale data warehouses, pig, python, spark, sql, visualization"
Sr. Python/AWS Data Engineer - Remote,CGI,"Columbia, NC",https://www.linkedin.com/jobs/view/sr-python-aws-data-engineer-remote-at-cgi-3781685311,2023-12-17,North Carolina,United States,Mid senior,Onsite,"Position Description
Are you motivated by the opportunity to delight your clients by providing them with innovative solutions to sophisticated technical problems? These are exciting times for CGI, and we are looking for hardworking individuals to innovate with us. If you have a groundbreaking spirit and thrive on innovation where you can influence the direction of technical strategy, come, and join our team!
We have immediately needed for a Sr. Python Developer/AWS Data Engineer to join our financial services tea. This is an exciting opportunity to work in a fast-paced team environment supporting one of the most prominent leaders in the secondary mortgage industry. We take an innovative approach to helping our clients, working side-by-side in an agile environment using new technologies.
We partner with 15 of the top 20 banks globally, and our top 10 banking clients have worked with us for an average of 26 years!
We have over 92,000+ CGI Members in 40 countries and over 5k+ loyal Clients who are using our end-to-end services across the globe.
The preferred location is Columbia, SC, OR Belton, TX but the position is open to Remote work anywhere in the US where there is a local CGI Office.
Your future duties and responsibilities
We are looking for a robust Senior Python Developer/AWS Data Engineer with Production migration experience to work on real cloud migration projects. In this role, you will be responsible for setting up, maintaining, and evolving the cloud infrastructure of web applications. The platform allows us to provide clients with enterprise-level web solutions. We seek a self-motivated AWS Developer to lead the architecture and database design for our external data integration services.
Required Qualifications To Be Successful In This Role
8+ years of IT experience with 4+ years of AWS Development and Cloud Architecture.
4+ years of experience with Python (must have)
Must have strong experience with SQL, Databases (Oracle, PostgreSQL, Aurora)
2+ years of experience with Data engineering (EMR, PySpark, Redshift, Glue)
Strong experience with data migration, cloud migration, and ETL.
Strong experience with AWS Lambda, Fargate, SNS, SQS, Elastic Beanstalk, ECS, Sagemaker and CloudWatch.
Experience with enterprise data lakes, data warehouses, data marts, and big data.
Excellent communication skills to ask questions, clarify requirements, and engage with the team and stakeholders.
Strong logic, reasoning, and critical thinking skills to solve problems as they arise.
Adaptive to change: Demonstrated ability to problem solve on the fly and tailor your approach to the resources at hand.
Must be an independent problem solver who can evaluate a situation and build solution options.
Strong Python, Strong working AWS experience, Databases (Oracle, PostgreSQL, Aurora) Data engineering (EMR, PySpark, Redshift, Glue), Serverless experience (Lambda, step functions), containerization (ECS with Fargate)
Desired Skillset
SAS knowledge
DevOps knowledge (Jenkins, Bitbucket, Terraform/UCD/CloudFormation)
Testing Automation
AWS CERTIFICATION(S) DESIRED
EDUCATION REQUIREMENT:
Bachelor's degree in computer science, Information Systems or related field
#DICE
CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $59,700.00 - $146,900.00.
CGI anticipates accepting applications for this position through February 6, 2024.
At CGI we call our professionals “members” to reinforce that all who join our team are, as owners, empowered to participate in the challenges and rewards that come from building a world-class company. CGI’s benefits include:
Competitive base salaries
Eligibility to participate in an attractive Share Purchase Plan (SPP) in which the company matches dollar-for-dollar contributions made by eligible employees, up to a maximum, for their job category
401(k) Plan and Profit Participation for eligible members
Generous holidays, vacation, and sick leave plans
Comprehensive insurance plans that include, among other benefits, medical, dental, vision, life, disability, out-of-county emergency coverage in all countries of employment.
Back-up childcare, Pet insurance, a Member Assistance Program, a 529 college savings program, a personal financial management tool, lifestyle management programs and more.
CGI anticipates accepting applications for this position through January 31, 2024.
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees “members” because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.
CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you.
Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned
.
We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.
All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.
CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information.
Show more
Show less","Python, AWS, Data Engineering, Cloud Migration, ETL, SQL, Databases (Oracle PostgreSQL Aurora), Data Warehouses, Data Lakes, Big Data, Communication Skills, Problem Solving, Adaptive to Change, Independent Problem Solver, SAS, DevOps, Jenkins, Bitbucket, Terraform, UCD, CloudFormation, Testing Automation, AWS CERTIFICATION(S), Computer Science, Information Systems","python, aws, data engineering, cloud migration, etl, sql, databases oracle postgresql aurora, data warehouses, data lakes, big data, communication skills, problem solving, adaptive to change, independent problem solver, sas, devops, jenkins, bitbucket, terraform, ucd, cloudformation, testing automation, aws certifications, computer science, information systems","adaptive to change, aws, aws certifications, big data, bitbucket, cloud migration, cloudformation, communication skills, computer science, data engineering, data lakes, data warehouses, databases oracle postgresql aurora, devops, etl, independent problem solver, information systems, jenkins, problem solving, python, sas, sql, terraform, testing automation, ucd"
"Data Engineer 2 (Menlo Park, CA hybrid) #3328",BioSpace,"Raleigh, NC",https://www.linkedin.com/jobs/view/data-engineer-2-menlo-park-ca-hybrid-%233328-at-biospace-3775206322,2023-12-17,North Carolina,United States,Mid senior,Onsite,"GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. GRAIL, LLC is a wholly-owned subsidiary of Illumina, Inc. (NASDAQ:ILMN). For more information, please visit www.grail.com.
Operational Intelligence is seeking an experienced Data Engineer to help grow our ability to organize, transform, analyze and monitor operational data streams to derive significant insights while maintaining high standards of data integrity and reproducibility. In this role, you will play a critical role in amplifying the signal present in the noise of information generated by day-to-day operations. The varied nature of this role requires excellent interpersonal skills and the ability to easily translate information between functional groups within the organization. Working closely with different partners, you will collaborate with various teams, from Operations to Software Engineering to build and maintain access to data. The insights and processes you generate will help drive the informed decision-making process and ensure GRAIL’s robust products continue to detect cancer early!
You Will
Build and maintain end-to-end data analytics pipelines that include: design, data gathering, processing, analysis, iteration with partners, and presentation of results
Define and develop data workflows from data pre-processing and automation through data dissemination
Perform critical data analyses to support operations such as real-time process monitoring or troubleshooting
Process, analyze, and present a diverse data set including customer service, sample processing, engineering, and many other sources
Assist in developing and handling interactive data visualization and analytics tools for reporting and trending
Work closely with the Lab Operations teams to generate useful metrics for monitoring efficacy and preventing failures before they occur
Assist in the preparation of presentation materials for Process Monitoring meetings where analyses and graphs are presented to various partners
Interact cross-functionally with other teams including software, operations, research, and product development
Your Background Will Include
BS/MS in a quantitative scientific field (computer science, engineering, mathematics, statistics, bioinformatics, etc.) with 3 - 5 years of experience
Experience with R or Python programming and at least one system-level programming language like Go or C++
Experience with SQL development and data warehousing concepts (e.g. ETL/ELT)
Effective written and verbal communication skills
Excellent teammate with a proven track record of success in a multi-functional team environment
Technical leadership, self-direction, and willingness to both teach others and learn new techniques is highly desirable
$115,000 - $130,000 a year
The expected, full-time, annual base pay scale for this position is $115,000 - $130,000. Actual base pay will consider skills, experience, and location.
The location for this role is Menlo Park and you will be expected to work a hybrid schedule which means two days in the office.
Based on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time.
In addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings.
GRAIL is an Equal Employment Office and Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.
Show more
Show less","Data Engineering, Data Analytics, R, Python, Go, C++, SQL, ETL/ELT, Data Warehousing, Data Preprocessing, Data Dissemination, Data Visualization, Analytics Tools, Process Monitoring, Technical Leadership, Team Collaboration","data engineering, data analytics, r, python, go, c, sql, etlelt, data warehousing, data preprocessing, data dissemination, data visualization, analytics tools, process monitoring, technical leadership, team collaboration","analytics tools, c, data dissemination, data engineering, data preprocessing, dataanalytics, datawarehouse, etlelt, go, process monitoring, python, r, sql, team collaboration, technical leadership, visualization"
Senior Data Engineer,Juniper Square,"Raleigh, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-juniper-square-3705287629,2023-12-17,North Carolina,United States,Mid senior,Remote,"About Juniper Square
Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.
GP Experience
Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital& Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.
The Team
The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.
About Your Role
Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.
What You’ll Do
Design and implement sophisticated data models in SQL.
Work closely with the other Software Engineers to ensure sound, scalable implementation.
Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
Introduce new technologies to evolve and enhance our data pipeline capabilities.
Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Qualifications
Bachelor's degree in Computer Science, or equivalent work experience
5+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
Strong command of relational databases (Postgresql preferred), data modeling and database design
Strong command of Python and experience using Python for Data Pipelines
Experience with cloud based services (AWS RDS preferred)
Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.
Benefits
Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000 and the Canadian base salary range for this role is $200,000 to $250,000 CAD. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. Your recruiter can provide further details.
Competitive salary and meaningful equity
Health, dental, and vision care for you and your family
Unlimited vacation policy and paid holidays
Generous paid family leave, medical leave, and bereavement leave policies
401k retirement savings plan
Healthcare FSA and commuter benefits programs
Freedom to customize your work and technology setup as you see fit
Professional development stipend
Monthly work from home wellness stipend while we're all remote
Mental wellness coverage including live coaching and therapy sessions
Home office productivity allowance to help create an ideal work from home setup
#Juniper-US
#Juniper-Canada
Show more
Show less","Python, AWS RDS, Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView, SQL, SQL, Data Modeling, Relational Databases, Python, ETL, ELT, Data Warehouses, Amazon Redshift, Google BigQuery, Snowflake","python, aws rds, looker, tableau, powerbi, mode, data studio, domo, qlikview, sql, sql, data modeling, relational databases, python, etl, elt, data warehouses, amazon redshift, google bigquery, snowflake","amazon redshift, aws rds, data studio, data warehouses, datamodeling, domo, elt, etl, google bigquery, looker, mode, powerbi, python, qlikview, relational databases, snowflake, sql, tableau"
Data Analyst,Motion Recruitment,"Durham, NC",https://www.linkedin.com/jobs/view/data-analyst-at-motion-recruitment-3781600915,2023-12-17,North Carolina,United States,Mid senior,Remote,"Our client, a nationally recognized and award-winning company in the health insurance vertical, has a 6+ month contract opening for a Data Analyst. They have over 4 million customers and 5,000+ employees dedicated to providing innovative solutions that simplify the healthcare system, improve efficiency and outcomes while reducing costs..
* While the position is Remote, work from home, you must reside in North Carolina or one of the following states: Alabama, Arizona, Arkansas, Colorado, Florida, Georgia, Idaho, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maryland, Michigan, Mississippi, Missouri, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Wisconsin, and Wyoming
Required Skills & Experience
Bachelor's degree or advanced degree (where required). In lieu of degree, 5+ years of experience in related field.
3+ years of experience in related field.
What You Will Be Doing
Daily Responsibilities
Excel experience required must be able to do Pivot Tables, VLOOKUP and analyze data.
Must be able to demonstrate the ability to prioritize and strong desktop management skills.
Provide programming support to extract and analyze data from different enterprise systems.
Develop new reports and report templates to support business needs by utilizing various reporting tools and data mining techniques.
Develop, implement, and maintain consistent processes for data quality controls to ensure data integrity and make recommendations to improve data integrity,
Monitor and analyze data to determine department improvement areas.
Work collaboratively with customers to complete effective needs analyses to successfully provide analytic support and problem solving assistance to business areas.
Identify trends and participate in problem-solving, investigation, and analyses to make recommendations to business areas to effectively assist them with decision making on operational or other business issues.
Communicate meaningful insights by creating charts and slide presentations and presenting findings and recommendations to business areas.
Generate reports for regular distribution and provide ongoing user support for ad hoc reporting, questions, queries, and problem support.
May maintain departmental inventory of queries – looking for opportunities to move to a production state with system generated reports, eliminating duplicates, and creating a single source process for repository, and guidelines for execution to minimize taxing the production systems.
May serve as point-of-contact for audit processes and procedures:
Suggest procedural changes as necessary to provide the level of reporting required.
Research, gather, prepare, and substantially edit reports, graphics and other materials.
Develop and maintain document processes to generate reports.
Evaluate data for presence of PHI fields. Consult with subject matter experts within the department for resolution of security and privacy issues.
Represent department on multiple project teams and process optimization efforts as needed.
To be eligible to contract at this client you must be able to pass a drug test and criminal background check. Posted By: Andrew Chadwick
Show more
Show less","Excel, Pivot Tables, VLOOKUP, Data analysis, Data mining, Programming, Data quality controls, Data integrity, Data visualization, Chart creation, Slide presentation, Business intelligence, Reporting, Problemsolving, Decision making, Data extraction, Data management, Data governance, Data security, Data privacy, Data optimization, Data integration, Data warehousing, Data mining, Machine learning, Artificial intelligence","excel, pivot tables, vlookup, data analysis, data mining, programming, data quality controls, data integrity, data visualization, chart creation, slide presentation, business intelligence, reporting, problemsolving, decision making, data extraction, data management, data governance, data security, data privacy, data optimization, data integration, data warehousing, data mining, machine learning, artificial intelligence","artificial intelligence, business intelligence, chart creation, data extraction, data governance, data integration, data integrity, data management, data mining, data optimization, data privacy, data quality controls, data security, dataanalytics, datawarehouse, decision making, excel, machine learning, pivot tables, problemsolving, programming, reporting, slide presentation, visualization, vlookup"
Senior Database Platform Engineer,Dimensional Fund Advisors,"North Carolina, United States",https://www.linkedin.com/jobs/view/senior-database-platform-engineer-at-dimensional-fund-advisors-3761176954,2023-12-17,North Carolina,United States,Mid senior,Remote,"Notes To Applicants
This position is eligible for full-time remote work in Texas, or North Carolina, or, in the alternative, to work in accordance with Dimensional’s best-of-both hybrid working model, which involves working in the office on Tuesdays, Wednesdays and Thursdays, and choosing to work in the office or remotely on Mondays and Fridays.
Resumes and portfolios (when applicable) are required as part of your application. When applying from a mobile device or tablet, you may not be able to attach a resume. If you cannot include an attachment at the time of your application, you will receive a follow up email asking you to attach your resume from a computer.
Here at Dimensional, we strive to be an inclusive workplace for all. Even if you do not match every qualification listed, if you are interested in who we are, what we do, and why we do it, we suggest and encourage you to apply.
If you require reasonable accommodation in completing this application, interviewing, or otherwise participating in the employee selection process, such accommodation is available upon request.
Job Description
JOB PURPOSE
The Senior Database Platform Engineer will work as part of a team to create and refine innovative solutions to empower our business. This multi-discipline role will enable the business and technology teams to leverage various data platform technologies at a high level. The Engineer will enable users to provision, analyze and consume data via performant, secure and scalable database platforms.
Duties & Responsibilities
Architects and automates database platform provisioning, configuration, monitoring, issue remediation, and security.
Applies expert understanding and troubleshooting of complex database systems.
Establishes, implements, and maintains best practice standards around code quality, deployment, security, architecture, and automation.
Collaborates with business and technology teams to accomplish engineering project goals.
Qualifications
Bachelor’s degree in Information Technology, Computer Science, Computer Engineering, or related field.
7+ years of experience engineering for database engines (ex: SQL Server, PostgreSQL, MongoDB)
5+ years of experience working with cloud platforms in various cloud service providers (ex: Snowflake, BigQuery, Redshift, RDS, AWS, Azure, Google Cloud, edetc.)
7+ years of experience using SQL or NoSQL based languages
5+ years of experience using interpreted scripting languages (ex: Python, Powershell, etc.)
5+ years of experience using and developing with configuration management products and applying Infrastructure as Code principles (ex; Terraform, Ansible, Powershell DSC, Puppet, Chef, Cloud Formation, etc.)
Expert knowledge of relevant database platform engine internals, infrastructure and concepts
Experience working and developing with/on REST APIs and other non-DB direct interfaces
Experience working with CI/CD and SDLC workflow (Ex: JIRA, Git, Bitbucket, Github, Liquibase, Jenkins, Octopus, DbUp, etc.)
Applies a high bar for database platform architecture and security to solutions
Embodies and applies an engineering mindset to all aspects of their role
Excellent verbal and written communication skills. Able to communicate complex concepts to various audiences helping them to understand the benefits of technologies and recommended course of action
Preferred
Working with containerized platforms (ex: Docker, Kubernetes, etc.)
Experience working with or interest in object-oriented programming languages (ex: C#, Java, C++, etc.)
Experience working with or interest in data distribution systems (ex: Kafka)
SCOPE & DIRECT REPORTS
This position does not manage employees.
Dimensional offers a variety of programs to help take care of you, your family, and your career, including comprehensive benefits, educational initiatives, and special celebrations of our history, culture, and growth.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.
Show more
Show less","Database platform engineering, Cloud platforms, SQL, NoSQL, Python, Powershell, Terraform, Ansible, Puppet, Chef, Cloud Formation, REST APIs, CI/CD, SDLC, Docker, Kubernetes, C#, Java, C++, Kafka","database platform engineering, cloud platforms, sql, nosql, python, powershell, terraform, ansible, puppet, chef, cloud formation, rest apis, cicd, sdlc, docker, kubernetes, c, java, c, kafka","ansible, c, chef, cicd, cloud formation, cloud platforms, database platform engineering, docker, java, kafka, kubernetes, nosql, powershell, puppet, python, rest apis, sdlc, sql, terraform"
Data Engineer (Python),PMG GLOBAL,Raleigh-Durham-Chapel Hill Area,https://www.linkedin.com/jobs/view/data-engineer-python-at-pmg-global-3768713428,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"*Open for W2 only*
We are currently sourcing for a Data Engineer (Python / ETL / AWS / SQL) to work at our client's location in Smithfield, RI, or Durham, NC!
Role Description
The team is hiring a Data Engineer, and you will be responsible for working with Architecture, Data Governance, and Business Intelligence teams to support our client's Data Management tools and evaluate new technologies. You will possess a passion for continuous learning and upskilling in new technologies and a flair for exploring.
Required Qualifications
Heavy Python experience is required.
ETL build experience with Python. NOT looking for an Informatica developer.
AWS
Strong SQL – Snowflake and SQL Server will be a huge plus.
Strong Analysis skills
Working knowledge of Unix OS /Shell scripting
Good working knowledge of Control-M/Automation tools.
Production Support will be required – one week every 3 months.
Excellent interpersonal and communication skills
Excellent collaboration skills to work with multiple teams in the organization.
Additional Experience
Experience with Metadata management solutions / Data lineage is a plus.
Learn New technologies and evaluate new products, participating in Proof of Concepts (POCs) is a plus.
Vendor management is a plus.
Some QA/Testing experience is a plus.
Some Kubernetes / Docker experience is a plus.
Strong communication and presentation skills
Location
:
Smithfield-RI
Durham-NC
Show more
Show less","Python, ETL, AWS, SQL, Snowflake, SQL Server, Unix OS, Shell scripting, ControlM/Automation tools, Metadata management solutions, Data lineage, Kubernetes, Docker","python, etl, aws, sql, snowflake, sql server, unix os, shell scripting, controlmautomation tools, metadata management solutions, data lineage, kubernetes, docker","aws, controlmautomation tools, data lineage, docker, etl, kubernetes, metadata management solutions, python, shell scripting, snowflake, sql, sql server, unix os"
Data Engineer (Azure) Hybrid,Irvine Technology Corporation,"North Carolina, United States",https://www.linkedin.com/jobs/view/data-engineer-azure-hybrid-at-irvine-technology-corporation-3780003507,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"Sr Data Engineer (Azure) Hybrid in either Charlotte or Raleigh.
Well branded industry leader is seeking a Sr Data Engineer who can lead a team of Data Engineers working on Azure Data Lake technology.
What you will do:
Oversee and execute data engineering roadmaps to drive business objectives.
Set up DevOps and CI/CD processes for efficient data engineering.
Manage advanced analytics initiatives supporting the Global initiatives.
Collaborate with architects to design robust data architectures.
Scale and manage the engineering team, attracting and mentoring top talent.
Coach, mentor, and lead the Data Engineering team, ensuring development, engagement, and workflow management.
Manage project priorities, product deliveries, and budgets/costs associated with projects.
Collaborate with key business and technical stakeholders to align initiatives with overall company and project goals.
Drive technical initiatives from execution to delivery.
What gets you the job:
Bachelor's degree in Computer Science, Computer Engineering, or related majors.
6+ years of data engineering experience in a high-scale distributed environment.
3+ years of engineering leadership experience, including people management.
Strong data engineering background on one or more cloud technologies – GCP, AWS, Azure.
Proficiency in Azure Databricks
Proficiency in ETL design, PL/SQL development, Python, and performance tuning in a multi-dimensional Data Warehousing environment.
Expertise in data management, operations, and security.
Experience in Enterprise Data solution architecture.
Demonstrated ability to define and enable data quality standards for auditing and monitoring.
Strong leadership capabilities, aptitude, and interest.
Ability to lead through our core values.
Experience driving collaboration across cross-functional teams.
Strong communication and teamwork skills.
Experience with SAFe Agile framework, JIRA for work & resource management.
Retail background strongly preferred; comparable skills
Show more
Show less","Azure Data Lake, DevOps, CI/CD, Data engineering, Data architecture, Data management, Data operations, Data security, Enterprise data solution architecture, Data quality standards, SAFe Agile framework, JIRA, Azure Databricks, PL/SQL, Python, Multidimensional Data Warehousing","azure data lake, devops, cicd, data engineering, data architecture, data management, data operations, data security, enterprise data solution architecture, data quality standards, safe agile framework, jira, azure databricks, plsql, python, multidimensional data warehousing","azure data lake, azure databricks, cicd, data architecture, data engineering, data management, data operations, data quality standards, data security, devops, enterprise data solution architecture, jira, multidimensional data warehousing, plsql, python, safe agile framework"
"Data Engineer, Data Platform",Grammarly,"North Carolina, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689961982,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, AWS, Azure, GCE, SQL, Data Engineering, Data Lakes, Data Warehousing, Streaming Data Processing, System Design, DevOps, Agile Development, Software Engineering","python, scala, java, aws, azure, gce, sql, data engineering, data lakes, data warehousing, streaming data processing, system design, devops, agile development, software engineering","agile development, aws, azure, data engineering, data lakes, datawarehouse, devops, gce, java, python, scala, software engineering, sql, streaming data processing, system design"
Data Engineer,PMG GLOBAL,Raleigh-Durham-Chapel Hill Area,https://www.linkedin.com/jobs/view/data-engineer-at-pmg-global-3769523975,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"*Open for W2 only*
The Role
The Customer Data & Marketing Technology group is seeking a Senior Full Stack Engineer to design, develop, and implement AWS cloud-based solutions. This role is critical in enhancing our Salesforce CRM platform to extend its capabilities in support of our client's world-class customer support staff. This role has an emphasis on the data and database components of AWS development as part of a full-stack development squad practicing Agile delivery and deployment. In addition, there will be technology leadership opportunities working with other members of the squad.
The Expertise and Skills You Bring
5+ years in Enterprise application development (design, development, testing, deployment, and support)
5+ years of development experience in Database Development
Writing SQL queries and debugging stored procedures within an Oracle environment.
Solid background in developing applications for Aurora Postgres, Oracle RDS, or similar.
Strong data background in terms of schema design (tables, indexes, 3rd normal form, denormalization to optimize for performance, etc.)
1+ year of Snowflake experience is a strong plus.
Salesforce development experience is a strong plus.
AWS Cloud environments
Containerization and Orchestration technologies
Application web server technologies
Strong hands-on solid understanding of Scripting
Experience and/or certification with Amazon Web Services, Google Cloud Platform, or Microsoft Azure is a plus.
Knowledge of Informatica and/or ETL tools
5+ years of Java development experience (microservices, Lambda, database applications)
2+ years of Python and/or PySQL development experience
AWS developer certification
Hands-on experience with API development supporting large-scale, continuously available enterprise-class platforms.
Unix shell scripting experience a plus
Control-M experience a strong plus
Experience developing in an Agile team setting (Kanban and/or SCRUM)
Experience working in a DevOps and CI/CD environment.
A Bachelor’s or Master’s degree in Computer Science, Information Technology, or equivalent experience.
Ability to think out of the box and design end-to-end solutions.
Passion and intellectually curious to learn new technologies and implement robust, scalable solutions.
Ability to work in a fast-paced environment.
Excellent verbal and written communication skills
Excellent collaboration skills to work with multiple teams in the organization.
Location
Raleigh-NC
West Lake-TX
Show more
Show less","AWS, Salesforce CRM, Oracle, Aurora Postgres, Snowflake, SQL, Java, Python, PySQL, Unix, ControlM, Microservices, Lambda, ETL, API Development, LargeScale Platforms, Docker, Kubernetes, Kanban, SCRUM, DevOps, CI/CD","aws, salesforce crm, oracle, aurora postgres, snowflake, sql, java, python, pysql, unix, controlm, microservices, lambda, etl, api development, largescale platforms, docker, kubernetes, kanban, scrum, devops, cicd","api development, aurora postgres, aws, cicd, controlm, devops, docker, etl, java, kanban, kubernetes, lambda, largescale platforms, microservices, oracle, pysql, python, salesforce crm, scrum, snowflake, sql, unix"
Sr. Data Engineer,Park Place Technologies,"Cleveland, NC",https://www.linkedin.com/jobs/view/sr-data-engineer-at-park-place-technologies-3771226940,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"Senior Data Engineer
The Senior Data Engineer is responsible for collecting, designing, and converting complex data into information that can be interpreted by Data Scientists and Business Analysts. Data accessibility is the ultimate goal, which enables our organization to utilize data for performance evaluation and optimization. As a Senior Data Engineer, one of your primary responsibilities is to guide and mentor junior Data Engineers to ensure they have the necessary skills and knowledge to perform their tasks effectively. This includes setting up guardrails and protocols that help the team achieve their goals efficiently while maintaining the quality of their work. Your role is crucial in ensuring the team's success, as you play a significant part in streamlining their workflow and maximizing their productivity.
What You’ll Be Doing
Implementing pipelines to move raw data in Azure Synapse using spark, python, SQL and C# in line with established architectural standards related to Data Warehouse and Data Lakehouse modeling standards.
Developing machine learning and regression analysis skills in spark-python-pandas, openai and Azure ML.
Mentoring other members of the Data Engineering team, specifically those in a junior role
Work closely with Data Analysts and Business Analysts to clarify their requirements and provide clear guidance on the execution process.
Providing peer review support to work produced by others, confirming use of relevant coding standards.
Demonstrate mastery of the Software Development Life Cycle.
What We’re Looking For
4+ years of experience in Python development related to data engineering (spark, pandas, etc.).
4+ years of experience in SQL related to data engineering.
Experience in designing and implementing complex data pipelines ensuring data quality & consistency.
Solid understanding of data warehouse and delta lake design concepts
Solid data analytics background
Solid understanding of the Software Development Life Cycle
Bonus Points
Microsoft Cloud Certification
Familiarity with Machine Learning and AI
Web Development is a plus.
Education
Bachelor’s Degree or Relevant Certifications & equivalent years of experience
Show more
Show less","Azure Synapse, Spark, Python, SQL, C#, Data Warehouse, Data Lakehouse, Machine Learning, Regression Analysis, Pandas, OpenAI, Azure ML, Software Development Life Cycle, Data Pipeline, Data Quality, Data Consistency, Data Warehouse Design, Delta Lake Design","azure synapse, spark, python, sql, c, data warehouse, data lakehouse, machine learning, regression analysis, pandas, openai, azure ml, software development life cycle, data pipeline, data quality, data consistency, data warehouse design, delta lake design","azure ml, azure synapse, c, data consistency, data lakehouse, data pipeline, data quality, data warehouse design, datawarehouse, delta lake design, machine learning, openai, pandas, python, regression analysis, software development life cycle, spark, sql"
Senior Data Analytics DevOps Engineer,VF Corporation,"Greensboro, NC",https://www.linkedin.com/jobs/view/senior-data-analytics-devops-engineer-at-vf-corporation-3749744838,2023-12-17,North Carolina,United States,Mid senior,Hybrid,"Senior Data Analytics DevOps Engineer (VF Services, LLC, Greensboro, NC):
As part of the global Data Analytics DevOps Engineering team, work with architecture, engineering, business teams and operations teams to design, build, provision infrastructure, and support development activities. Responsible for DevOps tools management in terms of maintainability, security, and identity access management (IAM). Implement deployment pipelines in Continuous integration/Continuous Delivery (CI/CD) tools to release managed, scalable, highly available, and fault-tolerant Data Analytics solutions on Cloud systems like Amazon Web Service (AWS), Azure and GCP. Implement Data Analytics Environments in Cloud following the infrastructure best practices defined with Security, Architecture and DevOps teams. Manage scalable data analytics solutions on different regions. Support the security team to identify and patch the system vulnerabilities. Evaluate and implement advance solutions to improve the monitoring and the troubleshooting. Implement design solutions for data pipeline automation to improve the deployment efficiency and time-to-market. Work side-by-side with Architecture team to evaluate new solutions and to provide the preliminary results from a Proof-of-concept (POC). Review, evaluate and approve technical design solutions proposed by partners for data analytics pipelines. Optimize the resources and work on resource tagging to allocate costs and plan budgeting, governance, and reporting. Manage the work of external partners resources and contractors in multiple countries.
Full time employment, Monday – Friday, 40 hours per week.
MINIMUM REQUIREMENTS:
Must have a Bachelor’s degree in Information Technology, Computer Science, or a related field, and 5 years of progressive, post-baccalaureate experience in Web Development, DevOps, or Cloud Computing; OR a Master’s degree in Information Technology, Computer Science, or a related field, and 2 years of experience in Web Development, DevOps, or Cloud Computing.
Of the required experience, must have at least 2 years of experience in each of the following:
Designing, implementing and supporting products and services in cloud native environments;
Cloud computing including AWS, GCP, or Azure;
Utilizing software development tools, including Terraform, Jenkins, or Ansible;
Utilizing tracking and management tools, including JIRA or Confluence;
Scripting with bash and Python; and
Agile software development.
Telecommuting permitted 1 day per week.
Any suitable combination of education, training, or experience is acceptable.
Apply online at
vfc.com/careers under “find your calling here” and search for R-20221220-0019 Senior Data Analytics DevOps Engineer and click apply. Should you have any difficulty in applying for this position through our website, please contact vf_global_mobility@vfc.com for assistance in the application process.
Hiring Range:
$98,400.00 USD - $123,000.00 USD annually
Incentive Potential
: This position is eligible for additional compensation awards that may include an annual incentive plan, sales incentive, or commission potential. Specific details of the additional compensation eligibility for this position will be provided during the recruiting and interview process.
Benefits at VF Corporation
: You can review a general overview of each benefit program offered, including this year's medical plan rates on www.MyVFbenefits.com and by clicking
Looking to Join VF?
Detailed information on your benefits will be provided during the hiring process.
P
lease note, our hiring ranges are determined and built from market pay data. In determining the specific compensation for this position, we comply with all local, state, and federal laws.
At VF, we value a diverse, inclusive workforce and we provide equal employment opportunity for all applicants and employees. All qualified applicants for employment will be considered without regard to an individual’s race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. If you are unable to submit your application because of incompatible assistive technology or a disability, please contact us at peopleservices@vfc.com. VF will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.
Pursuant to all applicable local Fair Chance Ordinance requirements, including but not limited to the San Francisco Fair Chance Ordinance, VF will consider for employment qualified applicants with arrest and conviction records.
Show more
Show less","AWS, DevOps, Azure, GCP, Terraform, Jenkins, Ansible, JIRA, Confluence, Bash, Python, Cloud, Web Development, Agile, IAM, CI/CD","aws, devops, azure, gcp, terraform, jenkins, ansible, jira, confluence, bash, python, cloud, web development, agile, iam, cicd","agile, ansible, aws, azure, bash, cicd, cloud, confluence, devops, gcp, iam, jenkins, jira, python, terraform, web development"
Senior Data Analyst,Harnham,"Bristol, VA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-harnham-3788329756,2023-12-17,Elizabethton,United States,Mid senior,Onsite,"To Apply for this Job Click Here
SENIOR DATA ANALYST
BRISTOL - HYBRID
£45,000 - 65,000
This role does not have the ability to sponsor
An exciting opportunity at a data-driven organisation focused on growing their analytics function within the reinsurance sphere
The Company
This is a fast-growing company undergoing a huge data transformation and looking for a highly motivated individual to lead projects focused on embracing the rapid growth
The Role And Responsibilities
In this role, you will:
Own the reporting in Tableau which is used as the primary visualisation tools
Be a part of the data quality assurance focusing on ensuring data integrity for more informed data-driven business decisions
Create high-performing data sets for reporting and analytics that insight can be drawn from
Lead analytics projects and guide a junior team member to enhance their skills
Stakeholder management skills, being able to translate business requirements into tech specs
Your Experience
The successful candidate will:
Proficient in Python and have been using it in a professional setting
Familiarity with a visualisation tool ideally Tableau
Able to work cross-functionally in a fast-paced environment to identify and gather relevant data sources
Utilise statistical techniques and data analysis tools to interpret complex data sets
Conduct exploratory data analysis to identify patterns, trends, and outliers
Have an understanding of GCP
STEM degree and strong educational background
Experience in the reinsurance sector
How To Apply
Please do so by the site
To Apply for this Job Click Here
Show more
Show less","Data Analysis, Tableau, Data Quality Assurance, DataDriven Decision Making, Data Visualization, Business Requirements, Python, Statistical Techniques, Data Interpretation, Pattern Identification, Trend Analysis, Outlier Detection, GCP, STEM, Reinsurance","data analysis, tableau, data quality assurance, datadriven decision making, data visualization, business requirements, python, statistical techniques, data interpretation, pattern identification, trend analysis, outlier detection, gcp, stem, reinsurance","business requirements, data interpretation, data quality assurance, dataanalytics, datadriven decision making, gcp, outlier detection, pattern identification, python, reinsurance, statistical techniques, stem, tableau, trend analysis, visualization"
Data Center Ops Analyst,NTT DATA Services,"Quincy, WA",https://www.linkedin.com/jobs/view/data-center-ops-analyst-at-ntt-data-services-3787399382,2023-12-17,Ephrata,United States,Mid senior,Onsite,"Req ID:
263362
NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Center Ops Analyst to join our team in Quincy, Washington (US-WA), United States (US).
The Data Center Planning Associate (Operations Analyst) is responsible for capacity management of the data center. This is a direct customer (internal & external) facing role that gathers requirements, manages project schedules, and ensures SLA/OLA compliance. They review project’s technical requirement and ensuring the availability of appropriate infrastructure redundancy and capacity. They coordinate project deployments with the DC Hardware team and creates as-built documentation such as rack elevations & device/cabling DC Portal records. The DC Planning Associates are directly responsible for the accuracy of our DCIM data.
The candidate will be responsible for:
DCIM Data entry
Queue management: Prioritization, SLA/OLA, scheduling, & resource assignment
Complete assigned planning tickets
Evaluate projects for capacity requirements
Evaluate equipment specifications
Create rack elevation, floor space, and power assignments
Evaluate DC infrastructure in relation to project requirements
Coordinate project deployment and decommissions
Update request, change, and incident notes
Create change management requests
Audit DCIM tool & DC floor for data accuracy
Basic Qualifications:
Minimum 3 years of Data Center Operation; Tape Operations/Smart hands
Minimum 3 years of DCIM Data entry
Minimum 3 years Queue management: Prioritization, SLA/OLA, scheduling, & resource assignment
#INDICS
About NTT DATA Services
NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients’ long-term success. Visit nttdata.com or LinkedIn to learn more.
NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
Show more
Show less","Data Center Operations, Tape Operations, Smart Hands, DCIM Data Entry, Queue Management, Prioritization, SLA/OLA, Scheduling, Resource Assignment, Rack Elevation, Floor Space, Power Assignments, Change Management, DCIM Tool, DC Floor, Cloud, Data, Applications","data center operations, tape operations, smart hands, dcim data entry, queue management, prioritization, slaola, scheduling, resource assignment, rack elevation, floor space, power assignments, change management, dcim tool, dc floor, cloud, data, applications","applications, change management, cloud, data, data center operations, dc floor, dcim data entry, dcim tool, floor space, power assignments, prioritization, queue management, rack elevation, resource assignment, scheduling, slaola, smart hands, tape operations"
Data Center Operations Analyst,NTT DATA Services,"Quincy, WA",https://www.linkedin.com/jobs/view/data-center-operations-analyst-at-ntt-data-services-3749134517,2023-12-17,Ephrata,United States,Mid senior,Onsite,"Req ID:
258563
NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Center Operations Analyst to join our team in Quincy, Washington (US-WA), United States (US).
The Data Center Planning Associate (Operations Analyst) is responsible for capacity management of the data center. This is a direct customer (internal & external) facing role that gathers requirements, manages project schedules, and ensures SLA/OLA compliance. They review project’s technical requirement and ensuring the availability of appropriate infrastructure redundancy and capacity. They coordinate project deployments with the DC Hardware team and creates as-built documentation such as rack elevations & device/cabling DC Portal records. The DC Planning Associates are directly responsible for the accuracy of our DCIM data.
The candidate will be responsible for:
DCIM Data entry
Queue management: Prioritization, SLA/OLA, scheduling, & resource assignment
Complete assigned planning tickets
Evaluate projects for capacity requirements
Evaluate equipment specifications
Create rack elevation, floor space, and power assignments
Evaluate DC infrastructure in relation to project requirements
Coordinate project deployment and decommissions
Update request, change, and incident notes
Create change management requests
Audit DCIM tool & DC floor for data accuracy
Basic Qualifications:
Minimum 3 years of Data Center Operation; Tape Operations/Smart hands
Minimum 3 years of DCIM Data entry
Minimum 3 years Queue management: Prioritization, SLA/OLA, scheduling, & resource assignment
#INDICS
About NTT DATA Services
NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients’ long-term success. Visit nttdata.com or LinkedIn to learn more.
NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
Show more
Show less","Data Center Operations, Data Center Planning, DCIM, Queue Management, SLA/OLA, Project Scheduling, Resource Assignment, Rack Elevation, Floor Space, Power Assignments, Infrastructure Evaluation, Project Deployment, Decommissioning, Change Management, Data Accuracy, Data Entry, Prioritization, Scheduling","data center operations, data center planning, dcim, queue management, slaola, project scheduling, resource assignment, rack elevation, floor space, power assignments, infrastructure evaluation, project deployment, decommissioning, change management, data accuracy, data entry, prioritization, scheduling","change management, data accuracy, data center operations, data center planning, data entry, dcim, decommissioning, floor space, infrastructure evaluation, power assignments, prioritization, project deployment, project scheduling, queue management, rack elevation, resource assignment, scheduling, slaola"
Lead Data Engineer / Gaming Space / Hybrid / Los Angeles,Dice,"Irvine, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-gaming-space-hybrid-los-angeles-at-dice-3786289978,2023-12-17,San Juan Capistrano,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!
Our client is in the gaming space looking for a Senior Data Engineer with 6-10 years of experience to lead a team across the Data landscape. This person ideally has experience in the gaming industry and has experience across Data Engineering, Analytics, and machine learning. Located near Santa Monica this role will be mostly onsite.
Basic Qualifications (Required Skills & Experience)
6-10 years of experience
Data Engineering experience
Data Analytics experience
Machine Learning experience
Python, R, SQL, AWS, Tableau, PowerBI
Other Qualifications & Desired Competencies
Onsite in Santa Monica / El segundo
Bonus on top of base salary / equity / relocation package
You will receive the following benefits:
Medical Insurance
Dental Benefits
Vision Benefits
401(k) matching
Lead Data Engineer / Gaming Space / Hybrid / Los Angeles
Show more
Show less","Data Engineering, Data Analytics, Machine Learning, Python, R, SQL, AWS, Tableau, PowerBI","data engineering, data analytics, machine learning, python, r, sql, aws, tableau, powerbi","aws, data engineering, dataanalytics, machine learning, powerbi, python, r, sql, tableau"
