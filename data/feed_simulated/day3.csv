job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Engineer,The Reject Shop,"Kensington, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-the-reject-shop-3767520951,2023-12-17,Redcliffe, Australia,Associate,Hybrid,"The Reject Shop is an ASX listed Discount Variety retailer currently operating more than 380 stores nationally. We pride ourselves on our supportive team culture and understanding our customers, delivering the best value and variety possible.
At The Reject Shop we are Passionate about helping All Australians save money EVERY DAY! This is our heart and the pure essence of everything that we do!
Having embarked on a mission to completely re-imagine our technology landscape with our new data platform at the core, we are seeking data engineers to help us on this journey.
As part of the data platform team you will be helping to change the way that we think about technology outcomes.  You will have the opportunity to play a major part in our technology renewal, especially how we use data to drive our business.  This is an exciting chance to help drive change and influence technology direction.
In the role, you will work collaboratively with our business teams to identify and understand problems and opportunities.  With other members of the Technology Team you will help ensure solutions achieve desired business outcomes.  You will be laser-focused on finding ways to iteratively unlock value and seeking continuous improvement across our business and technology processes.
Most importantly, you will enjoy be calm under pressure and thrive of experimenting and innovating.
Develop data pipelines that enables data to be more accessible for business and technology teams
Design and develop solutions to business problems using a wide selection of Azure Data services
Develop integrations with cloud-based solutions that meet their needs without compromising the integrity of our internal architecture
Assist in the development of data models to enable self-service of data
Collaborate with business and technology teams to lift their data understanding and highlight opportunities to solve problems using modern data approaches
Ensure the operational efficiency of the platform
Perform proof of concepts on new approaches and technologies to support the evolution of our environment
Ideally, you will be tertiary qualified in IT with experience in a similar role.  You will work well with others both inside and outside of the team and have an appetite to continue to learn and improve.
Along with this, you will have:
Good experience working in a cloud based environment
Design and implementation experience using Azure data services including Synapse, Azure Data Factory, Logic Apps, Azure Databricks, Azure Logging and Monitoring, Synapse, Azure Data Lake, Azure Data Factory, Azure SQL Database, Azure Analysis Services, Power BI
Experience with data modelling
Great SQL skills
A good knowledge of CI/CD and experience in a programming language such as C#, Javascript or Python.
Whilst not essential, experience in integrating legacy systems with a modern data platform would be beneficial
What We Offer:
A supportive and diverse culture. At TRS we care about your wellbeing and are committed to providing a fun, safe and respectful environment
Hybrid working model (Monday - Wednesday in office - Thursday & Friday WFH)
Team Member discount for you and an immediate family member (permanent team members)
Thrive - Supporting financial wellbeing and providing Instant access to your pay
Paid parental leave program supporting primary and secondary care givers
Employee Assistance Program - free and confidential professional counselling for work and personal issues for you and your immediate family members
The chance to make a meaningful impact on the success of a leading retail brand.
If this sounds like you and you meet most of the competencies outlined above, then we encourage you to apply, Why not Join
The Reject Shop
, and Help all Australians save money every day!
Show more
Show less","Data engineering, Azure Data services, Synapse, Azure Data Factory, Azure Databricks, Azure Logging, Azure SQL Database, Azure Analysis Services, Power BI, Data modelling, SQL, C#, Javascript, Python, CI/CD","data engineering, azure data services, synapse, azure data factory, azure databricks, azure logging, azure sql database, azure analysis services, power bi, data modelling, sql, c, javascript, python, cicd","azure analysis services, azure data factory, azure data services, azure databricks, azure logging, azure sql database, c, cicd, data engineering, data modelling, javascript, powerbi, python, sql, synapse"
Data Analyst,Robert Half,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-robert-half-3780086337,2023-12-17,Redcliffe, Australia,Associate,Hybrid,"The Company
Robert Half has been exclusively retained to appoint a newly created role of Data Analyst for a Global Construction Solutions Enterprise, based at its Melbourne headquarters.
Our client is a global industry leader with ongoing acquisitive and organic growth plans, providing access solutions to the commercial, industrial and construction sectors.
The Role
Reporting directly to the Managing Director (APAC), this role will suit a developing Data Analyst professional, joining an organisation that invests in its employee's development.
This role has two key areas of responsibility:
1. To provide timely analysis on
business performance issues and data analysis management
across the Australian and APAC business operations to include:
APAC Reporting & Financial Statement Close Process (FSCP) required for global for monthly and quarterly deliverables.
Analysis of existing Financial & Business cases & Data Analysis, assessment of existing and future Project Pipeline
2. To assist in the Managing Director in
delivering business transformation projects in the APAC region
by supporting feasibilities and analysis of Transformation and Projects Initiatives
Some of the more specific responsibilities will include ;
Analysis & Reporting
Provide accurate and timely variance and financial analysis on the APAC region results.
Preparation of monthly/quarterly regional reports.
Developing business cases, feasibility reports and cost/benefit analysis for solutions and proposals.
Manage the administration of the Australian and APAC Division's job evaluation tool
Assist with the development of the APAC business strategy.
Compiling charts, tables, and other elements of internal and external data visualisation.
Business Transformation
Work with project teams to validate and document current state processes and contribute to the development of efficient and customer focused future state processes.
Provide appropriate data analysis to support project initiatives.
Map, analyse and model processes.
Establish, update, monitor and report on project progress, action registers, risk, and issues log to ensure transparency of issues, conflicting priorities, and escalations are actioned.
Your Profile
You are a highly competent Data Analyst, with sound financial literacy, with an ability to identify issues and areas for improvement through analysis and have demonstrated implementing new processes and systems.
Essential to the role are:
A 'hands on' problem solver and critical thinker.
Work collaboratively within the project team, internal stakeholders, subcontractors, clients & consultants to achieve positive project and community outcomes.
Strong 'can do' attitude with clear ownership of decisions.
Minimum of 2 years in a similar role within the construction industry.
Strong Microsoft Excel and PowerPoint skills.
Excellent written, numeracy and verbal communication skills.
Experience within the Engineering and or Construction industries, highly desirable.
You will be joining a transparent, non-hierarchical culture with a strong commitment to flexible/hybrid work.
Apply Today
Please send your resume by clicking on the apply button. Your application will be assessed within 3-5 working days. Please note only shortlisted candidates will be contacted.
Learn more about our recruitment services: https://www.roberthalf.com.au/contact-us
PLEASE NOTE THAT ONLY APPLICANTS WITH FULL WORKING RIGHTS IN AUSTRALIA WILL BE CONSIDERED
Show more
Show less","Data Analysis, Financial Analysis, Business Transformation, Data Visualization, Project Management, Microsoft Excel, PowerPoint, Financial Literacy, Problem Solving, Critical Thinking, Collaboration, Communication","data analysis, financial analysis, business transformation, data visualization, project management, microsoft excel, powerpoint, financial literacy, problem solving, critical thinking, collaboration, communication","business transformation, collaboration, communication, critical thinking, dataanalytics, financial analysis, financial literacy, microsoft excel, powerpoint, problem solving, project management, visualization"
Global HR Data Analyst,Incitec Pivot Limited,"Southbank, Victoria, Australia",https://au.linkedin.com/jobs/view/global-hr-data-analyst-at-incitec-pivot-limited-3756060273,2023-12-17,Redcliffe, Australia,Associate,Hybrid,"The role
Implementing standardised people metrics, analytics & reporting that support the strategic and operational needs of the business. You’ll also maintain our data integrity through SAP and develop a global auditing process.
Global accountability for IPL’s People
metrics, reporting, analytics & insight
including:
Define and produce people metrics in support of the business’ strategic (e.g. talent), operational (e.g. labour productivity) and transactional (e.g. data required by third parties) requirements
Develop a process for extracting and collating the required data for the defined metrics
Develop a process/system/tool to translate that data into metrics in appropriate reports in support of the business’s strategic, operational, and transactional requirements
Produce reports that meet the quality and time requirements of the business
Working closely with the HR community and business leaders to understand operational challenges and provide additional data-driven insight
Lead the interface with IT and other stakeholders to ensure values within our systems are configured and maintained to meet HR requirements
About you:
Ideally you will have formal qualifications in a related field with hands on experience within an enterprise HR Data, Systems or Analytics environment. We work in a SAP environment, hence individuals with prior SAP experience are preferred.
By leveraging your advanced Microsoft Excel skills, you will be able to easily interpret data requests, extract information from diverse sources and formats, and present the data in a manner aligned to the specified requirements.
You should possess experience in constructing HR dashboards, advanced in excel, and experience reporting on Workplace Gender Equality, and DJ Sustainability Index.
Why join us?
We’re a warm, inclusive, and supportive team who work well together (flexibly) in support of delivering an excellent service to the business. This opportunity provides the foundations to take on a wide range of career paths and we continually invest in the development of our people.
About us
Incitec Pivot Limited (IPL) and its subsidiary company Dyno Nobel, is a ASX 100 global leader in the resources and agricultural sectors with an unrelenting focus on Zero Harm. With a diverse leadership, we add value for our customers through manufacturing excellence, innovation, and world class services. We provide ground-breaking solutions through practical innovation in the mining services and agriculture industry. The group comprises more than 5,000 people across the sales, commercial operations, and manufacturing footprint globally, including the key mining markets in North America, Asia Pacific, South Africa and South America.
We regularly have exciting opportunities come up within the business and can offer a rewarding career for the long term
Show more
Show less","SAP, Microsoft Excel, HR Dashboards, HR Analytics, Workplace Gender Equality, DJ Sustainability Index","sap, microsoft excel, hr dashboards, hr analytics, workplace gender equality, dj sustainability index","dj sustainability index, hr analytics, hr dashboards, microsoft excel, sap, workplace gender equality"
Principal and Senior consultant level Microsoft Fabric Data & Analytics Consultants - sponsorship available for any Databricks gurus,BI & DW Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/principal-and-senior-consultant-level-microsoft-fabric-data-analytics-consultants-sponsorship-available-for-any-databricks-gurus-at-bi-dw-australia-3758238438,2023-12-17,Redcliffe, Australia,Associate,Hybrid,"Principal and Senior Consultant level - Microsoft Fabric Data & Analytics Consultants - Overseas sponsorship for any Databricks gurus with niche professional services company experience
A leading Australian based Data & Analytics Professional Services consultancy is looking Senior to Principal Consultants for their Australian practice.
The roles are based in Sydney or Melbourne
What's In It for You
A competitive permanent salary package with the opportunity to work with large enterprise clients in many verticals including financial services, utilities, government and more!
The role
This role is the epitome of the archetypal IT business consultant where you will apply your proven Microsoft Data & Analytics Fabric and/or Databricks development expertise within top ASX 500 style organisations. You will be working exclusively on high level projects and will utilise the Microsoft Fabric technologies, Databricks, Azure Data Factory (ADF) and Power BI. You will become a key member of the team assisting with the growth of the Microsoft Practice.
The skills needed
The ideal candidate for this true technical consulting role will have some or all of the following experience:
Microsoft Platform knowledge of Microsoft Fabric and on prem Microsoft BI technologies
Azure Data Factory experience is expected
Azure Synapse
Azure One Lake
Databricks experience
Power BI experience
Data modelling experience
Ideally previous Professional services experience with niche Data consultancies
Previous on prem SSIS, SSAS (tabular and modular) and SSRS, Power BI experience
High level client liaison skills
Other cloud skills such Snowflake, GCP, AWS or DBT
As mentioned salary levels are excellent and remuneration will match your experience.
There is the possibility of sponsorship for overseas Databricks gurus but you need to be exceptionally good with associated professional services experience.
Show more
Show less","Microsoft Data & Analytics Fabric, Databricks, Azure Data Factory, Power BI, Data modelling, Professional services, SSIS, SSAS, SSRS, Snowflake, GCP, AWS, DBT","microsoft data analytics fabric, databricks, azure data factory, power bi, data modelling, professional services, ssis, ssas, ssrs, snowflake, gcp, aws, dbt","aws, azure data factory, data modelling, databricks, dbt, gcp, microsoft data analytics fabric, powerbi, professional services, snowflake, ssas, ssis, ssrs"
Data Engineer,CareCone Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-carecone-group-3781964575,2023-12-17,Redcliffe, Australia,Mid senior,Onsite,"Position: Data Engineer - Redshift
Location: Melbourne/Sydney/Brisbane
Permanent Position
Job description
Must to have:
Previous knowledge of SDS and SDS2 environments
Extensive SQL Skills
Good understnaidng of Postgress & DB2
Redshift
Good to have:
Some data analysis experience
Interested candidates can send their updated resume to Dhivya.Natarajan@carecone.com.au or reach me @ M: 61281982279
Show more
Show less","Data Engineer, Redshift, SDS, SDS2, SQL, Postgress, DB2","data engineer, redshift, sds, sds2, sql, postgress, db2","dataengineering, db2, postgress, redshift, sds, sds2, sql"
Data Engineer - IBM MDM,Tech Mahindra,"Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-ibm-mdm-at-tech-mahindra-3769584857,2023-12-17,Redcliffe, Australia,Mid senior,Onsite,"Location :
Brisbane | Sydney | Melbourne
MUST Have ::
DATA Engineer
Hands-on development experience with
IBM MDM
experience
edition 11 +
Experience with
Kafka
and
streaming process
Experience with testing frameworks such as
JUNIT
and
Cucumber
Experience with
Devops
process and
cloud services
(
AWS
)
Grasp of best practice standards
and
patterns
Familiar with
MDM implementation
(physical and virtual) and should be able to understand and contribute to code quickly knowledge and experience
with CI/ CD practices
and
processes
.
Senior engineers
have all about plus and expectation of minimum of 5+ years of experience.
Comprehensive understanding and ability to contribute to all phases of
SDLC
Show more
Show less","Data Engineer, IBM MDM, Apache Kafka, Streaming Process, JUnit, Cucumber, DevOps, AWS, Best Practices, Patterns, MDM Implementation, CI/CD, SDLC","data engineer, ibm mdm, apache kafka, streaming process, junit, cucumber, devops, aws, best practices, patterns, mdm implementation, cicd, sdlc","apache kafka, aws, best practices, cicd, cucumber, dataengineering, devops, ibm mdm, junit, mdm implementation, patterns, sdlc, streaming process"
Data Engineer - REDSHIFT | SQL | DB2,Tech Mahindra,"Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-redshift-sql-db2-at-tech-mahindra-3769585617,2023-12-17,Redcliffe, Australia,Mid senior,Onsite,"Location :
Brisbane | Sydney | Melbourne
MUST Have ::
DATA Engineering
experience
with
Redshift, SQL, DB2, POSTGRESS.
Worked as
Data Engineer
in area of
Data Warehousing/Business Intelligence
Very hands on heavy on quickly learning
Data Application, Data Analysis, Scripting, Data Transformation and Reconciliation.
Data Migration
experience if possible
Must have experience working as
DWH modelling, developer, or analyst
for 8+ years
AWS Redshift,
Netezza and
Data Modelling
is a MUST
Must have led a team for minimum 5 people
Must have good collaboration and communication skills
Performance optimization skill and code review skill is a must
Good to have reporting skills like
Cognos, Tableau , ThoughtSpot
etc.
Should have experience in working on various detail designs (HLD and LLD).
Good to have Unix
Shell Scripting
Knowledge
Additional Skills
Control M, Bit bucket/ Source Tree, Jenkins
Should have experience in Cost estimates for development and technical tasks break down
Should have good exposure to
DBFit like ETL testing tools
for test driven development
Should have experience in
DataStage
and
DB2 flex
Show more
Show less","Data Engineering, Redshift, SQL, DB2, PostgreSQL, Data Warehousing, Business Intelligence, Data Application, Data Analysis, Scripting, Data Transformation, Data Reconciliation, Data Migration, DWH Modelling, Data Developer, Data Analyst, AWS Redshift, Netezza, Data Modelling, Cognos, Tableau, ThoughtSpot, Unix Shell Scripting, Control M, Bitbucket, Source Tree, Jenkins, Cost Estimates, DBFit, ETL Testing Tools, Test Driven Development, DataStage, DB2 flex","data engineering, redshift, sql, db2, postgresql, data warehousing, business intelligence, data application, data analysis, scripting, data transformation, data reconciliation, data migration, dwh modelling, data developer, data analyst, aws redshift, netezza, data modelling, cognos, tableau, thoughtspot, unix shell scripting, control m, bitbucket, source tree, jenkins, cost estimates, dbfit, etl testing tools, test driven development, datastage, db2 flex","aws redshift, bitbucket, business intelligence, cognos, control m, cost estimates, data application, data developer, data engineering, data migration, data modelling, data reconciliation, data transformation, dataanalytics, datastage, datawarehouse, db2, db2 flex, dbfit, dwh modelling, etl testing tools, jenkins, netezza, postgresql, redshift, scripting, source tree, sql, tableau, test driven development, thoughtspot, unix shell scripting"
Lead Data Engineer,conundrm.,"Prahran, Victoria, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-at-conundrm-3731041061,2023-12-17,Redcliffe, Australia,Mid senior,Onsite,"About Conundrm
Conundrm is a fast-growing, innovative, funded start-up at the forefront of predictive analytics. We believe that numeracy is the key to combating misinformation, and we use mathematics and AI to do so. We are committed to delivering exceptional value to our customers through cutting-edge solutions and software.
Position Overview
The Lead Data Engineer at Conundrm plays a pivotal role in ensuring the seamless integration and availability of data for our advanced marketing models. The ideal candidate will have a strong background in data engineering, infrastructure management, and security, with proficiency in Python and R programming. This role demands a proactive approach to ingesting new data sources, deploying models through APIs, maintaining cloud infrastructure, and ensuring a high standard of security.
That all sounds incredibly serious... and the role absolutely is, but the environment we work in is fun, high-performing and friendly.
Generous annual leave and wellbeing packages
4x Mental Health Days (no questions asked) per year
Monthly cultural events; from magicians to productions to competitions
and much much more...
How you'll be contributing:
Data Ingestion: Design and implement processes to ingest new data sources into the company's data ecosystem, ensuring data quality and consistency.
Model Deployment: Collaborate with data scientists and statisticians to expose predictive models through scalable and robust APIs.
Programming: Utilise Python and R for data processing, transformation and integration tasks.
Security and Compliance: Implement and maintain security protocols, ensuring that our data and models are protected against unauthorised access and potential branches.
Who you are:
Bachelor's or Master's degree in Marketing Analytics, Data Science, or a related field.
Strong (minimum 3 years) proficiency in ETL processes, data cleansing techniques and data quality assurance.
Must have expertise in R and Python for data analysis, statistical modelling, and data cleansing.
Experience with data manipulation libraries and frameworks (e.g. Pandas, NumPy, dplyr) and machine learning libraries (e.g. scikit-learn, TensorFlow, Keras)
Data visualisation skills using tools such as Matplotlib, ggplot2 or similar.
Familiarity with marketing analytics concepts such as customer segmentation, customer lifetime value and marketing attribution.
Strong problem-solving skills and the ability to collaborate effectively in a small, tight-knit team.
Excellent communication skills to convey technical findings and marketing insights to team members with diverse backgrounds.
If you are a marketing science enthusiast with a strong background in ETL and data cleansing, combined with proficiency in R and Python, and thrive in a collaborative, agile and innovative team environment, we invite you to apply for this exciting opportunity.
Show more
Show less","Data Engineering, Infrastructure Management, Security, Python, R programming, APIs, Cloud Infrastructure, Data Ingestion, Model Deployment, Data Quality, Data Consistency, Data Processing, Data Transformation, Data Integration, Data Manipulation, Data Cleansing, Data Quality Assurance, ETL Processes, Pandas, NumPy, dplyr, Scikitlearn, TensorFlow, Keras, Machine Learning, Matplotlib, ggplot2, Data Visualization, Marketing Analytics, Customer Segmentation, Customer Lifetime Value, Marketing Attribution, ProblemSolving, Collaboration, Communication","data engineering, infrastructure management, security, python, r programming, apis, cloud infrastructure, data ingestion, model deployment, data quality, data consistency, data processing, data transformation, data integration, data manipulation, data cleansing, data quality assurance, etl processes, pandas, numpy, dplyr, scikitlearn, tensorflow, keras, machine learning, matplotlib, ggplot2, data visualization, marketing analytics, customer segmentation, customer lifetime value, marketing attribution, problemsolving, collaboration, communication","apis, cloud infrastructure, collaboration, communication, customer lifetime value, customer segmentation, data consistency, data engineering, data ingestion, data integration, data manipulation, data processing, data quality, data quality assurance, data transformation, datacleaning, dplyr, etl, ggplot2, infrastructure management, keras, machine learning, marketing analytics, marketing attribution, matplotlib, model deployment, numpy, pandas, problemsolving, python, r programming, scikitlearn, security, tensorflow, visualization"
Software Engineer - Data platform,Gridcog,Australia,https://au.linkedin.com/jobs/view/software-engineer-data-platform-at-gridcog-3764925530,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"About Gridcog:
Gridcog provides advanced software to plan, simulate and optimise decentralised energy projects. Our software is used by large energy suppliers, energy project developers, technology providers and large energy users. We have a loyal and growing customer base across Australia, and are expanding to the UK and Europe.
We believe the future of energy is distributed, smart and clean, and that we can use software to accelerate decarbonisation and to help tackle climate change.
We're a fast growing technology startup and we want to have a global impact. We are looking for smart, savvy and curious learners to join our team and to help invent new technology to lead the world into a decentralised energy future.
About the role:
We’re looking for a Software Engineer with Python and AWS data processing experience to join our team. A key component of the Gridcog platform is ingestion and processing of a wide variety of data sources related to energy generation, usage and prices. From energy regulators, energy suppliers, and customer assets such as solar and wind farms, and large scale batteries. You will help evolve and maintain our data processing infrastructure ensuring reliable delivery of data to help customers design and deliver the most efficient and effective energy transition outcomes.
We're a remote-first team that values some in-person time - our largest cluster is in Perth, followed by Melbourne - but anywhere in Australia could work for the right candidate.
Requirements
Our ideal candidate has:
Proven experience as a software engineer or data engineer
Experience designing and building data integrations and APIs with Python
Experience with data engineering tools, and data processing with pandas, numpy, and similar
Experience with AWS, and familiarity with serverless and event-driven architectures
Experience with ETL/ELT pipelines and both structured and unstructured data stores
Solid algorithm development skills, good understanding of data structures
System design skills: design robust, reliable and highly available online services
Ability to communicate technical concepts clearly to technical and non-technical team members
Experience with API design, database schema design, and automated testing
CI/CD development experience and modern monitoring and observability techniques
What you’ll do:
Build and take ownership of key services for our SaaS product, with a focus on backend services and data flows
Utilise your in-depth knowledge of AWS services to build scalable, reliable, and highly available cloud solutions
Work on data ingestion, processing, aggregation, and data pipeline components to enable seamless data transformation
Design and implement APIs and Event to enable integration with other applications
Scalability and Performance: Optimise software components for performance and scalability to handle large data volumes efficiently
Documentation: Create and maintain clear and comprehensive documentation for software architecture and code
Collaboration: Collaborate with product managers, data engineers, and data scientists to understand and address customer requirements
Problem Solving: Troubleshoot and resolve software issues, including bug fixes, performance improvements, and enhancements
Benefits
Competitive salary package aligned with experience and skills
Opportunity to work in a remote-first business with flexible working arrangements
Weekly opportunities for in-person collaboration at co-working spaces and an annual whole company retreat
Join a high-performing, unapologetic energy and tech nerd team to tackle significant challenges
Engage in a high-trust distributed team environment that values innovation and creative problem-solving
Contribute to the decarbonisation of the world's energy system
Time and budget support for ongoing professional and personal development
Opportunity for ESOP participation
Show more
Show less","Python, AWS, Data engineering, Data integration, API design, ETL/ELT pipelines, Data structures, System design, CI/CD, Monitoring, Observability, SaaS, Cloud solutions, Scalability, Performance, Documentation, Collaboration, Problem solving","python, aws, data engineering, data integration, api design, etlelt pipelines, data structures, system design, cicd, monitoring, observability, saas, cloud solutions, scalability, performance, documentation, collaboration, problem solving","api design, aws, cicd, cloud solutions, collaboration, data engineering, data integration, data structures, documentation, etlelt pipelines, monitoring, observability, performance, problem solving, python, saas, scalability, system design"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),Australia,https://au.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787780083,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
7hvHi2rnV9
Show more
Show less","F4S work style assessment, Predictive analytics, Research, Team motivations, Team performance, Hiring needs, JazzHR","f4s work style assessment, predictive analytics, research, team motivations, team performance, hiring needs, jazzhr","f4s work style assessment, hiring needs, jazzhr, predictive analytics, research, team motivations, team performance"
Data Analyst,Employment Hero,Australia,https://au.linkedin.com/jobs/view/data-analyst-at-employment-hero-3783637941,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"Our mission and where you fit in...
At Employment Hero, we're an ambitious bunch of people on a mission to make employment easier and more valuable for everyone.
Since our inception in 2014, we've had some pretty impressive growth (100% YoY), now serving 300,000 businesses globally, with 2 million+ users on the platform, reaching unicorn status in 2022 - and we have no plans to slow down.
There's never been a more exciting time to join one of the fastest-growing SaaS unicorns, so let's see if we could be a match!
What might your days look like? 🚀
We're a team of global innovators, who cherish diverse perspectives that fuel our mission; to simplify the world of work for SMBs worldwide.
Are you ready to make a significant impact in a rapidly expanding team? Are you excited about turning data into actionable insights? If you're looking for a dynamic role where you can unleash your analytical skills and contribute to the growth of a company, then we have the perfect opportunity for you!
🚀
📊 As Our Data Analyst, You'll Be
Creating reports and dashboards: Use your data expertise to craft visually appealing reports and interactive dashboards that effectively communicate findings to stakeholders, from executives to team members.
Conducting ad hoc analyses: Dive deep into data to address specific questions and investigate important issues that drive decision-making and strategic initiatives.
Analysing and interpreting data: Utilise your statistical analysis prowess and data visualisation techniques to uncover trends, identify patterns, and draw valuable insights and conclusions.
Collaborating with stakeholders: Work closely with stakeholders across the organisation, partnering with them to understand their needs and provide data-driven insights and recommendations.
What will you bring? 🚀
✔️ Experienced - you have 3+ years of experience in a similar role within the SaaS industry or a Scale-up/start-up environment, project managing and delivering high-quality analytics projects to a deadline.
✔️ You will possess strong communication skills, not only in conveying complex technical concepts to non-technical stakeholders, including executives and other business leaders but also in effectively using data visualisation tools like
Tableau
and Sisense to captivate and engage stakeholders.
✔️ Proficient in SQL (preferably PostgreSQL): Extract, manipulate, and analyse data from relational databases, enabling you to unlock its potential and uncover meaningful insights.
✔️ Strong business acumen: Identify opportunities where data-driven insights can add value and help drive strategic business decisions.
✔️ Excellent communication skills: Translate complex technical concepts into simple language, enabling you to effectively communicate with non-technical stakeholders, including executives and business leaders.
✔️ Meticulous attention to detail: Ensure the accuracy, completeness, and consistency of your data analysis, leaving no stone unturned.
✔️ Have full unrestricted working rights in Australia.
Bonus Points
🌟 It would be highly advantageous if you have previous experience working with Sales and Marketing / GTM SaaS metrics such as MQL, SQL, SAO, ARR, Close Rates, as is experience working with CX metrics such as NPS, CSAT, and Financial/Cost-related metrics.
🌟 Experience with programming languages like Python or R: Leverage your programming skills to manipulate and analyse data efficiently, taking your analysis to the next level.
🌟 Experience with statistical modeling, regression analysis, and hypothesis testing: Utilise advanced techniques to extract meaningful insights from complex datasets.
Experience is important, but for us the biggest measure of success is people who can live and breathe our values. Show us what you can bring to the table, and we'll empower you to let your talents shine.
Benefits
Life at Employment Hero 🚀
Remote-first principles
At Employment Hero, we're not just working remotely; we're integrating flexibility and global reach into the heart of our daily operations.
We also recognise the value of face-to-face connection, and organise local and global gatherings throughout the year to celebrate our wins and make meaningful connections with our colleagues.
Work your way
Every hero has unique powers. Bound by a common purpose and trust, we encourage each other to work in ways that allow us to bring our best selves to work.
We've got your back!!
Whether you're a seasoned remote-first pro, or a first-timer, you're in good company. With 900+ heroes globally, team support and collaboration is at your fingertips.
But don't just take it from us, here's a quote from one of our AU heroes:
""Working for a company that has purpose and meaning is felt throughout the entire business. No one turns up to work just to get paid - we turn up to work because we care and take pride in what we do, but we also know how to create balance and flexibility in the important things in our life, such as family, friends and leading a lifestyle consistent with our values.""
Plus you'll get to enjoy a number of great perks, including:
Remote-first and flexible working arrangements
A generous budget to spend on setting up your home office (if you need a desk, chair, or screen? We've got you covered!)
We set you up for success with the latest and greatest hardware, tools and tech
Learning and development (including an external study policy, live monthly professional development classrooms, and premium online learning content!)
Reward and recognition programs - because great work should be recognised and rewarded
Including Lightning Award for delivering quality work at speed and Values Champion Awards
Swag app cashback offers and discounts on hundreds of your favourite brands and products
Self, health, wealth and happiness programs
Social events and team celebrations
Employee Share Option Program: be an owner of Employment Hero
Annual Global Gathering to get to know your global colleagues - so far we've been to Thailand, Vietnam and are excited to meet in Bali in September 2024
We also recognise that the same recruitment process doesn't fit all, so should you require any accommodations or adjustments, simply let us know.
Are we a match?
When you apply for this role, anticipate hearing from us within 72 hours. At Employment Hero, we ensure that every applicant receives a timely response, whether it's an invitation to the first-round interview with our Talent Acquisition Specialist or a notification that your application, while valued, wasn't successful this time. Your time and effort matter to us, and we're committed to keeping you informed throughout the process.
Not exactly the right fit for you?
If you're excited about Employment Hero, but this role is not the one for you, we encourage you to explore our careers page, packed with many more great opportunities.
Know someone who would be a great fit? Feel free to share the role with your network!
Note to recruiters: Employment Hero has a dedicated in-house recruitment team who are focused on finding the very best talent for our organisation and we kindly request that recruiters do not contact us regarding assisting with our job vacancies. While we appreciate your interest and expertise, we have everything we need in-house to attract and hire the right candidates for our team. Thank you.
Show more
Show less","Data Analysis, Report Creation, Dashboard Creation, Ad Hoc Analysis, Data Interpretation, Stakeholder Collaboration, Communication, Tableau, Sisense, SQL, PostgreSQL, Business Acumen, Attention to Detail, Sales and Marketing Metrics, Customer Experience Metrics, Financial Metrics, Python, R, Statistical Modeling, Regression Analysis, Hypothesis Testing","data analysis, report creation, dashboard creation, ad hoc analysis, data interpretation, stakeholder collaboration, communication, tableau, sisense, sql, postgresql, business acumen, attention to detail, sales and marketing metrics, customer experience metrics, financial metrics, python, r, statistical modeling, regression analysis, hypothesis testing","ad hoc analysis, attention to detail, business acumen, communication, customer experience metrics, dashboard creation, data interpretation, dataanalytics, financial metrics, hypothesis testing, postgresql, python, r, regression analysis, report creation, sales and marketing metrics, sisense, sql, stakeholder collaboration, statistical modeling, tableau"
Data Engineer - Security,CyberSec People,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-security-at-cybersec-people-3765182933,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"The Role
Join one of the largest Cyber Security Operations by data volume in the southern hemisphere as a Data Insights Engineer and play a crucial role in enhancing their data analysis and visualisation capabilities.
You'll have the opportunity to develop and refine their data analysis algorithms, craft dynamic dashboards, and work on middleware essential for data workflows across various diverse technologies. Your expertise will empower our team and clients with actionable insights, driving rapid, data-informed decisions.
Your Day to Day :
Develop robust data pipelines using advanced technologies like Spark, Flink, Hive, or Kafka, facilitating scalable data workflows.
Efficiently manage and optimise big data platforms, ensuring top performance for our Security Operations Center (SOC).
Create sophisticated algorithms for data analysis, aiding our SOC operations and Security Services product suite in detection, investigation, and response tasks.
Design and maintain dynamic dashboards offering insightful data visualisations for strategic and operational security activities.
Collaborate with SOC analysts to determine data needs and develop seamless integration solutions with our existing security infrastructure.
What you bring:
Strong programming skills in Python, Scala, Java, or C/C++.
Experience with relational and NoSQL databases with expert SQL skills
Proven ability in building scalable and robust data pipelines
Competency in managing and administrating major big data platforms and ecosystems.
Bonus Points:
Knowledge in machine learning and data mining techniques for enhanced security data insights.
Experience in report and dashboard creation using tools such as Tableau, Kibana, Grafana, or Superset.
Familiarity with SIEM platforms and integrating data insights within these frameworks.
Background in software development and CI/CD practices.
If you are wanting a chance to showcase your superb data skills in the Cyber Security industry protecting one of the most crucial threat landscapes in Australia this is not an opportunity you want to miss.
Show more
Show less","Data Insights Engineer, Data Analysis, Data Visualisation, Spark, Flink, Hive, Kafka, Big Data Platforms, Data Analysis Algorithms, Dashboards, Python, Scala, Java, C/C++, Relational Databases, NoSQL Databases, SQL, Data Pipelines, Machine Learning, Data Mining, Tableau, Kibana, Grafana, Superset, SIEM Platforms, Software Development, CI/CD Practices","data insights engineer, data analysis, data visualisation, spark, flink, hive, kafka, big data platforms, data analysis algorithms, dashboards, python, scala, java, cc, relational databases, nosql databases, sql, data pipelines, machine learning, data mining, tableau, kibana, grafana, superset, siem platforms, software development, cicd practices","big data platforms, cc, cicd practices, dashboard, data analysis algorithms, data insights engineer, data mining, data visualisation, dataanalytics, datapipeline, flink, grafana, hive, java, kafka, kibana, machine learning, nosql databases, python, relational databases, scala, siem platforms, software development, spark, sql, superset, tableau"
Data Engineer,Keypath Education,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-keypath-education-3784115470,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"Discover Your Path at Keypath!
About Us:
At Keypath, we invite you to be part of something transformative. As a global EdTech leader, we collaborate with prestigious universities to create and deliver online education solutions that tackle global social and economic challenges head-on.
Join our dynamic, fast-growing international team that thrives on collaboration, innovation, and lifelong learning. With offices in Melbourne, Sydney, Chicago, and SE Asia, we are truly a global company. Today, we're a family of over 700 passionate ""Keypathers"" worldwide.
Work Anywhere:
At Keypath Education Australia, flexibility is our strength. We put our people first, allowing you to choose where you work – from home, in the office, or both.
Job Description:
The successful candidate will work closely with the Data Engineering team to design & maintain the critical data infrastructure and pipelines that bolster thecompany’s tactical and strategic data-driven initiatives.
The Data Engineer will be a technical expert in the development of system-to-system integration solutions, complex data warehouse data models, and highly functional semantic layers that translate raw company data into reliable and actionable information.
Additionally, the Data Engineer will ensure data pipelines and systems are efficient and reliable, and support insight and growth across the global Keypath organization.
Responsibilities include but are not limited to the following:
Design and develop Azure Data Factory data integration pipelines and SQL Server data warehouse solutions that support continuous delivery of data insights to business stakeholders.
Develop complex SQL/TSQL data warehouse procedural processes and data models to drive business insights and support organizational decision making.
Develop resilient system integration solutions using Azure Functions, Python, and .Net codebases and related development frameworks.
Work closely with the IT Product team and business sponsors to gather information and data requirements based on business operations feedback and conveyed reporting needs.
Design and manage Azure Analysis Services semantic-layer models to support business user self-service Power BI analysis initiatives.
Collaborate with team members on SDLC activities and standards, including code Peer Review and/or Pull Request approvals in Azure DevOps.
Qualifications
Education:
Minimum BS/BA Degree, preferably in a Technology, Computer Science, Mathematics or Statistics related discipline.
Experience:
4+ years of data engineering experience with a proven history of successfully developing and deploying data warehouse and data integration solutions.
2+ years of knowledge and experience developing and implementing data integrations using Microsoft Azure technologies, including Data Factory and SQL Server for data processing, data warehouse, and data modeling solutions.
2+ years of software engineering experience using modern data engineering languages and platforms, such as Python, Microsoft .Net, and Azure Functions.
Proficiency with semantic layer and data packaging concepts (Cubes, Tabular Models), preferably with Microsoft Azure Analysis Services (AAS).
Expertise in the design and maintenance of Data Pipelines, Data Transformations, and Data Flows within Azure Data Factory.
Extensive understanding and experience with Agile and scrum engineering practices and principles.
Technical Skills:
Deep knowledge of Microsoft SQL Server database and Azure data engineering technologies required (ex. Data Factory, SQL/TSQL, Azure Functions, Analysis Services, and Power BI).
Strong proficiency with Business Intelligence and reporting tools, such as Microsoft Power BI or Tableau.
Advanced proficiency with Microsoft Azure Data Factory and Azure Functions.
Understanding of predictive analytics platforms, software, languages, and libraries such as Microsoft Azure ML, SAS, Python, and/or R.
Additional Information
Benefits and Rewards:
Opportunity to chose how you work - in office, hybrid or remotely
Comprehensive online remote training.
All the necessary equipment for success.
Internet Allowance for remote work.
Clear career progression pathways.
Paid Mental Health days for your well-being.
Toolkits for Digital Health and Mental Health support.
Why Keypath?
Embrace Your Authentic Self: A supportive culture recognized as a top workplace (Ranked #8 on 2023 the Best Places to Work Australia©).
Global EdTech Leader: Join an industry on the rise.
Continuous Growth: Upskill through discounted study opportunities.
Transform the World: Contribute to upskilling and reskilling globally.
Make an Impact: Help solve future economic and social challenges.
Join Keypath and make a difference. We welcome applicants from diverse backgrounds and offer inclusive support throughout the recruitment process.
Apply now to unlock your potential with Keypath!
Show more
Show less","Azure Data Factory, SQL Server, SQL/TSQL, Azure Functions, Python, .Net, Microsoft Power BI, Tableau, Microsoft Azure ML, SAS, R, Agile, Scrum, Data engineering, Data warehousing, Data integration, Data modeling, Semantic layer, Data packaging, Data pipelines, Data transformations, Data flows, Predictive analytics, Machine learning","azure data factory, sql server, sqltsql, azure functions, python, net, microsoft power bi, tableau, microsoft azure ml, sas, r, agile, scrum, data engineering, data warehousing, data integration, data modeling, semantic layer, data packaging, data pipelines, data transformations, data flows, predictive analytics, machine learning","agile, azure data factory, azure functions, data engineering, data flows, data integration, data packaging, data transformations, datamodeling, datapipeline, datawarehouse, machine learning, microsoft azure ml, microsoft power bi, net, predictive analytics, python, r, sas, scrum, semantic layer, sql server, sqltsql, tableau"
Senior Data Engineer,IBC Recruitment,Australia,https://au.linkedin.com/jobs/view/senior-data-engineer-at-ibc-recruitment-3779284347,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"Senior Data Engineer
💰 $170k + Super + stock options
🚀 Cutting-edge data engineering technologies 📊
🔝 Work on a greenfield project 🌟
🚀 Join our client in this exciting and challenging project advancing the data capabilities of their Fintech platform, facilitating data-driven decision-making and enhancing reporting functionalities. 📈
As a Senior / Lead Data Engineer, you will play a crucial role in the modernisation of the platform, manage a small data squad and be able to be able to demonstrate commercial viability of solutions you design to the business. Working alongside Software Engineers and Data Scientists and focusing on building data ingestion, transformation and architecting solutions, you will be instrumental in delivering good quality data. You'll be a data engineering guru who can navigate your way through various risk based systems with different complexities, data requirements and scale.
You'll be very comfortable in Python, PySpark, dbt, and be across data tools within AWS (Redshift, S3, Glue) and Apache (Spark, Airflow, etc). You will help build out the Data Warehouse, build robust pipelines, and help automate reporting.
Required Skills:
🔸 Strong proficiency in SQL and Python
🔸 Solid experience working with DBT
🔸 Experience working in AWS (Redshift, S3, Glue) and Apache (Spark, Airflow, etc)
🔸 Ability to mentor and lead a small squad
🔸 Architecture/Design skills and being able to navigate complex systems with varying degrees of scale.
🔸 Expert communication and being able to work with stakeholders in risk, engineering, product and data science.
Apply and attached your resume, applications closing soon. Please note, you must live and have working rights in Australia to apply this opportunity.
Show more
Show less","Python, PySpark, dbt, SQL, AWS, Redshift, S3, Glue, Apache, Spark, Airflow, Data Warehouse","python, pyspark, dbt, sql, aws, redshift, s3, glue, apache, spark, airflow, data warehouse","airflow, apache, aws, datawarehouse, dbt, glue, python, redshift, s3, spark, sql"
Database Engineer,Leidos Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/database-engineer-at-leidos-australia-3784259403,2023-12-17,Redcliffe, Australia,Mid senior,Remote,"Company Description
At Leidos, we do work that really matters inspired by our mission to make the world safer, healthier, and more efficient through technology, engineering, and science. With 25 years of local experience, our over 2000 team members, work together to solve Australia’s toughest challenges in government, defence, intelligence and border protection. We’re robust and ambitious, and we empower our people to do their best work. You’ll feel inspired by what you can achieve and will be supported by an inclusive and flexible culture that genuinely cares for your wellbeing. Together, we can be the difference.
Benefits
We've got so much to offer at Leidos, here are a just a few of the
Benefits
we provide our team:
12 Extra Days Leave: Life Days are the Leidos way of recognising that we all need some extra time out to take care of life. By working slightly more than the minimum weekly hours (2 hours per week for full timers) you can accrue up to an extra 12 days of leave per year.
Leidos Life Hub provides access to discount offers or cashback rewards with over 400 Australian and International retailers.
When you grow, we grow. Be it through our exciting pipeline line of projects or by partnering with a People Leader who will guide, mentor and support you.
Job Description
Leidos Australia have a permanent full-time or contract opportunity for a Database Engineer (Postgres on REHL) to join our team in a role that offers great flexible work arrangements including remote work. Reporting to the Program Technical Lead and working within our team responsible for the implementation, support and maintenance of a large enterprise environment you will be responsible for supporting the customer platform, tuning system performance, installing system wide software, and supporting database hardware and software.
As a talented member of our multidisciplinary teams, you will rapidly grow and progress with us across a diverse and influential Federal Government and Defence portfolio. This is a rare opportunity to join the team who gets to influence programs today that will redefine the customer’s business tomorrow.
In this role you will perform the following functions as individual assignments or as part of a team:
Perform Level 2 and 3 platform support including management and administration of PostgresSQL database products on RedHat;
Implement and test database upgrades;
Design and implement data migration activities;
Maintain database design and implementation documentation;
Implement organisational objectives for database improvement and compliance with industry standards;
Provide solutions to a variety of technical problems of moderate scope and complexity
Maintaining currency of the environment, where possible.
Develops and configures tools to enable automation of database administration tasks.
Source and analyse relevant subject matter documentation
Contribute to IT Support process and policy development
Providing input into policies, procedures, and SOP’s relating to maintenance and management of the operational database infrastructure. Interact with users and technical subject matter experts to ensure delivery is fit for purpose.
Produce enterprise database designs and make recommendations for upgrades, designs and implementations will be advantage.
Qualifications
About You and What You'll Bring
Education
Coupled with your education and practical experience, you will demonstrate a pro-active approach with the ability to understand the business, identify issues and develop relationships to achieve the company’s objectives. You will also have:
Substantial professional experience working in a role within an engineering or project environment (preferably in a similar industry)
Strong Skills/Experience with PostgreSQL is required.
Knowledge and application of a broad range of skills including software, systems and enterprise architecture, software and hardware engineering, interface protocols
Substantial experience working in a large enterprise environments
Skills
Strong individual contributor and team player in a professional environment
Highly effective in managing multiple, concurrent activities, while understanding and managing priorities, dependencies and risk
Excellent communication (verbal and written) and interpersonal skills
Good initiative in developing and improving systems and procedures
Ability to independently develop and determine approaches to solutions in a practical and creative manner
Strong conceptual, analytical and interpretive skills with high level of attention to details
Strong focus on customer service
Strong planning and organising skills
Experience with MS SQL and other database technologies are highly regarded.
Experience with Ansible or infrastructure as code preferred.
Experience with Liquibase highly regarded.
This role does require the successful applicant to be an Australian Citizen hold and maintain or be willing to undergo a Baseline Security Clearance.
Additional Information
We’ve been keeping Australia safer, healthier and more efficient for 25 years, here’s just three ways how
We’re building the next generation secret end user environment for the Australian Department of Defence across a number of strategic locations around Australia.
We deliver software development and operational support through to the integration of underwater autonomous vehicles and survey ships to meet mission objectives.
We support key networks for the Australian Cyber Security Centre.
Applicants may also need to meet International Traffic in Arms Regulations (ITAR) requirements. In certain circumstances this can place limitations on persons who hold dual nationality, permanent residency or are former nationals of certain countries as per ITAR 126.1.
At Leidos, we embrace diversity and are committed to creating a truly inclusive workplace. We welcome and encourage applications from Aboriginal and Torres Strait Islanders, culturally and linguistically diverse people, people with disabilities, veterans, neurodiverse people, and people of all genders, sexualities and age groups.
Show more
Show less","Database Engineering, PostgresSQL, RedHat, SQL, Data Migration, Database Design, Automation, IT Support, Enterprise Architecture, Software Engineering, Hardware Engineering, Interface Protocols, MS SQL, Ansible, Liquibase, Data Security, Network Security, Software Development, Underwater Vehicles, Survey Ships","database engineering, postgressql, redhat, sql, data migration, database design, automation, it support, enterprise architecture, software engineering, hardware engineering, interface protocols, ms sql, ansible, liquibase, data security, network security, software development, underwater vehicles, survey ships","ansible, automation, data migration, data security, database design, database engineering, enterprise architecture, hardware engineering, interface protocols, it support, liquibase, ms sql, network security, postgressql, redhat, software development, software engineering, sql, survey ships, underwater vehicles"
Senior Data Engineer,Littlepay,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-littlepay-3746571155,2023-12-17,Redcliffe, Australia,Mid senior,Hybrid,"We’re Littlepay - a growing fintech headquartered in Melbourne with operations in the UK, EU, LATAM, USA, and more recently, Australia. Our payments platform enables our customers to pay for public transport using any payment card, mobile wallet, or wearable device. Processing over 100 million transactions in 13 countries and continuing to expand globally, we’re proud to move more people through better payment experiences.
Our Littlepay values:
Trust and be trustworthy
Embrace challenge
Solve problems, together
Deliver with speed and agility
Be candid and kind
About The Opportunity
Reporting to the Delivery Lead, the Senior Data Engineer will take ownership for the delivery of products and insights, adding value for internal and external customers. You will have an impact by helping others in your team be better through mentoring, design and code reviews. You will need to be curious, proactive and motivated to provide the best experience for customers and partners in the transit and mobility sectors.
Day to day, you’ll:
Design, implement and maintain dependable data platforms, production-quality APIs, workflows, and tools
Develop and maintain ETL pipelines using Spark
Ensure compliance with data security and privacy requirements within the data processing platform
Write queries and develop pipelines to surface data analysis and visualisations for internal and external users
Mentor junior engineers
Collaborate closely with Service Operations, Platform and Engineering Management to plan your initiatives
Build performant, sustainable, and maintainable technologies and code
Our platform tech
Scala, Python, Go, AWS technologies (including Glue, Step Function, S3, CloudFormation, Lambda, Athena, QuickSight, IAM)
Requirements
Ideally, you’ll have:
At least 4 years of experience in data engineering, including data integration, ETL processes, and data modelling, in a cloud environment
Experience with data governance
At least 5 years of experience in Python or Go
A strong understanding of BI data structures
The willingness to learn, fail, and try again - people who are naturally curious, proactive, and self-motivated do very well at Littlepay
A positive attitude and collaborative nature, with the willingness to participate in all areas of engineering
The ability to comprehend, coordinate, and communicate clearly and effectively with colleagues
Good knowledge of CI/CD practices
Bonus points for having:
Experience in Scala
Our recruitment process
If you’ve been shortlisted for the role, you’ll first have a chat with our Head of Talent, who will set up an interview if the stars align. A short workplace preferences questionnaire will also be sent to help us learn more about the way you prefer to work. Following a successful first interview, you’ll be sent a coding challenge. If we like what we see, we’ll invite you to interview with a second set of Littlepay folks before presenting an offer.
If you have any questions regarding the recruitment process or if you require any accommodations for any stage in the interview process, please let us know. Additionally, if you don’t meet all of the criteria (we know that some underrepresented groups tend to avoid applying if they don’t), please still apply. We really enjoy meeting people who might bring a fresh perspective on a problem we need to solve!
Ultimately, being part of our small but mighty team means you'll have the opportunity to learn about micropayments, and have a hand in the provision of services that impact millions of people. You’ll help us deliver on our revolutionary product roadmap and help scale our platform as we continue to expand across the globe.
Benefits
We love to look after our people. At Littlepay, you'll be part of an inclusive and diverse team that celebrates the differences and unique gifts we each bring to work.
You’ll have access to:
A high trust working environment, with access to cloud collaboration tools like Notion, Slack, Miro, Google Workspace, and Jira to help us work together effectively
Paid professional development (including conferences, courses, learning subscriptions, etc.)
Harrison Assessments - our talent management provider that facilitates continuous professional development
A variety of flexible leave options (including annual, personal, volunteer, parental, grandparental, gender affirmation, and more)
Superannuation contributions on all paid parental leave payments
A company-wide paid day off
Our flexible public holiday program
Flexible (hybrid) working conditions
Wellbeing support- including a free School of Life subscription for all employees, and counselling via the School of Life (UK) or our EAP program (Australia)
Our generous employee referral program
Mid-year, quarterly, and end-of-year corporate and team events and workshops
Short-term remote working arrangements
International travel opportunities (dependent on role)
You’ll also be able to take up the numerous other benefits that working at Littlepay affords you - including being part of a growing community of innovators at Stone and Chalk, where we enjoy lunch ‘n’ learns, summertime barbeques, ping pong tournaments, fortnightly drinks, catered breakfasts and morning teas, and more.
If this opportunity interests you, hit apply! We look forward to learning about you.
Littlepay are a 2023 Circle Back Initiative Employer – we commit to respond to every applicant. Please note - the Littlepay Talent Acquisition team is taking a break from December 23rd - January 7th, inclusive. We will respond to all applications in the 2 weeks following this period.
Show more
Show less","Scala, Python, Go, Spark, AWS technologies, Glue, Step Function, S3, CloudFormation, Lambda, Athena, QuickSight, IAM, CI/CD practices, ETL processes, Data integration, Data governance, Data modelling, Data structures, Microservices, NoSQL, API development, Software development, Cloud computing, Data analysis, Data visualization, Machine learning, Artificial intelligence, DevOps, Agile development, Scrum, Kanban, Jira, Notion, Slack, Miro, Google Workspace, Harrison Assessments","scala, python, go, spark, aws technologies, glue, step function, s3, cloudformation, lambda, athena, quicksight, iam, cicd practices, etl processes, data integration, data governance, data modelling, data structures, microservices, nosql, api development, software development, cloud computing, data analysis, data visualization, machine learning, artificial intelligence, devops, agile development, scrum, kanban, jira, notion, slack, miro, google workspace, harrison assessments","agile development, api development, artificial intelligence, athena, aws technologies, cicd practices, cloud computing, cloudformation, data governance, data integration, data modelling, data structures, dataanalytics, devops, etl, glue, go, google workspace, harrison assessments, iam, jira, kanban, lambda, machine learning, microservices, miro, nosql, notion, python, quicksight, s3, scala, scrum, slack, software development, spark, step function, visualization"
Business Data Analyst,University of Utah,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/business-data-analyst-at-university-of-utah-3683095691,2023-12-17,Bountiful,United States,Mid senior,Onsite,"Details
Open Date
04/18/2023
Requisition Number
PRN34621B
Job Title
Business Data Analyst
Working Title
Business Data Analyst
Job Grade
F
FLSA Code
Computer Employee
Patient Sensitive Job Code?
No
Standard Hours per Week
40
Full Time or Part Time?
Full Time
Shift
Day
Work Schedule Summary
Monday – Friday, 8:00 AM – 5:00 PM. This position may be eligible for remote work, and requires occasional travel.
VP Area
U of U Health - Academics
Department
00850 - Pediatric Critical Care
Location
Campus
City
Salt Lake City, UT
Type of Recruitment
External Posting
Pay Rate Range
$47,600 - $79,699
Close Date
Open Until Filled
Yes
Job Summary
Job Summary
The Emergency Medical Services ( EMS ) profession is changing and information is essential to good
decision making. Be a catalyst for an Information‐Driven Future! Join the National Emergency Medical Services Information Systems ( NEMSIS ) team and help to transform information into evidence supported action.
The NEMSIS Business Data Analyst position will play a key role in the development and promotion of the use of EMS data to facilitate enhanced prehospital emergency care services.
The NEMSIS program is responsible for the collection, cleaning, storing and sharing of nationwide EMS data. By joining the NEMSIS Technical Assistance Center, you assist local, state and national stakeholders in utilizing that information to improve emergency medical services.
This position focuses on effectively communicating and promoting the use of EMS data to expert and lay audiences. It will require strong communication skills and the ability to maintain a high level of professionalism with local, state and federal stakeholders.
The University of Utah offers a comprehensive benefits package including:
Excellent health care coverage at affordable rates
14.2% retirement contributions that vest immediately
Generous paid leave time
11 paid Holidays per year
50% tuition reduction for employee, spouse, and dependent children
Flex spending accounts
Free transit on most UTA services
Employee discounts on a variety of products and services including cell phones & plans,
entertainment, health and fitness, restaurants, retail, and travel.
Professional development opportunities. +
Responsibilities
Additional benefits information is available at www.hr.utah.edu/benefits
Disclaimer
This job description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to the job.
Essential Functions
Provide support to and collaborate with EMS data partners in local, state, and national levels.
Build professional relationships with stakeholders and advocate on their behalf.
Develop tools and reports that visualize EMS data to expert and lay audiences.
Educate stakeholders on available resources.
Assist in development and facilitation of trainings and presentations.
Facilitate project management and tracking deliverables.
Actively participate in process improvement, team building and professional development.
Comments
Familiar with standard concepts, practices, and procedures within a particular field. Relies on limited experience and judgment to plan and accomplish goals. Performs a variety of tasks. Works under general supervision. A certain degree of creativity and latitude is required. Typically reports to a supervisor or manager
Work Environment And Level Of Frequency Typically Required
Nearly Continuously: Office environment.
Physical Requirements and Level of Frequency that may be required
Nearly Continuously: Sitting, hearing, listening, talking.
Often: Repetitive hand motion (such as typing), walking.
Seldom: Bending, reaching overhead.
Minimum Qualifications
Requires a bachelor’s degree in related area or equivalency (one year of education can be substituted for two years of related work experience). Two to four years of experience in the field or in a related area. Familiar with standard concepts, practices, and procedures within a particular field.
Applicants must demonstrate the potential ability to perform the essential functions of the job as outlined in the position description.
Preferences
Preferences
Experience as an EMS clinician (such as EMT , paramedic or flight nurse) OR experience with EMS documentation software products.
Experience with data analytics and reporting.
Experience with research or evidence-based practice.
Excellent verbal and written communication skills.
Ability to collaborate effectively and work as part of a team.
Initiative, creative solutioning, and flexibility under pressure.
Business and technical writing skills.
Applicants will be screened according to preferences.
Type
Benefited Staff
Special Instructions Summary
Additional Information
The University of Utah values candidates who have experience working in settings with students from diverse backgrounds and possess a strong commitment to improving access to higher education for historically underrepresented students.
Individuals from historically underrepresented groups, such as minorities, women, qualified persons with disabilities and protected veterans are encouraged to apply. Veterans’ preference is extended to qualified applicants, upon request and consistent with University policy and Utah state law. Upon request, reasonable accommodations in the application process will be provided to individuals with disabilities.
The University of Utah is an Affirmative Action/Equal Opportunity employer and does not discriminate based upon race, ethnicity, color, religion, national origin, age, disability, sex, sexual orientation, gender, gender identity, gender expression, pregnancy, pregnancy-related conditions, genetic information, or protected veteran’s status. The University does not discriminate on the basis of sex in the education program or activity that it operates, as required by Title IX and 34 CFR part 106. The requirement not to discriminate in education programs or activities extends to admission and employment. Inquiries about the application of Title IX and its regulations may be referred to the Title IX Coordinator, to the Department of Education, Office for Civil Rights, or both.
To Request a Reasonable Accommodation For a Disability Or If You Or Someone You Know Has Experienced Discrimination Or Sexual Misconduct Including Sexual Harassment, You May Contact The Director/Title IX Coordinator In The Office Of Equal Opportunity And Affirmative Action
Director/ Title IX Coordinator
Office of Equal Opportunity and Affirmative Action ( OEO /AA)
383 University Street, Level 1 OEO Suite
Salt Lake City, UT 84112
801-581-8365
oeo@utah.edu
Online reports may be submitted at oeo.utah.edu
For more information: https://www.utah.edu/nondiscrimination/
To inquire about this posting, email:
employment@utah.edu (%20employment@utah.edu) or call 801-581-2300.
The University is a participating employer with Utah Retirement Systems (“URS”). Eligible new hires with prior URS service, may elect to enroll in URS if they make the election before they become eligible for retirement (usually the first day of work). Contact Human Resources at (801) 581-7447 for information. Individuals who previously retired and are receiving monthly retirement benefits from URS are subject to URS’ post-retirement rules and restrictions. Please contact Utah Retirement Systems at (801) 366-7770 or (800) 695-4877 or University Human Resource Management at (801) 581-7447 if you have questions regarding the post-retirement rules.
This position may require the successful completion of a criminal background check and/or drug screen.
https://safety.utah.edu/safetyreport This report includes statistics about criminal offenses, hate crimes, arrests and referrals for disciplinary action, and Violence Against Women Act offenses. They also provide information about safety and security-related services offered by the University of Utah. A paper copy can be obtained by request at the Department of Public Safety located at 1658 East 500 South.
Show more
Show less","Data analytics, Data reporting, Data visualization, Stakeholder engagement, Project management, Deliverable tracking, Process improvement, Team building, Professional development, Communication skills, Collaboration skills, Creative solutioning, Flexibility, Business writing, Technical writing","data analytics, data reporting, data visualization, stakeholder engagement, project management, deliverable tracking, process improvement, team building, professional development, communication skills, collaboration skills, creative solutioning, flexibility, business writing, technical writing","business writing, collaboration skills, communication skills, creative solutioning, data reporting, dataanalytics, deliverable tracking, flexibility, process improvement, professional development, project management, stakeholder engagement, team building, technical writing, visualization"
Staff Data Engineer,Recruiting from Scratch,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398035,2023-12-17,Bountiful,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL pipelines, Data management tools, Data classification, Data retention, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Streamsprocessing systems","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data management tools, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment, streamsprocessing systems","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamsprocessing systems, tdd"
Data Analyst-Technical-Senior,Intermountain Health,"Murray, UT",https://www.linkedin.com/jobs/view/data-analyst-technical-senior-at-intermountain-health-3786861380,2023-12-17,Bountiful,United States,Mid senior,Onsite,"Job Description:
The Senior level Technical Data Analyst position is focused on generating impactful solutions for customers within a defined clinical or business area. The Senior level analyst works within a team and serves as the lead technical support for a system business or clinical domain. Within their team, they mentor junior caregivers providing technical guidance and adherence to best practices. In partnership with the Product Owner, they facilitate regular meetings with customer stakeholders to provide work updates, set project priorities and establish deliverable timelines. They have a deep understanding of the clinical and operating data for the areas they support and consult with Healthcare Data Advisors to ensure the integrity and accuracy of customer data. They drive information insights through data exploration, interpretation, report development and visual storytelling. They are a key member of an Agile Delivery Team comprised of professionals working together to develop technical products focused on enabling Intermountain strategic initiatives and operational targets.
This position is part of a highly skilled, high impact team with deep clinical and technical expertise in leveraging healthcare data to solve real-world problems. Our teams report to senior analytics leaders and work in strong partnership with key business and clinical leaders across all areas of Intermountain Healthcare. This team provides ongoing training, maintenance, monitoring and promotion of data applications and reports to ensure data insights are widely known, accessible, and used to its fullest potential. This position helps develop and deploy world class data analytics at Intermountain Healthcare, one of the nation’s leading health systems. Using the Scaled Agile Framework®, Data Analysts will partner with business and clinical leaders, and data professionals across the organization, supporting our Mission to help people live the healthiest lives possible.
In Brief, the Senior level Technical Data Analyst will demonstrate a deep understanding of the data and operations of the business/clinical areas you support. Partner with Analytic leadership to ensure customer priorities are understood and resourced. Provide a high level of expertise and customer service in order to delight business and clinical customers through innovative and impactful analytic deliverables. Mentor Staff and Associate team members. Detect and facilitate opportunities for intra/inter team collaboration to enhance product work. Contribute technical acumen and thought leadership to the success of your Agile Delivery Team and advance your professional skills though structured and personal learning opportunities
Posting Specifics
Entry Rate: Depending on experience
Benefits Eligible: Yes. Click here to check out our benefits
Shift Details: Monday – Friday Days 8:00 to 5:00
Department: Research Analytics area within Enterprise Analytics
Other: This is a remote position with available shared working space on site if desired.
Minimum Qualifications Data Analyst Technical Staff - Starting Pay rate $38.79
Must meet one of the following:
Bachelor's degree in an analytics related field such as statistics, mathematics, information systems, computer science, finance, business management, or economics with two years of relevant work experience in a role performing data analysis.
Or
Bachelor’s degree in another area of study with four years of relevant work experience in a role performing data analysis.
Experience without a related degree will be considered if skill set and experience are robust.
Minimum Qualifications Data Analyst Statistical Staff - Starting Pay rate $38.79
Must meet one of the following:
Bachelor's degree in an analytics related field such as statistics, mathematics, information systems, computer science, finance, business management, or economics and two years of relevant experience performing data analysis.
Or
Bachelor’s degree in another area of study with four years of relevant work experience in a role performing data analysis.
Experience without a related degree will be considered if skill set and experience are robust.
Minimum Qualifications Data Analyst Technical Senior - Starting pay rate $43.06
Must meet one of the following:
Bachelor's degree in an analytics related field such as statistics, mathematics, information systems, computer science, finance, business management, or economics with four years of relevant work experience in a role performing data analysis.
Or
Bachelor’s degree in another area of study with six years of relevant work experience in a role performing data analysis.
Experience without a related degree will be considered if skill set and experience are robust.
Plus:
Demonstration of these skills:
Advanced SQL ability, knowledge of database design, data modeling and standardized data structures
Significant Experience in creating visually appealing, content rich and consumer centric reports and dashboards (i.e. Cognos, Tableau)
Proficiency in using word processing, spreadsheet, internet, e-mail, and scheduling applications
Highly effective verbal, written and interpersonal communication skills
Subject matter expertise in healthcare or related data (Clinical Programs, Clinical Services, Compliance, Quality, Revenue Cycle, Strategic Planning, Population Health, etc.).
Lifelong learner with a creative, collaborative approach.
Temperament to thrive in scaled agile teams (SAFe®) or similar product management practice
Preferred Qualifications
Bachelor's degree in an analytics related field such as statistics, mathematics, information systems, computer science, finance, business management, or economics.
Physical Requirements:
Interact with others requiring the employee to verbally communicate information.
- and -
Operate computers and other IT equipment requiring the ability to move fingers and hands.
- and -
See and read computer monitors and documents
- and -
Remain sitting or standing for long periods of time to perform work on a computer, telephone, or other equipment.
- and -
Remain sitting or standing for long periods of time to perform work on a computer, telephone, or other equipment.
- and -
Anticipated job posting close date:
12/26/2023
Location:
Intermountain Medical Center
Work City:
Murray
Work State:
Utah
Scheduled Weekly Hours:
40
The hourly range for this position is listed below. Actual hourly rate dependent upon experience.
$43.06 - $67.80
We care about your well-being – mind, body, and spirit – which is why we provide our caregivers a generous benefits package that covers a wide range of programs to foster a sustainable culture of wellness that encompasses living healthy, happy, secure, connected, and engaged.
Learn more about our comprehensive benefits packages for our Idaho, Nevada, and Utah based caregivers, and for our Colorado, Montana, and Kansas based caregivers; and our commitment to diversity, equity, and inclusion.
Intermountain Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Show more
Show less","Scaled Agile Framework, Data analytics, Statistics, Mathematics, Information systems, Computer science, Finance, Business management, Economics, Data analysis, Advanced SQL, Database design, Data modeling, Data structures, Data visualization, Tableau, Cognos, Word processing, Spreadsheet applications, Internet, Email, Scheduling applications, Verbal communication, Written communication, Interpersonal communication, Subject matter expertise in healthcare, Clinical Services, Compliance, Quality, Revenue Cycle, Strategic Planning, Population Health","scaled agile framework, data analytics, statistics, mathematics, information systems, computer science, finance, business management, economics, data analysis, advanced sql, database design, data modeling, data structures, data visualization, tableau, cognos, word processing, spreadsheet applications, internet, email, scheduling applications, verbal communication, written communication, interpersonal communication, subject matter expertise in healthcare, clinical services, compliance, quality, revenue cycle, strategic planning, population health","advanced sql, business management, clinical services, cognos, compliance, computer science, data structures, dataanalytics, database design, datamodeling, economics, email, finance, information systems, internet, interpersonal communication, mathematics, population health, quality, revenue cycle, scaled agile framework, scheduling applications, spreadsheet applications, statistics, strategic planning, subject matter expertise in healthcare, tableau, verbal communication, visualization, word processing, written communication"
"Healthcare Data Analyst II or Senior, DOE",Cambia Health Solutions,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/healthcare-data-analyst-ii-or-senior-doe-at-cambia-health-solutions-3784208434,2023-12-17,Bountiful,United States,Mid senior,Remote,"Remote opportunity for residents of OR, WA, ID and UT
Primary Job Purpose
The Healthcare Data Analyst II or Senior, DOE drives superior data quality through a robust process of collecting, validating, analyzing, and reporting of key information. Propels effective and efficient business operations by instituting, enhancing, and overseeing process improvement efforts. This role is critical to ensuring providers receive accurate and timely insights to effectively manage our members. This position will support all analytics and reporting for our Episodes of Care Alternative Payment Model. This is an exciting time to join this program to help build this APM from the ground up. You will be working with a highly collaborative team made up of super talented analysts.
General Functions And Outcomes
Provides data analytics, primarily for Provider Partnership Innovations (PPI) department.
Pulls, validates, and analyzes data from all types of sources to identify and assess risks and prevent, detect, investigate, and correct data inconsistencies and anomalies.
Develops working partnerships with data and system owners across Cambia and relevant vendors to secure necessary access, training, and context for obtaining and analyzing data
Supports departmental and cross-departmental reporting, including performance reporting
Develops and maintains reporting commitments
Creates and manages supporting materials
Analyzes and identifies healthcare industry trends and shifts in consumers’ preferences, attitudes and perceptions.
Researches best business practices within and outside the organization to establish benchmark data
Designs, populates, maintains, and creates departmental and cross-departmental reporting, including performance reporting. Assists in analyzing monitoring results and compiling statistics on a weekly, monthly or quarterly basis.
Applies data and analysis to drive process optimization, controls analysis and improvement, and risk reduction, directly and through leading cross-functional teams.
Participates in remediating risk areas or identified issues and documents processes and controls; monitors and regularly reports findings.
Ensures that activities are executing as planned, including vendor activities, and provides a feedback loop and remediation plan if problems are identified.
Documents and reviews business requirements, internal controls and/or management controls used to generate work output. Analyzes controls for strengths and weaknesses.
At least 2 years experience with data visualization tools such as Tableau, PowerBI, etc.
At least 2 years experience with Alteryx
Minimum Requirements
Demonstrated ability in creating innovative and persuasive presentations.
Ability to communicate effectively, both orally and in writing, regarding complex or sensitive information or issues.
Demonstrated experience in report preparation, project documentation, or policy and procedure writing, including the ability to organize, review and interpret statistical data.
Demonstrated analytical ability to identify problems, develop solutions, and implement chosen course of action.
Strong computer skills including use of Microsoft Office products or equivalent software for creating and maintaining databases, spreadsheets and creating reports and documents.
Action oriented with the ability to prioritize workload and focus on highest priorities.
Demonstrated ability to organize, plan, prioritize and coordinate multiple projects within specified timelines with minimal supervision.
A proven team player with the ability to partner, maintain and develop relationships.
Ability to pull, manage, organize and analyze data
Ability to understand the data and when it may be incorrect, contradictory and/or incomplete
Knowledge of health insurance claims processing with the ability to pull, validate, and analyze health insurance claims data. Knowledge of health insurance data systems.
May have experience in one of the following: data mining tools (TOAD, SQL, BOE, etc.) or analytic tools, statistical analysis or methods
Demonstrated ability to learn quickly when faced with new situation.
Demonstrated competency in pulling, managing, organizing, and analyzing data.
Demonstrated competency in reconciling data that may be incorrect, contradictory, and/or incomplete into understandable framework and actionable recommendations.
Fluent with health insurance claims processing and proven ability to pull, validate, and analyze health insurance claims data. Proficient in health insurance data systems.
Experienced in data mining tools (TOAD, SQL, BOE, etc.), analytic tools, statistical analysis and methods (e.g., SQL, SAS, OLAP, SPSS or the advanced statistical capabilities of Excel).
Demonstrated competency in process optimization, including end-to-end process mapping, control point identification and improvement, stakeholder interviewing, and end-to-end control assessment.
Proven decision making and problem solving skills.
Proven influence, interpersonal, communication, and relationship-building skills. Proven ability to work as part of a team, across teams, and with key business partners at all levels of the organization.
Normally to be proficient in the competencies listed above
Healthcare Data Analyst II
would have a Bachelor’s Degree and 5 years progressive experience in data identification, collection, validation and analytics, including report design or equivalent combination of education and related work experience.
Healthcare Data Analyst Senior
would have a Bachelor’s Degree and 8 years progressive experience in data identification, collection, validation and analytics, including report design or equivalent combination of education and related work experience.
The expected hiring range for a
Healthcare Data Analyst II
is $80,500 - $109,500 for a
Healthcare Data Analyst Senior
is $97,000 - $132,500 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for the
Healthcare Data Analyst II
position is 10%.  The bonus target for the
Healthcare Data Analyst Senior
position is 15%. The current full salary range for the
Healthcare Data Analyst II
role is $76,000 - $123,500 and the
Healthcare Data Analyst Senior
, is $91,500 - 149,500.
Benefits
Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
medical, dental, and vision coverage for employees and their eligible family members
annual employer contribution to a health savings account ($1,200 or $2,500 depending on medical coverage, prorated based on hire date)
paid time off varying by role and tenure in addition to 10 company holidays
up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
one-time furniture and equipment allowance for employees working from home
up to $225 in Amazon gift cards for participating in various well-being activities. for a complete list see our External Total Rewards page.
We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
If you need accommodation for any part of the application process because of a medical condition or disability, please email CambiaCareers@cambiahealth.com. Information about how Cambia Health Solutions collects, uses, and discloses information is available in our Privacy Policy. As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our Careers site.
Show more
Show less","Data Analytics, Data Mining, Data Validation, Data Visualization, Health Insurance Data, Alteryx, PowerBI, Tableau, SQL, TOAD, BOE, Excel, Statistical Analysis, Process Optimization, SAS, OLAP, SPSS","data analytics, data mining, data validation, data visualization, health insurance data, alteryx, powerbi, tableau, sql, toad, boe, excel, statistical analysis, process optimization, sas, olap, spss","alteryx, boe, data mining, data validation, dataanalytics, excel, health insurance data, olap, powerbi, process optimization, sas, spss, sql, statistical analysis, tableau, toad, visualization"
"Data Conversion Developer, Senior Associate",PwC,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749933941,2023-12-17,Bountiful,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Data analysis techniques, Maximo's modules and functionalities, IBM Maximo, Relational databases, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), Data extraction transformation and loading processes, SQL, Database querying languages, ETL tools, Azure ADF, AWS Glue, SSIS, DataBricks, Data cleansing techniques, Maximo Business Object (MBO) definitions, Data conversion requirements, Automation Scripts, Java Customizations, Database Configuration, Application Designer, Integration patterns, Data synchronization, Data exchange protocols, Python, PySpark, Scala","data analysis techniques, maximos modules and functionalities, ibm maximo, relational databases, ibm db2, oracle, microsoft sql server, maximos integration framework mif, data extraction transformation and loading processes, sql, database querying languages, etl tools, azure adf, aws glue, ssis, databricks, data cleansing techniques, maximo business object mbo definitions, data conversion requirements, automation scripts, java customizations, database configuration, application designer, integration patterns, data synchronization, data exchange protocols, python, pyspark, scala","application designer, automation scripts, aws glue, azure adf, data analysis techniques, data cleansing techniques, data conversion requirements, data exchange protocols, data extraction transformation and loading processes, data synchronization, database configuration, database querying languages, databricks, etl tools, ibm db2, ibm maximo, integration patterns, java customizations, maximo business object mbo definitions, maximos integration framework mif, maximos modules and functionalities, microsoft sql server, oracle, python, relational databases, scala, spark, sql, ssis"
Senior Data Analyst,Cornerstone Building Brands,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-data-analyst-at-cornerstone-building-brands-3774575971,2023-12-17,Bountiful,United States,Mid senior,Hybrid,"The Sr. Data Analyst is responsible for managing and supporting Quality and Manufacturing initiatives throughout all Cornerstone Building Brands business segments. The Sr. Data Analyst is expected to work with and learn from Business Leaders how to manage data and facilitate reporting that will support events focused on process improvement. The Sr. Data Analysist is responsible for data collection and analytics supporting the Quality and Manufacturing organization. All activities performed by the Quality Analyst are in support of fostering a cultural transformation for Cornerstone Building Brands to be a leader in exemplifying world-class Quality and continuous improvement behaviors.
ESSENTIAL DUTIES AND RESPONSIBILITIES
Develop analytical models, automating reporting and providing simple yet effective visualizations for stakeholders, using Power BI, SharePoint, Excel, SQL and other software programs
Use effective communication skills, working closely with internal customers to understand the business need and determine what data can best be used to drive quality business decisions
This role will require a range of capabilities including but not limited to exploratory data analysis, data trending, statistical analysis, regression, and other mathematical techniques
Ability to work in teams to understand, define and implement solutions to problems
Define and track KPI (key performance indicators) for all quality essentials
Prepare and communicate reports on quality activities and efficiencies
Identify, lead and drive CTO (Cost Take Out) or CoPQ (Cost of Poor Quality) initiatives across business units
Qualifications
5 year of experience with analytics systems (particularly PowerBI, with recognition of experience with Tableau and other relevant experience)
Bachelor’s degree in engineering or data Analytics strongly preferred
Strong​ understanding of processes of data collection, management, cleaning, organization, reporting, visualization, and analysis
Strong computer and analytic skills including Power BI, SharePoint, Excel, PowerPoint, and Project management software. With the ability to learn new skills and programs quickly to adapt to technological advancements
Ability to work effectively to meet deadlines and exhibit strong judgment and decision-making skills
Self-motivated with a high degree of ownership and accountability for results
Highly flexible and able to react constructively and with a positive attitude to multiple demands, shifting priorities, changed and unexpected events
Knowledge of predictive analytics a plus
Knowledge of RPA/automation, PowerApps, Power Automate a plus
Ability to travel up to 25% of the time
Show more
Show less","Power BI, SharePoint, Excel, SQL, Tableau, Data Analytics, Statistical analysis, Regression, KPI, Cost Take Out, Cost of Poor Quality, Data collection, Data management, Data cleaning, Data organization, Data reporting, Data visualization, Data analysis, Predictive analytics, RPA/automation, PowerApps, Power Automate","power bi, sharepoint, excel, sql, tableau, data analytics, statistical analysis, regression, kpi, cost take out, cost of poor quality, data collection, data management, data cleaning, data organization, data reporting, data visualization, data analysis, predictive analytics, rpaautomation, powerapps, power automate","cost of poor quality, cost take out, data cleaning, data collection, data management, data organization, data reporting, dataanalytics, excel, kpi, power automate, powerapps, powerbi, predictive analytics, regression, rpaautomation, sharepoint, sql, statistical analysis, tableau, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711063,2023-12-17,Bountiful,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning (ML), Python, Java, Bash, SQL, Git, Pandas, R, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Retention, 401K, GenderAffirming Offerings, Included Health, HRT","data engineering, machine learning ml, python, java, bash, sql, git, pandas, r, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, retention, 401k, genderaffirming offerings, included health, hrt","401k, airflow, applied machine learning, aws, azure, bash, data classification, data engineering, docker, dynamodb, etl, gcp, genderaffirming offerings, git, helm, hrt, included health, java, kafka, kubernetes, machine learning ml, pandas, python, r, retention, snowflake, spark, sparkstreaming, sql, storm"
Business Data Analyst - Salesforce,UL Solutions,"Vancouver, WA",https://www.linkedin.com/jobs/view/business-data-analyst-salesforce-at-ul-solutions-3768720025,2023-12-17,Beaverton,United States,Associate,Hybrid,"This role is HYBRID, 3 Days a week at our UL Vancouver Office.
Assists with the definition and development of user-friendly reports and graphics that integrate and convey complex data/information and maintains data catalog to provide key data sources to internal and external stakeholders.
Conducts in-depth data analyses using traditional and advanced statistical methods.
Works within established guidelines and policy.
Aggregates data to test hypotheses, prepares and analyzes data to meet the needs of efficient data analysis, using technical experience and judgement.
Assists with developing programming solutions to conduct data preparation, analysis and visualization in support of statistical modeling, data analytics and interpretation.
Transforms complex data into creative graphs, tables, charts and other graphics in order to present and/or share the data in a comprehensive and concise manner.
Assesses trends and provides analysis to identify emerging business critical issues. Identifies and defines opportunities for safety solutions by analyzing historical patterns.
Assists in defining, developing and deploying data, analytics and reporting products and services.
Conducts data validation, statistical analyses, and data mapping.
Read and follow the Underwriters Laboratories Code of Conduct and follow all physical and digital security practices.
Qualifications for Candidates
University Degree (Equivalent to Bachelors degree) in Mathematics, Statistics, Finance or a related discipline and generally three plus years of experience in data and statistical analysis strongly preferred.
Strong in Microsoft Excel and Power BI, data management and data integration skills with an ability to organize, analyze and correlate data information.
Strong interpersonal skills with the ability to communicate with technical and non-technical stakeholders.
Previous experience doing large scale data analysis preferred.
Ability to organize, analyze and correlate data information.
Mission:
For UL, corporate and social responsibility isn’t new. Making the world a safer, more secure and sustainable place has been our business model for the last 129 years and is deeply engrained in everything we do.
People:
Ask any UL employee what they love most about working here, and you’ll almost always hear, “the people.” Going beyond what is possible is the standard at UL. We’re able to deliver the best because we employ the best.
Interesting work:
Every day is different for us here as we eagerly anticipate the next innovation that our customers’ create. We’re inspired to take on the challenge that will transform how people live, work and play. And as a global company, in many roles, you will get international experience working with colleagues around the world.
Grow & achieve:
We learn, work and grow together with targeted development, reward and recognition programs as well as our very own UL University that offers extensive training programs for employees at all stages, including a technical training track for applicable roles.
Total Rewards:
The salary range for this position is $50,000-$70,000 of the salary range and is based upon years of experience that is commensurate with the level of the position.
All employees at UL Solutions are eligible for annual bonus compensation. The target for this position is 10% of the base salary offered. Employees are eligible for health benefits such as medical, dental and vision; wellness benefits such as mental & financial health; and retirement savings (401K) commensurate with the standard rewards offered in each individual location or country, for the relevant position level.
We also provide employees with paid time off including vacation (15 days), holiday including floating holidays (12 days) and sick time off (72 hours).
Learn More:
Working at UL is an exciting journey that twists and turns daily. We thrive in the twists and revel in the turns. This is our every day. This is our normal.
Curious? To learn more about us and the work we do, visit UL.com.
Show more
Show less","Microsoft Excel, Power BI, Data management, Data integration, Data analysis, Data visualization, Statistical modeling, Data analytics, Data interpretation, Data preparation, Data catalog, Data validation, Data mapping, Statistical analyses, Hypothesis testing, Data security, Data governance, Business intelligence, Reporting, Data mining, Data warehousing, Data science, Machine learning, Artificial intelligence","microsoft excel, power bi, data management, data integration, data analysis, data visualization, statistical modeling, data analytics, data interpretation, data preparation, data catalog, data validation, data mapping, statistical analyses, hypothesis testing, data security, data governance, business intelligence, reporting, data mining, data warehousing, data science, machine learning, artificial intelligence","artificial intelligence, business intelligence, data catalog, data governance, data integration, data interpretation, data management, data mapping, data mining, data preparation, data science, data security, data validation, dataanalytics, datawarehouse, hypothesis testing, machine learning, microsoft excel, powerbi, reporting, statistical analyses, statistical modeling, visualization"
Senior Data Engineer,ZoomInfo,"Vancouver, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zoominfo-3770686821,2023-12-17,Beaverton,United States,Mid senior,Remote,"At ZoomInfo, we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. We value your take charge, take initiative, get stuff done attitude and will help you unlock your growth potential. One great choice can change everything. Thrive with us at ZoomInfo.
At
ZoomInfo
we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. If you are a take charge, take initiative, get stuff done individual we want to talk to you! We have high aspirations for the company and are looking for the right people to help fulfill the dream. We strive to continually improve every aspect of the company and use cutting edge technologies and processes to delight our customers and rapidly increase revenues.
As a Senior Data Engineer, you'll have a key role in building and designing the strategy of our finance analytics engineering team under the Enterprise Data Engineering group.
Our Technological Stack includes: Airflow, DBT, Python, Snowflake, AWS, GCP, Amplitude, Fivetran, and more.
What will you actually be doing?
Building, and continuously improving our data gathering, modeling, reporting capabilities and self-service data platforms.
Working closely with Data Engineers, Data Analysts, Data Scientists, Product Owners, and Domain Experts to identify data needs.
Required Experience:
Relevant Bachelor degree – preferably CS, Engineering/ Information Systems or other equivalent Software Engineering background.
8+ years of experience as a Data/BI engineer.
Strong SQL abilities and hands-on experience with SQL and no-SQL DBs, performing analysis and performance optimizations.
Hands-on experience in Python or equivalent programming language
Experience with data warehouse solutions (like BigQuery/ Redshift/ Snowflake)
Experience with data modeling, data catalog concepts, data formats, data pipelines/ETL design, implementation and maintenance.
Experience with AWS/GCP cloud services such as GCS/S3, Lambda/Cloud Function, EMR/Dataproc, Glue/Dataflow, Athena.
Experience with Airflow and DBT - Advantage.
Experience with data visualization tools and infrastructures (like Tableau/SiSense/Looker/other) - Advantage.
Experience with development practices – Agile, CI/CD, TDD - Advantage.
Experience with Infrastructure as Code practices - Terraform - Advantage
About Us:
For over a decade, ZoomInfo has helped companies achieve their most important objective: profitable growth. Backed by the world's most comprehensive B2B database, our platform puts sales and marketing professionals in position to identify, connect, and engage with qualified prospects.
Our mission is to provide every company with a 360-degree view of their ideal customer, empowering each phase of their go-to-market strategy and driving their ability to hit their number.
The US base salary range for this position is $133,600.00 to $170,000.00 variable compensation + benefits.
Actual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process.
We want our employees and their families to thrive. In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.
About Us:
ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.
ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.
ZoomInfo is proud to be an Equal Opportunity employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.
Show more
Show less","Airflow, DBT, Python, Snowflake, AWS, GCP, Amplitude, Fivetran, BI, SQL, Data Warehouse, ETL, Data Modeling, Data Catalog, Data Formats, Data Pipelines, Agile, CI/CD, TDD, Terraform, Tableau, Sisense, Looker, BigQuery, Redshift","airflow, dbt, python, snowflake, aws, gcp, amplitude, fivetran, bi, sql, data warehouse, etl, data modeling, data catalog, data formats, data pipelines, agile, cicd, tdd, terraform, tableau, sisense, looker, bigquery, redshift","agile, airflow, amplitude, aws, bi, bigquery, cicd, data catalog, data formats, datamodeling, datapipeline, datawarehouse, dbt, etl, fivetran, gcp, looker, python, redshift, sisense, snowflake, sql, tableau, tdd, terraform"
Data Engineer,Jobs for Humanity,"Cambridge, MA",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3785369888,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Location: McLean, Virginia, United States of America Job Title: Data Engineer At Capital One, we believe in creating technology solutions that address important problems and meet the needs of our customers. We are a diverse and inclusive group of innovators who are passionate about using data and emerging technologies to drive transformation. We are currently looking for a Senior Associate, Data Engineer to join our Finance Tech team. What You'll Do: - Proactively seek opportunities to address customer needs and collaborate with stakeholders to build the best solutions for complex business problems. - Support the design and development of scalable data architectures and systems for extracting, storing, and processing large amounts of data. - Build and optimize data pipelines to efficiently ingest, transform, and load data from various sources while ensuring data quality and integrity. - Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling. - Implement testing, validation, and pipeline observability to ensure data pipelines are meeting customer service level agreements (SLAs). Basic Qualifications: - Bachelor's Degree - At least 2 years of experience in application development (Internship experience does not apply) - At least 1 year of experience in big data technologies Preferred Qualifications: - 3+ years of experience developing data pipelines using Python or Scala - 2+ years of experience with distributed computing tools (Spark, EMR, Hadoop) - 2+ years of experience with UNIX/Linux including basic commands and shell scripting - 1+ years of experience with public cloud platforms (AWS, Microsoft Azure, Google Cloud) - 1+ years of data warehousing experience (Redshift or Snowflake) - 1+ years of experience with Agile engineering practices Capital One offers a comprehensive and competitive set of health, financial, and other benefits to support your total well-being. More information on benefits can be found on the Capital One Careers website. Eligibility varies based on employment status. Please note that for this position, Capital One will not sponsor a new applicant for employment authorization. At Capital One, we are committed to diversity and inclusion in the workplace. We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state, or local law. We promote a drug-free workplace and will consider applicants with a criminal history in accordance with applicable laws and regulations. If you require an accommodation during the application process, please contact Capital One Recruiting. All information provided will be kept confidential and used only to provide reasonable accommodations. For technical support or questions about our recruiting process, please email Careers@capitalone.com. Note: Capital One Financial is comprised of different entities. Positions posted in Canada are for Capital One Canada, positions posted in the United Kingdom are for Capital One Europe, and positions posted in the Philippines are for Capital One Philippines Service Corp. (COPSSC). Thank you for considering a career at Capital One!
Show more
Show less","Python, Scala, Spark, EMR, Hadoop, UNIX/Linux, AWS, Microsoft Azure, Google Cloud, Redshift, Snowflake, Agile, Data pipelines, Data warehousing, Data engineering, Data analysis, Data exploration, Data modeling, Machine learning, Distributed computing, Big data, Cloud computing, Data integrity, Data quality, Data architecture, Data visualization","python, scala, spark, emr, hadoop, unixlinux, aws, microsoft azure, google cloud, redshift, snowflake, agile, data pipelines, data warehousing, data engineering, data analysis, data exploration, data modeling, machine learning, distributed computing, big data, cloud computing, data integrity, data quality, data architecture, data visualization","agile, aws, big data, cloud computing, data architecture, data engineering, data exploration, data integrity, data quality, dataanalytics, datamodeling, datapipeline, datawarehouse, distributed computing, emr, google cloud, hadoop, machine learning, microsoft azure, python, redshift, scala, snowflake, spark, unixlinux, visualization"
Senior Data Engineer (Remote),MMS,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782264334,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data engineering, Data science, Data curation, Data modeling, Data warehousing, TSQL, Azure data factory, Microsoft SQL databases, Software development, Data privacy, Anonymization, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP, CDISC, FHIR, OMOP, Clinical trials, Pharmaceutical development","data engineering, data science, data curation, data modeling, data warehousing, tsql, azure data factory, microsoft sql databases, software development, data privacy, anonymization, iso 9001, iso 27001, 21 cfr part 11, fda, gcp, cdisc, fhir, omop, clinical trials, pharmaceutical development","21 cfr part 11, anonymization, azure data factory, cdisc, clinical trials, data curation, data engineering, data privacy, data science, datamodeling, datawarehouse, fda, fhir, gcp, iso 27001, iso 9001, microsoft sql databases, omop, pharmaceutical development, software development, tsql"
Data Engineer,Sony Music Publishing,"Nashville, TN",https://www.linkedin.com/jobs/view/data-engineer-at-sony-music-publishing-3770193671,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Sony
Music Publishing
is the leading global music publisher, which is home to world-class songwriters, legendary catalogues, and industry-leading synchronization licensing and production music businesses. With an international network of 38 offices, Sony Music Publishing represents many of the most iconic songs ever written by celebrated songwriters such as The Beatles, Bob Dylan, Aretha Franklin, Marvin Gaye, Michael Jackson, Carole King, Queen and The Rolling Stones, as well as contemporary superstars including Beyoncé, Ed Sheeran, Pharrell Williams, Lady Gaga, P!nk, and Sam Smith. To learn more: www.sonymusicpub.com/en.
What You’ll Do:
A Data Engineer within our Enterprise Systems will work with Lead Data Engineer. Product Owners and Solution/Enterprise Architect Teams on new solutions that seamlessly integrate both the back end of a website and/or application. The applicant must have an in-depth knowledge of design of modern programming languages & web technologies, backend infrastructure & databases, and AWS cloud services to be effective.
Who You Are:
Basic Qualifications
3+ years of AWS experience.
AWS Serverless, Cloud Security, DevOps, Containers.
Knowledgeable in SQL/NOSQL; Aurora, Redshift, Dynamo DB.
Knowledge in modern data engineering languages: Python, Event Bridge, Step Functions, Lambda, CloudWatch, CloudFront, Pyspark, R and Glue.
Ability to build API integration with Angular/React Material for front end development.
Collaborating with team members, Product Managers, subject matter experts, and other teams to refine requirements and translate into functional software using standardized coding techniques and conventions.
Knowledgeable about all development life cycle phases and solution delivery for cloud systems with experience in unit, integration testing and strong documentation.
Ability to work hybrid work schedule (remote & onsite from our Nashville office)
Must be authorized to work in the United States.
7.5-hour business workday but variations in work volume frequently require extended working hours for evening and late-night events.
Nice to Haves
Experience with CRM/Finance/Accounting systems a plus.
Knowledge of the software development lifecycle and concepts such as Agile, SAFE, scrum, CI/CD, and DevOps.
Knowledge of coding best practices such as CI/CD, Cloud Security, DevOps.
Knowledge on dashboards and Dashboard platforms such as AWS QuickSight, Google Looker, Tableau, PowerBI.
Knowledge of big data technologies like AWS Redshift, Hive and Spark.
Experience with AWS Database Migration Service (DMS)
AWS Certification.
What We Give You:
Relocation Assistance Available.
You join an inclusive, collaborative and global community where you have the opportunity to fuel the creative journey.
A modern office environment designed to foster productivity, creativity, and teamwork.
An attractive and comprehensive benefits package including medical, dental, vision, life & disability coverage, and 401K + employer matching.
Voluntary benefits like company-paid identity theft protection and resources for pets, mental health and meditation resources, industry-leading fertility coverage, fully paid leave for childbirth or bonding, fully paid leave for caregivers, programs for loved ones with developmental disabilities and neurodiversity, subsidized back-up child and elder care, and reimbursement for adoption, surrogacy, tuition and student loans.
We invest in your professional growth & development  Flexible Time Off.
Time off for a winter recess.
Sony is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy), gender, national origin, citizenship, ancestry, age, physical or mental disability, military status, status as a veteran or disabled veteran, sexual orientation, gender identity or expression, marital or family status, genetic information, medical condition, or any other basis protected by applicable federal, state, or local law, ordinance, or regulation.
EEO is the Law
EEO is the Law Supplement
Right to Work (English/Spanish)
E-Verify Participation (English/Spanish)
Show more
Show less","AWS, Serverless, Cloud Security, DevOps, Containers, SQL, NoSQL, Aurora, Redshift, DynamoDB, Python, Event Bridge, Step Functions, Lambda, CloudWatch, CloudFront, Pyspark, R, Glue, API, Angular, React Material, Agile, SAFE, Scrum, CI/CD, AWS QuickSight, Google Looker, Tableau, PowerBI, AWS Redshift, Hive, Spark, AWS Database Migration Service (DMS), AWS Certification","aws, serverless, cloud security, devops, containers, sql, nosql, aurora, redshift, dynamodb, python, event bridge, step functions, lambda, cloudwatch, cloudfront, pyspark, r, glue, api, angular, react material, agile, safe, scrum, cicd, aws quicksight, google looker, tableau, powerbi, aws redshift, hive, spark, aws database migration service dms, aws certification","agile, angular, api, aurora, aws, aws certification, aws database migration service dms, aws quicksight, aws redshift, cicd, cloud security, cloudfront, cloudwatch, containers, devops, dynamodb, event bridge, glue, google looker, hive, lambda, nosql, powerbi, python, r, react material, redshift, safe, scrum, serverless, spark, sql, step functions, tableau"
Data Engineer - Snowflake expert,Experfy,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-snowflake-expert-at-experfy-3686208632,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Opportunity Description
We are looking for a Data Engineer to join our digital data team in the data
architecture operation and governance team to build and operationalize data
pipelines necessary for the enterprise data and analytics and insights initiatives,
following industry standard practices and tools. The bulk of the work would be in
building, managing, and optimizing data pipelines and then moving them effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise. In addition, guarantee compliance with data governance and data security requirements while creating, improving, and
operationalizing these integrated and reusable data pipelines.
The data engineer will be the key interface in operationalizing data and analytics on behalf of the business unit(s) and organizational outcomes.
Tech Skills
Knowledge of AWS
Knowledge of Azure or GCP is a plus
Orchestration: Airflow
Project management & support: JIRA projects & service desk, Confluence, Teams
Expert in ELT and ETL
Expert in Relational database technologies and concepts:
Snowflake is a must have
Perform SQL queries
Create database models
Maintain and improve queries performance
Working knowledge of Python and familiar with other scripting languages
Good knowledge of cloud computing
Soft Skills
Pragmatic and capable of solving complex issues
Ability to understand business needs
Good communication
Push innovative solutions
Service-oriented, flexible & team player
Self-motivated, take initiative
Attention to detail & technical intuition
Experience
At least 5 years experiences in a data team as Data Engineer
Experience in a healthcare industry is a strong plus
Snowflake certified
Preferred Qualifications
BS or MS in Computer Science
Requirements
Responsibilities
Must work with business team to understand requirements, and translate them into technical needs
Gather and organize large and complex data assets, perform relevant analysis
Ensure the quality of the data in coordination with Data Analysts and Data Scientists (peer validation)
Propose and implement relevant data models for each business cases
Optimize data models and workflows
Communicate results and findings in a structured way
Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan
Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements
Leverage existing or create new ""standard pipelines"" within to bring value through business use cases
Ensure best practices in data manipulation are enforced end-to-end
Actively contribute to Data governance community
Show more
Show less","AWS, Azure, GCP, Airflow, JIRA, Confluence, Teams, ELT, ETL, Snowflake, SQL, Python, Cloud computing, Pragmatic, Problemsolving, Business needs, Communication, Innovation, Serviceoriented, Teamwork, Selfmotivation, Initiative, Attention to detail, Technical intuition, Data engineering, Healthcare industry, Data Analyst, Data Scientist, Data model, Data manipulation, Data governance, Business case, Pipeline implementation, Data quality","aws, azure, gcp, airflow, jira, confluence, teams, elt, etl, snowflake, sql, python, cloud computing, pragmatic, problemsolving, business needs, communication, innovation, serviceoriented, teamwork, selfmotivation, initiative, attention to detail, technical intuition, data engineering, healthcare industry, data analyst, data scientist, data model, data manipulation, data governance, business case, pipeline implementation, data quality","airflow, attention to detail, aws, azure, business case, business needs, cloud computing, communication, confluence, data engineering, data governance, data manipulation, data model, data quality, data scientist, dataanalytics, elt, etl, gcp, healthcare industry, initiative, innovation, jira, pipeline implementation, pragmatic, problemsolving, python, selfmotivation, serviceoriented, snowflake, sql, teams, teamwork, technical intuition"
Azure Dev Ops Data Engineer,IVY TECH SOLUTIONS INC,"Raleigh, NC",https://www.linkedin.com/jobs/view/azure-dev-ops-data-engineer-at-ivy-tech-solutions-inc-3787774797,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"­­
HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Azure DevOps/Data Engineer
Location: Raleigh, NC
Duration:12+Months
Initially Remote
Please send the resume to
or 847- 350-1008
Skills:
3+ years of experience of Azure Data Platform implementation
3+ years of experience in design, development, testing and deployment of microservices
3+ years of hands-on experience in implementation and performance tuning of Azure Data Factory, Synapse, Kafka, Data Bricks or similar implementations
Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
Experience developing software code in one or more programming languages (Java, Scala, Python, etc.)
Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
Experience with data migration
Excellent verbal and written communication
Experience defining system architectures and exploring technical feasibility trade-offs.
Ability to prototype and evaluate applications and interaction methodologies.
Required Qualifications
3+ years of experience of Azure Data Platform implementation
3+ years of experience in design, development, testing and deployment of microservices
3+ years of hands-on experience in implementation and performance tuning of Azure Data Factory, Synapse, Kafka, Data Bricks or similar implementations
Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
Experience developing software code in one or more programming languages (Java, Scala, Python, etc.)
Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
Experience with data migration
Excellent verbal and written communication
Experience defining system architectures and exploring technical feasibility trade-offs.
Ability to prototype and evaluate applications and interaction methodologies.
Charan Kumar | IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
HWtvcVUklY
Show more
Show less","Azure DevOps, Data Engineering, Azure Data Platform, Microservices, Azure Data Factory, Synapse, Apache Kafka, Apache Spark, Java, Scala, Python, Infrastructure as Code, Terraform, Data Migration, Data Architecture, Feasibility Analysis, Data Analysis, Microservices Design, Agile Development, Continuous Integration, Continuous Delivery","azure devops, data engineering, azure data platform, microservices, azure data factory, synapse, apache kafka, apache spark, java, scala, python, infrastructure as code, terraform, data migration, data architecture, feasibility analysis, data analysis, microservices design, agile development, continuous integration, continuous delivery","agile development, apache kafka, apache spark, azure data factory, azure data platform, azure devops, continuous delivery, continuous integration, data architecture, data engineering, data migration, dataanalytics, feasibility analysis, infrastructure as code, java, microservices, microservices design, python, scala, synapse, terraform"
Database Engineer 3,"Farfield Systems, Inc","Annapolis Junction, MD",https://www.linkedin.com/jobs/view/database-engineer-3-at-farfield-systems-inc-3787772449,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"About Farfield Systems, Inc
At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member.
Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years.
""Employee driven...customer focused."" We build, operate and secure networks and infrastructure.
*** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship***
Provides technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develops relational and/or Object-Oriented databases, database parser software, and database loading software. Projects long-range requirements for database administration and design. Responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls. The DBE works primarily at the front end of the lifecycle-requirements through system acceptance testing and Initial Operational Capability (IOC). Develops requirements from a projects inception to its conclusion for a particular business and Information Technology (IT) subject matter area (i.e., simple to complex systems). Assist with recommendations for, and analysis and evaluation of systems improvements, optimization, development, and/or maintenance efforts. Translates a set of requirements and data into a usable document by creating or recreating ad hoc queries, scripts, and macros; updates existing queries, creates new ones to manipulate data into a master file; and builds complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies.
Basic Qualifications:
Ten (10) years experience as a DBE in programs and contracts of similar scope, type, and complexity is required. Bachelors degree in a technical discipline from an accredited college or university is required. Five (5) years of DBE experience may be substituted for a bachelors degree.
Experience in Infomration Assurance: DoD 8570.01-M compliance with Information Assurance Technical (IAT) Level II is required
Desired Qualifications:
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities 2. Support the analysis and evaluation of system improvements, optimization, development and/or maintenance efforts
Support the development of long and short term requirements for database administration and design
Assist in developing databases, database parser software, and database loading software
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file
Assist in developing database structures that fit into the overall architecture of the system under development
Lead development of database structures that fit into the overall architecture of the system under development
Lead development of databases, database parser software, and database loading software
Develop requirement recommendations from a projects inception to its conclusion for a particular Business and IT subject matter area (i.e. simple to complex systems)
Develop a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls
Direct fulfillment of requirements from a projects inception to it conclusion
Direct organization of requirements and data into a usable database schema by directing development of ad hoc queries, scripts, macros, updates to existing queries
Direct the overall database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls
Direct the development of complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies
Farfield Systems will provide reasonable accommodations to applicants who are unable to utilize our online application system due to a disability. Please send your request to careers@farfieldsystems.com or call us for assistance at 410-874-9363.
Farfield Systems is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law
Powered by JazzHR
g11n75OCEX
Show more
Show less","Relational database, ObjectOriented database, Database parser software, Database loading software, Open Database Connectivity, Cloud methodologies, DoD 8570.01M compliance, Information Assurance Technical (IAT) Level II, Queries, Scripts, Macros, Business and IT subject matter area, Data volumes, Logical distribution, Physical distribution, Response times, Retention rules, Security, Domain controls, Analysis and evaluation of system improvements optimization development and/or maintenance efforts","relational database, objectoriented database, database parser software, database loading software, open database connectivity, cloud methodologies, dod 857001m compliance, information assurance technical iat level ii, queries, scripts, macros, business and it subject matter area, data volumes, logical distribution, physical distribution, response times, retention rules, security, domain controls, analysis and evaluation of system improvements optimization development andor maintenance efforts","analysis and evaluation of system improvements optimization development andor maintenance efforts, business and it subject matter area, cloud methodologies, data volumes, database loading software, database parser software, dod 857001m compliance, domain controls, information assurance technical iat level ii, logical distribution, macros, objectoriented database, open database connectivity, physical distribution, queries, relational database, response times, retention rules, scripts, security"
"Looking for Azure Data Engineer - Houston, TX - Fulltime",Extend Information Systems Inc.,"Houston, TX",https://www.linkedin.com/jobs/view/looking-for-azure-data-engineer-houston-tx-fulltime-at-extend-information-systems-inc-3716362615,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Hi,
I hope you are doing well!
We have an opportunity for
Azure Data Engineer
with one of our clients for
Houston, TX
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Azure Data Engineer
Location:
Houston, TX
Terms:
Fulltime
Job Details
Technical Skills:
You have a minimum of 4+ years' experience working with Azure Data tools (ADF,Data Catalog, event hub,IOT hub) etc
You have proficiency in PySpark ,Python and SQL
Experience working with Databricks ,SQL End Point, Synapse
Knowledge on Power BI and willingness to enhance PBI skill set.
You have already written code capable of efficiently handling large volumes of data across large numbers of tables, and brought this code into a controlled productive environment.
Knowledge and experience in DevOps environments is a plus
Roles & Responsibilities
You are strong on handling large and complex amounts of structured data, you have a track record of having delivered end-to-end data solutions in corporate environments.
As an all-rounder, you enjoy taking time to understand business requirement, articulate a data product solution and then roll up your sleeves and develop it into production grade.
You enjoy navigating ambiguity and working as part of a distributed team to solve business problems using data.
You are comfortable with agile development processes, with both scheduled and ad-hoc release cycles.
Good English communication skills are a must, both spoken and written
You are comfortable working with some level of independence in a multinational environment
You are happiest solving concrete problems
You are able to lead a discussion with Business and understand requirements
Thanks & Regards
Monika Singh
Extend Information System Inc
Phone: (571)-622-3980
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","Azure Data tools (ADF Data Catalog Event Hub IoT hub), PySpark, Python, SQL, Apache Databricks, SQL Endpoint, Synapse, Power BI, DevOps, Agile development, English communication skills (spoken and written)","azure data tools adf data catalog event hub iot hub, pyspark, python, sql, apache databricks, sql endpoint, synapse, power bi, devops, agile development, english communication skills spoken and written","agile development, apache databricks, azure data tools adf data catalog event hub iot hub, devops, english communication skills spoken and written, powerbi, python, spark, sql, sql endpoint, synapse"
Data Engineer Hybrid,Avani Tech Solutions Private Limited,"Wayzata, MN",https://www.linkedin.com/jobs/view/data-engineer-hybrid-at-avani-tech-solutions-private-limited-3758755186,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Pay: 75-78/hr
Monday through Friday 8:00AM - 5:00PM CST
Hybrid Work
Need SQL
Job Description
The Data Engineer III will design, build and operate high performance data centric solutions utilizing the comprehensive big data capabilities for the company’s data platform environment.
In this role, you will act as an authority for data access pathways and techniques working with analysts within the functional data analytics team. You will design data structures and pipelines to collect data and design and implement data transformations, combinations or aggregations.
Independently handle complex issues with minimal supervision, while escalating only the most complex issues to appropriate staff. *Other duties as assigned*Help drive the adoption of new technologies and methods within the functional data and analytics team and be a role model and mentor for data engineers.
Build prototypes to test new concepts and be a key contributor of ideas and code that improve the core software infrastructure, patterns and standards.
Provide necessary technical support through all phases of solution life cycle.*Perform data modeling and prepare data in databases for use in various analytics tools and configurate and develop data pipelines to move and optimize data assets.
Develop technical solutions utilizing big data and cloud-based technologies and ensuring they are designed and built to be sustainable and robust.
Participate in the decision-making process related to architecting solutions.
Collaborate with businesses, application and process owners, and product team members to define requirements and design solutions for the company’s big data and analytics solutions.
Minimum Qualifications
Bachelor's degree in a related field or equivalent experience
Four years of related work experience.
Preferred Qualifications
Experience developing data or software applications including analysis, design, coding, testing, deploying and supporting of applications.
Experience working with big data platform.
Experience with reporting tools and data sources.
Show more
Show less","SQL, Data Engineering, Big Data, Data Structures, Data Pipelines, Data Transformations, Data Aggregation, Data Modeling, Data Analytics, Cloud Computing, Software Development, Application Development, Testing, Deployment, Prototyping, Data Architecture, Requirements Gathering, Solution Design, Collaboration","sql, data engineering, big data, data structures, data pipelines, data transformations, data aggregation, data modeling, data analytics, cloud computing, software development, application development, testing, deployment, prototyping, data architecture, requirements gathering, solution design, collaboration","application development, big data, cloud computing, collaboration, data aggregation, data architecture, data engineering, data structures, data transformations, dataanalytics, datamodeling, datapipeline, deployment, prototyping, requirements gathering, software development, solution design, sql, testing"
Senior Staff Data Engineer,Career Renew,"Austin, TX",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-career-renew-3786585845,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Career Renew is recruiting for one of its clients, a public cloud IT transformation services company, a Senior Staff Data Engineer in Austin, Texas - this is an onsite position.
As a Senior Staff Data Engineer, Corporate, you will have the opportunity to work with big data and emerging Google Cloud technologies to drive corporate services. You will have an opportunity to design, develop, and maintain the best Enterprise Data Warehouse solution to fit our corporate needs. You will be interacting with all of our business units and Google Cloud subject matter experts.
From transforming business requirements, solution architecture, data modeling, architecting, ETL, metadata, and business continuity, you will have the opportunity to work collaboratively with architects and other engineers to recommend, prototype, build, and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and covering a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes, and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as guide client-facing technical discussions for established projects.
Requirements
Mastery in the following domain area:
Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines, and reporting/analytic tools. Must have expert-level experience working with Google's batch or streaming data processing solutions (such as BigQuery, Dataform, and BI Engine)
Proficiency in the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming, and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data Catalog: Managing Data Catalogs, definitions, and data lineage.
Data Quality: Must have experience with DataForm, or other DQ solutions.
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. It may involve conversion between relational and NoSQL data stores, or vice versa
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale
4+ years of experience with Data modeling, SQL, ETL, Data Warehousing, and Data Lakes
4+ years experience in writing production-grade data solutions (relational and NoSQL)in an enterprise-class RDBMS
2+ years of experience with enterprise-class Business Intelligence tools such as Looker, PowerBI, Tableau, etc.
Mastery in writing software in Python
Experience writing software in one or more languages, such as Javascript, Java, R, or Go
Experience with systems monitoring/alerting, capacity planning, and performance tuning
Hands-on experience building frontend applications with React
Hands-on experience with CI/CD solutions (Cloud Build / Terraform)
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc.)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Ability to balance and prioritize multiple conflicting requirements with great attention to detail
Excellent verbal/written communication & data presentation skills, including the ability to succinctly summarize key findings and effectively communicate with both business and technical teams
Benefits
Unlimited PTO, paid parental leave, competitive and attractive compensation, performance-based bonuses, paid holidays, generous medical, dental, vision plans, life, short and long-term disability insurance, 401K/RRSP with match, as well as Google-certified training programs and a professional development stipend.
Show more
Show less","Google Cloud Platform (GCP), BigQuery, Dataform, BI Engine, Hadoop, Cassandra, HBase, Spark, Spark Streaming, Apache Beam, Pub/Sub, Kafka, RabbitMQ, Data Catalog, DataForm, SQL, ETL, Data Warehousing, Data Lakes, Looker, PowerBI, Tableau, Python, Javascript, Java, R, Go, React, Cloud Build, Terraform, CloudSQL, Spanner, Cloud Storage, Dataflow, Dataproc, Bigtable, Dataprep, Composer","google cloud platform gcp, bigquery, dataform, bi engine, hadoop, cassandra, hbase, spark, spark streaming, apache beam, pubsub, kafka, rabbitmq, data catalog, dataform, sql, etl, data warehousing, data lakes, looker, powerbi, tableau, python, javascript, java, r, go, react, cloud build, terraform, cloudsql, spanner, cloud storage, dataflow, dataproc, bigtable, dataprep, composer","apache beam, bi engine, bigquery, bigtable, cassandra, cloud build, cloud storage, cloudsql, composer, data catalog, data lakes, dataflow, dataform, dataprep, dataproc, datawarehouse, etl, go, google cloud platform gcp, hadoop, hbase, java, javascript, kafka, looker, powerbi, pubsub, python, r, rabbitmq, react, spanner, spark, spark streaming, sql, tableau, terraform"
Senior Data Engineer (FT),"Double Line, Inc.","Austin, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-ft-at-double-line-inc-3783141492,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Please DO NOT send applications/resume via email. Use the link below to APPLY ONLINE:
https://doubleline.applytojob.com/apply/FcLpeWsMBJ/Senior-Data-Engineer
(This is a remote position open to candidates residing near Austin, TX, Raleigh, NC, or Nashville, TN. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)
Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can
Fly the Airplane
, not just be a passenger in the back. We're a growing company focused on expanding our Development team with an experienced and innovative Senior Data Engineer with impressive analytical skills. Sound interesting?
If so, we're looking for a motivated and driven person like you who has:
Successfully completed multiple projects where you designed and executed ways to solve complex problems for clients around data integration, data quality, data warehousing, and analytics.
Demonstrated proficiency in deciding which ETL and data streaming technologies to use in AWS, Google Cloud, and Azure-based solutions, and propensity to pick something new when you want to push yourself and the team to innovate.
Mastery of T-SQL and experience with postgreSQL or other forms of SQL.
Experience in an Agile environment with Lean software development principles.
Drive to amplify the skills of teammates through mentoring and training junior and mid-level data engineers.
Mindset of continuous improvement and setting best practices.
Deadline-driven mentality.
Bonus points if you're bringing knowledge of or really want to learn the following:
A wide variety of data processing tools and approaches, from Python to Google BigQuery to SSIS to AWS Lambda to Azure Data Factory and others.
Performance implications of memory and disk usage at different data volumes.
Business intelligence tools and dashboard design theory.
We Do Not Want You To Make The Leap Without Knowing What We Need, So Here Is How We Define Success For This Position
Bring a new idea to our team of brilliant data engineers in the first 30 days.
Lead the collaborative design process of a data engineering solution in one of our projects in the first 2 months.
Become a mentor to a data engineer within your first 6 months.
In Return, We Offer
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that challenge state and local governments, particularly in education, healthcare, and similar fields.
A home where your voice matters and you can affect real change.
Direct connection to the Executive team where you can help drive the future of the company.
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture.
We need to know - can you make this happen? If so, we definitely need to talk to you.
Please DO NOT send applications/resume via email. Use the link below to APPLY ONLINE:
https://doubleline.applytojob.com/apply/FcLpeWsMBJ/Senior-Data-Engineer
Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.
Double Line does not currently offer relocation assistance.
About Double Line, Inc.
We are a technology consulting company providing customized data solutions for educational and other public organizations.
Show more
Show less","Data Integration, Data Quality, Data Warehousing, Analytics, ETL, Data Streaming, AWS, Google Cloud, Azure, TSQL, postgreSQL, SQL, Agile, Lean Software Development, Mentoring, Training, Python, Google BigQuery, SSIS, AWS Lambda, Azure Data Factory, Business Intelligence, Dashboard Design, Memory Usage, Disk Usage, Data Volumes","data integration, data quality, data warehousing, analytics, etl, data streaming, aws, google cloud, azure, tsql, postgresql, sql, agile, lean software development, mentoring, training, python, google bigquery, ssis, aws lambda, azure data factory, business intelligence, dashboard design, memory usage, disk usage, data volumes","agile, analytics, aws, aws lambda, azure, azure data factory, business intelligence, dashboard design, data integration, data quality, data streaming, data volumes, datawarehouse, disk usage, etl, google bigquery, google cloud, lean software development, memory usage, mentoring, postgresql, python, sql, ssis, training, tsql"
Azure Data Engineer,Accroid Inc,"Chicago, IL",https://www.linkedin.com/jobs/view/azure-data-engineer-at-accroid-inc-3750193045,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Exp: 10-12+ Years
Job Description And Our Notes Are Listed Below.
Senior IT Engineer. The role is a long-term contract engagement. Candidates should be available to be onsite at least two days weekly ine control, test and production release).
Experience with Performance Tuning and Optimization using native monitoring and troubleshooting tools.
Experience with backups, restores and recovery models.
Knowledge of High Availability and Disaster Recovery options for database technologies.
Experience migrating on-prem environments to public cloud (Azure-preferred)
Knowledge of best practices in the areas of application design, performance, security, scalability and maintainability
What STERIS Offers
The opportunity to join a company that will invest in you for the long-term. STERIS couldn’t be where it is today without our incredible people. That’s why we share in our success together by rewarding you for your hard work. Hiring people who are in it for the long run with STERIS is our ultimate goal. We do this by providing competitive salaries, healthcare benefits, tuition assistance, paid-time off, holidays, matching 401(k), annual merit, and incentive plans. Join us and help write our next chapter.
STERIS is a leading provider of products and services that meet the needs of growth areas within Healthcare: procedures, devices, vaccines and biologics. We exist to fulfill our MISSION TO HELP OUR CUSTOMERS CREATE A HEALTHIER AND SAFER WORLD. STERIS is a $3B, publicly traded (NYSE: STE) company with approximately 16,000 associates and Customers in more than 100 countries.
If you need assistance completing the application process, please call 1 (440) 392.7047. This contact information is for accommodation inquiries only and cannot be used to check application status.
STERIS is an Equal Opportunity Employer. We are committed to equal employment opportunity and the use of affirmative action programs to ensure that persons are recruited, hired, trained, transferred and promoted in all job groups regardless of race, color, religion, age, disability, national origin, citizenship status, military or veteran status, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, genetic information, and any other category protected by federal, state or local law. We are not only committed to this policy by our status as a federal government contractor, but also we are strongly bound by the principle of equal employment opportunity.
The full affirmative action program, absent the data metrics required by
60-741.44(k), shall be available to all employees and applicants for employment for inspection upon request. The program may be obtained at your location’s HR Office during normal business hours.
Show more
Show less","Microsoft SQL Server, Database Administration, Data Modeling, High Availability, Disaster Recovery, Performance Monitoring, Performance Tuning, Cybersecurity, Data Security, Data Encryption, Capacity Management, Problem Resolution, Process Automation, Query Tuning, Schema Refinement, OnCall Support, Database Migration, Cloud Computing, Azure, PaaS, IaaS, Database Development Methodologies, Database Design, Database Implementation, Database Administration Skills, Change Control, Test and Production Release, Performance Tuning and Optimization, Backup and Restore, Recovery Models, Application Design, Performance, Security, Scalability, Maintainability","microsoft sql server, database administration, data modeling, high availability, disaster recovery, performance monitoring, performance tuning, cybersecurity, data security, data encryption, capacity management, problem resolution, process automation, query tuning, schema refinement, oncall support, database migration, cloud computing, azure, paas, iaas, database development methodologies, database design, database implementation, database administration skills, change control, test and production release, performance tuning and optimization, backup and restore, recovery models, application design, performance, security, scalability, maintainability","application design, azure, backup and restore, capacity management, change control, cloud computing, cybersecurity, data encryption, data security, database administration, database administration skills, database design, database development methodologies, database implementation, database migration, datamodeling, disaster recovery, high availability, iaas, maintainability, microsoft sql server, oncall support, paas, performance, performance monitoring, performance tuning, performance tuning and optimization, problem resolution, process automation, query tuning, recovery models, scalability, schema refinement, security, test and production release"
Master Data Analyst,Applied Industrial Technologies,"Cleveland, OH",https://www.linkedin.com/jobs/view/master-data-analyst-at-applied-industrial-technologies-3751756247,2023-12-17,Cleveland,United States,Mid senior,Hybrid,"The Master Data Analyst is responsible for working with Service Centers, Field Management, Finance, Supply Chain, and IT to analyze data to maximize revenue and operating efficiency as it pertains to all vendor and customer related master data. Maintain data integrity. Assist with creating requirements for changes to vendor and customer data models. The Master Data Analyst will be involved in designing and enforcing Data Governance rules and standards and provide consulting expertise to Service Centers. Problem solving and resolution for an array of data related issues. Work in a highly collaborative manner with several departments across the company.
This is a permanent full-time position at our corporate headquarters campus in the Midtown Corridor of Cleveland, OH.
**Job Duties**
Manage the daily operations by creating, reviewing, validating, and maintaining vendor and customer master data in SAP
Organizes and determines course of action necessary to complete master data requests within the established Service Level Agreements
Perform root-cause analysis on master data integration problems. Recommend and execute corrective action
Routinely audit data against established rule sets to ensure consistency and accuracy of information, and update data as necessary.
Assist with the design, testing, and implementation of new process and/or enhancements to the systems that support master data
Leads process standardization, data governance, cleansing activities, maintenance, and data quality improvement efforts in alignment with data strategy goals
Supports SAP deployments across the company by ensuring the data cleansing meets the global and business specific standards
Coach and advise the business through data quality procedures to ensure data is complete, accurate, and timely
Communicate timely and professionally to users, management, and senior leadership
Develop standard procedures, user guides, and other necessary training and job aid materials. Provide training / coaching as needed
**Requirements**
Bachelor's degree or equivalent of work experience
Up to 1 year of relevant experience
Solid analytical & problem-solving skills
Process orientation, detail focus, ability to multi-task. Sense of humor.
Results oriented. Self-Motivated. Ability to handle stress & deadlines.
Applied Industrial Technologies (NYSE: AIT)** is a leading value-added distributor and technical solutions provider of industrial motion, fluid power, flow control, automation technologies, and related maintenance supplies. Our leading brands, specialized services, and comprehensive knowledge serve MRO and OEM end users in virtually all industrial markets through our multi-channel capabilities that provide choice, convenience, and expertise.
**We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, gender, sexual orientation, gender identity, age, disability, protected veteran status, marital status, medical condition or any other characteristic protected by law.**
If you need accommodation for any part of the employment process because of a disability, please send an email to hiring@applied.com or call 216-426-4389 to let us know the nature of your request.
Show more
Show less","SAP, Master data, Data governance, Data quality, Process standardization, Data cleansing, Data analysis, Problemsolving, Communication, Training, Analytical skills, Detail focus, Multitasking, Stress management, Deadline management","sap, master data, data governance, data quality, process standardization, data cleansing, data analysis, problemsolving, communication, training, analytical skills, detail focus, multitasking, stress management, deadline management","analytical skills, communication, data governance, data quality, dataanalytics, datacleaning, deadline management, detail focus, master data, multitasking, problemsolving, process standardization, sap, stress management, training"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cleveland, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086845,2023-12-17,Cleveland,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, AI, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Platforms, Data Frameworks, Data Visualization, Pandas, R, Airflow, KubeFlow, Pipeline Tools, NLP, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, machine learning, ai, data mining, data cleaning, data normalization, data modeling, data platforms, data frameworks, data visualization, pandas, r, airflow, kubeflow, pipeline tools, nlp, large language models, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","ai, airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data frameworks, data management tools, data mining, data normalization, data platforms, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, nlp, nosql, pandas, pipeline tools, python, r, snowflake, spark, sparkstreaming, sql, storm, visualization"
SAP Master Data Analyst,"TechnoSmarts, Inc.","Cleveland, OH",https://www.linkedin.com/jobs/view/sap-master-data-analyst-at-technosmarts-inc-3777128835,2023-12-17,Cleveland,United States,Mid senior,Hybrid,"W2 ONLY. NO C2C / CORP-CORP / THIRD PARTY / 1099 CANDIDATES.
12 month renewable term contract. Likely to extend multi-year/option to hire.
Client: Major global corporation and a leader in their industry with annual revenue of $19 B and employing over 18,000.
Overview:
Position will play a key role to meet the objectives associated with maintaining material Master Data accuracy, compliance and process management. This position is responsible for driving these objectives through the functional groups of the divisions. Key activities include optimizing the material life cycle, facilitating the overall material master data process for a given division, and making adjustments and enhancements as needed. Educate users on data standards, policies and procedures. Work with division data owners across all functional teams (supply chain, technical, finance, and marketing) to drive accuracy, timeliness, and provide expertise regarding definitions, cleansing tools and workflow processes. Initiate connections into data synchronization tools allowing retailers visibility into product data.
Train users from a material master data perspective and resolve key performance indicators including the coordination with affiliate markets, working on timely workflow processing and testing new tools to measure the effectiveness and ownership of the users. In addition, test new automation techniques to effectively utilize resources within the divisions. Additionally the position will ensure that any special projects which have significant impact on the underlying master data (e.g. data accuracy audits/validation, packaging changes, brand realignments) are aligned with the industry standards (GS1).
Role:
Execute SAP material workflow process, changes, extensions and activation of materials.
Review & resolve data errors and determine the root cause.
Look over business processes and recommend improvements to align with division’s needs.
Manage material life cycle process, health & validity checks, pre-activation of materials.
Support and lead Master Data management software solutions and implementations.
Audit and validate the quality and procedures associated with current master data.
Communicate and educate cross function & depts on Master Data standards.
Manage the SKU rationalization initiatives through the deactivation process.
Respond and consult on outside customer challenges and inquiries regarding material master.
Work with business users/owners to complete data mapping for clean and accurate transformation of data
Learn and understand the end-to-end data flow, including all cross functional connections and inputs with ability to articulate the flow clearly and succinctly to all levels of the organization
Proactively develop new procedures to support the health and sustainment of master data.
Educate users on data standards, policies, and procedures, including industry standards (GS1).
Work with-in the function to drive accuracy, timeliness, and provide expertise regarding definitions, cleansing tools and workflow processes.
Build and test new automation techniques to effectively utilize resources within the function and to make repetitive tasks less labor intensive.
Support new acquisitions, merger activities, and organization realignment activities, as needed.
Align work within supply chain and with other master data team activities.
Execute SAP material workflow process, changes, extensions and activation of materials.
Review & resolve data errors and determine the root cause.
Look over business processes and recommend improvements to align with division’s needs.
Manage material life cycle process, health & validity checks, pre-activation of materials.
Support and lead Master Data management software solutions and implementations.
Audit and validate the quality and procedures associated with current master data.
Communicate and educate cross functional teams and department on Master Data standards.
Manage the SKU rationalization initiatives through the deactivation process.
Respond and consult on outside customer challenges and inquiries regarding material master. Work with business users/owners to complete data mapping for clean and accurate transformation of data.
Learn and understand the end-to-end data flow, including all cross functional connections and inputs with ability to articulate the flow clearly and succinctly to all levels of the organization.
Qualifications:
Must have 8+ years work experience in the United States.
Bachelor’s Degree or equivalent relevant experience.
Prior experience with an integrated ERP systems environment and required data dependencies.
5 – 8+ years experience in SAP Master Data management.
Material Master data maintenance.
Automation experience is a plus.
Strong skills in Excel and Access.
Show more
Show less","SAP, Master Data, GS1, ERP, Excel, Access","sap, master data, gs1, erp, excel, access","access, erp, excel, gs1, master data, sap"
Data Analyst Part Time,Toyandsons,"Saskatoon, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-toyandsons-3756819621,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Mining, Business Intelligence, Data Visualization, SQL, R, Python, Tableau, Power BI, Data Management, ETL, A/B Testing, Hypothesis Testing, Machine Learning, Communication, Collaboration, Problem Solving","data analysis, statistical techniques, data mining, business intelligence, data visualization, sql, r, python, tableau, power bi, data management, etl, ab testing, hypothesis testing, machine learning, communication, collaboration, problem solving","ab testing, business intelligence, collaboration, communication, data management, data mining, dataanalytics, etl, hypothesis testing, machine learning, powerbi, problem solving, python, r, sql, statistical techniques, tableau, visualization"
Data Analyst Part Time,Voxmediallc,"Saskatoon, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3758280261,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Visualization, Data Management, ETL","data analysis, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data visualization, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Tractorsupplycompany,"Saskatoon, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-tractorsupplycompany-3740518054,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Analysis, Trend Analysis, Reporting, Data Visualization, Tableau, SQL, R, Python, Data Quality, Data Cleansing, Data Management, ETL, A/B Testing, Machine Learning, DataDriven Decision Making, Data Mining, Data modeling, Software Development","data analysis, statistical analysis, trend analysis, reporting, data visualization, tableau, sql, r, python, data quality, data cleansing, data management, etl, ab testing, machine learning, datadriven decision making, data mining, data modeling, software development","ab testing, data management, data mining, data quality, dataanalytics, datacleaning, datadriven decision making, datamodeling, etl, machine learning, python, r, reporting, software development, sql, statistical analysis, tableau, trend analysis, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Univisioncommunicationsinc,"Prince Albert, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-univisioncommunicationsinc-3757805477,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Visualization, Data Management, ETL, Data Manipulation, Machine Learning, Big Data","data analysis, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data visualization, data management, etl, data manipulation, machine learning, big data","ab testing, big data, data management, data manipulation, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Prince Albert, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751468571,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Advanced Analytics, Data Visualization, A/B Testing, Data Integrity, Data Manipulation, DataDriven DecisionMaking, Data Quality, Data Collection, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, advanced analytics, data visualization, ab testing, data integrity, data manipulation, datadriven decisionmaking, data quality, data collection, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, advanced analytics, data collection, data integrity, data manipulation, data quality, dataanalytics, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Mayorsofficeofcontractservices,"Swift Current, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-mayorsofficeofcontractservices-3759044027,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hrteam@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Statistical modeling, Hypothesis testing, A/B testing, ETL, Data visualization","data analysis, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, etl, data visualization","ab testing, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Americanlandscapesystems,"Shellbrook, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-americanlandscapesystems-3742883508,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Statistics, Machine Learning, Big Data, SQL, R, Python, Tableau, Power BI, Advanced Data Visualization, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL Processes, Data Warehousing, Data Integration, Data Quality Management, Data Mining, Data Governance, Business Intelligence, Decision Making, Reporting","data analysis, statistics, machine learning, big data, sql, r, python, tableau, power bi, advanced data visualization, statistical modeling, hypothesis testing, ab testing, data management, etl processes, data warehousing, data integration, data quality management, data mining, data governance, business intelligence, decision making, reporting","ab testing, advanced data visualization, big data, business intelligence, data governance, data integration, data management, data mining, data quality management, dataanalytics, datawarehouse, decision making, etl, hypothesis testing, machine learning, powerbi, python, r, reporting, sql, statistical modeling, statistics, tableau"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Majorleaguebaseball,"Prince Albert, Saskatchewan, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-majorleaguebaseball-3752011650,2023-12-17,Saskatchewan, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Hypothesis testing, A/B testing, Data visualization, Data management, ETL","data analysis, sql, r, python, tableau, power bi, hypothesis testing, ab testing, data visualization, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, tableau, visualization"
Senior Application Engineer - Data Simulation,Ansys,"Canonsburg, PA",https://www.linkedin.com/jobs/view/senior-application-engineer-data-simulation-at-ansys-3725038990,2023-12-17,Weirton,United States,Mid senior,Onsite,"Requisition #:
13274
When visionary companies need to know how their world-changing ideas will perform, they close the gap between design and reality with Ansys simulation. For more than 50 years, Ansys software has enabled innovators across industries to push boundaries by using the predictive power of simulation. From sustainable transportation to advanced semiconductors, from satellite systems to life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.
Take a leap of certainty … with Ansys.
Summary
/ Role Purpose
Join the Ansys Customer Excellence team to partner with our customers to engineer what's ahead, solve their real-world engineering problems, deploy Ansys software in their design workflows, and grow Ansys’ business. As a hands-on subject matter expert, you will use expert-level engineering knowledge to provide technical pre-sales support, perform professional services, and help translate customer requirements into exciting new product features. You will be working within multi-disciplinary teams to create pervasive simulation solutions, advance your industry knowledge, and grow the business impact.
Key Duties And Responsibilities
Lead in coordinating and executing all technical activities throughout the sales opportunity lifecycle such as technical discovery, negotiate technical success criteria, product presentations, demonstrations, and evaluations. Work independently within multi-disciplinary teams
Interact with customers to understand their product design needs and engineering design workflows; analyze how to address customers’ requirements using Ansys products and platform, articulate Ansys’ value proposition
Assist in creating differentiating simulation solutions using the Ansys platform and products; deploy the solutions within customers’ design workflows
Develop competence as a subject matter expert and industry expert
Collaborate with the Ansys product development teams to translate customer requirements into exciting new product features; test new releases of Ansys products on industrial problems, develop application best practices
Support Ansys field and digital marketing, author conference presentations
Contribute to consulting services,
Minimum Education/Certification Requirements And Experience
Required education and degree type: BS or MS or PhD in Mechanical/Chemical/Aerospace/Electrical Engineering or related field
Required minimum years of professional experience in an engineering software environment: BS+5, MS+3, or PhD+0
Proven track record of analyzing customer’s business and technical needs, requirements, and their state of current infrastructure, operations, and simulation & other engineering workflows
Proven track record of designing & proposing enterprise-level solutions based on data management solutions and implementing such solutions
Logical problem-solving, strong interpersonal and communication skills, fluent in writing and speaking English
Strong organizational and time management skills, possesses a sense of urgency
Projects a professional image and demonstrates business acumen, driven to succeed
Ability to travel domestically up to 25% of time
Preferred Qualifications And Skills
Preferred education and years of professional experience in an engineering software environment: BS+8, MS+6, or PhD+3
5 years of experience in application engineering, or consulting services type customer facing roles using engineering software
Good understanding of enterprise class product development systems like SLM, SPDM, ERP, ALM, TDM, MIM/IMM, PDM, PLM (e.g. Aras Innovator, Siemens Teamcenter, Dassault ENOVIA or 3DEXPERIENCE, MSc SimManager, MSc MaterialCenter)
Demonstrated use of relevant Ansys software or knowledge of other commercial CAE, CAD, EDA, PLM software packages
Practical knowledge of agility and agile project management
Ability to interact effectively with senior business managers and C-level executives
Ability to travel domestically up to 50% of time
This role is not available for sponsorship.
At Ansys, we know that changing the world takes vision, skill, and each other. We fuel new ideas, build relationships, and help each other realize our greatest potential in the knowledge that every day is an opportunity to observe, teach, inspire, and be inspired.
Together as One Ansys, we are powering innovation that drives human advancement
.
Our Commitments
Amaze with innovative products and solutions
Make our customers incredibly successful
Act with integrity
Ensure employees thrive and shareholders prosper
Our Values
Adaptability: Be open, welcome what’s next
Courage: Be courageous, move forward passionately
Generosity: Be generous, share, listen, serve
Authenticity: Be you, make us stronger
Our Actions
We commit to audacious goals
We work seamlessly as a team
We demonstrate mastery
We deliver outstanding results
OUR ONE ANSYS CULTURE HAS INCLUSION AT ITS CORE
We believe diverse thinking leads to better outcomes. We are committed to creating and nurturing a workplace that fuels this by welcoming people, no matter their background, identity, or experience, to a workplace where they are valued and where diversity, inclusion, equity, and belonging thrive.
TAKE A LEAP OF CERTAINTY IN YOUR CAREER AT ANSYS
At Ansys, you will find yourself among the sharpest minds and most visionary leaders across the globe. Collectively we strive to change the world with innovative technology and transformational solutions. With a prestigious reputation in working with well-known, world-class companies, standards at Ansys are high – met by those willing to rise to the occasion and meet those challenges head on. Our team is passionate about pushing the limits of world-class simulation technology, empowering our customers to turn their design concepts into successful, innovative products faster and at a lower cost.
At Ansys, it’s about the learning, the discovery, and the collaboration. It’s about the “what’s next” as much as the “mission accomplished.” And it’s about the melding of disciplined intellect with strategic direction and results that have, can, and do impact real people in real ways. All this is forged within a working environment built on respect, autonomy, and ethics.
CREATING A PLACE WE’RE PROUD TO BE
Ansys is an S&P 500 company and a member of the NASDAQ-100. We are proud to have been recognized for the following more recent awards, although our list goes on: America’s Most Loved Workplaces, Gold Stevie Award Winner, America’s Most Responsible Companies, Fast Company World Changing Ideas, Great Place to Work Certified (China, Greece, France, India, Japan, Korea, Spain, Sweden, Taiwan, U.K.).
For more information, please visit us at www.ansys.com
Ansys is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics.
Ansys does not accept unsolicited referrals for vacancies, and any unsolicited referral will become the property of Ansys. Upon hire, no fee will be owed to the agency, person, or entity.
Show more
Show less","Ansys software, Simulation, Presales support, Product presentations, Product demonstrations, Product evaluations, Data management solutions, CAD, CAE, EDA, PLM, SLM, SPDM, ERP, ALM, TDM, MIM/IMM, PDM, Aras Innovator, Siemens Teamcenter, Dassault ENOVIA, 3DEXPERIENCE, MSc SimManager, MSc MaterialCenter","ansys software, simulation, presales support, product presentations, product demonstrations, product evaluations, data management solutions, cad, cae, eda, plm, slm, spdm, erp, alm, tdm, mimimm, pdm, aras innovator, siemens teamcenter, dassault enovia, 3dexperience, msc simmanager, msc materialcenter","3dexperience, alm, ansys software, aras innovator, cad, cae, dassault enovia, data management solutions, eda, erp, mimimm, msc materialcenter, msc simmanager, pdm, plm, presales support, product demonstrations, product evaluations, product presentations, siemens teamcenter, simulation, slm, spdm, tdm"
"Senior Data Analyst, Payer Relations",Eleanor Health,"Waltham, MA",https://www.linkedin.com/jobs/view/senior-data-analyst-payer-relations-at-eleanor-health-3779277809,2023-12-17,Malden,United States,Mid senior,Remote,"Position Description
We're looking for a Senior Analyst to join our team with a culture rooted in kindness, empathy, and product-mindedness. This analyst will be focused on helping support the Payer Partnerships team by delivering performance reports, strategic insights, and data feeds to maintain and expand our partnerships. This role will be a member of the growing Analytics team but will work closely with the Partner Relationship Manager.
Some of the exciting projects that this role will get to work on include creating insights for quarterly business review meetings with our Payer Partners to help demonstrate the impact our care model is having on our members, performing regular analysis on our contract health to find opportunities to grow outcomes, and working with our payers to help maintain contractual reporting compliance through the creation of data feeds.
We use a variety of technologies at Eleanor Health including Google Cloud Platform, Mode, dbt, Salesforce, Callrail, Rudderstack, and Google Analytics.
Candidate Responsibilities
Proactively develop insights on how our care model is positively impacting the health outcomes of our members
Help build business cases through analyses and insights to land new and expand existing payer contracts.
Access data provided by the Data Engineering and Analytics Engineering team to develop reports and analyses pertaining to contract performance for key members of the Partnerships team.
Create dashboards for the Partnerships teams following the principles of ""Data Product"" development
Create SQL views containing member-level and activity-level data to transmit to relevant payer partners
Attend payer partner meetings as needed to address data issues, gather reporting requirements, and educate stakeholders on insights
Communicate to internal colleagues and external payer partners with all levels of familiarity with data.
Candidate Qualifications:
Have 4-6 years of analytics experience
Fantastic data insight communication and presentation skills
SQL Expert with dimensional modeling familiarity
Ability to take an ask from a stakeholder and find the underlying business need
Have a keen eye for spotting patterns in stakeholder asks that can be automated by Analytics Engineering team
Highly collaborative individual who can become a relied upon member of both the Analytics team and Partnerships team
Are keen on staying up to date on current technology trends and continuously improving team and company performance by incorporating modern engineering and best practices.
Value and display a high level of empathy and humility as well as personal accountability and ownership for meeting commitments.
Have strong written and verbal communication skills.
Are excited to work with a fully remote team.
Are currently authorized to work for any US employer without visa sponsorship.
Have the ability to complete all other responsibilities as required
Preferred:
Data analysis with Python experience
Familiarity with dbt development
Previous experience wireframing dashboards for stakeholder feedback
Prior healthcare experience
Compensation & Benefits:
The total target compensation range for this position is $110,000 - $152,000. The actual compensation offered depends on a variety of factors, which may include, as applicable, the applicant's qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job.
Eleanor Health offers a generous benefits package to full-time employees, which includes:
Flexible PTO policy and a remote work environment- unplug, relax, and recharge!
9 observed company holidays + 3 floating holidays- We encourage you to use the additional 3 floating holidays to accommodate personal beliefs/practices
Wellness Days - In lieu of ""Sick Time"" which typically applies only when you are ill, we encourage you to proactively manage your overall wellbeing, both physical and mental, as well as the wellbeing of those who play important roles in your life.
Fully covered medical and dental insurance plan, with affordable vision coverage.- We are a health first company, and we strive to make our plans affordable and accessible
401(k) plan with 3% match. We are excited to be able to support the long-term financial well-being of our team in a way that reinforces Eleanor's commitment to equity.
Short-term disability- We understand that things happen, we want you to feel comfortable to take the time to get better.
Long Term Disability - Picks up where Short Term Disability leaves off.
Life Insurance - Both Eleanor and employee-paid options are available.
Family Medical Leave- Eleanor Health's Paid Family & Medical Leave (""PFML"") is designed to provide flexibility and financial peace of mind for approved family and medical reasons such as the birth, adoption, or fostering of a child, and for serious health conditions that they or a family member/significant other might be facing.
Wellness Perks & Benefits- Mental Health is important to us and we want our employees to have the accessibility they deserve to talk things through, zen with a mindfulness app, or seek assistance from health advocates
Mindfulness App Reimbursement
1 year subscription to TalkSpace
Paid Membership to Health Advocate, One Medical, and Teladoc
About Eleanor Health
Eleanor Health is the first outpatient addiction and mental health provider delivering convenient and comprehensive care through a value-based payment structure. Committed to health and wellbeing without judgment, Eleanor Health is focused on delivering whole-person, comprehensive care to transform the quality, delivery, and accessibility of care for people affected by addiction.
To date, Eleanor Health operates multiple clinics and a fully virtual model statewide across Louisiana, Massachusetts, New Jersey, North Carolina, Ohio, Texas, Florida, and Washington, delivering care through population and value-based partnerships with Medicare, Medicaid, and employers.
If you are passionate about providing high quality, evidence based care for individuals with substance use disorder through an innovative practice and about building a great business that makes a difference, Eleanor Health is an ideal opportunity for you. We seek highly skilled, motivated and compassionate individuals who take responsibility and adapt quickly to change to join our deeply committed and collaborative team.
Job Types: Full-time
Show more
Show less","Google Cloud Platform, Mode, dbt, Salesforce, Callrail, Rudderstack, Google Analytics, SQL Expert, SQL views, Python, data analysis with Python, dbt development, wireframing dashboards, healthcare experience, empathy, humility, ownership","google cloud platform, mode, dbt, salesforce, callrail, rudderstack, google analytics, sql expert, sql views, python, data analysis with python, dbt development, wireframing dashboards, healthcare experience, empathy, humility, ownership","callrail, data analysis with python, dbt, dbt development, empathy, google analytics, google cloud platform, healthcare experience, humility, mode, ownership, python, rudderstack, salesforce, sql expert, sql views, wireframing dashboards"
Reverse Engineer - Data Restoration Specialist,Motion Recruitment,"Boston, MA",https://www.linkedin.com/jobs/view/reverse-engineer-data-restoration-specialist-at-motion-recruitment-3764626305,2023-12-17,Malden,United States,Mid senior,Remote,"Job Opportunity: Reverse Engineer - Data Restoration Specialist
Location: Fully Remote
Position: Reverse Engineer - Data Restoration Specialist
About Us:
We are a dynamic and innovative cybersecurity firm dedicated to providing cutting-edge solutions for organizations affected by ransomware attacks. Our team of experts specializes in reverse engineering and the development of custom utilities to restore critical data from virtual machines, backup files, and databases. We pride ourselves on being at the forefront of technology, enabling our clients to recover from even the most sophisticated ransomware incidents.
Job Overview:
We are seeking a talented and experienced Reverse Engineer to join our team. The ideal candidate will have a profound understanding of virtual machine technologies, particularly in the context of data organization, storage, and retrieval. In this role, you will play a crucial part in developing advanced tools and utilities for data restoration, working with clients who have fallen victim to ransomware attacks.
Key Responsibilities
Conduct reverse engineering activities to analyze and understand virtual machine technologies.
Develop custom utilities for advanced data retrieval and restoration.
Create tools to repair and make bootable virtual machines.
Carve and extract backups from various backup systems.
Merge encrypted or corrupt offsets of originally encrypted data with backups.
Collaborate with cross-functional teams to enhance data restoration capabilities.
Stay updated on the latest advancements in reverse engineering and cybersecurity.
Qualifications
Bachelor's or advanced degree in Computer Science, Information Security, or related field.
Proven experience in reverse engineering with a focus on virtual machine technologies.
Strong understanding of data organization, storage, and retrieval in virtual environments.
Proficiency in developing custom utilities for advanced data retrieval.
Experience working with organizations affected by ransomware attacks is a plus.
Solid programming skills in languages such as C, C++, or Python.
Excellent problem-solving and analytical skills.
What We Offer
Competitive salary and benefits package.
Opportunity to work on cutting-edge projects in the cybersecurity domain.
Collaborative and dynamic work environment.
Professional development opportunities.
How to Apply:
If you are passionate about cybersecurity, possess the required skills, and are ready to make a difference in the fight against ransomware, please submit your resume.
Join us in our mission to protect organizations from the impact of ransomware attacks and contribute to the development of innovative solutions in the cybersecurity landscape.
Posted By:
Lauren Proctor
Show more
Show less","Reverse Engineering, Data Restoration, Virtual Machine Technologies, Data Organization, Storage, Retrieval, Development, Utilities, Data Restoration, Ransomware, Attacks, C, C++, Python, ProblemSolving, Analytical Skills, Cybersecurity","reverse engineering, data restoration, virtual machine technologies, data organization, storage, retrieval, development, utilities, data restoration, ransomware, attacks, c, c, python, problemsolving, analytical skills, cybersecurity","analytical skills, attacks, c, cybersecurity, data organization, data restoration, development, problemsolving, python, ransomware, retrieval, reverse engineering, storage, utilities, virtual machine technologies"
Principal/Sr. Engineer Systems Test (Data Acquisition Engineer) with Security Clearance,ClearanceJobs,"Palmdale, CA",https://www.linkedin.com/jobs/view/principal-sr-engineer-systems-test-data-acquisition-engineer-with-security-clearance-at-clearancejobs-3759644256,2023-12-17,Palmdale,United States,Mid senior,Onsite,"Responsibilities
At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history. This role may be selected at the higher grade based on requirements listed below. Northrop Grumman Aeronautics Systems has an opening for a Principal or Senior Principal Electronics Engineer (Data Acquisition) to join our team of qualified, diverse individuals within our Test and Evaluation organization. This role is located in Palmdale. The primary job responsibilities are, but not limited, to the following:
Responsible for turning engineer requirement into data acquisition.
Provide mission load files to Curtiss Wright KSM-500, Omega Next, System 550, MCS, and IADS.
Develop software application for instrumentation.
Performs a variety of duties, including development, testing, and procedure writing. This role may be selected at the higher grade based on requirements listed below. Essential Functions
Responsible for turning engineer requirement into data acquisition.
Provide mission load files to Curtiss Wright KSM-500, Omega Next, System 550, MCS, and IADS.
Develop software application for instrumentation.
Performs a variety of duties, including development, testing, and procedure writing.Basic Qualifications for a Principal level 3:
BS in STEM (Science, Technology, Engineering or Mathematics) and 5 years equivalent experience In Instrumentation / Data Acquisition career field or 3 years with a Master's degree and 0 years with a Ph D.
DOD Secret clearance is required to be considered
Ability to obtain Special Program Access prior to start
Familiar with ACRA Airborne Data Acquisition System KSM-500 series or industry equivalent.
Experience with ground telemetry frontends and IADS
IT working experience, candidate must have solid understanding of client and server applications
Experience with software development and maintenance
Overtime, odd shifts, and weekend work will occasionally be requiredBasic Qualifications for a Sr. Principal level 4:
BS in STEM (Science, Technology, Engineering or Mathematics) and 9 years equivalent experience In Instrumentation / Data Acquisition career field or 7 years with a Master's degree and 4 years with a Ph D
DOD Secret clearance is required to be considered
Ability to obtain Special Program Access prior to start
Familiar with ACRA Airborne Data Acquisition System KSM-500 series or industry equivalent.
Experience with ground telemetry frontends and IADS
IT working experience, candidate must have solid understanding of client and server applications
Experience with software development and maintenance
Overtime, odd shifts, and weekend work will occasionally be requiredPreferred Qualifications: * Active DOD Top Secret * Experience working with IRIG-106 standard, particularly chapters 4, 7, and 10
Experience working with SQL, XML, and C#, and is willing to adapt to special software projects
Active Security+ certification
9+ years of data acquisition experience working with over-the-air RF telemetry transmission and onboard recording systems
9+ experience working with ground station mission control room and test ranges
9+ years of experience working with various ICDs written for various digital bus mediums such as Ethernet, Mil-Std-1553, Arinc-429, Serial, 1394, etc
Ideal candidate must have an ability to digest different data formats and apply best filtering criteria for capturing data
5+ years of experience working with RT Station IENA data format
Experience in supporting test activities, anomaly resolution and process improvement initiatives
Familiar with the engineering development cycle, along with engineering configuration management conceptsThe selected candidate will work in a dynamic people-focused environment while interacting with customers and other design engineers. The position will be based out of Palmdale, CA. Salary Range: $95,000 - $142,400
Salary Range 2: $117,700 - $176,500 The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions. Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Instrumentation, Data Acquisition, Curtiss Wright KSM500, Omega Next, System 550, MCS, IADS, SQL, XML, C#, RT Station IENA data format, Engineering Development Cycle, Engineering Configuration Management, IRIG106 standard, MilStd1553, Arinc429","instrumentation, data acquisition, curtiss wright ksm500, omega next, system 550, mcs, iads, sql, xml, c, rt station iena data format, engineering development cycle, engineering configuration management, irig106 standard, milstd1553, arinc429","arinc429, c, curtiss wright ksm500, data acquisition, engineering configuration management, engineering development cycle, iads, instrumentation, irig106 standard, mcs, milstd1553, omega next, rt station iena data format, sql, system 550, xml"
PL/SQL Developer / Data Analyst,HireKeyz Inc,"Annapolis, MD",https://www.linkedin.com/jobs/view/pl-sql-developer-data-analyst-at-hirekeyz-inc-3692568508,2023-12-17,Maryland,United States,Associate,Onsite,"Job Details
Role: PL/SQL Developer / Data Analyst
Location: Hybrid (Annapolis, MD) ; 3 days remote ; 2 days onsite
Duration: 12-month contract
SCOPE
Analyzing, testing, debugging and repairing PL/SQL based applications & packaging and configuring Oracle object components for deployment
Required Skills
Active Oracle PL/SQL certification
Twelve (12) or more years of SQL programming experience
Twelve (12) or more years of PL/SQL programming experience
The ability to:
Break down complex programming concepts into organized and concise units.
Work in an agile, self-motivated environment with multiple, concurrent priorities.
An understanding of relational data concepts and the ability to apply these concepts to concrete solutions.
Development skills with Oracle PL/SQL (Packages, Functions, Procedures, triggers), Materialized Views, DBLinks, and Oracle Jobs.
Strong analytical skills for evaluating, documenting, and enhancing existing and new data models.
Experience with:
Reverse Engineering and Refactoring PL/SQL code
Extraction, Transform and Load programming
Data Warehouse/ Data Mart design
Asynchronous event-driven applications
Knowledge of the following:
BI Publisher,
Java syntax,
SQL Server Reporting Services (SSRS),
XML syntax and parsing,
Domain knowledge of court related concepts and business activities.
TASKS
Analyzing, testing, debugging and repairing existing PL/SQL based applications.
Analyzing new business rules and interfaces in order to design and implement new Oracle objects.
Preparing and maintaining documentation on new and existing data models & programming objects
Packaging and configuring Oracle object components for deployment across Development, Test and Production Database Environments.
Working with Architects and Oracle DBA staff to establish standards for developing and deploying Oracle objects.
Assisting Business Intelligence staff with reporting requirements and solutions.
Collaborating with stakeholders across different business units to align data initiatives with business strategies.
Evaluating and enhancing SQL performance.
Preparing regular and ad hoc reports
Show more
Show less","PL/SQL, Oracle, SQL, Data Warehouse, Data Mart, Asynchronous eventdriven applications, Java, SQL Server Reporting Services (SSRS), XML, BI Publisher, Reverse Engineering, Refactoring, Extraction Transform and Load programming, Domain knowledge of court related concepts and business activities, Materialized Views, DBLinks, Oracle Jobs, Packages, Functions, Procedures, Triggers","plsql, oracle, sql, data warehouse, data mart, asynchronous eventdriven applications, java, sql server reporting services ssrs, xml, bi publisher, reverse engineering, refactoring, extraction transform and load programming, domain knowledge of court related concepts and business activities, materialized views, dblinks, oracle jobs, packages, functions, procedures, triggers","asynchronous eventdriven applications, bi publisher, data mart, datawarehouse, dblinks, domain knowledge of court related concepts and business activities, extraction transform and load programming, functions, java, materialized views, oracle, oracle jobs, packages, plsql, procedures, refactoring, reverse engineering, sql, sql server reporting services ssrs, triggers, xml"
Data Engineer,"Merit321, Launching Careers","Sykesville, MD",https://www.linkedin.com/jobs/view/data-engineer-at-merit321-launching-careers-3689230526,2023-12-17,Maryland,United States,Associate,Onsite,"Position
: Data Engineer
Location
: Columbia, Maryland (On-Site)
Clearance
: Secret Clearance Required
Description
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer.
Requirements
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Qualifications
Security Clearance -
Must have a current Secret level security clearance
and therefore all candidates must be a U.S. Citizen with a willingness to go to PLACEMENT MANAGER/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Engineer, Python, Java, Data Acquisition, Network Data, Kibana, Elasticsearch, JSON, XML, Kafka, NiFi, AWS S3 and SQS, Dataflow, Troubleshooting, Security+, NOSQL Databases, Accumulo, DevOps, AWS S3, AWS SQS, ETL, Data Labelling, Data Preparation","data engineer, python, java, data acquisition, network data, kibana, elasticsearch, json, xml, kafka, nifi, aws s3 and sqs, dataflow, troubleshooting, security, nosql databases, accumulo, devops, aws s3, aws sqs, etl, data labelling, data preparation","accumulo, aws s3, aws s3 and sqs, aws sqs, data acquisition, data labelling, data preparation, dataengineering, dataflow, devops, elasticsearch, etl, java, json, kafka, kibana, network data, nifi, nosql databases, python, security, troubleshooting, xml"
Data Engineer,"Merit321, Launching Careers","Sykesville, MD",https://www.linkedin.com/jobs/view/data-engineer-at-merit321-launching-careers-3713419751,2023-12-17,Maryland,United States,Associate,Onsite,"Position
: (6010) Data Engineer
Location
: Columbia, Maryland
Clearance
: Active TS, SCI required
Description
We are looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer.
Requirements
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a team to accomplish client objectives.
Qualifications
Security Clearance-Current active TS, SCI eligible
5 years total relevant experience
BS in Engineering, Computer Science, org technical degree or industry experience equivalent (+4 years relevant experience can substitute for a BS degree)
Experience with programming languages such as Python and Java.
Fluency with data extraction, custom translation development, and loading including data prep and labeling to enable data analytics.
Familiarity with various log formats such as JSON, XML, and others.
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must be willing to achieve required skill certification promptly
Willingness to do a programming challenge during the interview process.
Place of Performance: either Columbia, MD or San Antonio, TX. Both positions would involve Hybrid work.
7 years of total relevant experience
Experience with NOSQL databases such as Accumulo desired
CI Poly preferred
Experience with NOSQL databases such as Accumulo desired
Experience developing with Kubernetes environments
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Familiarity with the Agile Project team and task management environment.
Good communication skills
Experience with Zoom/Teams/Meet style team meetings
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Engineering, Programming Languages: Python Java, Data Extraction, Custom Translation Development, Data Loading, Data Preparation, Data Labeling, Data Analytics, Log Formats: JSON XML, Troubleshooting, System and Dataflow Issues, NOSQL Databases: Accumulo, Kubernetes Environments, Cyber and Network Security Operations, Data Flow Management, Data Storage Solutions: Kafka NiFi AWS S3 and SQS, Agile Project Management, Communication Skills","data engineering, programming languages python java, data extraction, custom translation development, data loading, data preparation, data labeling, data analytics, log formats json xml, troubleshooting, system and dataflow issues, nosql databases accumulo, kubernetes environments, cyber and network security operations, data flow management, data storage solutions kafka nifi aws s3 and sqs, agile project management, communication skills","agile project management, communication skills, custom translation development, cyber and network security operations, data engineering, data extraction, data flow management, data labeling, data loading, data preparation, data storage solutions kafka nifi aws s3 and sqs, dataanalytics, kubernetes environments, log formats json xml, nosql databases accumulo, programming languages python java, system and dataflow issues, troubleshooting"
Data Integration Analyst,"Changing Technologies, Inc.","Annapolis, MD",https://www.linkedin.com/jobs/view/data-integration-analyst-at-changing-technologies-inc-3715674974,2023-12-17,Maryland,United States,Associate,Onsite,"Job Description
Job Title:
Data Integration Analyst
Location:
1 Abney Ln, Annapolis, MD 21401
Job Type:
Hybrid
Contract:
1 Year with possible extensions
Pay Rate
: $62 /hr.
Candidate must have US Citizenship or Green Card/Permanent Residency in the US to be considered for this position.
Responsibilities
Installing, upgrading, and maintaining Oracle Data Integrator (ODI) in Production, test and QA servers.
Installing, upgrading, and maintaining Oracle Business Intelligence Enterprise Edition (OBIEE) in Production, test and QA servers.
Installing, upgrading, and maintaining Oracle Business Intelligence Applications (OBIA) in Production, test and QA servers.
Working with users to gather requirements for OBIA procurement, finance, custom OBIEE, OAS and BI publisher reports
Modifying OBIA repository, creating custom tables and joins for OBIA reports.
Creating custom reports in OBIA procurement and finance.
Writing custom SQL queries using PeopleSoft finance and procurement tables, to validate data in OBIA reports.
Customizing out of the box scenarios in ODI and adding it to the existing load plan.
Generating load plans in ODI and optimizing those using custom OBIA tables, modifying ODI interfaces and creating table indexes to reduce the ODI load running time.
Migrating ODI objects (Interface, Scenarios and Packages) between environments using smart export and import.
Creating report in OBIEE to track ODI load status and errors using SNP tables.
Working with DBAs and oracle support to rectify the runtime errors on ODI load plan.
Importing and exporting service instance using WebLogic scripting tool in AIX server to export and import presentation services catalog, data model and security policy metadata between environments in OBIEE 12C.
Running commands in Linux to import SSL certificate and key stores for active directory authentication.
Configuring security providers, and active directory using WebLogic administration console.
Creating users and adding them to groups in WebLogic admin console.
Creating roles and adding groups & users in WebLogic enterprise manager.
Using Linux commands to start, stop and navigate through files and folders in all Linux servers for OBIEE and ODI
Performing error diagnostics in OBIEE and ODI using server logs.
Installing, configuring, and migrating reports in OBIEE 12.2.1.0 to 12.2.1.3, and OAS 5.5 to 6.4.
Performing Authentication and Authorization (implement object and row level security).
Using Write back feature in OBIEE to implement and maintain the reports where users are able to enter data for expected budget and expenditure values and plan current and future Fiscal year Budget and expenses.
Planning, developing data model, and creating custom reports in OBIEE for Alternative Dispute Reports (ADRESS) based on user requirements.
Modeling the ADRESS repository by importing data warehouse tables, creating new physical and logical joins.
Creating custom templates in BI publisher, using bursting definition and schedule reports to be delivered to an FTP folder in a remote machine based on the requirement for domestic violence reports users.
Creating custom queries and views using oracle internet directory tables for security reports.
Maintaining EBAR, license, foster care and security reports using BI publisher.
Running Linux scripts on the server to rename DV reports BI publisher files and adding them to the server.
Creating and modifying repository and session variables in OBIEE.
Creating new tables and views in data warehouse
The Ability To
Develop reports, design rpd, administer server, and perform server upgrade in OBIEE 12C and OAS 5.5/6.4;
Create xml templates and associate with SQL code;
Read user requirements and understand the dependencies on roles;
Work closely with Oracle database administrators and Oracle xml servers' administrators and enterprise security groups; and,
Distinguish who the current user of the report is so that roles and report output can be designed accordingly.
Write technical communications for non-technical staff.
Experience
OBIEE security configuration (authentication and authorization),
Configuring active directory security in OBIEE 12C and OAS 5.5/6.4,
OBIA 12C, OAS 5.5/6.4, Procurement and Finance,
Oracle Data Integrator 12.2.1.4,
ODI customization, migration between environments and load plan generation
Skills Required
Bachelor degree in computer science or related field
Must have at least 10 year (or more) of experience in Oracle Business Intelligence Applications
Must have at least 10 year of experience in Web logic
Familiarity
OBIEE (Oracle Business Intelligence Enterprise Edition) 12C, OAS 5.5/6.4, WebLogic 12.2.1.4,
Write back feature in OBIEE 12C,
Oracle Bl publisher 12C and OAS 5.5/6.4,
Oracle PL/SQL and xml publisher
Schedule
8 hour shift
Monday to Friday
Show more
Show less","Oracle Data Integrator (ODI), Oracle Business Intelligence Enterprise Edition (OBIEE), Oracle Business Intelligence Applications (OBIA), SQL, Oracle Internet Directory, OBIEE 12C and OAS 5.5/6.4, RPD, XML templates, Active Directory, Linux, WebLogic, WebLogic 12.2.1.4, Oracle PL/SQL, OBIEE 12C, BI Publisher 12C and OAS 5.5/6.4","oracle data integrator odi, oracle business intelligence enterprise edition obiee, oracle business intelligence applications obia, sql, oracle internet directory, obiee 12c and oas 5564, rpd, xml templates, active directory, linux, weblogic, weblogic 12214, oracle plsql, obiee 12c, bi publisher 12c and oas 5564","active directory, bi publisher 12c and oas 5564, linux, obiee 12c, obiee 12c and oas 5564, oracle business intelligence applications obia, oracle business intelligence enterprise edition obiee, oracle data integrator odi, oracle internet directory, oracle plsql, rpd, sql, weblogic, weblogic 12214, xml templates"
Product Data Analyst,Kellton,"Baltimore, MD",https://www.linkedin.com/jobs/view/product-data-analyst-at-kellton-3747499881,2023-12-17,Maryland,United States,Associate,Onsite,"We have this W2 opportunity for Product Data Analyst with our Top Financial Client. Please let us know if this is something that interests you.
Title
: Product Data Analyst
Location
: Baltimore, MD(Hybrid 3 days onsite 2 days WFH)
Max Pay Rate
: $21/hr on W2
Primary Responsibilities
The team is responsible for ensuring the accuracy and completeness of Product data for the firm.
Responsibilities
The candidate will report to the Product Operations team leader and have diverse responsibilities, including but not limited to the following:
Identifying, defining and coordinating development of rules to monitor the data quality of securities reference data (incl. equities, listed derivatives and fixed-income products and pricing) against external data providers (incl. Bloomberg and Reuters) and direct exchange feeds.
Acquiring an intricate knowledge of the Firms product data, flow of the data in the firm systems, impact of poor quality data, building the ability to understand the wider environment and question data integrity.
Root-cause analysis of data quality exceptions to determine trends in the data, identify inaccuracies in external data providers' feeds and recommend opportunities to increase efficiency and productivity.
Communication and organizational skills as well as attention to detail and a readiness to escalate issues with a sense of urgency are of importance.
Escalation of potential/current risks.
Skills Required
Some understanding of financial services.
Excellent verbal and written communication abilities.
Ability to work in a team.
Ability to quickly digest new information and learn new businesses and processes.
Result-focused mind-set.
Skills Desired
(Highly Preferred) Finance Graduate (B.Com/M.Com/MBA in Finance/Equivalent Degree in Finance)
Ability to work well under pressure.
Proficient in Microsoft Office.
Exposure to Technical Skills (sql, database, macros, etc)
Show more
Show less","Product Data Analysis, Data Quality Assurance, Securities Reference Data, Bloomberg, Reuters, Data Integrity, RootCause Analysis, Financial Services, Verbal Communication, Written Communication, Teamwork, Information Absorption, Process Learning, Result Orientation, Finance Graduate, Microsoft Office, SQL, Database, Macros","product data analysis, data quality assurance, securities reference data, bloomberg, reuters, data integrity, rootcause analysis, financial services, verbal communication, written communication, teamwork, information absorption, process learning, result orientation, finance graduate, microsoft office, sql, database, macros","bloomberg, data integrity, data quality assurance, database, finance graduate, financial services, information absorption, macros, microsoft office, process learning, product data analysis, result orientation, reuters, rootcause analysis, securities reference data, sql, teamwork, verbal communication, written communication"
Data Operations Engineer - Digital Marketing,"Stanley Black & Decker, Inc.","Towson, MD",https://www.linkedin.com/jobs/view/data-operations-engineer-digital-marketing-at-stanley-black-decker-inc-3656748387,2023-12-17,Maryland,United States,Associate,Onsite,"The Global Tools & Storage (GTS) division of Stanley Black & Decker is currently hiring a full-time Data Ops Engineer to join the Global Customer Experience (GCX) team.
In this role, you will support the marketing organization in operationalizing data. You will work with cross-functional and external resources to build and optimize the environment and processes needed to efficiently manage data and derive value – from marketing personalization to analytics.
The Data Ops Engineer will hold primary responsibility for expanding the scope of our Customer Data Platform (CDP):
Build and maintain the data-based structures and systems to collect and arrange data for 12+ brands, 3 regions and 30+ markets
Automate the data flow across the organization, from aggregation to reporting, supporting the Brand marketing, email marketing and analytics teams
Serve as a trusted partner between IT and the business to ensure constant connectivity between IT and the marketing owned systems, including building new data pipelines to Treasure Data CDP
Ensure compliance of all data into defined global customer data platform architecture, ensuring constant connectivity and data privacy
Serve as an administrator of the Treasure Data Customer Data Platform
Assist in the guidance of the overall data and personalization strategy
Operate within an agile process to support always-on marketing and campaign audience delivery to the digital experience team
Manage technical relationship with our customer data technology partner
Provide oversight, governance, and measurement of customer data technology initiatives
Track and measure the overall health and performance of customer data technology delivery initiatives
Support world-class, enterprise digital solutions that meet business and brand objectives and customer needs while respecting time, budget, and technical considerations
Who You Are:
BA or BS required.
Minimum of Five (5) years’ experience in Data Ops, data engineering or related technical field.
Deep understanding and experience utilizing Treasure Data CDP (certification a plus)
Development of ETL pipelines using Python & SQL
Experience in SQL optimization and performance tuning
Experience with data modeling and building high-volume ETL pipelines
Experience in architecting and deploying customer data platforms
Working knowledge and understanding of Google Tag Manager, marketing platforms like Iterable, dashboard software like Datorama, and content management systems like Drupal
Rich understanding of evolving data privacy landscape and the impact on business
Excellent time- and project-management skills with ability to own multiple projects
Strong leadership and communication skills – verbal, written, and interpersonal
Superb interpersonal skills to work in a complex, matrixed organization through being a team player and working independently
Proven track record of managing cross-platform digital projects with large groups of distributed stakeholders and project team members
We Don’t Just Build The World, We Build Innovative Technology Too.
Joining the Stanley Black & Decker team means working in an innovative, tech-driven and highly collaborative team environment supported by over 58,000 professionals in 60 countries across the globe. Here, you’ll get the unique chance to impact some of the world’s most iconic brands including STANLEY TOOLS, DEWALT, CRAFTSMAN, MAC TOOLS and Black + Decker. Your ideas and solutions have the potential to reach millions of customers as we work together to write the next chapter in our history. Come build with us and take your career to new heights.
Who We Are
We’re the World’s largest tool company. We’re industry visionaries. We’re solving problems and advancing the manufacturing trade through innovative technology and our Industry 4.0 Initiative. We are committed to ensuring our state-of-the-art “smart factory” products and services provide greater quality to our customers & greater environmental and social value to our planet. We are unique in that we have a rich and storied history dating back to 1843, but that hasn't stopped us from evolving into a vibrant, diverse, global growth company.
Benefits & Perks
You’ll get a competitive salary and a comprehensive benefits plan that includes medical, dental, life, vision, wellness program, disability, retirement benefits, Employee Stock Purchase Plan, Paid Time Off, including paid vacation, holidays & personal days, and tuition reimbursement. And, of course, discounts on Stanley Black & Decker tools and products and well as discount programs for many other vendors and partners.
What You’ll Also Get
Career Opportunity: Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.
Learning & Development:
Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities).
Diverse & Inclusive Culture:
We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too.
Purpose-Driven Company:
You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices.
EEO Statement:
All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran’s status or any other protected characteristic.
Show more
Show less","Data Ops Engineering, ETL pipelines, Data modeling, Python, SQL, SQL optimization, Google Tag Manager, Iterable, Datorama, Drupal, Data privacy, Treasure Data CDP","data ops engineering, etl pipelines, data modeling, python, sql, sql optimization, google tag manager, iterable, datorama, drupal, data privacy, treasure data cdp","data ops engineering, data privacy, datamodeling, datorama, drupal, etl pipelines, google tag manager, iterable, python, sql, sql optimization, treasure data cdp"
Customer Data Platform & Marketing Automation Developer,W. L. Gore & Associates,"Elkton, MD",https://www.linkedin.com/jobs/view/customer-data-platform-marketing-automation-developer-at-w-l-gore-associates-3745507747,2023-12-17,Maryland,United States,Associate,Onsite,"About the Role:
We are looking for a Developer for our Customer Data Platform (CDP) and Marketing Automation Tool who will be responsible for strategic and tactical initiatives around this capability and technologies. To include design, configuration, development, testing, deployment, and life cycle activities within the applications to meet business needs. As the developer, you will partner with the Application Owner, Business Analyst, Architect, and Product Owner to support and maintain the application to meet business outcomes and IT standards. The developer is responsible for ensuring documentation and development activities. You will act as a representative of the application to business and IT stakeholders to coordinate the needed IT services to ensure effective engagement between the business process and IT.
This position will be located at a US East facility (Newark, Delaware or Elkton, Maryland) or US West facility (Flagstaff or Phoenix, Arizona), with the possibility of a hybrid remote work arrangement, depending upon the responsibilities of the role and business needs.
Responsibilities:
Develop, guide and perform the CDP/marketing automation solution application design and customizations
Design CDP/marketing automation solutions and technologies, working closely with delivery leads, business analysts, solution architects, developers and product owners to architect technology solutions to meet business needs
Develop solutions within CDP/marketing automation based on business requirements
Drive the creation of application and technical design documents which leverage Gore best practices, author technical design specifications
Stay up to date with technology trends to ensure reference architecture and strategies stays current; understand successes and failures in the marketplace to better guide the firm's architecture strategy
Act as the Tier 2 and 3 support for CDP/marketing automation issue resolution
Coordinate and manage vendors supporting CDP/marketing automation applications
Create and oversee all testing activities in the CDP/marketing automation applications
Analyze periodic application releases and conduct impact analysis on the existing solution design, and develop and/or validate all application technical design specs
Evaluate the product backlog and own all ‘level of effort’ estimation activities
Required Qualifications:
Minimum 3 years of experience integrating with disparate systems including AWS, DQ Services and Salesforce.com
Prior experience and proficiency in JavaScript, Python, .NET and related technologies
Experience in web and micro-services architectures including knowledge in API security, and standards for REST, SOAP and API Gateways
Knowledge of REST based services executing POST, PUT, PATCH, DELETE, etc.
Familiarity with marketing automation and data management capabilities
Demonstrated ability in Middleware technologies to ETL data across systems
Ability and desire to learn more about machine learning, artificial intelligence and other automations to improve efficiencies
Ability to influence organizational priorities and lead an environment driven by customer service and teamwork
Excellent verbal/written communication, collaboration, analytical and presentation skills
Ability to travel up to 5% since this is a hybrid work opportunity; candidates must be flexible to work across Global time zones
Desired Qualifications:
Experience with Microsoft Dynamics Tech Stack including Power Platform, Power Query, Power Automate, Power BI, DataVerse and Azure platform
Knowledge of Salesforce Sales Cloud, the development of customer 360 use cases into the CRM user experience, and experience with developing with LWC’s
Hybrid Working Arrangements are permitted for Associates in the continental United States (US) and Canada, with appropriate approval and compliance with Gore’s hybrid working policies, from the country in which they are employed.
What We Offer:
Our success is based on the capability and creativity of our Associates, and we are proud to offer a comprehensive and competitive total rewards program that supports your everyday and helps you build your tomorrow.
We provide benefits that offer choice and flexibility and promote overall well-being. And in keeping with our belief that every Associate should share in the collective success of the enterprise; we provide a distinctive Associate Stock Ownership Plan in each country as well as potential opportunities for “profit-sharing”. Learn more at gore.com/careers/benefits
We believe in the strength of a diverse and inclusive workplace. With diverse perspectives, ideas and experiences, we uncover new possibilities and make a greater impact in the world. We are proud of Associates for building on our rich history of innovation, upholding our values and supporting an inclusive work environment where we treat each other and our external partners with fairness, dignity and respect.
Gore is an equal opportunity employer. We welcome all applications irrespective of race, color, religion, sex, gender, national origin, ancestry, age, status as a qualified individual with a disability, genetic information, pregnancy status, medical condition, marital status, sexual orientation, status as a covered veteran, gender identity and expression, and any other characteristic protected by applicable laws and regulations.
Gore is committed to a drug-free workplace. All employment is contingent upon successful completion of drug and background screening. Gore will consider qualified applicants with criminal histories, e.g., arrest and conviction records, in a manner consistent with the requirements of applicable laws.
Gore requires all applicants to be eligible to work within the U.S. Gore generally will not sponsor visas unless otherwise noted on the position description.
Our Talent Acquisition Team welcomes your questions at gore.com/careers/contact
Show more
Show less","CDP, AWS, Salesforce.com, JavaScript, Python, .NET, REST, SOAP, API Gateways, Machine Learning, Artificial Intelligence, Power Platform, Power Query, Power Automate, Power BI, DataVerse, Azure, Salesforce Sales Cloud, LWC's","cdp, aws, salesforcecom, javascript, python, net, rest, soap, api gateways, machine learning, artificial intelligence, power platform, power query, power automate, power bi, dataverse, azure, salesforce sales cloud, lwcs","api gateways, artificial intelligence, aws, azure, cdp, dataverse, javascript, lwcs, machine learning, net, power automate, power platform, power query, powerbi, python, rest, salesforce sales cloud, salesforcecom, soap"
(5461) Data Analyst,"Merit321, Launching Careers","Sykesville, MD",https://www.linkedin.com/jobs/view/5461-data-analyst-at-merit321-launching-careers-3698421325,2023-12-17,Maryland,United States,Associate,Hybrid,"Position:
(5461) Data Analyst
Location:
Ft. Gordon, GA (Hybrid)
Clearance:
Active Secret Clearance
Our client is seeking an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customers mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on-customer site support.
Essential Job Responsibilities
Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Minimum Qualifications
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, Client terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Desired Skills (Optional)
Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Analysis, Statistical Techniques, Data Exploration, Data Analytics, Data Integrity, Data Normalization, Data Sources, Data Security, Metadata, Data Storage Structures, Data Mining, Data Cleansing, SQL, XML, Javascript, ETL Frameworks, Business Objects, Reporting Packages, Databases, Relational Databases, Data Models, Database Design Development, Data Mining Techniques, Segmentation Techniques","data analysis, statistical techniques, data exploration, data analytics, data integrity, data normalization, data sources, data security, metadata, data storage structures, data mining, data cleansing, sql, xml, javascript, etl frameworks, business objects, reporting packages, databases, relational databases, data models, database design development, data mining techniques, segmentation techniques","business objects, data exploration, data integrity, data mining, data mining techniques, data models, data normalization, data security, data sources, data storage structures, dataanalytics, database design development, databases, datacleaning, etl frameworks, javascript, metadata, relational databases, reporting packages, segmentation techniques, sql, statistical techniques, xml"
Database Engineer,GliaCell Technologies,"Linthicum Heights, MD",https://www.linkedin.com/jobs/view/database-engineer-at-gliacell-technologies-3787755552,2023-12-17,Maryland,United States,Mid senior,Onsite,"Are you a Database Engineer who is ready for a new challenge that will launch your career to the next level?
Tired of being treated like a company drone?
Tired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?
Our engineers were certainly tired of the same.
At GliaCell our slogan is “We make It happen”.
We will immerse you in the latest technologies.
We will develop and support your own personalized training program to continue your individual growth.
We will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.
Culture isn’t something you need to talk about…if it just exists.
If this sounds interesting to you, then we’d like to have a discussion regarding your next adventure! If you want to be a drone, this isn’t the place for you.
We Make It Happen!
GliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.
GliaCell’s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.
We Offer:
Long term job security
Competitive salaries & bonus opportunities
Challenging work you are passionate about
Ability to work with some amazingly talented people
Job Description:
GliaCell is seeking a
Database Engineer
on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.
Position Description:
The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis.
Develop or maintain database structure to fit into the overall architecture of the system.
Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software.
Support the development and test of various Python-based REST end points, microservices, and data model management capabilities utilizing Django and Flask frameworks to interact with data models such as MariaDB, MongoDB, and PostgreSQL and send data upon request, in JSON format, to UI front ends.
Required Skills:
Database experience using MongoDB or MariaDB; including deployment and management of the database itself, debugging of optimization issues, and scaling.
Desired Skills:
Python, Django or Flask, REST Endpoint Development, Micro-Service Model, Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment
Key Requirements:
To be considered for this position you must have the following:
Possess an active or rein-statable TS/SCI with Polygraph security clearance.
U.S. Citizenship.
Works well independently as well as on a team.
5+ years with a Bachelor’s in Computer Science (or related field) or 10 years without the degree.
Strong communication skills.
Location:
Linthicum Heights, MD
Salary:
Based on Education, Years of Experience, Skill and Abilities
Check Out Our Benefits:
Paid Time Off
Medical, Dental & Vision Benefits
Life & Disability Insurance
Tuition, Training & Certification Reimbursement
401K Contribution
Employee Referral Bonus Program
Equipment Reimbursement
Team Engagement & Outings
Swag
…And more!
Learn more about GliaCell Technologies:
To apply for this position, respond to this job posting and attach an updated resume for us to review.
GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Powered by JazzHR
JtCmXg9W2w
Show more
Show less","Database design, Database development, Database implementation, Data storage and retrieval, Data analysis, Python, Django, Flask, REST Endpoint Development, MicroService Model, Swagger, AWS, C2S, Docker, Visual Studio Code, IDEs, JSON, XML, Jira, Confluence, Git, Polygraph security clearance, Active TS/SCI security clearance, U.S. Citizenship, Agile environment, MariaDB, MongoDB, PostgreSQL","database design, database development, database implementation, data storage and retrieval, data analysis, python, django, flask, rest endpoint development, microservice model, swagger, aws, c2s, docker, visual studio code, ides, json, xml, jira, confluence, git, polygraph security clearance, active tssci security clearance, us citizenship, agile environment, mariadb, mongodb, postgresql","active tssci security clearance, agile environment, aws, c2s, confluence, data storage and retrieval, dataanalytics, database design, database development, database implementation, django, docker, flask, git, ides, jira, json, mariadb, microservice model, mongodb, polygraph security clearance, postgresql, python, rest endpoint development, swagger, us citizenship, visual studio code, xml"
Database Engineer,"Farfield Systems, Inc","Linthicum, MD",https://www.linkedin.com/jobs/view/database-engineer-at-farfield-systems-inc-3787762774,2023-12-17,Maryland,United States,Mid senior,Onsite,"About Farfield Systems, Inc
At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member.
Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years.
""Employee driven...customer focused."" We build, operate and secure networks and infrastructure.
*** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship***
Database Engineer 2:
As part of the Secure the Enterprise initiative, develop capabilities to shift from the current manual system security evaluation and authorization process to a new model that emphasizes automation, streamlined processes and approvals, continuous monitoring and assessment, and network data gathering across the entire life cycle of a project.
The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis.
The Database Engineer will develop or maintain database structure to fit into the overall architecture of the system.
The Database Engineer will create new workflows to take over existing processes as needed as well as provide break/fix requests or updates.
The Database Engineer will lead development of database structures, database parser software, and database loading software.
The Database Engineer will support the development and test of various Python based ReST end points, microservices, and data model management capabilities utilizing Django and Flask frameworks to interact with data models such as mariaDB, MongoDB, and PostgreSQL and send data upon request, in JSON format, to UI front ends.
What we are expecting from you (i.e. the qualifications you must have):
Active TS/SCI w/ Polygraph Clearance
Python
Experience using MongoDB or MariaDB
Experience using Django or Flask
ReST endpoint
Micro services
Five (5) years of experience as a DBE in programs and contracts of similar scope, type, and complexity within the Federal Government is required. Bachelor’s Degree in a technical discipline from an accredited college or university is required. Five (5) years of DBE experience may be substituted for a bachelor’s degree
What we are desiring from you (i.e. the nice-to-have qualifications):
Swagger
AWS, C2S or other cloud experience
Docker
Visual Studio Code or similar IDEs
JSON and/or XML serialization
Jira
Confluence
Git version control
Agile
Database Engineer 3:
Take responsibility for directing the development of complex systems using queries, tables, and database storage and retrieval using Cloud methodologies for the design, development, implementation, information storage and retrieval, data flow, and analysis. The Database Engineer will work closely with our development team and be responsible for leading the development of database structures, database parser software, and database loading software. The successful candidate will be required to fulfill requirements from project inception to conclusion and provide break/fix requests or updates.
Essential Duties:
Direct the development of complex systems using queries, tables, and database storage and retrieval using Cloud methodologies for the design, development, implementation, information storage and retrieval, data flow, and analysis.
Direct the overall database structure to fit into the overall architecture of the system.
Create new workflows to take over existing processes as needed.
Provide break/fix requests or updates.
Lead the development of database structures, database parser software, and database loading software.
Direct fulfillment of requirements from project inception to conclusion.
You will excel in this role if you are:
Looking to play a critical role in our team; you are motivated and detail-oriented candidate who can work independently and as part of a team.
What we are expecting from you (i.e. the qualifications you must have):
Active TS/SCI Clearance w/ Polygraph
Ten (10) years of experience in programs and contracts of similar scope, type, and complexity within the Federal Government is required. Bachelor’s Degree in a technical discipline from an accredited college or university is required. Five (5) years of additional database engineering experience may be substituted for a Bachelor’s Degree.
Strong programming skills in Python, Django or Flask.
Experience with database development using MongoDB or MariaDB.
Experience with ReST endpoint development and microservice models.
Knowledge of JSON and/or XML serialization.
Experience working in an Agile environment.
What we are desiring from you (i.e. the nice-to-have qualifications):
Experience with Swagger, AWS, C2S, or other cloud experience.
Knowledge of Docker and Visual Studio Code or similar IDEs.
Familiarity with Jira, Confluence, and Git version control.
Database Engineer 2
(Data Management):
Essential Duties: The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develop or maintain database structure to fit into the overall architecture of the system. Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software. Support the development and test of various Python based ReST end points, microservices, and data model management capabilities utilizing Django and Flask frameworks to interact with data models such as mariaDB, MongoDB, and PostgreSQL and send data upon request, in JSON format, to UI front ends.
Required: Python, Django or Flask, Database experience using MongoDB or MariaDB, ReST endpoint development, Micro service model
Desired: Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment
Database Engineer 3
(Data Management)
Essential Duties: The Database Engineer will direct the development of complex systems using queries, tables, and database storage and retrieval using Cloud methodologies for the design, development, implementation, information storage and retrieval, data flow and analysis. Direct the overall database structure to fit into the overall architecture of the system. Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software. Direct fulfillment of requirements from a project inception to conclusion.
Required: Python, Django or Flask, Database experience using MongoDB or MariaDB, ReST endpoint development, Micro service model
Desired: Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment
Powered by JazzHR
xZyLetJskQ
Show more
Show less","Django, Flask, MariaDB, MongoDB, PostgreSQL, Python, Docker, AWS, C2S, Visual Studio Code, Jira, Confluence, Git, Swagger, Agile, SQL, Database design, ReST, Microservice, Cloud computing, Data analysis, Data storage, Data retrieval, Data flow, JSON, XML","django, flask, mariadb, mongodb, postgresql, python, docker, aws, c2s, visual studio code, jira, confluence, git, swagger, agile, sql, database design, rest, microservice, cloud computing, data analysis, data storage, data retrieval, data flow, json, xml","agile, aws, c2s, cloud computing, confluence, data flow, data retrieval, data storage, dataanalytics, database design, django, docker, flask, git, jira, json, mariadb, microservice, mongodb, postgresql, python, rest, sql, swagger, visual studio code, xml"
Senior Database Engineer,Columbia Technology Partners,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-at-columbia-technology-partners-3787913617,2023-12-17,Maryland,United States,Mid senior,Onsite,"Description:
Provides technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develops relations and/or Object-Oriented databases, database parser software, and database loading software. Project long-range requirements for database administration and design. Responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls.
The DBE works primarily;
At the front end of the life-cycle requirements through system acceptance testing and Initial Operation Capability (IOC)
Assist with recommendations for, and analysis and evaluation of systems improvements, optimization, development, and/or maintenance efforts.
Administer and engineer database solutions to enable the efficient SQL licensing posture and a more reliable undercarriage for IT services and OS management software.
Translates a set of requirements and data into a useable document by creating or recreating ad hoc queries, scripts and macros;
Updates existing queries, creates new ones to manipulate data into a master file; and builds complex systems using queries, tables, Open Database Connectivity and database storage.
Manage SQL Databases and perform database performance tuning and backup.
Follow and improve ELT/ETL data base design patterns, document and provide support.
Qualifications:
An Active TS/SCI clearance + FS polygraph
Ten (10) years’ experience as a DBE in programs and contracts of similar scope, type and complexity is required.
Bachelor’s degree in a technical discipline from an accredited college or university is required.
Five (5) years of DBE experience may be substituted for a bachelor’s degree.
The candidate must have IAT level 2 certification to meet DoD 8570 requirement.
Demonstrated experience with SQL Server
Columbia Technology Partners is an Equal Opportunity Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Our EEO policy reflects our commitment to ensure equality and promote diversity and inclusion in the workplace. Our policy applies to all employees, job candidates, contractors, stakeholders, partners, and visitors.
Powered by JazzHR
KVZPtIrHN6
Show more
Show less","Database Design, Database Development, Database Implementation, Information Storage, Information Retrieval, Data Flow, Data Analysis, Relational Databases, ObjectOriented Databases, Database Parser Software, Database Loading Software, Database Architecture, Data Volumes, Number of Users, Logical Distribution, Physical Distribution, Response Times, Retention Rules, Security Controls, Domain Controls, System Acceptance Testing, Initial Operation Capability (IOC), Systems Improvements, Systems Optimization, Systems Development, Systems Maintenance, SQL Licensing, IT Services, OS Management Software, Ad Hoc Queries, Scripts, Macros, Data Manipulation, Master Files, Complex Systems, Open Database Connectivity, Database Storage, Database Performance Tuning, Database Backup, ELT Data Base Design Patterns, ETL Data Base Design Patterns, IAT Level 2 Certification, SQL Server","database design, database development, database implementation, information storage, information retrieval, data flow, data analysis, relational databases, objectoriented databases, database parser software, database loading software, database architecture, data volumes, number of users, logical distribution, physical distribution, response times, retention rules, security controls, domain controls, system acceptance testing, initial operation capability ioc, systems improvements, systems optimization, systems development, systems maintenance, sql licensing, it services, os management software, ad hoc queries, scripts, macros, data manipulation, master files, complex systems, open database connectivity, database storage, database performance tuning, database backup, elt data base design patterns, etl data base design patterns, iat level 2 certification, sql server","ad hoc queries, complex systems, data flow, data manipulation, data volumes, dataanalytics, database architecture, database backup, database design, database development, database implementation, database loading software, database parser software, database performance tuning, database storage, domain controls, elt data base design patterns, etl data base design patterns, iat level 2 certification, information retrieval, information storage, initial operation capability ioc, it services, logical distribution, macros, master files, number of users, objectoriented databases, open database connectivity, os management software, physical distribution, relational databases, response times, retention rules, scripts, security controls, sql licensing, sql server, system acceptance testing, systems development, systems improvements, systems maintenance, systems optimization"
Data Management Software Engineer,Columbia Technology Partners,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/data-management-software-engineer-at-columbia-technology-partners-3787909777,2023-12-17,Maryland,United States,Mid senior,Onsite,"Description:
Facilitate the incremental migration of existing Agency Human Capital Management (HCM) data from the PeopleSoft HCM system to the new Cloud HCM environment.
Develop and support modifications/extensions to the project's source data extraction, source data error identification, and source data transformation processes
Support loading data, monitoring progress, and reporting errors.
Support, compile, and summarize identified data issues and proposed fixes to the Maryland Program Office (MPO) for review and decision on resolution.
Correct data ingest/validation issues.
Take a structured approach to decompose, identify, and define data handling solutions to meet role based access control (RBAC) and attribute based access control (ABAC) requirements through a variety of tools and techniques.
Address client data tagging and Enterprise Data Header (EDH) requirements enabling the existing data model to be extended to capture information such as classification, sensitivity tags, and controls at the element level.
Use the Cloud HCM data disposal tool to perform data deletion and purging.
Support data governance using the Cloud HCM single authority data source to the element level and controls.
Provide a publicly accessible data dictionary with element level conformal definitions that are continuously maintained; providing and capturing all additional client required elements during implementation.
Move historical data from the legacy HCM system to a Database as a Service (DBaaS) Virtual Machine (VM) instance.
Use Cloud HCM to manage data quality by leveraging build edits and a rules engine for data entry to control data as it is entered, and through workflows.
Implement Mass Personnel Action Processing (PAR) corrections using automated methods to correct data.
Provides support to client Master Data Management (MDM) initiatives through data models, data controls, integration capabilities.
Qualifications:
An Active TS/SCI clearance + FS polygraph
Security+ Certification
Fourteen (14) years’ experience as an engineer in programs and contracts of similar scope, type, and complexity
Functional experience in Peoplesoft HRMS 9.2
Technical experience in PeopleTools, including, but not limited to, developing & supporting Application Engines, PeopleCode, Data Mover, and Peoplesoft integration tools
Technical experience in Oracle RDBMS development, preferably SQL, PL/SQL
Experience or demonstrable ability to learn other technologies and tools, including, but not limited to, Oracle ODI and Oracle Cloud SaaS and PaaS solutions
Technical flexibility to work with on-premises and cloud-based tools and applications
Bachelor’s degree in a technical discipline from an accredited college or university is required
Five (5) years of additional experience on projects with similar processes may be substituted for a bachelor’s degree
Columbia Technology Partners is an Equal Opportunity Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Our EEO policy reflects our commitment to ensure equality and promote diversity and inclusion in the workplace. Our policy applies to all employees, job candidates, contractors, stakeholders, partners, and visitors.
Powered by JazzHR
Z2ZiNSEOZJ
Show more
Show less","PeopleSoft HCM, Cloud HCM, Data extraction, Data transformation, Data loading, Error monitoring, Data reporting, Data governance, Role based access control (RBAC), Attribute based access control (ABAC), Data tagging, Enterprise Data Header (EDH), Cloud HCM data disposal tool, Data quality management, Build edits, Rules engine, Mass Personnel Action Processing (PAR), Master Data Management (MDM), PeopleTools, Application Engines, PeopleCode, Data Mover, Oracle RDBMS, SQL, PL/SQL, Oracle ODI, Oracle Cloud SaaS, Oracle Cloud PaaS","peoplesoft hcm, cloud hcm, data extraction, data transformation, data loading, error monitoring, data reporting, data governance, role based access control rbac, attribute based access control abac, data tagging, enterprise data header edh, cloud hcm data disposal tool, data quality management, build edits, rules engine, mass personnel action processing par, master data management mdm, peopletools, application engines, peoplecode, data mover, oracle rdbms, sql, plsql, oracle odi, oracle cloud saas, oracle cloud paas","application engines, attribute based access control abac, build edits, cloud hcm, cloud hcm data disposal tool, data extraction, data governance, data loading, data mover, data quality management, data reporting, data tagging, data transformation, enterprise data header edh, error monitoring, mass personnel action processing par, master data management mdm, oracle cloud paas, oracle cloud saas, oracle odi, oracle rdbms, peoplecode, peoplesoft hcm, peopletools, plsql, role based access control rbac, rules engine, sql"
Senior Database Engineer,GliaCell Technologies,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-at-gliacell-technologies-3787754622,2023-12-17,Maryland,United States,Mid senior,Onsite,"Are you a Senior Database Engineer who is ready for a new challenge that will launch your career to the next level?
Tired of being treated like a company drone?
Tired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?
Our engineers were certainly tired of the same.
At GliaCell our slogan is “We make It happen”.
We will immerse you in the latest technologies.
We will develop and support your own personalized training program to continue your individual growth.
We will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.
Culture isn’t something you need to talk about…if it just exists.
If this sounds interesting to you, then we’d like to have a discussion regarding your next adventure! If you want to be a drone, this isn’t the place for you.
We Make It Happen!
GliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.
GliaCell’s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.
We Offer:
Long term job security
Competitive salaries & bonus opportunities
Challenging work you are passionate about
Ability to work with some amazingly talented people
Job Description:
GliaCell is seeking a
Senior Database Engineer
on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.
Key Requirements:
To be considered for this position you must have the following:
Possess an active or rein-statable TS/SCI with Polygraph security clearance.
Be a U.S. Citizen.
Bachelor of Science Degree in Computer Science (or a related subject) (4 years of work experience can be substituted for the degree).
20+ years of experience.
Experience with Oracle database administration.
Experience with Extract, Translate, and Load (ETL) operations.
An understanding of dataflow into Oracle.
An understanding of TechSIGINT data.
Location:
Annapolis Junction, Maryland
Salary:
Based on Education, Years of Experience, Skill, and Abilities
Check Out Our Benefits:
Paid Time Off
Medical, Dental & Vision Benefits
Life & Disability Insurance
Tuition, Training & Certification Reimbursement
401K Contribution
Employee Referral Bonus Program
Equipment Reimbursement
Team Engagement & Outings
Swag
…And more!
Learn more about GliaCell Technologies:
To apply for this position, respond to this job posting and attach an updated resume for us to review.
GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Powered by JazzHR
SE468twMSF
Show more
Show less","Active TS/SCI with Polygraph security clearance, U.S. Citizen, Oracle database administration, ETL operations, Dataflow into Oracle, TechSIGINT data, Agile Software Development, FullStack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, DevOps Containerization, CND, CNE, CNO, Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, Prevent Advanced Persistent Threat","active tssci with polygraph security clearance, us citizen, oracle database administration, etl operations, dataflow into oracle, techsigint data, agile software development, fullstack application development, big data, cloud technologies, analytics, machine learning, ai, devops containerization, cnd, cne, cno, threat mitigation, vulnerability exposure, penetration testing, threat hunting, prevent advanced persistent threat","active tssci with polygraph security clearance, agile software development, ai, analytics, big data, cloud technologies, cnd, cne, cno, dataflow into oracle, devops containerization, etl operations, fullstack application development, machine learning, oracle database administration, penetration testing, prevent advanced persistent threat, techsigint data, threat hunting, threat mitigation, us citizen, vulnerability exposure"
Senior Database Engineer,GliaCell Technologies,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-at-gliacell-technologies-3787757344,2023-12-17,Maryland,United States,Mid senior,Onsite,"Are you a Senior Database Engineer who is ready for a new challenge that will launch your career to the next level?
Tired of being treated like a company drone?
Tired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?
Our engineers were certainly tired of the same.
At GliaCell our slogan is “We make It happen”.
We will immerse you in the latest technologies.
We will develop and support your own personalized training program to continue your individual growth.
We will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.
Culture isn’t something you need to talk about…if it just exists.
If this sounds interesting to you, then we’d like to have a discussion regarding your next adventure! If you want to be a drone, this isn’t the place for you.
We Make It Happen!
GliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.
GliaCell’s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.
We Offer:
Long term job security
Competitive salaries & bonus opportunities
Challenging work you are passionate about
Ability to work with some amazingly talented people
Job Description:
GliaCell is seeking a
Senior Database Engineer
on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.
Position Description:
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.
Support the development of long and short term requirements for database administration and design.
Support the analysis and evaluation of system improvements, optimization, development and/or maintenance efforts.
Translate a set of requirements and data into a usable database schema.
Create and update data models using PowerDesigner
Assist in developing database structures that fit into the overall architecture of the system under development.
Lead development of database structures that fit into the overall architecture of the system under development.
Develop requirement recommendations from a project’s inception to its conclusion for a particular Business and IT subject matter area (i.e. simple to complex systems).
Develop a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls.
Direct the overall database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls.
Direct fulfillment of requirements from a project’s inception to it conclusion.
Direct organization of requirements and data into a usable database schema by directing development of ad hoc queries, scripts, macros, updates to existing queries.
Direct the development of complex systems using queries, tables, Open - Database Connectivity and database storage and retrieval
Required Skills:
Oracle, Linux, SQL, scripting
Desired Skills:
Oracle Data Guard, Jira, Confluence, relational database design tools such as PowerDesigner
Key Requirements:
To be considered for this position you must have the following:
Possess an active or rein-statable TS/SCI with Polygraph security clearance.
U.S. Citizenship.
Works well independently as well as on a team.
10+ years of experience as a DBE in programs and contracts of similar scope, type, and complexity is required. A bachelor’s degree in a technical discipline from an accredited college or university is required. Five (5) years of DBE experience may be substituted for a bachelor’s degree.
Strong communication skills.
May require database engineering on-call support for troubleshooting and correcting operational system issues. 4 hours of inconvenience pay will be provided for any weeks on call.
Location:
Annapolis Junction, MD / Hanover, MD
Salary:
Based on Education, Years of Experience, Skill and Abilities
Check Out Our Benefits:
Paid Time Off
Medical, Dental & Vision Benefits
Life & Disability Insurance
Tuition, Training & Certification Reimbursement
401K Contribution
Employee Referral Bonus Program
Equipment Reimbursement
Team Engagement & Outings
Swag
…And more!
Learn more about GliaCell Technologies:
To apply for this position, respond to this job posting and attach an updated resume for us to review.
GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Powered by JazzHR
cAqWrTTS0P
Show more
Show less","Oracle, Linux, SQL, Scripting, Oracle Data Guard, Jira, Confluence, PowerDesigner, FullStack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, DevOps Containerization, CND, CNE, CNO, Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, Preventing Advanced Persistent Threat, Agile Software Development","oracle, linux, sql, scripting, oracle data guard, jira, confluence, powerdesigner, fullstack application development, big data, cloud technologies, analytics, machine learning, ai, devops containerization, cnd, cne, cno, threat mitigation, vulnerability exposure, penetration testing, threat hunting, preventing advanced persistent threat, agile software development","agile software development, ai, analytics, big data, cloud technologies, cnd, cne, cno, confluence, devops containerization, fullstack application development, jira, linux, machine learning, oracle, oracle data guard, penetration testing, powerdesigner, preventing advanced persistent threat, scripting, sql, threat hunting, threat mitigation, vulnerability exposure"
Software Engineer 1 - Data Analysis,Captivation,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/software-engineer-1-data-analysis-at-captivation-3686212108,2023-12-17,Maryland,United States,Mid senior,Onsite,"Build Something to Be Proud Of.
Captivation Software has built a reputation on providing customers exactly what is needed in a timely manner. Our team of engineers take pride in what they develop and constantly innovate to provide the best solution. Captivation Software has an immediate need for a software engineer with data analysis experience
Requirements
Security Clearance:
Must currently hold a Top Secret / SCI U.S. Government security clearance with a favorable Polygraph, therefore all candidates must be a U.S. citizen
Minimum Qualifications:
Bachelor's degree in Computer Science or similar
7 years of software engineering experience needed
Required Skills:
Data Analysis experience
This position is open for direct hires only. We will not consider candidates from third party staffing/recruiting firms.
Benefits
Annual Salary: $180,000 - $200,000 (Depends on the years of experience)
Up to 20% 401k contribution (no matching required)
Above market hourly rates
$3,000 HSA Contribution
5 Weeks Paid Time Off
Company Paid Employee Medical / Dental / Vision Insurance / Life Insurance / Short-Term & Long-Term Disability / AD&D
Show more
Show less","Data Analysis, Software Engineering","data analysis, software engineering","dataanalytics, software engineering"
AWS Developer (data Engineer) and Date Governance Manager,IVY TECH SOLUTIONS INC,"Maryland City, MD",https://www.linkedin.com/jobs/view/aws-developer-data-engineer-and-date-governance-manager-at-ivy-tech-solutions-inc-3787773508,2023-12-17,Maryland,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
AWS Developer (data Engineer) and Date Governance Manager
Location: MD
Duration: 12+Months
Initially Remote
Please send the resume to
or 847- 350-1008
Skills:
(data Engineer) and Date Govervance Manager
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
mHLW2CY4MA
Show more
Show less","AWS, Data Engineer, Data Governance, Governance Manager","aws, data engineer, data governance, governance manager","aws, data governance, dataengineering, governance manager"
Senior Data Engineer,Merkle,"Columbia, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-at-merkle-3778606349,2023-12-17,Maryland,United States,Mid senior,Remote,"Company Description
Dentsu/Merkle is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsu mcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Why Join Us?
Merkle is the leader in data-driven performance marketing. We help brands achieve tremendous competitive advantage. We thrive because we employ the best in the business. Merkle's energy lives in everything we do, and it shows in the way we deliver to our clients. Additionally, we are recognized as top global partners by some of the world's leading technology firms including Adobe, Google, AWS, Salesforce, and more.
Job Description
As a Sr. Data Engineer, you will be a core member of the data engineering team: developing new or enhancing existing data products as we build a meta data driven, big data solution, processing and transforming data to produce high quality data assets for our customers. You will work with passionate, goal-oriented Data Engineers to solve complex problems. You will collaborate with Operations and Delivery teams to provide market focused data solutions to our customers. You will participate in a growing, high performing data engineering team that embraces change, is continuously improving, and rapidly builds, tests, enhances, and scales data transformational solutions that drive incremental business value for our customers and partners.
Key Responsibilities:
Create or assemble large, complex data sets that meet functional and non-functional business requirements
Document requirements from End Users to development user stories, capturing all the details and acceptance criteria required
Identify, design, and implement internal process improvements: automating manual processes, optimizing data builds, and implementing data QA and reporting
Coordinate with cross-functional teams, such as Identity and Data Science teams, to develop new data products, making sure each team has the data required
Work with data and analytic experts to strive for greater functionality of the data
Develop Proof of Concepts for new or updated data needs as required
Triage data build or release issues as required
Support integration with internal solutions
Qualifications
Bachelor's degree in Mathematics, Statistics, Economics, Computer Science or other equivalent experience
10+ year experience with data engineering, data modeling and/or data processing
5+ years' experience with SQL, relational databases, and data warehouses
Experience with Python
Experience with Snowflake
Experience in cloud base technologies (AWS, GCP, Azure)
Experience working in Agile teams and CI/CD environments
Experience with Linux and Windows is a plus
Experience with Jenkins, Airflow, or other orchestration tools is a plus
Experience with code repositories such as Git, Bitbucket, Github, etc.
Experience in Tableau is a plus
Skills:
Strong analytic skills working with B2B, B2C, and Digital data assets for the purposes of customer experience marketing and analytics
Able to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Able to build processes supporting data transformation, data structures, metadata, dependency and workload management
Demonstrated ability to manipulate, process and extract value from large disconnected datasets
Demonstrated strong organizational, leadership, and communication skills
Demonstrated ability to work with cross-functional teams in a dynamic environment
Familiarity with statistical concepts, modeling, and the ability to integrate data and analytics
Able to manage ambiguity, asking questions to gain clarity and understanding
Able to learn new emerging technologies quickly and apply innovative ideas to resolve problems
Able to investigate and determine solutions to solve complex problems, offering options and recommendations to business stakeholders or leadership for decisions
Able to analyze data, with a high attention to detail, and identify data patterns and anomalies
Able to assess and manage big data and data that scales
Able to focus on results and business outcomes, meeting business expectations
Additional Information
The anticipated salary range for this position is $94,000 - $152,375. Salary is based on a variety of factors including relevant experience, knowledge, skills and other factors permitted by law.
A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit www.dentsubenefitsplus.com.
About Dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
Show more
Show less","Data engineering, Python, Snowflake, SQL, Data modeling, Data processing, Data warehouses, Agile, CI/CD, Linux, Windows, Jenkins, Airflow, Orchestration tools, Git, Bitbucket, Github, Tableau, B2B, B2C, Marketing, Analytics, Root cause analysis, Data transformation, Data structures, Metadata, Dependency management, Workload management, Organizational skills, Leadership skills, Communication skills, Crossfunctional teams, Statistical concepts, Modeling, Data integration, Cloud base technologies, AWS, GCP, Azure","data engineering, python, snowflake, sql, data modeling, data processing, data warehouses, agile, cicd, linux, windows, jenkins, airflow, orchestration tools, git, bitbucket, github, tableau, b2b, b2c, marketing, analytics, root cause analysis, data transformation, data structures, metadata, dependency management, workload management, organizational skills, leadership skills, communication skills, crossfunctional teams, statistical concepts, modeling, data integration, cloud base technologies, aws, gcp, azure","agile, airflow, analytics, aws, azure, b2b, b2c, bitbucket, cicd, cloud base technologies, communication skills, crossfunctional teams, data engineering, data integration, data processing, data structures, data transformation, data warehouses, datamodeling, dependency management, gcp, git, github, jenkins, leadership skills, linux, marketing, metadata, modeling, orchestration tools, organizational skills, python, root cause analysis, snowflake, sql, statistical concepts, tableau, windows, workload management"
Sr. Data Analytics Consultant,"Bytecode IO, Inc","Baltimore, MD",https://www.linkedin.com/jobs/view/sr-data-analytics-consultant-at-bytecode-io-inc-3778234884,2023-12-17,Maryland,United States,Mid senior,Remote,"Bytecode IO is a growing data consulting company that is looking for a customer-focused Data Analytics Consultant to join our team. We work on the latest technologies - Snowflake, BigQuery, Stitch, Looker - with a wide range of fast growing companies.
The ideal candidate can work with minimal direction delivering analytic solutions to unlock data value. You will engage on a variety of client projects using both new and proven technologies.
This is a remote full-time W2 position - US only based
You must prove you are authorized to work in the US - if hired
ABOUT THE ROLE
Develop data models, reports and dashboards from ideation through production
Integrate and transform data for analysis using SQL, ETL tools and API integrations
Validate data to ensure accuracy
Provide guidance to clients on optimizing their data environment
Work with clients (product managers, marketers, engineering team) to define requirements, establish priorities, offer solutions and execute development
Perform technical and business user training
Execute projects with minimal guidance
WHO YOU ARE
Required:
5+ years of SQL experience
Consulting experience leading projects
Technical expertise with data modeling and database design
Experience with business intelligence software
Excellent communication and problem-solving skills
Ability to manage multiple clients concurrently
Availability to take conference calls with clients during business hours
Interest in mentoring
Great if you have (not a must):
Experience working remotely
Experience with Looker
Familiarity with Python, Javascript, HTML
Experience with SQL-type database administration
BS in Computer Science, Computer Information Systems, Engineering, Statistics or Mathematics
BENEFITS
Flexible daily schedule to maximize work / life balance
Health Benefits
Vacation, Sick Leave, Paid public holiday
401K
Training and Certification on Looker, AWS and Google Cloud
Company Laptop
Show more
Show less","Data Analytics, Snowflake, BigQuery, Stitch, Looker, SQL, ETL, API integration, Data modeling, Database design, Business intelligence, Communication, Problemsolving, Multitasking, Python, Javascript, HTML, SQLtype database administration, Computer Science, Computer Information Systems, Engineering, Statistics, Mathematics","data analytics, snowflake, bigquery, stitch, looker, sql, etl, api integration, data modeling, database design, business intelligence, communication, problemsolving, multitasking, python, javascript, html, sqltype database administration, computer science, computer information systems, engineering, statistics, mathematics","api integration, bigquery, business intelligence, communication, computer information systems, computer science, dataanalytics, database design, datamodeling, engineering, etl, html, javascript, looker, mathematics, multitasking, problemsolving, python, snowflake, sql, sqltype database administration, statistics, stitch"
Senior Data Engineer,Global Credit Union,"Street, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-at-global-credit-union-3781791324,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Overview
Reports to:
Manager, Data Analytics
Functions Supervised:
None
Primary Functions:
Lead and implement data engineering projects, support and maintain data pipelines, and provide expertise and best practices regarding data engineering. Typical data engineering projects focus on improving performance and adding features to existing data pipelines.
Duties And Responsibilities
Maintain in-depth understanding of relevant data engineering best practices
Display expertise with tools needed to debug and diagnose issues
Design, implement, and deploy a solution for medium to large understood problems
Design and develop new data engineering pipelines
Continuously evaluate and provide recommendations to improve pipeline, systems, and infrastructure, based on data engineering best practices
Act as project manager for data engineering projects, typically
Show initiative and work well individually, as part of the Data Engineering team, and with other teams within the organization
Deliver feedback in a constructive manner
Provide guidance to other engineers and workflow developers
Work well with technical leads, incorporating feedback as needed
Perform other duties as assigned.
Qualifications
Education:
Bachelor's degree in computer science, software or computer engineering, applied math, physics, statistics, or a related field.
Creditable Experience in Lieu of Education:
Minimum of ten years relevant experience.
Experience/Skills:
Strong Experience in Design, Development, Implementation, Unit Testing, Troubleshooting and Support of ETL/ELT Process Using Wherescape RED and Other In-House Tools. Experience in the Snowflake Environment. Strong Experience in SQL Query Design and Implementation. Experience in DWH Concepts and other DWH Related Terms. Experience in SQL Performance Tuning and Query Optimization by Indexing, Partitioning and Denormalization. Well Versed with Migration. Working Knowledge of BI Reporting (preferably Tableau and SSRS). Experience with workflow orchestration. Understanding of security & privacy principals.
Tenure:
Not Applicable.
Compensation
Senior Data Engineer (Category 06)
Salary Pay Range:
$90,000 - $155,000 annually. Starting base salary will be determined based on candidate experience, qualifications, education, and local or state wage requirements, if applicable and will fall within the range provided above.
In accordance with our Salary Administration policy, new hire base salaries generally fall within the minimum to midpoint of the listed range.
Benefits
Short-term and long-term incentives
Comprehensive medical, dental and vision insurance plan that has HSA and FSA options
401(k) plan with a 5% match
Employee Assistance Program (EAP)
Life and disability coverage
Voluntary cash benefits for accident, hospitalization and critical illness
Tuition Reimbursement
Generous leave programs to include Paid Time Off accrual, Paid Sick Leave, Paid Holidays
Click here to view Global’s comprehensive Benefits Programs .
Equal Opportunity Employer
Show more
Show less","Data engineering, Data pipelines, Data integration, Data quality, Data governance, ETL/ELT, Workflow orchestration, Cloud computing, SQL, Snowflake, Tableau, SSRS, Python, Java, Scala, Hadoop, Spark","data engineering, data pipelines, data integration, data quality, data governance, etlelt, workflow orchestration, cloud computing, sql, snowflake, tableau, ssrs, python, java, scala, hadoop, spark","cloud computing, data engineering, data governance, data integration, data quality, datapipeline, etlelt, hadoop, java, python, scala, snowflake, spark, sql, ssrs, tableau, workflow orchestration"
Database Engineer 2,SilverEdge,"Fort Meade, MD",https://www.linkedin.com/jobs/view/database-engineer-2-at-silveredge-3709835610,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Provides technical expertise for database design, development,
implementation, information storage and retrieval, data flow and
analysis. Develops relational and/or Object-Oriented databases,
database parser software, and database loading software. Projects
long-range requirements for database administration and design.
Responsible for developing a database structure that fits into the
overall architecture of the system under development and has to make
trades among data volumes, number of users, logical and physical
distribution, response times, retention rules, security and domain
controls. The DBE works primarily at the front end of the lifecyclerequirements
through system acceptance testing and Initial Operational
Capability (IOC). Develops requirements from a project’s inception to
its conclusion for a particular business and Information Technology
(IT) subject matter area (i.e., simple to complex systems). Assist with
recommendations for, and analysis and evaluation of systems
improvements, optimization, development, and/or maintenance efforts.
Translates a set of requirements and data into a usable document by
creating or recreating ad hoc queries, scripts, and macros; updates
existing queries, creates new ones to manipulate data into a master
file; and builds complex systems using queries, tables, Open Database
Connectivity and database storage and retrieval using Cloud
methodologies.
Support the database design, development, implementation, information storage and retrieval,
data flow and analysis activities
Support the analysis and evaluation of system improvements, optimization, development and/or
Support the development of long and short term requirements for database administration and
design
Assist in developing databases, database parser software, and database loading software
Translate a set of requirements and data into a usable database schema by creating or recreating
ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data
into a master file
Assist in developing database structures that fit into the overall architecture of the system under
development
Lead development of database structures that fit into the overall architecture of the system
under development
Lead development of databases, database parser software, and database loading software
Develop requirement recommendations from a project’s inception to its conclusion for a
particular Business and IT subject matter area (i.e. simple to complex systems)
Five (5) years experience as a DE in programs and contracts of similar scope, type, and complexity
is required. Bachelor’s degree in a technical discipline from an accredited college or university is
required. Five (5) years of DE experience may be substituted for a bachelor’s degree.
Show more
Show less","Database design, Database development, Database implementation, Data storage and retrieval, Data flow and analysis, Relational databases, Objectoriented databases, Database parser software, Database loading software, Cloud methodologies, Open Database Connectivity, Ad hoc queries, Scripts, Macros, Master files, System improvements, System optimization, System development, System maintenance, Data manipulation, Database schema, Database structures, Database architecture, Business and IT subject matter area, Requirements gathering, Requirements analysis, Requirements management, System testing, Initial Operational Capability (IOC)","database design, database development, database implementation, data storage and retrieval, data flow and analysis, relational databases, objectoriented databases, database parser software, database loading software, cloud methodologies, open database connectivity, ad hoc queries, scripts, macros, master files, system improvements, system optimization, system development, system maintenance, data manipulation, database schema, database structures, database architecture, business and it subject matter area, requirements gathering, requirements analysis, requirements management, system testing, initial operational capability ioc","ad hoc queries, business and it subject matter area, cloud methodologies, data flow and analysis, data manipulation, data storage and retrieval, database architecture, database design, database development, database implementation, database loading software, database parser software, database schema, database structures, initial operational capability ioc, macros, master files, objectoriented databases, open database connectivity, relational databases, requirements analysis, requirements gathering, requirements management, scripts, system development, system improvements, system maintenance, system optimization, system testing"
Database Engineer 3,SilverEdge,"Fort Meade, MD",https://www.linkedin.com/jobs/view/database-engineer-3-at-silveredge-3709832787,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Provides technical expertise for database design, development,
implementation, information storage and retrieval, data flow and
analysis. Develops relational and/or Object-Oriented databases,
database parser software, and database loading software. Projects
long-range requirements for database administration and design.
Responsible for developing a database structure that fits into the
overall architecture of the system under development and has to make
trades among data volumes, number of users, logical and physical
distribution, response times, retention rules, security and domain
controls. The DBE works primarily at the front end of the lifecyclerequirements
through system acceptance testing and Initial Operational
Capability (IOC). Develops requirements from a project’s inception to
its conclusion for a particular business and Information Technology
(IT) subject matter area (i.e., simple to complex systems). Assist with
recommendations for, and analysis and evaluation of systems
improvements, optimization, development, and/or maintenance efforts.
Translates a set of requirements and data into a usable document by
creating or recreating ad hoc queries, scripts, and macros; updates
existing queries, creates new ones to manipulate data into a master
file; and builds complex systems using queries, tables, Open Database
Connectivity and database storage and retrieval using Cloud
methodologies.
Support the database design, development, implementation, information storage and retrieval,
data flow and analysis activities
Support the analysis and evaluation of system improvements, optimization, development and/or
maintenance efforts
Support the development of long and short term requirements for database administration and
design
Assist in developing databases, database parser software, and database loading software
Translate a set of requirements and data into a usable database schema by creating or recreating
ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data
into a master file
Assist in developing database structures that fit into the overall architecture of the system under
development
Lead development of database structures that fit into the overall architecture of the system
under development
Lead development of databases, database parser software, and database loading software
Develop requirement recommendations from a project’s inception to its conclusion for a
particular Business and IT subject matter area (i.e. simple to complex systems)
Develop a database structure that fits into the overall architecture of the system under
development and has to make trades among data volumes, number of users, logical and physical
distribution, response times, retention rules, security and domain controls
Direct fulfillment of requirements from a project’s inception to it conclusion
Direct organization of requirements and data into a usable database schema by directing
development of ad hoc queries, scripts, macros, updates to existing queries
Direct the overall database structure that fits into the overall architecture of the system under
development and has to make trades among data volumes, number of users, logical and physical
distribution, response times, retention rules, security and domain controls
Direct the development of complex systems using queries, tables, Open Database Connectivity
and database storage and retrieval using Cloud methodologies
Ten (10) years experience as a DE in programs and contracts of similar scope, type, and complexity
is required. Bachelor’s degree in a technical discipline from an accredited college or university is
required. Five (5) years of DE experience may be substituted for a bachelor’s degree.
Show more
Show less","Database design, Database development, Database implementation, Information storage, Information retrieval, Data flow, Data analysis, Database structure, Relational databases, ObjectOriented databases, Database parser software, Database loading software, Data storage, Data retrieval, Cloud methodologies, Open Database Connectivity, Ad hoc queries, Scripts, Macros, Data manipulation, Data volumes, Number of users, Logical distribution, Physical distribution, Response times, Retention rules, Security, Domain controls, Initial Operational Capability (IOC), Business and IT subject matter area, System improvements, Optimization, Development, Maintenance efforts, Usable document, Master file, Complex systems, Database schema","database design, database development, database implementation, information storage, information retrieval, data flow, data analysis, database structure, relational databases, objectoriented databases, database parser software, database loading software, data storage, data retrieval, cloud methodologies, open database connectivity, ad hoc queries, scripts, macros, data manipulation, data volumes, number of users, logical distribution, physical distribution, response times, retention rules, security, domain controls, initial operational capability ioc, business and it subject matter area, system improvements, optimization, development, maintenance efforts, usable document, master file, complex systems, database schema","ad hoc queries, business and it subject matter area, cloud methodologies, complex systems, data flow, data manipulation, data retrieval, data storage, data volumes, dataanalytics, database design, database development, database implementation, database loading software, database parser software, database schema, database structure, development, domain controls, information retrieval, information storage, initial operational capability ioc, logical distribution, macros, maintenance efforts, master file, number of users, objectoriented databases, open database connectivity, optimization, physical distribution, relational databases, response times, retention rules, scripts, security, system improvements, usable document"
Data Science Engineer (Senior),"Integral Federal, Inc.","Annapolis Junction, MD",https://www.linkedin.com/jobs/view/data-science-engineer-senior-at-integral-federal-inc-3580061411,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Integral Federal is looking to hire a Data Science Engineer to support the Transportation Security Administration (TSA) Performance Engineering Analytics (PEA) program. The PEA program will support TSA as they continue to look for ways to enhance its layered approach to security through new state-of-the-art technologies, expanded use of existing and proven technologies, improved passenger identification techniques, and other developments that will continue to strengthen anti-terrorism capabilities.
Analyze and organize raw data, build data systems and pipelines, and develop code/toolsto Extract, Transform, and Load (ETL) data into the appropriate structures for storage
Combine raw information from different sources and explore ways to enhance data quality, performance, and reliability
Create performance metrics for data ingest, storage, and quality to track performance indicators for communication to relevant stakeholders.
Apply continuous improvement methodologies, such as ITIL (Information Technology Infrastructure Library) and/or Six Sigma, towards analyzing, measuring, and documenting system improvements, and assessing data quality.
Create and present consumable summaries of findings through effective communication and visualizations.
Perform data processing, cleansing, mining and analysis utilizing data sources having relevant context that are internal and external to the business’ domain data repositories/data warehouses.
Interpret, understand, and assess customer requirements to provide custom tool development, query optimization, aggregation, categorization, and interpretation of data.
Monitors for data quality events such as missing data, non-conformant data, and unexpected values and appropriately responds and resolves data quality issues
Follows appropriate protocols and procedures to handle purging and masking information deemed to be sensitive in nature by privacy stakeholders
Capture data lineage and produce lineage reports to inform end users and customers how the data was sourced, transformed, and where it resides Extending data with third party sources of information when needed
Implement data governance practices and procedures that serve as the guides for data science projects
Required
10+ years experience and a Bachelor's Degree
Expertise developing interrogating databases with Structure Query Language (SQL) to develop complex queries involving joins, subqueries, aggregation, indices, common table expressions, and query optimization/plans. The ability to code and debug stored procedures is desirable.
Skills in applying pattern recognition techniques to large structured, semi-structured, and unstructured data sets to automate and optimize data wrangling (extract, transform, clean, evaluate, normalize/standardize, and organize) from disparate sources.
Experience with developing ETL capabilities using tools such as IBM Infosphere/Datastage, Informatica, or similar tools
Previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, data cleansing, and segmentation techniques
Knowledge of programming languages (e.g. Java, Python, SQL)
Hands-on experience with SQL database design
Great numerical and analytical skills
Preferred
Experience with TSA systems and environment
Integral Federal is united by a shared passion of excellence in service. We are an Equal Opportunity Employer that cultivates a culture of diversity, equity, and inclusion, and are headquartered in Rockville, MD with offices in Charlottesville, Fredericksburg, DC and Aberdeen.
We offer a comprehensive total rewards package including paid parental leave and immediate vesting in our 401K. That means you don’t have to wait an entire year to earn matching funds for your retirement contributions! Give us a try and become part of a curated group of intelligence professionals at Integral Federal Inc.
Our package also includes
Medical, Dental & Vision Insurance
Flexible Spending Accounts
Short-Term and Long-Term Disability Insurance
Life Insurance
Paid Time Off – Holidays, Vacation & Sick Days
Earned bonuses and awards
Professional Training Reimbursement
Paid Parking
Employee Assistance Program
Show more
Show less","Data engineering, Data wrangling, Data modeling, Data mining, Data cleansing, Data segmentation, Java, Python, SQL, IBM Infosphere/Datastage, Informatica, ETL, ITIL, Six Sigma, Structure Query Language (SQL), Stored procedures, Data governance, Data lineage, Data quality, Data visualization, Data analysis, Customer requirements, Sensitivity Analysis","data engineering, data wrangling, data modeling, data mining, data cleansing, data segmentation, java, python, sql, ibm infospheredatastage, informatica, etl, itil, six sigma, structure query language sql, stored procedures, data governance, data lineage, data quality, data visualization, data analysis, customer requirements, sensitivity analysis","customer requirements, data engineering, data governance, data lineage, data mining, data quality, data segmentation, data wrangling, dataanalytics, datacleaning, datamodeling, etl, ibm infospheredatastage, informatica, itil, java, python, sensitivity analysis, six sigma, sql, stored procedures, structure query language sql, visualization"
Data Analyst I,Global Credit Union,"Street, MD",https://www.linkedin.com/jobs/view/data-analyst-i-at-global-credit-union-3781791325,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Overview
Reports to:
Manager, Data Analytics
Functions Supervised:
None
Primary Functions:
As a Data Analyst I, individuals possess a basic understanding of data analytical concepts, are familiar with SQL syntax and can utilize out-of-the-box SSRS and Tableau features. They can write straightforward queries, perform simple data shaping, and provide insight from analysis. With this foundation, they create simple reports, lists, and extracts and work with the business on end user acceptance. Reporting requirements are usually well defined for the Data Analyst I, with development following pre-determined specifications.
Duties And Responsibilities
Works with Business users and domain SMEs to understand requirements.
Focuses on processing and preparing data to uncover patterns using tools such as Snowflake, Tableau, and Python for data manipulation and analysis
Assists Data Warehouse Operations in data mapping activities by specifying logic and business rules for transforming and preparing data.
Supports the development, automation and publication of reports and dashboards.
Assists with Data Quality (DQ) and Master Data Management (MDM) initiatives.
Assists in developing governance processes around data management and the warehouse development.
Develops knowledge of credit union operations and knowledge of current industry trends as well as current methods and technologies that relate to Data Warehousing, data preparation, data presentation practices and operations.
Performs other duties as assigned.
Qualifications
Education:
Bachelor's degree in Business, Computer Science, Management Information Systems or related field.
Creditable Experience in Lieu of Education:
Minimum of one year of Data Analytics and reporting.
Experience/Skills:
Strong oral, written, administrative, organizational, and inter-personal skills. The ability to analyze problems, develop alternatives and recommendations. Proficiency in SQL and self-service analytics tools such as Excel, Power Pivot, YellowFin, Snowflake, PowerBI, Tableau etc. Familiarity with Visual Studio, Master Data Management, data quality tools, data integration tools, R, and Python is desired. Financial Services experience and familiarity of big data concepts is a plus.
Tenure:
Assignment to the Data Analyst I category 10, Data Analyst II category 09, Data Analyst III category 08, or Senior Data Analyst category 07 will be determined by the candidate's education or experience. Advancement requires management recommendation and will be based on the candidate's certifications and/or performance.
Compensation
Salary Pay Range:
Data Analyst I (Category 10): $57,484 - $90,825 annually
Data Analyst II (Category 09): $63,233 - $101,804 annually
Data Analyst III (Category 08): $72,717 - $119,983 annually
Senior Data Analyst (Category 07): $80,000 - $137,981 annually
Starting base salary will be determined based on candidate experience, qualifications, education, and local or state wage requirements, if applicable and will fall within the range provided above.
In accordance with our Salary Administration policy, new hire base salaries generally fall within the minimum to midpoint of the listed range.
Benefits
Short-term and long-term incentives
Comprehensive medical, dental and vision insurance plan that has HSA and FSA options
401(k) plan with a 5% match
Employee Assistance Program (EAP)
Life and disability coverage
Voluntary cash benefits for accident, hospitalization and critical illness
Tuition Reimbursement
Generous leave programs to include Paid Time Off accrual, Paid Sick Leave, Paid Holidays
Click here to view Global’s comprehensive Benefits Programs .
Equal Opportunity Employer
Show more
Show less","SQL, Data Warehousing, Tableau, Snowflake, Python, SSRS, Power Pivot, Excel, YellowFin, PowerBI, R, Master Data Management, Data Quality tools, Data Integration tools, Visual Studio, Financial Services, Big Data","sql, data warehousing, tableau, snowflake, python, ssrs, power pivot, excel, yellowfin, powerbi, r, master data management, data quality tools, data integration tools, visual studio, financial services, big data","big data, data integration tools, data quality tools, datawarehouse, excel, financial services, master data management, power pivot, powerbi, python, r, snowflake, sql, ssrs, tableau, visual studio, yellowfin"
Data Engineer (Hybrid) - 16861,"Mission Technologies, a division of HII","Fort Meade, MD",https://www.linkedin.com/jobs/view/data-engineer-hybrid-16861-at-mission-technologies-a-division-of-hii-3764506005,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers’ greatest challenges.
We recognize that the most effective environment for your projects doesn’t always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done—while having the flexibility to work from home when needed to achieve focused results.
Why Enlighten?
Benefits
At Enlighten, our team’s unwavering work ethic, top talent and celebration of innovative ideas have helped us thrive. We know that our employees are essential to our company’s success, so we seek to take care of you as much as you take care of us. Here are a few highlights of our benefits package:
100% paid employee premium for healthcare, vision and dental plans.
10% 401k benefit.
Generous PTO + 10 paid holidays.
Education/training allowances.
Job Description
Enlighten is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is mostly on-site in Ft. Meade, Maryland with some flexibility to work from home.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current TS/SCI with a polygraph level security clearance and therefore all candidates must be a U.S. Citizen.
5+ years of experience as a developer, analyst, or engineer with Bachelors in related field; 3 years relevant experience with Masters in related field; in related field; or High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must be able to work on customer site most of the time (Right now about 3-4 days/week).
Preferred Requirements
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com].
Enlighten, an HII Company, is an Equal Opportunity/Veterans and Disabled Employer. U.S. citizenship may be required for certain positions. HII Is committed to cultivating an inclusive company culture to promote collaboration and enhance creativity by hiring a diverse work force.
Show more
Show less","Data Engineering, Big Data Systems, Cybersecurity, Data Acquisition, Data Analysis, Data Sharing, Python, Java, Kibana, Elasticsearch, JSON, XML, Kafka, NiFi, AWS S3, AWS SQS, NOSQL, Accumulo","data engineering, big data systems, cybersecurity, data acquisition, data analysis, data sharing, python, java, kibana, elasticsearch, json, xml, kafka, nifi, aws s3, aws sqs, nosql, accumulo","accumulo, aws s3, aws sqs, big data systems, cybersecurity, data acquisition, data engineering, data sharing, dataanalytics, elasticsearch, java, json, kafka, kibana, nifi, nosql, python, xml"
Senior Data Analyst Level 2,"Riverside Technology, inc.","College Park, MD",https://www.linkedin.com/jobs/view/senior-data-analyst-level-2-at-riverside-technology-inc-3744793514,2023-12-17,Maryland,United States,Mid senior,Hybrid,"The NOAA Center for Satellite Applications and Research (STAR) is the science arm of NOAA’s Satellite and Information Services, which acquires and manages the nation’s environmental satellites. STAR scientists lead efforts to develop, test, validate, and refine the science algorithms needed to drive user-defined products. They also investigate both enhanced and new sensor technology for future NOAA satellite missions and conduct research to examine which products users will need to carry out NOAA's mission goals. STAR supports the calibration and validation of all data in NOAA's satellite operations and develops new methods for inter-calibrating data from NOAA polar and geostationary satellites with other satellites in the evolving international system. Additionally, STAR collaboratively develops efficient methods and technology to transfer new products from research to operations.
Riverside is seeking a mid-level scientific analyst to support NOAA’s Center for Satellite Applications and Research (STAR). The work will entail using knowledge and expertise to research and develop new and innovative uses of satellite data to help coordinate efforts between NESDIS/STAR and commercial, university, and/or private partners/companies.
It includes investigating
new techniques and new observing systems, and evaluating their potential impact on products. It also includes developing product assessment tools, designing and developing scientific applications, code integration, porting, testing and debugging, releasing and maintaining code and documenting software applications in both standard and High Performance Computing (HPC) environments. The successful candidate will have knowledge and skills associated with both regional and global data assimilation systems and techniques and is ideally familiar with observation system experiments (OSE) and observation system simulation experiments (OSSE).
Requirements
Minimum bachelor’s degree in computer science, information systems, software engineering, or remote sensing-related scientific field to include atmospheric sciences, oceanography, physics or similar; Master’s Degree desired
4-10 years’ experience in scientific analysis
Work with a team of Federal and contractor personnel to perform advanced scientific programming and numerical analysis to solve physical problems
Strong Coding Computer Skills in Python
Knowledge of satellite data assimilation, numerical weather prediction, radiative transfer, retrieval algorithms, and satellite observations data
Operating Systems: Linux, High Performance Computing Platforms
Knowledge of applying advanced numerical methods, artificial intelligence, and machine learning to large datasets using software libraries such as Keras, TensorFlow, and/or PyTorch
Experience performing data analysis on large environmental datasets and with scientific code
Ability to perform independent assessments of proposed concepts
Ability to produce scientific reports and presentations
Ability to effectively communicate technical information and work status accurately and reliably to project leads and managers
US citizen preferred; Must be able to obtain and maintain a national agency check and background investigation after hire
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Short Term & Long Term Disability
Training & Development
Work From Home hybrid
Show more
Show less","Python, Satellite data assimilation, Numerical weather prediction, Radiative transfer, Retrieval algorithms, Satellite observations, Linux, High Performance Computing Platforms, Artificial intelligence, Machine learning, Keras, TensorFlow, PyTorch, Data analysis, Scientific code, Numerical methods","python, satellite data assimilation, numerical weather prediction, radiative transfer, retrieval algorithms, satellite observations, linux, high performance computing platforms, artificial intelligence, machine learning, keras, tensorflow, pytorch, data analysis, scientific code, numerical methods","artificial intelligence, dataanalytics, high performance computing platforms, keras, linux, machine learning, numerical methods, numerical weather prediction, python, pytorch, radiative transfer, retrieval algorithms, satellite data assimilation, satellite observations, scientific code, tensorflow"
Database Engineer III,C2 Technology Solutions Inc.,"Columbia, MD",https://www.linkedin.com/jobs/view/database-engineer-iii-at-c2-technology-solutions-inc-3679613594,2023-12-17,Maryland,United States,Mid senior,Hybrid,"C2 Technology Solutions is a small business with a global perspective, offering a complete spectrum of insightful expertise in system and software engineering, architecture design, business process re-engineering, and IT infrastructures. Since 2010, we have provided performance and value-driven consulting services to both government and commercial clients.
The key to C2 Technology Solutions’ success is in its primary asset…People. We understand that our growth is parallel to the growth of every individual on our team.
At C2 Technology Solutions, community and ownership inspires creativity and the working environment fosters excellence. In addition to a great work environment, we offer a competitive compensation package with outstanding benefits and career development opportunities.
Position Overview
Our firm is currently seeking an experienced Database Engineer in the Columbia, Maryland area. The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develop relational and/or Object-Oriented databases, database parser software, and database loading software. Project long-range requirements for database administration and design. Responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls. Work primarily at the front end of the lifecycle requirements through system acceptance testing and Initial Operational Capability (IOC). Develop requirement from a project's inception to its conclusion for a particular business and Information Technology (IT) subject matter area (i.e., simple to complex systems). Assist with recommendations for, and analysis and evaluation of systems improvements, optimization, development, and/or maintenance efforts. Translate a set of requirements and data into a usable document by creating or recreating ad hoc queries, scripts, and macros; update existing queries, create new ones to manipulate data into a master file; and build complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies. The Database Engineer will direct the development of complex systems using queries, tables, and database storage and retrieval using Cloud methodologies for the design, development, implementation, information storage and retrieval, data flow and analysis. Direct the overall database structure to fit into the overall architecture of the system. Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software. Direct fulfillment of requirements from a project inception to conclusion.
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities
Support the analysis and evaluation of system improvements, optimization, development and/or maintenance efforts
Support the development of long and short term requirements for database administration and design
Assist in developing databases, database parser software, and database loading software
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file
Assist in developing database structures that fit into the overall architecture of the system under development
Lead development of database structures that fit into the overall architecture of the system under development
Lead development of databases, database parser software, and database loading software
Develop requirement recommendations from a project's inception to its conclusion for a particular Business and IT subject matter area (i.e. simple to complex systems)
Develop a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls
Direct fulfillment of requirements from a project's inception to it conclusion
Direct organization of requirements and data into a usable database schema by directing development of ad hoc queries, scripts, macros, updates to existing queries
Direct the overall database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls
Direct the development of complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies
Qualifications/Experience Requirements
Ten (10) years experience as a DBE in programs and contracts of similar scope, type, and complexity is required. Bachelor's degree in a technical discipline from an accredited college or university is required. Five (5) years of DBE experience may be substituted for a bachelor's degree.
Required: Database experience using MongoDB or MariaDB; including deployment and management of the database itself, debugging of optimization issues, and scaling.
Desired: Python, Django or Flask, ReST Endpoint Development, Micro-Service Model, Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment
https://c2techsol.applicantstack.com/x/openings
Security Clearance with an appropriate agency Polygraph is required.
C2 Technology Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, or disability.
Show more
Show less","Database Design, Database Development, Database Implementation, Information Storage and Retrieval, Data Flow and Analysis, Relational Database, ObjectOriented Database, Database Parser Software, Database Loading Software, MongoDB, MariaDB, Python, Django, Flask, ReST Endpoint Development, MicroService Model, Swagger, AWS, C2S, Docker, Visual Studio Code, JSON, XML, Jira, Confluence, Git, Agile","database design, database development, database implementation, information storage and retrieval, data flow and analysis, relational database, objectoriented database, database parser software, database loading software, mongodb, mariadb, python, django, flask, rest endpoint development, microservice model, swagger, aws, c2s, docker, visual studio code, json, xml, jira, confluence, git, agile","agile, aws, c2s, confluence, data flow and analysis, database design, database development, database implementation, database loading software, database parser software, django, docker, flask, git, information storage and retrieval, jira, json, mariadb, microservice model, mongodb, objectoriented database, python, relational database, rest endpoint development, swagger, visual studio code, xml"
Senior Cloud Data Engineer,BDO USA,"Columbia, MD",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470288,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics Solutions, Azure, AWS, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, Artificial Intelligence, Machine Learning, Data Lake Medallion Architecture, Batch and/or Streaming Data Ingestion, AI Algorithms, Automation Tools, UiPath, Alteryx, Computer Vision based AI Technologies, Git, DevOps, Linux, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Star Schema, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics solutions, azure, aws, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, artificial intelligence, machine learning, data lake medallion architecture, batch andor streaming data ingestion, ai algorithms, automation tools, uipath, alteryx, computer vision based ai technologies, git, devops, linux, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, star schema, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithms, alteryx, artificial intelligence, automation tools, aws, azure, azure analysis services, batch andor streaming data ingestion, bicep, business intelligence, c, cloud data analytics solutions, computer vision based ai technologies, data definition language ddl, data lake medallion architecture, data manipulation language dml, data ops, dataanalytics, datamodeling, datawarehouse, dbt, delta, devops, functions, git, java, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, tabular modeling, terraform, uipath, views"
Senior Cloud Data Engineer,BDO USA,"Baltimore, MD",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765473106,2023-12-17,Maryland,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, Artificial Intelligence, Data Science, Computer Science, Information Systems, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, Azure, AWS, SQL, Python, Java, Scala, C#, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Automation Tools, Computer Vision, DataOps, MLOps, Tableau, Power BI, Microsoft Fabric, Azure Analysis Services, UiPath, Alteryx, Synapse, IoT, Microsoft SQL Server, .Net, Qlik, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, SSIS, SSAS, SSRS, dbt, Terraform, Bicep, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, machine learning, artificial intelligence, data science, computer science, information systems, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, azure, aws, sql, python, java, scala, c, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, automation tools, computer vision, dataops, mlops, tableau, power bi, microsoft fabric, azure analysis services, uipath, alteryx, synapse, iot, microsoft sql server, net, qlik, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, ssis, ssas, ssrs, dbt, terraform, bicep, purview, delta, pandas, spark sql","ai algorithms, alteryx, artificial intelligence, automation tools, aws, aws lake formation, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer science, computer vision, data lake medallion architecture, data science, dataanalytics, databricks, datamodeling, dataops, datawarehouse, dbt, delta, devops, git, information systems, iot, java, linux, machine learning, microsoft fabric, microsoft sql server, mlops, net, pandas, powerbi, purview, python, qlik, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, synapse, tableau, terraform, uipath"
Senior Data Analyst Level 3,"Riverside Technology, inc.","College Park, MD",https://www.linkedin.com/jobs/view/senior-data-analyst-level-3-at-riverside-technology-inc-3744793563,2023-12-17,Maryland,United States,Mid senior,Hybrid,"The NOAA Center for Satellite Applications and Research (STAR) is the science arm of NOAA’s Satellite and Information Services, which acquires and manages the nation’s environmental satellites. STAR scientists lead efforts to develop, test, validate, and refine the science algorithms needed to drive user-defined products. They also investigate both enhanced and new sensor technology for future NOAA satellite missions and conduct research to examine which products users will need to carry out NOAA's mission goals. STAR supports the calibration and validation of all data in NOAA's satellite operations and develops new methods for inter-calibrating data from NOAA polar and geostationary satellites with other satellites in the evolving international system. Additionally, STAR collaboratively develops efficient methods and technology to transfer new products from research to operations.
Riverside is seeking a mid-level scientific analyst to support NOAA’s Center for Satellite Applications and Research (STAR). The work will entail using knowledge and expertise to research and develop new and innovative uses of satellite data to help coordinate efforts between NESDIS/STAR and commercial, university, and/or private partners/companies.
It includes investigating
new techniques and new observing systems, and evaluating their potential impact on products. It also includes developing product assessment tools, designing and developing scientific applications, code integration, porting, testing and debugging, releasing and maintaining code and documenting software applications in both standard and High Performance Computing (HPC) environments. The successful candidate will have knowledge and skills associated with both regional and global data assimilation systems and techniques and is ideally familiar with observation system experiments (OSE) and observation system simulation experiments (OSSE).
Requirements
Minimum bachelor’s degree in computer science, information systems, software engineering, or remote sensing-related scientific field to include atmospheric sciences, oceanography, physics or similar; Master’s Degree desired
4-10 years’ experience in scientific analysis
Work with a team of Federal and contractor personnel to perform advanced scientific programming and numerical analysis to solve physical problems
Strong Coding Computer Skills in Python
Knowledge of satellite data assimilation, numerical weather prediction, radiative transfer, retrieval algorithms, and satellite observations data
Operating Systems: Linux, High Performance Computing Platforms
Knowledge of applying advanced numerical methods, artificial intelligence, and machine learning to large datasets using software libraries such as Keras, TensorFlow, and/or PyTorch
Experience performing data analysis on large environmental datasets and with scientific code
Ability to perform independent assessments of proposed concepts
Ability to produce scientific reports and presentations
Ability to effectively communicate technical information and work status accurately and reliably to project leads and managers
US citizen preferred; Must be able to obtain and maintain a national agency check and background investigation after hire
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Short Term & Long Term Disability
Training & Development
Work From Home hybrid
Show more
Show less","Python, Satellite data assimilation, Numerical weather prediction, Radiative transfer, Retrieval algorithms, Satellite observations data, Linux, High Performance Computing Platforms, Keras, TensorFlow, PyTorch, Data analysis, Scientific code, Artificial intelligence, Machine learning, Scientific reports, Presentations, Technical communication","python, satellite data assimilation, numerical weather prediction, radiative transfer, retrieval algorithms, satellite observations data, linux, high performance computing platforms, keras, tensorflow, pytorch, data analysis, scientific code, artificial intelligence, machine learning, scientific reports, presentations, technical communication","artificial intelligence, dataanalytics, high performance computing platforms, keras, linux, machine learning, numerical weather prediction, presentations, python, pytorch, radiative transfer, retrieval algorithms, satellite data assimilation, satellite observations data, scientific code, scientific reports, technical communication, tensorflow"
Digital Health Systems and Data Analyst - Remote,Northeast Health Wangaratta,"Wangaratta, Victoria, Australia",https://au.linkedin.com/jobs/view/digital-health-systems-and-data-analyst-remote-at-northeast-health-wangaratta-3787310338,2023-12-17,Beechworth, Australia,Mid senior,Remote,"Work options:
Hybrid
Full Time, 15 Month Fixed Contract
80 hours per fortnight, including an ADO
Northeast Health Wangaratta (NHW) is the major health provider in the Central Hume region with an extensive range of acute, sub-acute, aged care and community services.
Good with data ? Good with process ? Good with systems ?
NHW needs YOU to be our Digital Health Systems and Data Analyst
NHW have an exciting opportunity for an experienced ICT Business / Reporting Analyst to support the requirements definition, process mapping, and data analysis for a suite of projects that support the Hume Region's Digital Health roadmap.
The Digital Health Systems and Data Analyst is a critical member of these project teams and for the provision of support to subject matter experts within the operational areas. The successful applicant will offer high levels of pragmatism, attention to detail, and consultation required to deliver quality outcomes for the organisation and our patients.
The Digital Health Systems and Data Analyst will additionally provide support and development of operational reporting capabilities. This role is central to the successful delivery and maintenance of these critical information and data resources for Northeast Health Wangaratta (NHW)
Experience with implementation or administration of healthcare systems and the associated health information management principles is considered beneficial.
Our Region
Wangaratta is a large regional centre located in Northeast Victoria, with an approximate population of 30,000 people.
Wangaratta offers a diverse range of shopping, cafes, bars, galleries and art precinct.
Wangaratta boasts great parks and gardens and scenic walking/hiking trails in our local area, including the majestic Mt. Buffalo.
Located just 2.5 hours to Melbourne and approx. 2 hours from the snow fields.
We are surrounded by world class Gourmet Food and Wine Region, including King Valley, Rutherglen, Glenrowan and Milawa Gourmet region.
Check out our region at www.visitwangaratta.com.au/Home
The Successful Applicant Will Benefit From
Competitive remuneration, novated leasing and salary packaging benefits available. If you have relocated permanently you may also be able to substantially increase your take-home pay by salary packaging some or all of your relocation costs.
5 weeks annual leave and an addition week for any employee working 10 or more weekend shifts. There is also the option for full time employees to be able to purchase additional leave if they wish.
Option to be able to swap any existing public holiday to another religious holiday or day of significance of your choice.
Employee Assistant Programs (EAP)
Well being programs and discounted corporate gym membership
Social Club Benefits
Support for our diverse workforce
Applying For The Role
Enquiries can be made with Cam Saunders on (03) 5722 5806
Confirmation of full Covid-19 and Influenza Vaccination or an approved medical exemption is required upon application
A PD for this role can be downloaded below or at www.northeasthealth.org.au/careers/jobs/
Applications must address the key selection criteria from the PD
Employment Statutory Declaration form must be submitted with your application
To apply for this position hit Apply Now/Join Now or head over to our careers page at www.northeasthealth.org.au/careers/jobs/
All additional documents can be uploaded within the application after filling in the key criteria
Applications close Tuesday, 26th December 2023
Show more
Show less","Data analysis, Process mapping, Data reporting, Requirements definition, Healthcare systems, Health information management, Project management, Consultation, Attention to detail, Pragmatism","data analysis, process mapping, data reporting, requirements definition, healthcare systems, health information management, project management, consultation, attention to detail, pragmatism","attention to detail, consultation, data reporting, dataanalytics, health information management, healthcare systems, pragmatism, process mapping, project management, requirements definition"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Temiskaming Shores, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751469384,2023-12-17,Temiskaming Shores, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Interpretation, Trend Analysis, DataDriven Decision Making, Performance Metrics, Reporting, Data Visualization, SQL, R, Python, Data Modeling, Algorithms, A/B Testing, Data Quality, Data Integrity, Data Cleansing, Data Manipulation, Stakeholder Engagement, Communication, Collaboration, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, data interpretation, trend analysis, datadriven decision making, performance metrics, reporting, data visualization, sql, r, python, data modeling, algorithms, ab testing, data quality, data integrity, data cleansing, data manipulation, stakeholder engagement, communication, collaboration, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, algorithms, collaboration, communication, data integrity, data interpretation, data manipulation, data quality, dataanalytics, datacleaning, datadriven decision making, datamodeling, etl, hypothesis testing, performance metrics, powerbi, python, r, reporting, sql, stakeholder engagement, statistical modeling, statistical techniques, tableau, trend analysis, visualization"
Lead Data Analyst,Kforce Inc,"Beaverton, OR",https://www.linkedin.com/jobs/view/lead-data-analyst-at-kforce-inc-3770149981,2023-12-17,Oregon,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is seeking a Lead Data Analyst in Beaverton, OR. Duties Include:
Lead Data Analyst generates reports and regular datasets or report information for end- users using system tools and database or data warehouse queries and scripts
Integrates data from multiple sources to produce requested or required data elements
As a Lead Data Analyst, you will program and maintain report forms and formats, information dashboards, data generators, canned reports. and other end- user information portals or resources
May create specifications for reports based on business requests
Requirements
7+ years of proven work experience in Data Analyst or Business Data Analyst role
Strong Experience in SQL and a scripting language such as Python or R
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Experience with data warehouses/RDBMS like Snowflake & Teradata
Experience working with BI tools like Tableau, Power BI, or DOMO
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling)
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc. preferred
Good understanding of Hadoop and Big Data processing frameworks
Experience with source control tools such as GitHub
Ability to cleanse, curate, and manipulate data; As well as visualize and story-telling the results of an analysis
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment
Self-starter, highly organized and result oriented; and should be able to work with minimal direction
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $59 - $73 per hour
Show more
Show less","Python, R, SQL, Snowflake, Teradata, Tableau, Power BI, DOMO, AWS, EMR, S3, Redshift, Athena, Hadoop, GitHub, Data warehouses, RDBMS, Data modeling, Analytical algorithms, Forecasting, Modeling, Big data processing frameworks","python, r, sql, snowflake, teradata, tableau, power bi, domo, aws, emr, s3, redshift, athena, hadoop, github, data warehouses, rdbms, data modeling, analytical algorithms, forecasting, modeling, big data processing frameworks","analytical algorithms, athena, aws, big data processing frameworks, data warehouses, datamodeling, domo, emr, forecasting, github, hadoop, modeling, powerbi, python, r, rdbms, redshift, s3, snowflake, sql, tableau, teradata"
CaSA Marketplace Data Analyst,Kforce Inc,"Beaverton, OR",https://www.linkedin.com/jobs/view/casa-marketplace-data-analyst-at-kforce-inc-3778827165,2023-12-17,Oregon,United States,Associate,Onsite,"Responsibilities
Kforce has a client seeking a Marketplace Data Analyst in Beaverton, OR. Summary: The Marketplace Data Analyst will generate reports and regular datasets or report information for end- users using system tools and database or data warehouse queries and scripts. This person integrates data from multiple sources to produce requested or required data elements. The candidate programs and maintains report forms and formats, information dashboards, data generators, canned reports, and other end- user information portals or resources. They may create specifications for reports based on business requests. The Marketplace Data Analyst will be responsible for Commercial/Retail data tools & solutions for the company's business throughout the Central & South American region. Some of the key areas of ownership are data management (quality control, troubleshooting data issues, attributing datapoints, etc.), Tableau dashboards management (maintenance, enhancements required by business, new product deployments, etc.) and maintenance & enhancements in local databases in Microsoft Access.
Requirements
To be considered for this position, candidates must have experience in a similar role, or they must possess significant knowledge, experience, and abilities to successfully perform the responsibilities listed
Relevant education and/or training will be considered a plus
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $55 - $65 per hour
Show more
Show less","Data Analysis, Data Management, Data Visualization, Data Warehousing, Tableau, SQL, Microsoft Access, Databases, Data Integration, Data Quality Control, Data Troubleshooting, Data Attribution, Data Reports, Data Portals, Data Resources, Business Intelligence","data analysis, data management, data visualization, data warehousing, tableau, sql, microsoft access, databases, data integration, data quality control, data troubleshooting, data attribution, data reports, data portals, data resources, business intelligence","business intelligence, data attribution, data integration, data management, data portals, data quality control, data reports, data resources, data troubleshooting, dataanalytics, databases, datawarehouse, microsoft access, sql, tableau, visualization"
PRINCIPAL DATA ENGINEER,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/principal-data-engineer-at-nike-3743907374,2023-12-17,Oregon,United States,Mid senior,Onsite,"Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.
Who Are We Looking For
We are looking for an experienced Principal Data Engineer to guide and influence multiple engineering teams to deliver scalable data and analytics solutions, implement new technologies, participate in solution design and architecture discussions. The ideal candidate will have outstanding communication skills, shown data design and implementation capabilities, strong eye for business, and a drive to deliver results. The person in this role will be technically proficient and excel at collaborating with engineers, analysts, and business partners. This person will be a self-starter, comfortable with ambiguity, and will enjoy working in a fast-paced dynamic environment.
What Will You Work On
In this role, you will build and deliver scalable data and analytics solutions focused on Nike Direct, Supply Chain, and Commercial space. You will design, implement and integrate new technologies and evolve data and analytics products. You will be chipping in to all aspects of data engineering from ingestion, transformation, and consumption in addition to designing and building test-driven development, reusable frameworks, automated workflows, and libraries at scale to support analytics products. You will also participate in architecture and design discussions to process and store high-volume data sets.
Who Will You Work With
This person will join the North America Data & Analytics Organization and you will work with world-class talent in the field of Data Engineering with a goal of better business insights and driving data-driven decisions across the organization. Youll be working closely with hardworking teams - Internal partners, Product Owners, Engineering Leaders, Data Analysts, Big Data Leads, and Engineers. One of Nikes maxims is ""Win as a Team"" and you will be working in a very collaborative environment and will find success in teamwork, a positive attitude, and hard work.
What You Bring
Bachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience
10+ years relevant work experience in the Data Engineering field
5+ years validated experience working with Hadoop and Big Data processing frameworks (Spark, Hive, Nifi, Spark-Streaming, Flink, etc.)
2+ years experience building scalable, real-time and high-performance cloud data lake solutions
Strong experience with relational SQL and programming languages such as Python, Scala, or Java
Experience with source control tools such as GitHub and related CI/CD processes
Experience working with Big Data streaming services such as Kinesis, Kafka, etc
Experience working with NoSQL data stores such as HBase, DynamoDB, etc
Experience provisioning RESTful APIs to enable real-time data consumption
Experience working in AWS environment primarily EMR, S3, Kinesis, Redshift, Athena, etc
Experience with data warehouses/RDBMS like Snowflake & Teradata
Experience with workflow scheduling tools like Airflow
Experience providing guidance and mentorship to other engineers
Strong understanding of algorithms, data structures, data architecture, and technical designs
Proven track record of rapidly learning new technologies and developing and implementing proof of concepts and practical, working code
Proven track record to influence and communicate effectively with team members and business partners
Proven track record to partner with teams in solving complex problems by taking a broad perspective to identify innovative solutions
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
Benefits
Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.
Show more
Show less","Data engineering, Hadoop, Spark, Hive, Nifi, SparkStreaming, Flink, SQL, Python, Scala, Java, GitHub, CI/CD, Kinesis, Kafka, HBase, DynamoDB, RESTful APIs, EMR, S3, Redshift, Athena, Snowflake, Teradata, Airflow, AWS, Algorithms, Data structures, Data architecture, Technical designs, Big data processing frameworks, Cloud data lake solutions, Data warehouses, RDBMS, Workflow scheduling tools, Data consumption, Data analysis, Data visualization, Data modeling, Machine learning, Agile development","data engineering, hadoop, spark, hive, nifi, sparkstreaming, flink, sql, python, scala, java, github, cicd, kinesis, kafka, hbase, dynamodb, restful apis, emr, s3, redshift, athena, snowflake, teradata, airflow, aws, algorithms, data structures, data architecture, technical designs, big data processing frameworks, cloud data lake solutions, data warehouses, rdbms, workflow scheduling tools, data consumption, data analysis, data visualization, data modeling, machine learning, agile development","agile development, airflow, algorithms, athena, aws, big data processing frameworks, cicd, cloud data lake solutions, data architecture, data consumption, data engineering, data structures, data warehouses, dataanalytics, datamodeling, dynamodb, emr, flink, github, hadoop, hbase, hive, java, kafka, kinesis, machine learning, nifi, python, rdbms, redshift, restful apis, s3, scala, snowflake, spark, sparkstreaming, sql, technical designs, teradata, visualization, workflow scheduling tools"
Data Engineer - 2,SPECTRAFORCE,"Portland, OR",https://www.linkedin.com/jobs/view/data-engineer-2-at-spectraforce-3787365473,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Title:
Data Engineer - 2
Duration:
12 Months
Location:
Portland, OR
Pay Rate:
Starting from $55/hr on W2
Top Skills
Data Pipelines
Data Architecture
ETL
Python
Data Warehouse
Roles And Responsibilities
Develop ETL processes to move third-party data into client's data warehouse, including housing and rental supply, census and demographics, legislation that may/does impact client, and social listening. Implement a refresh cadence for each source based on value and how often it’s updated.
Design, implement, and maintain data pipelines to connect client's internal and external data. This foundational data set drives ML models that predict where legislative challenges/opportunities are likely to arise and the users users most likely to engage politically.
Partner with E&I and DS to build data visualizations that enable service reporting on client’s legislative and regulatory landscape, incorporating available internal data and new external data sources as ETL processes are implemented.
Support the launch and optimization of Iterable (Email marketing automation platform) that Public Policy is using to engage legislators, journalists, partners, and Hosts by moving audience data into Iterable for automated communications and extracting engagement events and marketing campaign metadata to support data visualization.
Implement the integration of regulatory product and compliance data from client’s data warehouse - the Policy Cloud (Salesforce CRM) where the Public Policy team manages legislative and regulatory efforts.
Maintain and expand upon data pipelines that move structured internal client data into the Policy Cloud including host, listing, regulatory, compliance, and business value.
Required Skills
Expertise in data and analytics engineering/data architecture.
Expertise in Python and SQL. Expertise in R is a plus.
Experience leveraging disparate data sets, in particular legislative/regulatory/economic/geospatial.
Expertise in transforming data to be leveraged for self-service data visualization resources.
Experience building and implementing ML models is a plus
Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders.
Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals.
Ability to self-serve and take the initiative to find answers to technical questions.
Show more
Show less","Data Pipelines, Data Architecture, ETL, Python, SQL, R, Data Warehouse, ML Models, Data Visualization, Iterable, Salesforce CRM, Policy Cloud, Geospatial Data","data pipelines, data architecture, etl, python, sql, r, data warehouse, ml models, data visualization, iterable, salesforce crm, policy cloud, geospatial data","data architecture, datapipeline, datawarehouse, etl, geospatial data, iterable, ml models, policy cloud, python, r, salesforce crm, sql, visualization"
Data Engineer - 2,Russell Tobin,"Portland, OR",https://www.linkedin.com/jobs/view/data-engineer-2-at-russell-tobin-3782810256,2023-12-17,Oregon,United States,Mid senior,Onsite,"What are we looking for in our Data Engineer - 2?
Title- Data Engineer
Location Portland OR
Duration 12 months
pay $60-$63.hr
Skills:
Expertise in data and analytics engineering/data architecture.
Expertise in Python and SQL. Expertise in R is a plus.
Experience leveraging disparate data sets, in particular legislative/regulatory/economic/geospatial.
Expertise in transforming data to be leveraged for self-service data visualization resources.
Experience building and implementing ML models is a plus
Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders.
Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals.
Ability to self-serve and take the initiative to find answers to technical questions.
Education:
Bachelor degree in Computer Science or Computer Engineering
Rate/Salary: $60-$63
Show more
Show less","Data Engineering, Data Architecture, Python, SQL, R, Data Visualization Tools, Machine Learning, Data Analytics, Geospatial Data, Legislative Data, Regulatory Data, Economic Data, Complex Data Systems, Big Data Systems","data engineering, data architecture, python, sql, r, data visualization tools, machine learning, data analytics, geospatial data, legislative data, regulatory data, economic data, complex data systems, big data systems","big data systems, complex data systems, data architecture, data engineering, data visualization tools, dataanalytics, economic data, geospatial data, legislative data, machine learning, python, r, regulatory data, sql"
Software Engineer-Data Acquisition and Modelling,USNR,"Eugene, OR",https://www.linkedin.com/jobs/view/software-engineer-data-acquisition-and-modelling-at-usnr-3787913851,2023-12-17,Oregon,United States,Mid senior,Onsite,"$1,000 Retention Bonus Offered!
USNR is a global leader in the forest products equipment industry manufacturing systems to produce lumber, plywood, finger-jointed components, and engineered wood products. Pushing the limits of technology has made USNR the premier producer of forest products equipment in the world.
USNR offers a safe and rewarding work environment with opportunities and challenges that help employees grow professionally. Home to a dynamic team of people from a wide range of backgrounds, USNR provides everyone with the means to achieve their full potential. It’s the key to our success and why we encourage individual effort and teamwork.
We are actively seeking a
Software Engineer
to join our team of talented people to help continue our tradition of innovation and market leadership. This position works out of our facility in
Eugene, OR
. Come and grow with us!
In this role, you will be a member of a team dedicated to developing leading edge veneer processing algorithms for grade and moisture, used in the processing of veneer and plywood products.
How you Will Make a Positive Impact:
Gather development specifications to ensure their full range of responsibilities and project requirements are understood prior to beginning a task
Develop features from design through to completion. Works with a mentor to understand how the task fits within the broader architecture, project and/or total system
Provide support for legacy systems including troubleshooting and resolving issues
Follow prescribed coding standards and methodologies
When updating code or working on existing projects, you will read the existing code and consider the implications to the broader project when making changes
Complete required documentation and participate in code reviews
Complete unit level quality assurance and resolves defect tracking issues independently
The Value You Bring:
Diploma or degree in one of the following areas: technology, computer science, or engineering
Minimum of 5+ years software development experience is required
Experience applying mathematics to software development and analytics
Expert level in C++
Visual Studio experience
Ability to work effectively in a team and with customers
Positive attitude and willingness to learn.
Excellent interpersonal communication skills, both oral and written
Experience in the forestry industry a huge plus!
Git and MS SQL experience preferred
USNR is pleased to offer the following benefits to our full-time employees:
Medical, dental, and vision insurance available to employees and their dependents on the 1st of the month following date of hire
Optional flexible spending accounts (FSA) and health savings account (HSA)
Company-paid basic life and short-term disability insurance with employee-paid options for additional life insurance and long-term disability
Up to $300 per year in wellness activity reimbursement
401k employer match contribution at 50% of your contribution percentage, up to a total of 3%
Up to 96 hours of Paid Time Off (PTO) accrued per year with no waiting period to use accrued time
8 paid scheduled holidays and 1 paid floating holiday per calendar year
Up to $300 per year in Wellness Activity Reimbursement
Up to $5,250 per year tuition reimbursement for continuing education
Annual reimbursement for safety boots and safety glasses
This job posting is not meant to be an all-inclusive list of duties, responsibilities, and activities. It is intended to provide a summary of the position’s general scope and function within the company. Duties, responsibilities, and activities may change at any time with or without notice.
USNR and its affiliates offer competitive salaries, excellent benefits, and opportunities for growth. We take pride in and value diversity in our workforce, and we seek to recruit, develop, and retain the most talented people from a diverse candidate pool. We are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and employees without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, military and veteran status, and any other characteristic protected by applicable law. All employment is decided on the basis of qualifications, merit, and business need.
Any offer of employment is contingent on the successful completion of a background investigation and drug screen.
USNR participates in E-Verify and will only employ individuals who are verified as legally authorized to work in the United States. This position is not eligible for visa sponsorship, currently or in the future.
Current employees of USNR and affiliates, and rehires within less than one year, are not eligible for the retention bonus.
To learn more about our company, and the products and services we provide, please visit
.
Powered by JazzHR
VMGcddTh3I
Show more
Show less","C++, Visual Studio, Git, MS SQL, Software Development, Mathematics, Forestry Industry","c, visual studio, git, ms sql, software development, mathematics, forestry industry","c, forestry industry, git, mathematics, ms sql, software development, visual studio"
Senior Data Engineer,Mercury,"Portland, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mercury-3768472930,2023-12-17,Oregon,United States,Mid senior,Onsite,"In the 1880s, Herman Hollerith noticed the US Census was taking over 8 years to calculate. To solve this, he invented a tabulating machine using punch cards that dramatically sped up the process and served as the foundation for innovation in high-quality data gathering.
We’re looking for Senior Data Engineers who can help us build our high-quality data engine that informs how we invest in and build Mercury’s future. You’ll be early to building a data-informed culture across Mercury so that we can all determine what’s happening, react quickly, and invest intelligently.
Here are some things you’ll do on the job:
Partner with leadership, engineers, and data scientists to understand data needs and build systems that deliver high-quality and reliable data
Own and maintain the data systems that extract, transform, and load data into internal and external tooling
Apply proven expertise and build high-performance scalable data warehouses.
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts).
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data.
You should:
Have 2+ years of experience working with analytics teams on building high-quality and reliable data infrastructure.
Be able to navigate from architecture and implementation decisions related to data infrastructure to guide teams towards building reliable and accurate pipelines and company-critical data sets.
Have familiarity with postgres backend data, Snowflake, and data transformation tools like dbt.
Value quality in data tools, testing and innovation.
The total rewards package at Mercury includes base salary, equity (stock options), and benefits.
Our salary and equity ranges are highly competitive within the SaaS and fintech industry and are updated regularly using the most reliable compensation survey data for our industry. New hire offers are made based on a candidate’s experience, expertise, geographic location, and internal pay equity relative to peers.
Our target new hire base salary ranges for this role are the following:
US employees (any location): $173,600 - $204,200 USD
Canadian employees (any location): CAD 158,000-185,800
Show more
Show less","Data Engineering, Postgres, Snowflake, dbt, Data Warehousing, Data Pipelines, Data Infrastructure, Data Quality, Data Testing, Data Innovation, Data Analytics, Software Engineering, DataInformed Culture, DataDriven Decision Making, Data Visualization, Data Analysis, SQL","data engineering, postgres, snowflake, dbt, data warehousing, data pipelines, data infrastructure, data quality, data testing, data innovation, data analytics, software engineering, datainformed culture, datadriven decision making, data visualization, data analysis, sql","data engineering, data infrastructure, data innovation, data quality, data testing, dataanalytics, datadriven decision making, datainformed culture, datapipeline, datawarehouse, dbt, postgres, snowflake, software engineering, sql, visualization"
Data Analyst,Insight Global,"Beaverton, OR",https://www.linkedin.com/jobs/view/data-analyst-at-insight-global-3778866319,2023-12-17,Oregon,United States,Mid senior,Onsite,"***Cannot work on a C2C basis***
Global Product Writing is seeking a Product Data Analyst to join our on-site team with Evergreen to support the ongoing development of our data infrastructure in partnership with the Global Product Writing Team. This person will aggregate and analyze sets of data, produce metrics and KPIs, and construct visualizations using data visualization and reporting platforms.
In this role you will…
● Work side by side with the Sr. Manager and Evergreen Program Manager across a variety of data models, systems, and tools to ensure ongoing success of the 2H priorities.
● Work in Airtable. A lot of Airtable in fact. You will be partnering with Global Product Writing Ops which manages all work performed from an Airtable environment.
● Write SQL queries to pull data from internal systems into the Airtable environment.
● Be a team-player. We work with a large team and have many partnering teams that rely on our ability to drive business-needs and deliver results.
● See data in a three dimensional way - taking metrics and KPIs and bringing them to life through visual mediums that are digestible by experts and laymen alike.
● Expand the current toolkit of data reporting using platforms like Power BI and Tableau
Responsibilities:
● Partner with the Evergreen Delivery Team to bring together the greater vision of Global Product Writing through data, metrics, and organization-wide KPIs.
● Working closely with the Global Product Writing Ops Team and attending their weekly meetings to ensure that the SPR is properly plugged into the right data sets and drive the ongoing projects we are working on in collaboration with that team.
● Aggregate and analyze often complex and disparate sets of data, combining data-sets to surface important insights to drive business decisions.
● Perform data validation and data cleansing to ensure high-quality and accurate data.
What We’re Looking For:
● Strong analytical skills
● Ability to create custom SQL queries
● Advanced proficiency in Airtable Automations and Interfaces
● Experience with Airtable configuration and project management
● Project management skills including the ability track work independently and prioritize requests from a variety of stakeholders
● Ability to work with multiple technologies such as Tableau, Power BI, and SQL
● Self-starter who can pivot between macro and micro trend identification and be able to deliver a comprehensive narrative of data insights and projected operational opportunities
Bonus Skills:
● Experience within Content Production
Show more
Show less","Data Analysis, SQL, Data Visualization, Airtable, Power BI, Tableau, Project Management, Trend Identification, Data Validation, Data Cleansing","data analysis, sql, data visualization, airtable, power bi, tableau, project management, trend identification, data validation, data cleansing","airtable, data validation, dataanalytics, datacleaning, powerbi, project management, sql, tableau, trend identification, visualization"
Sr. Data Engineers,"GSPANN Technologies, Inc","Portland, OR",https://www.linkedin.com/jobs/view/sr-data-engineers-at-gspann-technologies-inc-3647472405,2023-12-17,Oregon,United States,Mid senior,Onsite,"AWS, PySpark, Hive, Snowflake, Apache Airflow
Description
We are looking for Sr. Data Engineers who hold H1B Visa/TN Permit/Green Card, or are US Citizens. The person will join our team in Portland, Oregon. Once you become a part of our dynamic team, we will help you elevate your career with incredible opportunities.
Who We Are
GSPANN has been in business for over a decade, with over 2000 employees worldwide, and servicing some of the largest retail, high technology, and manufacturing clients in North America. We provide an environment that enables career growth while still interacting with company leadership.
Visit Why GSPANN for more information.
Location: Portland
Role Type: Full Time / Contract
Published On: 17 August 2022
Experience: 4+ Years
Share this job
Description
We are looking for Sr. Data Engineers who hold H1B Visa/TN Permit/Green Card, or are US Citizens. The person will join our team in Portland, Oregon. Once you become a part of our dynamic team, we will help you elevate your career with incredible opportunities.
Role And Responsibilities
Analyze and organize raw data.
Build data systems and pipelines.
Evaluate business needs and objectives.
Interpret trends and patterns.
Conduct complex data analysis and report the results.
Prepare data for prescriptive and predictive modeling.
Build algorithms and prototypes.
Combine raw information from different sources.
Explore ways to enhance data quality and reliability.
Identify opportunities for data acquisition.
Develop analytical tools and programs.
Collaborate with data scientists and architects on several projects.
Develop and maintain datasets.
Improve data quality and efficiency.
Skills And Experience
Minimum 4 years of work experience as a Data Engineer.
Prior experience in building scalable, real-real time and high-performance data solutions.
Must have expert-level SQL skills.
Hands-on with scripting languages such as Shell, Python.
Expertise in source control tools such as GitHub and related development processes.
Good understanding of workflow scheduling tools.
Able to look at a problem/existing landscape, propose and help build the solution.
Prior experience in working with data structures and algorithms.
Good understanding of solution and technical designs.
Must have a strong problem-solving and analytical mindset.
Able to communicate effectively, both verbally and written, with the team members.
Show more
Show less","AWS, PySpark, Hive, Snowflake, Apache Airflow, SQL, Shell, Python, GitHub, Data structures, Algorithms, Problemsolving, Analytical mindset, Communication","aws, pyspark, hive, snowflake, apache airflow, sql, shell, python, github, data structures, algorithms, problemsolving, analytical mindset, communication","algorithms, analytical mindset, apache airflow, aws, communication, data structures, github, hive, problemsolving, python, shell, snowflake, spark, sql"
Senior Software Engineer — Data Center Modeling,NVIDIA,"Hillsboro, OR",https://www.linkedin.com/jobs/view/senior-software-engineer-%E2%80%94-data-center-modeling-at-nvidia-3750224397,2023-12-17,Oregon,United States,Mid senior,Onsite,"Widely considered to be one of the technology world’s most desirable employers, NVIDIA is an industry leader with groundbreaking developments in High-Performance Computing, Artificial Intelligence and Visualization. The GPU, our invention, serves as the visual cortex of modern computers and is at the heart of our products and services.
Our work opens up new universes to explore, enables outstanding creativity and discovery and powers what were once science fiction inventions from artificial intelligence to autonomous cars. We are looking for outstanding computer architects to design and develop models for the next generation of GPU-accelerated data centers.
What You'll Be Doing
Collaborate with a team of architects and software engineers to develop tools and methodologies to model power, cooling, performance, reliability, and TCO at the datacenter scale.
Constantly learn about hardware architecture and the performance analysis process to improve the efficiency and effectiveness of our tools
What We Need To See
Degree in Computer Engineering, Computer Science, Electrical Engineering or related fields or equivalent experience
10+ years of experience in systems architecture covering power, performance, and modeling.
Strong programming skills in Python well as strong mathematical/analytical skills.
A demonstrated ability to design, deliver and support production quality software, tools and infrastructure.
Excellent interpersonal skills with success leading projects across multi-discipline teams.
Ways To Stand Out From The Crowd
Experience with accelerated server architecture design and deployment.
Background in data center/cloud computing design.
Expertise in data analysis and visualization.
NVIDIA offers highly competitive salaries and a comprehensive benefits package. We have some of the most brilliant and talented people in the world working for us and, due to unprecedented growth, our world-class engineering teams are growing fast. If you're a creative and autonomous engineer with real passion for technology, we want to hear from you!
The base salary range is 176,000 USD - 333,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","HighPerformance Computing, Artificial Intelligence, Visualization, GPU, Data Center, Power, Cooling, Performance, Reliability, TCO, Modeling, Hardware Architecture, Performance Analysis, Python, Mathematical/Analytical Skills, Data Analysis, Visualization, Accelerated Server Architecture, Data Center/Cloud Computing, Software Design, Software Development, Software Delivery, Software Support, Infrastructure, Interpersonal Skills, Project Leadership, Collaboration, Teamwork","highperformance computing, artificial intelligence, visualization, gpu, data center, power, cooling, performance, reliability, tco, modeling, hardware architecture, performance analysis, python, mathematicalanalytical skills, data analysis, visualization, accelerated server architecture, data centercloud computing, software design, software development, software delivery, software support, infrastructure, interpersonal skills, project leadership, collaboration, teamwork","accelerated server architecture, artificial intelligence, collaboration, cooling, data center, data centercloud computing, dataanalytics, gpu, hardware architecture, highperformance computing, infrastructure, interpersonal skills, mathematicalanalytical skills, modeling, performance, performance analysis, power, project leadership, python, reliability, software delivery, software design, software development, software support, tco, teamwork, visualization"
Lead Data Analyst,"Aroghia Group, LLC","Oregon, United States",https://www.linkedin.com/jobs/view/lead-data-analyst-at-aroghia-group-llc-3782734050,2023-12-17,Oregon,United States,Mid senior,Onsite,"Must have experience working as a Lead
10+ years proven work experience in Data Analyst or Business Data Analyst role.
Strong Experience in SQL and a scripting language such as Python or R
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Experience with data warehouses/RDBMS like Snowflake & Teradata
Experience working with BI tools like Tableau, Power BI or DOMO
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling)
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc preferred.
Good understanding of Hadoop and Big Data processing frameworks
Experience with source control tools such as GitHub
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis.
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment.
Self-starter, highly organized and result oriented; and should be able to work with minimal direction.
Show more
Show less","Data Analyst, Business Data Analyst, SQL, Python, R, Problem Solving, Analytical Skills, Data Warehousing, Data Modeling, Analytical Algorithms, Forecasting, Modeling, AWS, EMR, S3, Redshift, Athena, Hadoop, Big Data Processing Frameworks, Source Control Tools, GitHub, Data Cleansing, Data Curation, Data Manipulation, Data Visualization, Storytelling, Key Performance Metrics, Reports, Data Delivery, Agile Environment, SelfStarter, Organization, Result Orientation","data analyst, business data analyst, sql, python, r, problem solving, analytical skills, data warehousing, data modeling, analytical algorithms, forecasting, modeling, aws, emr, s3, redshift, athena, hadoop, big data processing frameworks, source control tools, github, data cleansing, data curation, data manipulation, data visualization, storytelling, key performance metrics, reports, data delivery, agile environment, selfstarter, organization, result orientation","agile environment, analytical algorithms, analytical skills, athena, aws, big data processing frameworks, business data analyst, data curation, data delivery, data manipulation, dataanalytics, datacleaning, datamodeling, datawarehouse, emr, forecasting, github, hadoop, key performance metrics, modeling, organization, problem solving, python, r, redshift, reports, result orientation, s3, selfstarter, source control tools, sql, storytelling, visualization"
Senior DataCenter System Engineer,NVIDIA,"Hillsboro, OR",https://www.linkedin.com/jobs/view/senior-datacenter-system-engineer-at-nvidia-3654699406,2023-12-17,Oregon,United States,Mid senior,Onsite,"NVIDIA’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics, and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU-accelerated servers acting as the brain of modern data centers. The NVIDIA Datacenter System group is looking for a Senior System Diagnostics Engineer with an interest in design automation, tools, integration, verification and validation. This position offers the opportunity to make a broad impact across many projects while developing on internal solutions to enable system creation in the most optimized way.
What You'll Be Doing
Lead planning, defining test requirements and optimizing the production line to deliver new NVIDIA GPU datacenter products.
Be instrumental in driving the best-in-class quality metrics by tracking and ensuring adequate test coverage of all aspects of the product.
Collaborate with Test Engineering, Product Engineering and SW teams to ensure successful production test diagnostics SW package releases through various stages of NPI, product ramp and entire lifecycle.
Leverage your in-depth experience with statistical analysis tools and data parsing scripts to define manufacturing test spec limits for various parameters.
Analyze, debug and resolve critical firmware and software issues, often under tight time schedules.
Use your knowledge of system power-up and handshakes during boot to debug sophisticated interactions between HW, FW and SW on faulty boards.
Early engagement with HW/FW/SW engineering teams, and other groups, to build end-to-end solutions and optimize datacenter product designs.
Parse manufacturing logs and manipulate databases to analyze failures.
Collaborate and establish continuous improvements in our manufacturing flows and production test diagnostics.
Innovating!
What We Need To See
Bachelors or Masters degree in Math, Computer Science, or Engineering field or equivalent experience
6+ years of experience on server systems.
Strong problem solving and software engineering skills, a passion for applying them to new challenges and a commitment to high quality work.
Expertise in Python or a similar language and an understanding of object- oriented programming
Proven experience on Server architectures, CPU baseboards and GPU technology in order to productize new GPU boards and GPU-accelerated Server architectures.
Consistent track record of conceptualizing, designing, and implementing modular and robust software components with well-thought-out APIs and interfaces.
Deep knowledge of server systems including SBIOS, BMC, network, power, rack layouts, cabling, and experience with compute, storage and GPU servers in both air and water cooled environments.
Knowledge of IPMI/SNMP/Redfish.
Ability to multitask effectively in a dynamic environment.
You love solving hard problems and can work independently or as part of a team.
Widely considered to be one of the technology world’s most desirable employers, NVIDIA leads the way in groundbreaking developments in Artificial Intelligence, High-Performance Computing and Visualization. The GPU, our invention, serves as the visual cortex of modern computers and is at the heart of our products and services. Our work opens up new universes to explore, enables amazing creativity and discovery, and powers what were once science fiction inventions from artificial intelligence to autonomous cars. NVIDIA is looking for great people in multiple teams to help us accelerate the next wave of artificial intelligence, in Software, Hardware, Research and more. If you are creative and passionate, we want to hear from you.
The base salary range is 152,000 USD - 287,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","Python, Objectoriented programming, Server architectures, CPU baseboards, GPU technology, SBIOS, BMC, Network, Power, IPMI, SNMP, Redfish, C++, Statistical analysis tools, Data parsing scripts","python, objectoriented programming, server architectures, cpu baseboards, gpu technology, sbios, bmc, network, power, ipmi, snmp, redfish, c, statistical analysis tools, data parsing scripts","bmc, c, cpu baseboards, data parsing scripts, gpu technology, ipmi, network, objectoriented programming, power, python, redfish, sbios, server architectures, snmp, statistical analysis tools"
Finance Master Data Analyst,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/finance-master-data-analyst-at-nike-3759877385,2023-12-17,Oregon,United States,Mid senior,Onsite,"Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.
With an aggressive long-term growth strategy, NIKE is counting on the Finance function to thrive in its mission to drive and deliver value to every NIKE shareholder. Our consumers are demanding more, increasing the complexity of our business models and need to move faster with greater precision. Nike's Finance function is on a journey to transform and is strategically investing in tools that allow us to evolve with the Consumer Direct Offense. The Finance Business Intelligence team's mission is to re-invent reporting and analytics capabilities globally, and we're seeking world-class talent with a passion for data and analytics, a disciplined focus on business value, and ability to influence change!
Who We Are Looking For
We are looking for a Finance Master Data Analyst who is passionate about data management, data transformation, and process excellence. The ideal candidate is skilled in information systems, data structures, and problem solving. This candidate should be comfortable working with a diverse group of partners and communicating and executing work you're doing with leadership. You will lead transformation efforts in Nike's Finance Master Data Management strategy and ensure data integrity across Nike's system landscape.
What You Will Work On
As the Finance Master Data Analyst, you will be responsible for administering key processes and projects associated with the timeliness and accuracy of our financial data while adhering to data governance policies. This position partners across all Finance functions to assist business partners in understanding project impacts to Finance Master Data as well as provides education to the Finance community on Master Data and Master Data Governance concepts. You will also partner closely with the Finance Data Governance team to provide input on policies and sustain the policies through operation procedures.
Who You Will Work With
You will engage with Global and Geo Finance Planning and Controlling teams, Nike Tech teams, Finance Project teams, and the Finance Business Intelligence Team.
What You Bring
Bachelor's degree (B.S.) in quantitative field such as Statistics, Mathematics, Economics, Business, or Finance or combination of relevant education, experience and training
2+ years of relevant experience- preferably including experience with a national public accounting firm or with a Fortune 500/multinational consumer products company
Experience using SAP, BPC, EPM
Excellent oral, written, and interpersonal communication skills; ability to effectively communicate status, issues, and risks with the team
A proven track record of delivering results (quality, time, budget) on globally and regionally based projects preferred
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
Benefits
Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.
Show more
Show less","Data management, Data transformation, Process excellence, Information systems, Data structures, Problem solving, Data governance, Policies, SAP, BPC, EPM, Communication skills, Oral communication, Written communication, Interpersonal communication, Project delivery, Global projects, Regional projects","data management, data transformation, process excellence, information systems, data structures, problem solving, data governance, policies, sap, bpc, epm, communication skills, oral communication, written communication, interpersonal communication, project delivery, global projects, regional projects","bpc, communication skills, data governance, data management, data structures, data transformation, epm, global projects, information systems, interpersonal communication, oral communication, policies, problem solving, process excellence, project delivery, regional projects, sap, written communication"
Lead Data Analyst,"Centizen, Inc.","Beaverton, OR",https://www.linkedin.com/jobs/view/lead-data-analyst-at-centizen-inc-3774809225,2023-12-17,Oregon,United States,Mid senior,Onsite,"Lead Data Analyst
7+ years proven work experience in Data Analyst or Business Data Analyst role
Strong Experience in
SQL and a scripting language
such as Python or R
Strong
problem solving and analytical skills
with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Experience with
data warehouses/RDBMS
like Snowflake & Teradata
Experience working with
BI tools
like Tableau, Power BI or DOMO
Strong understanding of
data modeling and analytical algorithms
(including forecasting and modeling)
Experience working in
AWS environment
primarily EMR, S3, Redshift, Athena, etc preferred
Good understanding of Hadoop and Big Data processing frameworks
Experience with source control tools such as GitHub
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment
Self-starter, highly organized and result oriented; and should be able to work with minimal direction
Required
DATA ANALYSIS
DATA MODELING
DATASETS
DATABASE
DATA CLEANING
Show more
Show less","Data Analysis, Data Modeling, Datasets, Database, Data Cleaning, SQL, Python, R, Tableau, Power BI, DOMO, Snowflake, Teradata, EMR, S3, Redshift, Athena, AWS, Hadoop, Big Data, GitHub, Data Warehousing, RDBMS, Forecasting, Modeling, BI Tools, Data Visualization, StoryTelling, Agile, Communication, Presentations, Key Performance Metrics, Reports","data analysis, data modeling, datasets, database, data cleaning, sql, python, r, tableau, power bi, domo, snowflake, teradata, emr, s3, redshift, athena, aws, hadoop, big data, github, data warehousing, rdbms, forecasting, modeling, bi tools, data visualization, storytelling, agile, communication, presentations, key performance metrics, reports","agile, athena, aws, bi tools, big data, communication, data cleaning, dataanalytics, database, datamodeling, datasets, datawarehouse, domo, emr, forecasting, github, hadoop, key performance metrics, modeling, powerbi, presentations, python, r, rdbms, redshift, reports, s3, snowflake, sql, storytelling, tableau, teradata, visualization"
CaSA Marketplace Data Analyst,"Centizen, Inc.","Beaverton, OR",https://www.linkedin.com/jobs/view/casa-marketplace-data-analyst-at-centizen-inc-3777077840,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Description
The candidate will be responsible for commercial and retail data tools & solutions for Client’s business throughout the Central & South American region. Some of the key areas of ownership are:
1) data management (quality control, troubleshooting data issues, attributing datapoints, etc),
2) Tableau dashboard management (maintenance, enhancements required by business, new product deployments, etc) and
3) maintenance & enhancements in local databases in Microsoft Access.
Show more
Show less","Data management, Quality control, Troubleshooting, Datapoints, Tableau, Dashboard management, Microsoft Access, SQL, Data analysis, Reporting","data management, quality control, troubleshooting, datapoints, tableau, dashboard management, microsoft access, sql, data analysis, reporting","dashboard management, data management, dataanalytics, datapoints, microsoft access, quality control, reporting, sql, tableau, troubleshooting"
Data Center/Systems Administrator II,ASK Consulting,"Hillsboro, OR",https://www.linkedin.com/jobs/view/data-center-systems-administrator-ii-at-ask-consulting-3781008341,2023-12-17,Oregon,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: Data Center/Systems Administrator II
Location: Hillsboro, OR (97124)
Duration: 12 Months
Pay rate: $45.00 – $48.92/hr. on W2.
Job description:
Key responsibilities include performing general server administration tasks, monitoring and optimizing system performance and reliability, operational workflow development, and managing enhancements/upgrades and providing - Various Ievels of support.
Develops and maintain system documentation for Iab/Data Center configuration and customizations.
Designs, develops, and maintains custom templates and reports for performance and proof of concept testing.
Provides application and server Ievel support for server environments and serve as the Subject Matter Expert for asset management system.
Lead Technical efforts for software upgrade, patch updates, data migration, for server software.
Apply configuration and tuning standards in accordance with Microsoft and Iinux recommendations and clients’ requirements.
Develop and maintain system documentation for server/software configuration and customizations.
Conduct system performance analysis and performance improvements in collaboration with Architects, Engineers, and Network Engineer to insure system efficiency.
Provide Tier 2 and Tier 3 Ievel support. Strong knowledge of - Virtualization Technology including - VMWare ESX(i), Citrix Administration Skills and Netscaler.
Minimum 3-5 years’ experience with Windows and Iinux (UNIX) server administration including Active Directory, Domain Name Services and DHCP.
Knowledge of IP-KVM would be a plus Years of experience required for these positions: At Ieast five years direct I.T. industry experience.
Associate degree is required. Bachelor's degree is preferred.
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Server Administration, System Performance Optimization, System Reliability, Operational Workflow Development, Systems Integration, System Documentation, Customization, Performance Testing, Proof of Concept Testing, Application Support, Server Level Support, Software Upgrade, Patch Updates, Data Migration, Configuration Management, Tuning, Microsoft Standards, Linux Recommendations, Clients’ Requirements, Virtualization Technology, VMWare ESX(i), Citrix Administration Skills, Netscaler, Windows Server Administration, Linux Server Administration, Active Directory, Domain Name Services, DHCP, IPKVM","server administration, system performance optimization, system reliability, operational workflow development, systems integration, system documentation, customization, performance testing, proof of concept testing, application support, server level support, software upgrade, patch updates, data migration, configuration management, tuning, microsoft standards, linux recommendations, clients requirements, virtualization technology, vmware esxi, citrix administration skills, netscaler, windows server administration, linux server administration, active directory, domain name services, dhcp, ipkvm","active directory, application support, citrix administration skills, clients requirements, configuration management, customization, data migration, dhcp, domain name services, ipkvm, linux recommendations, linux server administration, microsoft standards, netscaler, operational workflow development, patch updates, performance testing, proof of concept testing, server administration, server level support, software upgrade, system documentation, system performance optimization, system reliability, systems integration, tuning, virtualization technology, vmware esxi, windows server administration"
"Data Center\/Systems Administrator @ Hillsboro, OR - 100% onsite",Infobahn Softworld Inc,"Hillsboro, OR",https://www.linkedin.com/jobs/view/data-center-systems-administrator-%40-hillsboro-or-100%25-onsite-at-infobahn-softworld-inc-3756471720,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Title: Data Center\/Systems Administrator
Location: Hillsboro, OR - 100% onsite
Duration: 1 Year
""candidate must possess a BS in Computer science\software engineering.""
Looking for Customer facing System Administrator
Must Haves
Factory support
EMC
NetApp
Networking, Installations, Cables, switch, routers, backup, recovery
Build windows operating server
Able to change parts on server
knowledge of data center parts and assemblies
Technical Skills
Troubleshooting: Windows, Linux, Storage systems, Data Center
Databases: SQL, Oracle
Nice To Haves
VMware ESX(i) environment, Citrix
NPS-natural power supply
Job Description
Key responsibilities include performing general server administration tasks, monitoring and optimizing system performance and reliability, operational workflow development, and managing enhancements/upgrades and providing - Various Ievels of support.
Develops and maintain system documentation for Iab/Data Center configuration and customizations.
Designs, develops, and maintains custom templates and reports for performance and proof of concept testing.
Provides application and server Ievel support for server environments and serve as the Subject Matter Expert for asset management system.
Lead Technical efforts for software upgrade, patch updates, data migration, for server software.
Apply configuration and tuning standards in accordance with Microsoft and Iinux recommendations and clients requirements.
Develop and maintain system documentation for server/software configuration and customizations.
Conduct system performance analysis and performance improvements in collaboration with Architects, Engineers and Network Engineer to insure system efficiency.
Provide Tier 2 and Tier 3 Ievel support. Strong knowledge of - Virtualization Technology including - VMWare ESX(i), Citrix Administration Skills and Netscaler.
Minimum 3-5 years' experience with Windows and Iinux (UNIX) server administration including Active Directory, Domain Name Services and DHCP.
Expertise in administration of IIS (Internet Information Server) and Apache Web Servers including installation, configuration, monitoring, upgrade and Web Application installation.
Knowledge of computer diagnostics and installation, to include hardware/software troubleshooting and networking.
Demonstrated experience working with infrastructure related components such as running network cables, installing servers in equipment racks.
Familiarity with the operation and configuration of networking protocols, network interface card installation, switches , routers and similar components
Ability to work in a fast paced environment and offer effective solutions under tight deadlines
Strong Problem-solving And Root Cause Analysis Skills
Ability to work after hours support on a rotating schedule
Must be capable of lifting 1U and 2U rack mounted servers up to 40 pounds and familiar with the use of Rack Jacks and general data center safety procedures
Follow written and/or - Verbal instructions for custom operating system and application installs
Maintaining and auditing Iab assets and routine inventory control
Additional Skills And Or Education
Capable of Iearning and using custom built asset and inventory tracking software (training will be provided)
Knowledge of data center power system i.e. PDU, 120/240v would be helpful.
Knowledge of wire and cable management would be a plus.
Knowledge of IP-KVM would be a plus
Years of experience required for this positions: At Ieast 3-5 years direct I.T. industry experience
Show more
Show less","Windows Server, Linux Administration, Active Directory, Domain Name Services, DHCP, IIS, Apache Web Server, VMware ESX(i), Citrix Administration Skills, Netscaler, SQL, Oracle, Networking Protocols, Network Interface Card Installation, Switches, Routers, Rack Jacks, PDU, IPKVM, Data Center Power Systems, Wire and Cable Management, Custom Operating System Installs, Application Installs, Asset and Inventory Tracking Software, Data Center Safety Procedures","windows server, linux administration, active directory, domain name services, dhcp, iis, apache web server, vmware esxi, citrix administration skills, netscaler, sql, oracle, networking protocols, network interface card installation, switches, routers, rack jacks, pdu, ipkvm, data center power systems, wire and cable management, custom operating system installs, application installs, asset and inventory tracking software, data center safety procedures","active directory, apache web server, application installs, asset and inventory tracking software, citrix administration skills, custom operating system installs, data center power systems, data center safety procedures, dhcp, domain name services, iis, ipkvm, linux administration, netscaler, network interface card installation, networking protocols, oracle, pdu, rack jacks, routers, sql, switches, vmware esxi, windows server, wire and cable management"
Lead Data Analyst,Mavensoft Technologies,"Hillsboro, OR",https://www.linkedin.com/jobs/view/lead-data-analyst-at-mavensoft-technologies-3775857162,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Title
: Lead Data Analyst
Duration
:  5 months (contract)
Location
: Portland, OR (Onsite)
Key Skills:
SQL, (Python or R), AWS, Data warehouse, BI, Data Analysis, Data Cleansing, Data Curate, Data Manipulate, and Documentation.
Job Requirements
:
7+ years proven work experience in Data Analyst or Business Data Analyst role
Strong Experience in SQL and a scripting language such as Python or R
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Experience with data warehouses/RDBMS like Snowflake & Teradata
Experience working with BI tools like Tableau, Power BI or DOMO
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling)
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc preferred
Good understanding of Hadoop and Big Data processing frameworks
Experience with source control tools such as GitHub
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment
Self-starter, highly organized and result oriented; and should be able to work with minimal direction
Email your resume to usjobs@mavensoft.com
To learn more about Mavensoft visit us online at http://www.mavensoft.com/
Show more
Show less","SQL, Python, R, AWS, Data warehouse, BI, Data Analysis, Data Cleansing, Data Curate, Data Manipulation, Documentation, Data Modeling, Analytical Algorithms, Hadoop, Big Data Processing Frameworks, Source Control Tools, GitHub, Visualization, Storytelling, Presentations, Key Performance Metrics, Reports, Data","sql, python, r, aws, data warehouse, bi, data analysis, data cleansing, data curate, data manipulation, documentation, data modeling, analytical algorithms, hadoop, big data processing frameworks, source control tools, github, visualization, storytelling, presentations, key performance metrics, reports, data","analytical algorithms, aws, bi, big data processing frameworks, data, data curate, data manipulation, dataanalytics, datacleaning, datamodeling, datawarehouse, documentation, github, hadoop, key performance metrics, presentations, python, r, reports, source control tools, sql, storytelling, visualization"
Lead Azure Data Engineer,Dice,"Tualatin, OR",https://www.linkedin.com/jobs/view/lead-azure-data-engineer-at-dice-3786292489,2023-12-17,Oregon,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, KPI Partners, Inc., is seeking the following. Apply via Dice today!
KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces.
We enable your growth
At KPI, you can become who you want to be and learn skills that will take you further in your career
Continuously upgrade yourself
Develop as a future leader
Drive cloud enablement around the world
Engineering Excellence
Enhance your engineering expertise with our unique approach
This program gives engineers the opportunity to excel in product and software engineering by learning our industry-leading practices, tools, and technologies to build excellence by enhancing their competencies and skills
Visit to Know more :
Title: Lead Azure Data Engineer – 6 Months Contract (Hybrid Role)
Location: San Francisco Bay Area, Dallas, TX, Portland, OR
Key Skills:
ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql
Description
Enterprise Data modelling / Design
Azure SQL Data Warehouse (Synapse)
T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..)
ADF / Any Enterprise ETL Tool
ADLS / any cloud storage
Should be able to understand the technical specifications and able to work independently with minimal or no supervision
Must have Skills :
Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric.
Proven experience with SQL, namely schema design and dimensional data modelling
Solid knowledge of data warehouse best practices, development standards and methodologies
Strong experience with Azure Cloud on data integration with Databricks
Be an independent self-learner with the “let’s get this done” approach and ability to work in Fast paced and Dynamic environment
Nice-to-Have Skills:
Basic understanding on ML Studio, AI/ML, MLOps etc.
Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge.
Good to have SAP Hana knowledge
Intermediate knowledge on Power BI
Good to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes.
Show more
Show less","Azure, Data Factory, Synapse, TSQL, Enterprise Data Modelling, Azure SQL Data Warehouse, ADF, ADLS, Cloud Storage, Azure Services, Data Migration, Data Processing, SQL, Schema Design, Dimensional Data Modelling, Data Warehouse Best Practices, Development Standards, Methodologies, Azure Cloud, Data Integration, Databricks, ML Studio, AI/ML, MLOps, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db, SAP Hana, Power BI, DevOps, CI/CD Deployments, Cloud Migration Methodologies, Data Modelling","azure, data factory, synapse, tsql, enterprise data modelling, azure sql data warehouse, adf, adls, cloud storage, azure services, data migration, data processing, sql, schema design, dimensional data modelling, data warehouse best practices, development standards, methodologies, azure cloud, data integration, databricks, ml studio, aiml, mlops, event hub, iot hub, azure stream analytics, azure analysis service, cosmo db, sap hana, power bi, devops, cicd deployments, cloud migration methodologies, data modelling","adf, adls, aiml, azure, azure analysis service, azure cloud, azure services, azure sql data warehouse, azure stream analytics, cicd deployments, cloud migration methodologies, cloud storage, cosmo db, data factory, data integration, data migration, data modelling, data processing, data warehouse best practices, databricks, development standards, devops, dimensional data modelling, enterprise data modelling, event hub, iot hub, methodologies, ml studio, mlops, powerbi, sap hana, schema design, sql, synapse, tsql"
Marketplace Data Analyst #: 23-07210,HireTalent - Diversity Staffing & Recruiting Firm,"Beaverton, OR",https://www.linkedin.com/jobs/view/marketplace-data-analyst-%23-23-07210-at-hiretalent-diversity-staffing-recruiting-firm-3781964572,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Title: Marketplace Data Analyst
Job Location: Beaverton, OR
Job Duration: 9 Months on W2
Top 3 Skills
Tableau: In this position we need someone capable of designing & maintaining dashboards
Data Management: Ensuring data quality flows according to the expectation of tools.
Microsoft Access: Data ingestion tools & database design/management is required as well.
Job Description
The candidate will be responsible for Commercial/Retail data tools & solutions for Client’s business throughout the Central & South American region. Some of the key areas of ownership are:
data management (quality control, troubleshooting data issues, attributing datapoints, etc),
Tableau dashboards management (maintenance, enhancements required by business, new product deployments, etc) and
maintenance & enhancements in local databases in Microsoft Access.
Duties
Generates reports and regular datasets or report information for end- users using system tools and database or data warehouse queries and scripts. Integrates data from multiple sources to produce requested or required data elements. Programs and maintains report forms and formats, information dashboards, data generators, canned reports and other end- user information portals or resources. May create specifications for reports based on business requests.
Skills
Typical Office: This is a typical office job, with no special physical requirements or unusual work environment.
Show more
Show less","Tableau, Data Management, Microsoft Access, Data Quality Control, Data Troubleshooting, Data Attribution, Dashboard Maintenance, Dashboard Enhancement, Database Maintenance, Database Enhancement, Data Visualization, Data Integration, Data Warehousing, SQL, Reporting","tableau, data management, microsoft access, data quality control, data troubleshooting, data attribution, dashboard maintenance, dashboard enhancement, database maintenance, database enhancement, data visualization, data integration, data warehousing, sql, reporting","dashboard enhancement, dashboard maintenance, data attribution, data integration, data management, data quality control, data troubleshooting, database enhancement, database maintenance, datawarehouse, microsoft access, reporting, sql, tableau, visualization"
Healthcare Data Analyst – Operational Informatics,Energy Jobline,"Beaverton, OR",https://www.linkedin.com/jobs/view/healthcare-data-analyst-%E2%80%93-operational-informatics-at-energy-jobline-3773700548,2023-12-17,Oregon,United States,Mid senior,Onsite,"Base Salary – USD $71,303 to $89,898
Location: Beaverton, Oregon
( Remote for commutable distance of Portland, OR and Clark County in Washington State ).
Job Description
Healthcare Data Analyst, Operational Informatics who will:
Be responsible for designing, implementing, and coordinating data analysis projects
Provide key analytic and statistical support of clinical and operational improvement activities
Develop, prepare, and distribute complex clinical and operational reports utilizing multiple data systems
Develop and provide support for relational database-based solutions, including reporting, design, and data management
Effectively summarize complex information into appropriate charts, tables, and figures using Tableau in order to convey the meaning of the data to customers and decision-makers
Required Qualifications For This Position Include
Bachelor’s Degree in highly quantitative field (e.g., Statistics, Mathematics, Actuarial Science, Economics, Engineering/Physics, Accounting, or Computer Science)
5 years of experience in data analysis -OR- a combination of equivalent education and experience
Preferred Qualifications For This Position Include
Healthcare organization/health insurer experience
Statistical modeling experience
2+ years’ experience working with data visualization to create dashboards in Tableau or PowerBI
2+ years’ experience designing and writing SQL queries for reporting & analysis.
Show more
Show less","Tableau, PowerBI, SQL, Data Analysis, Statistical Modeling, Data Visualization, Relational Databases, Healthcare, Statistics, Mathematics, Actuarial Science, Economics, Engineering, Physics, Accounting, Computer Science","tableau, powerbi, sql, data analysis, statistical modeling, data visualization, relational databases, healthcare, statistics, mathematics, actuarial science, economics, engineering, physics, accounting, computer science","accounting, actuarial science, computer science, dataanalytics, economics, engineering, healthcare, mathematics, physics, powerbi, relational databases, sql, statistical modeling, statistics, tableau, visualization"
"Staff Software Engineer, Data",NAVEX,"Lake Oswego, OR",https://www.linkedin.com/jobs/view/staff-software-engineer-data-at-navex-3775578917,2023-12-17,Oregon,United States,Mid senior,Onsite,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.
Position Summary
At NAVEX, you will help design and implement our NAVEX One data platform part of our newest engineering team. Our Product Engineering team shares a passion for designing quality solutions, embracing new technologies and delivering powerful products that help our customers protect their reputation and bottom line.
As a Data Staff Software Engineer, you will influence technical designs and implement our new data platform. You will focus on quality implementation while guiding the other data engineers. You will help us build a data platform that will ingest other teams’ content and then provide application specific data sets. We are looking for a candidate who is strong in data engineering. In this role, you will have ample opportunity to explore new value-added capabilities, invest in data development and tool research, mentor software developers and grow your career all while balancing your life priorities.
We Offer You
An Inspiring Culture. Invested teammates, belonging groups, and a socially determined culture
Meaningful Work. Innovative products and solutions with real life impact for people and organizations
Career Growth. Stellar training and an unwavering commitment to your growth and success
Life Flexibility. Time to care for yourself, your loved ones, and your community
Industry Leadership. A highly reputable, fast growing and consistently profitable organization
Real Rewards. Competitive and transparent pay practices, wellbeing programs and benefits with choice
What You Will Do
Work with a team of data engineers and be accountable for designs and high-quality deliveries as an individual contributor
Help team members grow by mentoring newer engineers
Participate in the innovative advancements of our product platform and collaborate with our awesome agile team members
Promote opportunities for refactoring and identify areas of optimization
Research and leverage commercial products, libraries, and tools that can be used to solve problems
Participate in design sessions with other engineers, architects, and product managers, providing constructive and honest feedback during sprint retrospectives with a team mindset
Use automation, including continuous integration, automated deployments, and automated unit and functional testing
What You Will Need
A Bachelor’s degree in Computer Science or be good enough that we won’t notice through equivalent prior work-related experience
5+ years’ experience in an Agile, full-stack software development environment with a focus on big data designs and implementations, ideally with SaaS and/or micro service-based systems
Expert knowledge of data management and pipeline systems, practices, and standards
Expert analytical and design skills, including the ability to abstract information requirements from real-world processes to understand information flows in computer systems
Expertise in the fields of data transformations (ELT, ETL), data quality, data cleansing, and data profiling using dbt Labs’ DBT
Expertise in Data Cataloging and Master Data Management concepts
Expertise in both SQL and NoSQL implementations; experience with Microsoft SQL Server, Snowflake, and Postgres database platforms
Experience with SQL profiling, performance tuning, and data ingestion into Data Warehouses
Strong problem solving and critical thinking skills with the ability to identify and influence others on the best solution
Ability to work well in a team environment and attitude to focus on team specific goals and objectives
Excellent verbal and written communication skills and a commitment to engage and collaborate with people across a variety of levels with diverse backgrounds
We believe each member of our team deserves to see a path forward to achieving their career and financial goals.
Each team member is required to have a career plan in place and reviewed with their manager after six months with our team.
The minimum starting pay range for this role is $110,000 per annum with 5% MBO.
Pay progression is based on performance.
Our pay programs are just one element of our commitment to Be the ONE place you want to thrive in life. Check out NAVEX’s career page to learn about our innovative people programs designed to create one powerful life experience for YOU!
NAVEX is an equal opportunity employer, including disability/vets.
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!
Show more
Show less","Data engineering, Data platform, Agile, Data transformations, ELT, ETL, DBT Labs' DBT, Data Cataloging, Master Data Management, SQL, NoSQL, Microsoft SQL Server, Snowflake, Postgres, SQL profiling, Data ingestion, Data Warehouses, Critical thinking, Team environment, Communication skills","data engineering, data platform, agile, data transformations, elt, etl, dbt labs dbt, data cataloging, master data management, sql, nosql, microsoft sql server, snowflake, postgres, sql profiling, data ingestion, data warehouses, critical thinking, team environment, communication skills","agile, communication skills, critical thinking, data cataloging, data engineering, data ingestion, data platform, data transformations, data warehouses, dbt labs dbt, elt, etl, master data management, microsoft sql server, nosql, postgres, snowflake, sql, sql profiling, team environment"
Data Analytics Specialist,Intelliswift Software,"Hillsboro, OR",https://www.linkedin.com/jobs/view/data-analytics-specialist-at-intelliswift-software-3773576749,2023-12-17,Oregon,United States,Mid senior,Onsite,"Job Title: Data Analytics Specialist
Duration: 12 Months
Location: Hillsboro, OR/Onsite
Intelliswift Software Inc. conceptualizes, builds, and supports the world's most amazing technology products and solutions. Our team of rich experts from diverse backgrounds contributes to making Intelliswift one of the most reliable partners in IT and Talent solutions. We specialize in delivering world-class Digital Product Engineering, Data Management and Analytics, and Staffing Solutions services to Fortune companies, SMBs, ISVs, and fast-growing startups.
Job Description:
The IT Data & Analytics Specialist supports the business in providing data and tools to support the digitalization and Industry 4.0 transformation.
This include activities like:
Understand and translate requirements for reports, key figures and dashboards into a solution context.
Creatively and proactively provide your expertise in the identification, evaluation, design, development, testing, training and go-live of reports and key figures.
Responsible for the validation of GMP-relevant reports, advice on report design to improve the use of the reports and for continuous monitoring of the report portfolio in terms of benefit, use as well as the efficiency and effectiveness of the respective reports.
Take care of supporting users with questions and problems regarding our data products and analyze and prioritize the incidents in order to enable an effective and fastest possible solution to the existing errors.
Ensure the establishment and active maintenance of expert networks to ensure the support and further development of your own regional key user community including relevant stakeholders.
Operate change management and support the departments in use and continuous improvement.
Role Minimum Requirements
Successfully completed your studies in the field of computer science, data science, (business) computer science, (business) engineering or business administration. Alternatively, you have a comparable qualification.
At least 3 years of practical professional experience in user-oriented reporting and process data processing and/or in a GMP/CSV regulated environment. In addition, you have at least 2 years of experience in the management of local and global projects.
Good general GxP knowledge, in particular for Computerized System Validation (CSV) and in applications in the reporting and/or statistical environment.
Knowledge in the areas of gSuite software and methodical business analysis.
Passionate about consulting, implementation and support (ideally in the life sciences industry).
Complete the tasks assigned to you independently, on your own initiative and conscientiously with a very high degree of customer/service orientation.
Successful teamwork is your driver for progressive work and even under exceptional stress you can proceed in a result-oriented and quality-oriented manner.
Required Skills:
Tableau/Power BI, GMP/GxP, Veeva, Jira.
Equal Employment Opportunity Statement
Intelliswift celebrates a diverse and inclusive workforce. We offer equal employment opportunities to all applicants and employees. All qualified applicants will be considered regardless of race, color, sex, gender identity, gender expressions, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other protected basis under the law.
Americans with Disabilities Act (ADA)
If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please contact Intelliswift Human Resources Department
Other Employment Statements
Intelliswift participates in the E-Verify program.
Learn More
For information on Intelliswift Software, Inc., visit our website at www.intelliswift.com.
Show more
Show less","Tableau, Power BI, GMP, GxP, Veeva, Jira, Computerized System Validation (CSV), gSuite","tableau, power bi, gmp, gxp, veeva, jira, computerized system validation csv, gsuite","computerized system validation csv, gmp, gsuite, gxp, jira, powerbi, tableau, veeva"
Senior Azure Data Engineer,OmniData,"Portland, OR",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-omnidata-3766867639,2023-12-17,Oregon,United States,Mid senior,Remote,"What We Are Looking For –
A passionate, hungry, and motivated individual that is eager for a chance to join a young startup, experiencing rapid growth. At OmniData, we are searching for a remote Senior Azure Data Engineer that has hands on production experience in developing PySpark solutions in Synapse Analytics on data warehousing and analytics projects. We need a team player who has a an exceptional reputation for mentoring less experienced teammates. We seek someone with a strong technical aptitude with expertise in translating complex technical information that appropriately meets the needs of the client while skillfully deploying the best strategies for client analytics goals. In return, we offer deep mentorship, a great work/life balance, and the opportunity to be part of creating a consulting firm that makes a difference for our clients!
What You Will Do –
You will work on various Big Data, Data Warehouse and Analytics projects for our world class customers. In addressing complex client needs, you will be integrated into appropriately sized and skilled teams. This will give you the opportunity to analyze requirements, develop data and analytical solutions, and execute as part of the project team, all while working with the latest tools, such as Azure Synapse Analytics and related Microsoft technologies.
Your Duties And Responsibilities –
Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients.
Instill confidence in the client as well as your teammates
Work independently toward client success, at the same time knowing your own limitations and when to call on others for help.
What you must have to be considered
–
7+ years of experience in Analytics and Data Warehousing on the Microsoft platform
1-2 years advanced experience in PySpark solutions in Synapse analytics
Experience working with the Microsoft Azure stack (e.g. Synapse, Databricks, DataFactory etc.)
What Would Be Nice For You To Have –
Experience with Python
Experience gathering requirements and working within various project delivery methodologies
Experienced working as a customer facing consultant
Exposure to DAX
Strong communication skills tying together technologies and architectures to business results
Benefits And Perks
Health Coverage: 100% Employee Coverage (Up to a 1500 PPO Plan), 60% Coverage for dependents
Dental/Vision
Life Insurance, Aflac, Short/Long term disability, HSA etc.
9 Company holidays (Ability to use them as 'floating' Holidays)
Paid time off (PTO) 15 days
Flexible Sick Time
Maternity/Paternity Leave (Birth Parent: 3 months, Non-birth parent: 1 month
401k - up to a 5% match. Eligibility to contribute the month after start date. Vests as soon as you are eligible for contribution.
Opportunities for career growth and advancement, as well as helping to shape a young consulting firm
Ability to learn from highly skilled consultants with years of industry experience
Exposure to the latest and greatest data warehousing, analytics, and cloud technologies
Flexible schedules in a hybrid work environment
Basic Life Insurance (50k)
LT and ST Disability (paid by employee)
Flexible Spending Account (FSA) and Healthcare Savings Account (HSA)
Employee Assistance Program (EAP)Mental health and substance abuse conditions are serious and sometimes require 24/7 access to resources.
Ethics and Compliance Reporting & Pulse Surveys
Employee Engagement & Cultural initiatives: Health & Wellness, Pulse Surveys, and Kolbe Instinctive Strengths
Commuter Benefits
WeWork office provision
About OmniData
Learn more about our culture and read the testimonials from some members by visiting our
Careers Page.
OmniData is a Portland, Oregon based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come. To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.
OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next-generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team. You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance. At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients.
OmniData Values
We build partnerships that last. We are ambitious and set aggressive goals. We embody professional humility. We are prepared. Visit
www.omnidata.com/AboutUs
to learn more.
OmniData is a Portland - based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come. To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team. You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance. At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients.
Show more
Show less","PySpark, Synapse Analytics, Data warehousing, Analytics, Azure Synapse Analytics, Microsoft Azure Stack, Python, DAX, SQL, Machine Learning, AI, Solution Architecture","pyspark, synapse analytics, data warehousing, analytics, azure synapse analytics, microsoft azure stack, python, dax, sql, machine learning, ai, solution architecture","ai, analytics, azure synapse analytics, datawarehouse, dax, machine learning, microsoft azure stack, python, solution architecture, spark, sql, synapse analytics"
Staff Software Engineer - Datalake,Dremio,"Portland, OR",https://www.linkedin.com/jobs/view/staff-software-engineer-datalake-at-dremio-3783882985,2023-12-17,Oregon,United States,Mid senior,Remote,"Be Part of Building the Future
Dremio is The Easy and Open Data Lakehouse, providing self-service analytics with data warehouse functionality and data lake flexibility across all of your data. Dremio increases agility with a revolutionary data-as-code approach that adopts Git concepts to enable data experimentation, version control, and governance. In addition, Dremio breaks down data silos by simplifying ingestion into the lakehouse, and also allowing queries directly on databases and data warehouses. All of this is available through a fully managed service that not only eliminates the need to maintain infrastructure and software, but also automatically optimizes the data in the lakehouse to maximize performance for every workload.
Founded in 2015, Dremio is headquartered in Santa Clara, CA. Investors include Cisco Investments, Insight Partners, Lightspeed Venture Partners, Norwest Venture Partners, Redpoint Ventures, and Sapphire Ventures. For more information, visit www.dremio.com. Connect with Dremio on GitHub, LinkedIn, Twitter, and Facebook.
If you, like us, say “bring it on” to exciting challenges that really do change the world, we have endless opportunities where you can make your mark.
About The Role
We are looking for an experienced Staff Software Engineer to enhance Dremio’s data warehouse capabilities on top of the datalakes across all major table/file formats and object stores. These capability advancements will increase our competitive position in the market and enable Dremio adoption for a larger set of customers.
What You’ll Be Doing
Develop core components for Dremio’s query engine
Deliver key features and feature enhancements for our customers in the Datalake like DML operations, time travel, schema evolution along with performance and reliability improvements
Work with open source projects like Apache Iceberg, Parquet, Arrow and Calcite
Own design, implementation, testing, and support of next-generation features related to scalability, reliability, robustness, and performance of the product
Collaborate with Product Management to innovate and deliver on customer requirements and with Support and field teams to ensure customer success
Understand and reason about concurrency and parallelization to deliver scalability and performance in a multithreaded and distributed environment
Solve complex technical problems and customer issues while improving our telemetry and instrumentation to proactively detect issues before they arise and make debugging more efficient
Work with engineering leaders to establish solid designs/architecture for upcoming features.
Develop the future leaders of Dremio by providing continuous mentorship and coaching of junior software engineers, help with hiring and onboarding
What We’re Looking For
8+ year of industry experience
B.S./M.S/Equivalent in Computer Science or a related technical field or equivalent experience
Fluency in Java, C++ or another modern language
Strong database fundamentals including SQL, performance, and schema design and background in large scale data processing systems (e.g., Hadoop, Spark, etc.)
Understanding of distributed file systems such as S3, ADLS, or HDFS
Experience with Apache Iceberg, Parquet, AVRO and/or Delta Lake
Experience with Hive and AWS Glue
Ability to solve ambiguous, unexplored, and cross-team problems effectively
Interested and motivated to be part of a fast-moving startup with a fun and accomplished team
Big picture thinking, ability to scope and plan solutions for big problems and mentor others on the same
Bonus points if you have
Hands-on experience with distributed query engines, query processing or optimization, distributed systems, concurrency control, data replication, code generation, or storage systems
Hands on experience with AWS, Azure and Google Cloud Platform
What We Offer
Medical, dental and vision insurance
401(k) Plan
Short term / long term disability and life insurance
Pre-IPO stock options
Flexible PTO
16 hours of volunteer time off
12 company paid holidays, including Juneteenth
Remote work options
Paid parental leave
Employee Assistance Program (EAP)
Biannual swag surprise
Certain benefits are only allowed to full-time Dremio employees and may not be the same across all locations.
The base salary range for this position is $154,545 to $209,091 per year. The base salary actually offered to a successful candidate will take into account various relevant and non-discriminatory business factors including, without limitation, the candidate’s geographic location, job-related experience, knowledge, and skills, and education, as well as internal equity considerations. A successful candidate may also be eligible to earn additional compensation including commissions and/or bonuses.
What We Value
At Dremio, we hold ourselves to high standards when it comes to People, Thinking, and Action. Our Gnarlies (that's what we call our employees) communicate with clarity, drive accountability, and are respectful towards each other. We confront brutal facts and focus on results while operating with a sense of urgency and building a ""flywheel"". People who like to jump in and drive momentum will thrive in our #GnarlyLife.
Dremio is an equal opportunity employer supporting workforce diversity. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, age, marital status, protected veteran status, disability status, or any other unlawful factor.
Dremio is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request accommodation due to a disability, please inform your recruiter.
Dremio has policies in place to protect the personal information that employees and applicants disclose to us. Please click
here
to review the privacy notice.
Show more
Show less","Data Lakehouse, Dremio, SQL, Java, C++, Python, Apache Iceberg, Parquet, AVRO, Delta Lake, Hive, AWS Glue, Distributed Query Engines, Query Processing, Query Optimization, Distributed Systems, Concurrency Control, Data Replication, Code Generation, Storage Systems, AWS, Azure, Google Cloud Platform","data lakehouse, dremio, sql, java, c, python, apache iceberg, parquet, avro, delta lake, hive, aws glue, distributed query engines, query processing, query optimization, distributed systems, concurrency control, data replication, code generation, storage systems, aws, azure, google cloud platform","apache iceberg, avro, aws, aws glue, azure, c, code generation, concurrency control, data lakehouse, data replication, delta lake, distributed query engines, distributed systems, dremio, google cloud platform, hive, java, parquet, python, query optimization, query processing, sql, storage systems"
Staff Cybersecurity Data Platform Engineer,Adobe,"Oregon, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767924187,2023-12-17,Oregon,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, AWS, Kafka, Flink, Spark, PySpark, Delta Lakehouse, Orchestration, Data Ingestion, Medallion Architectures, Unity Catalog, Autoloader Jobs, Distributed Cloud Architectures, Data Models, Schemas, Security Operations Center (SOC), Threat Management, Incident Response, Enterprise Security, Security Management Workflows","databricks, aws, kafka, flink, spark, pyspark, delta lakehouse, orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, distributed cloud architectures, data models, schemas, security operations center soc, threat management, incident response, enterprise security, security management workflows","autoloader jobs, aws, data ingestion, data models, databricks, delta lakehouse, distributed cloud architectures, enterprise security, flink, incident response, kafka, medallion architectures, orchestration, schemas, security management workflows, security operations center soc, spark, threat management, unity catalog"
"Senior Software Engineer, Data Transcoding - Autonomous Vehicles",NVIDIA,"Oregon, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-transcoding-autonomous-vehicles-at-nvidia-3761513608,2023-12-17,Oregon,United States,Mid senior,Remote,"NVIDIA is seeking to hire senior software engineers to develop and scale its Data Transcode framework! Our Data Transcode framework is the highly optimized transcoding engine widely used by AV Curation and Perception teams for data export and operations. In order to support this activity, you will need to have strong programming skills in C/C++/GoLang, and a decent knowledge of video coding fundamentals and mathematics. In addition, it is desired that you have experience with different types of sensor technologies and an understanding of cloud-based technologies.
What You'll Be Doing
Write flawless C++/Go code.
Design and build scalable data transcoding services that ensure the performance and robustness of the service.
Analyze root cause and triage issues.
Closely collaborate with our partner's team to implement new features and improve existing ones.
What We Need To See
Masters or equivalent experience in Computer Science, Electrical Engineering or related field with 12+ years of Work or Research Experience.
Strong programming background that incorporates methodologies like data structures, design patterns, OOP, and test-driven development.
Advanced programming skills to build efficient solutions optimized for runtime and memory footprint.
A specialist programmer in C/C++ and knowledge of Go.
Experience with or solid understanding of hardware and sensor technologies.
Switch effectively between long-term strategic management and near-term tactical management.
Highly motivated with strong interpersonal skills, you have the ability to work successfully with multi-functional teams, principles and architects and coordinate effectively across organizational boundaries and geographies.
Ways To Stand Out From The Crowd
Experience working with automotive sensor technologies.
Experience with hardware-accelerated video codecs API.
Strong mathematical skills.
A go-getter attitude to dive deep and understand technical requirements.
With competitive salaries and a generous benefits package, NVIDIA is widely considered to be one of the technology industry's most desirable employers. We have some of the most forward-thinking and talented people in the world working with us and our engineering teams are growing fast in some of the most impactful fields of our generation: Deep Learning, Artificial Intelligence, and Autonomous Vehicles. If you're a creative engineer who enjoys autonomy and shares our passion for technology, we want to hear from you!
The base salary range is 216,000 USD - 333,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","C, C++, Go, OOP, Data Structures, Design Patterns, TDD, Data Transcoding, Sensor Technologies, Cloud Computing, Video Coding, Mathematics, Hardware Accelerated Video Codecs, Automotive Sensor Technologies","c, c, go, oop, data structures, design patterns, tdd, data transcoding, sensor technologies, cloud computing, video coding, mathematics, hardware accelerated video codecs, automotive sensor technologies","automotive sensor technologies, c, cloud computing, data structures, data transcoding, design patterns, go, hardware accelerated video codecs, mathematics, oop, sensor technologies, tdd, video coding"
Business Intelligence and Data Analyst (Remote),EMyth,"Ashland, OR",https://www.linkedin.com/jobs/view/business-intelligence-and-data-analyst-remote-at-emyth-3787732981,2023-12-17,Oregon,United States,Mid senior,Remote,"EMyth Business Intelligence and Data Analyst
About us:
At EMyth, our mission is to help small business owners create a business that can grow predictably and sustainably and can operate self-sufficiently so that the business serves the owner’s life rather than the other way around. Our clients become leaders who can question their assumptions, find the most authentic version of themselves and develop the discernment to make the best choices for their business, their team, their customers and themselves.
We pioneered the business coaching industry in 1977 and established ourselves as the leading authority in business coaching with the release of The E-Myth Revisited by EMyth’s founder, Michael E. Gerber. One of the 10 best-selling business books of all time, The E-Myth Revisited has inspired millions of business owners to “work on their business, not just in it.” Our Coaching Programs are designed to guide business owners in making the transition from Technicians, whose businesses work because of them, to Managers, Entrepreneurs and Leaders, whose businesses can flourish without them.
We welcome applicants of any gender identity and expression, sexual orientation, religion, ethnicity, race, age, socioeconomic and family status, disability and veteran status. Employment at EMyth is based on merit and an applicant’s qualifications for each position. We’re committed to maintaining a workplace free of bias, prejudice, bullying, discrimination, verbal and physical violations of any kind. We’re looking for people who are curious, open-minded and self-aware, who can build co-creative relationships with their teammates that support small business owners in building a business and a life they love leading.
Your role:
As our Business Intelligence and Data Analyst, you’ll be accountable for refining and continuing to build our data infrastructure, generating and improving reports and dashboards, and making observations and recommendations for improvement from your perspective at the intersection of all our data. Above all else, you’ll work to ensure that our data is accurate, clean, simply displayed and easy to understand so EMyth’s Leadership Team can make informed decisions.
You’ll need to be experienced working with data and comfortable exploring, investigating and asking thoughtful questions to get a full understanding of what the data are showing us while ensuring its accuracy. You’ll need to be highly detail-oriented to spot and clear up any inconsistencies in how data is gathered, measured and reported. Competence in linear algebra, probability and statistics is required.
We’re looking for people who are:
Self-aware
Able to manage their emotional reactivity
Exceptional listeners
Comfortable giving honest and direct feedback
Welcoming of honest and direct feedback, delivered respectfully, in a group setting as well as one-to-one
Results-oriented
Systems-oriented
Always seeking opportunities to learn and grow
Emotionally intelligent
Curious
Discerning
Confident in what they know and open to what they don’t know
Tenacious
Have a high attention to detail
Place importance on accuracy
Good with numbers
Passionate about technology
Enjoy problem solving
Able to explain complex concepts or issues to any type of audience
EMyth could be your place if you:
Are looking for work with a sense of mission and purpose
Are able to pivot when shown a better way, even when it’s uncomfortable
Share your opinions and perspectives easily even though they may be unpopular and/or you might be wrong
Seek your own personal growth more than you seek approval from others
Place a high value on your own authenticity
Are results focused
Gravitate to the bigger picture, challenging yourself and others to produce the best results for the business.
What you'll need:
A BA or BS degree or demonstrated skill in lieu of a degree
At least 6 years of combined experience in data, analytics and business intelligence roles including:
A minimum of 5 years experience translating business needs into data architecture, reports and analyses
A minimum of 3 years experience with HubSpot
A minimum of 1 year experience with Salesforce
A minimum of years experience with other platforms that are used to manage and analyze data
Comfort in documenting your work so your processes, logic, standards and assumptions are systematized for the benefit of the company and other team members. Passion for delivering accurate, clear information to others. An ability to understand complexity and turn it into simplicity for the benefit of your team.
Desire and experience in teaching others how to work and interact with our data systems
Ease in:
Managing projects with both employees and outside contractors.
Collaborating with team members who have less technical skill.
Navigating a broad set of platforms, including Quickbooks, Asana and Zapier
Using a variety of data-related tools, like Dataloader, Tableau, Snowflake, Microsoft Power BI, Looker, Google Analytics, SOQL and SOSL as well as SQL or similar query languages
An ability to maximize the results of any project or initiative within the limits of available resources.
The ability to both work independently and co-creatively
A quiet and professional home office space
What we offer:
Salary range between $80,000 - $90,000 depending on experience
Remote work
Three weeks paid vacation
One week paid personal time
Two weeks paid sick time
14 paid holidays
401(k) with matching
Health, dental and vision insurance
Flexible Spending Account (FSA) and Dependent Care FSA
Life insurance
A world-class, committed team of people who:
hold themselves and each other accountable for results
trust each other by speaking up vulnerably and authentically
listen with an open mind, care and curiosity
help each other see their blind spots and break unproductive habits
reject artificial harmony in favor of productive conflict
share a dedication to the transformation of small business owners and their businesses
An open and apolitical culture in which your voice is heard and you matter
How to apply:
If this sounds like you, please submit your most recent resume and a cover letter by December 19, 2023. In your cover letter, tell us about yourself, why you’re attracted to this work and EMyth in particular and answer this question:
What have you learned, from personal experience, is necessary to make meaningful, lasting change in yourself?
Because who you are is as important to us as what you’ve done, only applications with cover letters will be considered. For this position, we’re also only considering applicants who are United States residents and eligible to work in the United States.
Powered by JazzHR
E5HKz5DCCr
Show more
Show less","Data infrastructure, Reporting, Dashboards, Data accuracy, Data cleaning, Data visualization, Data analysis, Linear algebra, Probability, Statistics, HubSpot, Salesforce, Data management platforms, Data architecture, Data analysis, Data management, Data visualization, Data communication, SQL, SOQL, SOSL, Snowflake, Tableau, Microsoft Power BI, Dataloader, Looker, Google Analytics, Zapier, Asana, Quickbooks","data infrastructure, reporting, dashboards, data accuracy, data cleaning, data visualization, data analysis, linear algebra, probability, statistics, hubspot, salesforce, data management platforms, data architecture, data analysis, data management, data visualization, data communication, sql, soql, sosl, snowflake, tableau, microsoft power bi, dataloader, looker, google analytics, zapier, asana, quickbooks","asana, dashboard, data accuracy, data architecture, data cleaning, data communication, data infrastructure, data management, data management platforms, dataanalytics, dataloader, google analytics, hubspot, linear algebra, looker, microsoft power bi, probability, quickbooks, reporting, salesforce, snowflake, soql, sosl, sql, statistics, tableau, visualization, zapier"
Master Data Engineer,Marcus & Millichap,"Portland, OR",https://www.linkedin.com/jobs/view/master-data-engineer-at-marcus-millichap-3783508388,2023-12-17,Oregon,United States,Mid senior,Remote,"This position will support data and analytics by monitoring and analyzing master data, key data, and master relationship data within the organization.
Essential Functions
Ensures master data integrity in key systems as well as maintaining the processes to support the data quality.
Identifies areas for data quality improvements and helps to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies.
Ensures quality of master data in key systems, as well as, development and documentation of processes with other functional data owners to support ongoing maintenance and data integrity.
In collaboration with subject matter experts and data stewards, defines and implements data strategy, policies, controls, and programs to ensure the enterprise data is accurate, complete, secure, and reliable.
Provides assistance in resolving data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies collaborating with subject matter experts (SMEs) and data stewards.
Ensure master data integrity in key systems as well as maintaining the processes to the data quality. Identify areas for data quality improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies.
Manages, analyzes, and resolves data initiative issues and manages revisions needed to best meet internal and customer requirements while adhering to published data standards.
Assists in data management, governance, and data quality of master data requirements with other functional data owners to ensure functional master data integrity across the operation of financial systems is consistent and meets stated business rules and requirements.
Work closely with the business/IT to ensure alignment of master data rules and the operations of the application meet all requirements.
Education & Experience
Bachelor's degree or equivalent combination of education and experience.
Five or more years of experience in data analysis or computer programming.
Experience working with SQL and relational databases.
Strong analytical skills and independent learning style
Our mission is to help our clients create and preserve wealth by providing the best real estate investment sales, financing, research, and advisory services available.
Founded in 1971
, Marcus & Millichap (NYSE: MMI) is a leading commercial real estate brokerage firm focusing exclusively on investment sales, financing, research, and advisory services, with nearly
2,000 investment sales and financing professionals in 80+ offices throughout the United States and Canada
.
Marcus & Millichap
closes more transactions than any other real estate investment brokerage firm in the nation.
In 2022, the firm closed 12,272 transactions with a sales volume of approximately $86.3 billion
.
The firm has perfected a powerful property marketing system that integrates broker specialization by property type and market area; the industry’s most comprehensive investment research; a long-standing culture of information sharing; relationships with the largest pool of qualified investors; and state-of-the-art technology that matches buyers and sellers.
Marcus & Millichap provides equal employment opportunities to all employees and applicants for employment without discrimination with regard to race, religious belief (including dress or grooming practices), color, sex, sex stereotype, pregnancy, childbirth or related medical conditions (including breast feeding), age, national origin, ancestry, sexual orientation, gender identification and expression, transgender status, physical or mental disability, medical condition, genetic characteristics, genetic information, family care, marital status, enrollment in any public assistance program, status as military, a veteran or qualified disabled veteran, status as an unpaid intern or volunteer, or any other classification protected by law. We also prohibit discrimination based on the perception that anyone has any of those characteristics, or is associated with a person who has or is perceived as having any of those characteristics. In addition to federal law requirements, Marcus & Millichap complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Show more
Show less","SQL, Relational databases, Master data management, Data governance, Data quality, Data analysis, Data strategy, Data policies, Data controls, Data programs, Data standards, Error detection, Error correction, Process control, Process improvement, Process design, Data initiative management, Data integrity, Data governance, Data quality, Master data requirements, Analytical skills, Independent learning style","sql, relational databases, master data management, data governance, data quality, data analysis, data strategy, data policies, data controls, data programs, data standards, error detection, error correction, process control, process improvement, process design, data initiative management, data integrity, data governance, data quality, master data requirements, analytical skills, independent learning style","analytical skills, data controls, data governance, data initiative management, data integrity, data policies, data programs, data quality, data standards, data strategy, dataanalytics, error correction, error detection, independent learning style, master data management, master data requirements, process control, process design, process improvement, relational databases, sql"
Sr. Data Engineer,US Tech Solutions,"Beaverton, OR",https://www.linkedin.com/jobs/view/sr-data-engineer-at-us-tech-solutions-3766874994,2023-12-17,Oregon,United States,Mid senior,Hybrid,"Sr. Data Engineer
Location : Beaverton, Oregon, 97003
Duration : 6+ Months Contract(Strong possibility of Extension)
Skills:
PySpark, SQL, Airflow.
Job Description:
A sr data engineer is a professional responsible for designing, developing, and maintaining the infrastructure and systems that enable the collection, storage, processing, and analysis of data within an organization.
Data engineers play a crucial role in ensuring that data is available & accurate, accessible, and properly structured for use by data analysts, data scientists, visualization tools and other stakeholders.
About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.
Recruiter Details:
Adil Saifi - Team Lead (IT Recruitment)
Telelphone: (201) 524 9600 Ext: 7222 | Direct: (503) 207 6215
Fax: (201) 524 9601
Email: adil@ustechsolutionsinc.com
Show more
Show less","Data Engineering, PySpark, SQL, Airflow, Data Collection, Data Storage, Data Processing, Data Analysis, Data Visualization, Database Management","data engineering, pyspark, sql, airflow, data collection, data storage, data processing, data analysis, data visualization, database management","airflow, data collection, data engineering, data processing, data storage, dataanalytics, database management, spark, sql, visualization"
Senior Data Analyst,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/senior-data-analyst-at-nike-3759020516,2023-12-17,Oregon,United States,Mid senior,Hybrid,"Work options:
Hybrid
Hybrid
Title
: Senior Data Analyst
Location
: Beaverton, OR
Duration
: 7 Months
Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders, and visionaries. Our teams are innovative, diverse, multidisciplinary, and collaborative, taking technology into the future and bringing the world with it. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.
Who are we looking for
We are looking for an experienced Senior Data Analyst in our North America Data & Analytics team to conduct full life cycle analysis, produce reports that provide valuable insight for business to enable continuous improvements. The ideal candidate will have outstanding communication skills, proven data design and implementation capabilities, good eye for business, and an innate drive to deliver results. The person in this role will be technically proficient and excel at collaborating with engineers, analysts, and business partners. This person will be a self-starter, comfortable with ambiguity, and will enjoy working in a fast-paced dynamic environment.
What Will You Work On
Acquire data from primary or secondary data sources and maintain databases/data systems
Organize and transform information into comprehensible structures
Use data to predict trends and perform statistical analysis
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
Use tools and techniques to visualize data in easy-to-understand formats, such as diagrams and graphs
Use data mining to extract information from data sets and identify correlations and patterns
Monitor data quality and remove corrupt data
Manage and design the reporting environment, including data sources, security, and metadata
Evaluate changes and updates to source systems
Communicate with business partners to understand data content and business requirements
Locate and define new process improvement opportunities
Who Will You Work With
This role is part of the North America Data & Analytics Organization and you will work with world-class talent in the field of Data Engineering with a goal of better business insights and driving data-driven decisions across the organization. You’ll be working closely with Internal partners, Product Owners, Engineering Leaders, Data Analysts, Big Data Leads, and Engineers. One of Nike’s maxims is “Win as a Team” and you will be working in a very collaborative environment and will find success in teamwork, a positive attitude, and hard work. You thrive when surrounded by hardworking colleagues and aim to never stop learning.
What You Bring
Bachelor/Master degree in Mathematics, Economics, Computer Science, Information Management or Statistics or related technical subject area or equivalent combination of education and experience
4+ years proven work experience in Data Analyst or Business Data Analyst role
Experience in SQL and a scripting language such as Python or R
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Good understanding of Hadoop and Big Data processing frameworks
Experience working with BI tools like Tableau, Power BI or DOMO
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling)
Experience with source control tools such as GitHub
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc preferred
Experience with data warehouses/RDBMS like Snowflake & Teradata
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment
Self-starter, highly organized and result oriented; and should be able to work with minimal direction
Show more
Show less","Data Analysis, Data Engineering, SQL, Python, R, Apache Hadoop, Apache Spark, AWS, Tableau, Power BI, DOMO, Data Modeling, Snowflake, Teradata, GitHub, Data Visualization, Data Storytelling","data analysis, data engineering, sql, python, r, apache hadoop, apache spark, aws, tableau, power bi, domo, data modeling, snowflake, teradata, github, data visualization, data storytelling","apache hadoop, apache spark, aws, data engineering, data storytelling, dataanalytics, datamodeling, domo, github, powerbi, python, r, snowflake, sql, tableau, teradata, visualization"
Sr. Data Engineer,Concora Credit,"Beaverton, OR",https://www.linkedin.com/jobs/view/sr-data-engineer-at-concora-credit-3766099034,2023-12-17,Oregon,United States,Mid senior,Hybrid,"We hire people, not positions. That's because, at Concora Credit, we put people first, including our customers, partners, and Team Members.
Concora Credit is guided by a single purpose: to help non-prime customers
do more
with credit.
Today, we have helped millions of customers access credit. Our industry leadership, resilience, and willingness to adapt ensure we can help our partners responsibly say yes to millions more.
As a company grounded in entrepreneurship, we're looking to expand our team and are looking for people who foster innovation, strive to make an impact, and want to Do More!
We’re an established company with over 20 years of experience, but now we’re taking things to the next level. We're seeking someone who wants to impact the business and play a pivotal role in leading the charge for change.
Join the nation’s leader in second-look finance servicing as our Sr. Data Engineer - Master Data Management!
The impact you’ll have at Concora Credit:
We are seeking a highly skilled and experienced Master Data Management (MDM) Engineer to join our data management team. The ideal candidate will have a solid understanding of data governance principles and possess extensive experience in designing, developing, and maintaining robust data integration workflows. As an MDM Engineer, you will play a critical role in ensuring the integrity, consistency, and reliability of the company’s master data assets across the enterprise.
As our Sr. Data Engineer - Master Data Management, you will:
Develop and maintain a secure and effective master data management architecture that aligns with business objectives and data governance policies.
Design and implement data integration processes and workflows, ensuring seamless data interchange between disparate systems.
Administer and maintain metadata repositories, utilizing platforms like Collibra or Alation, to support data lineage, governance, and stewardship.
Collaborate with IT and business stakeholders to identify and define master data needs, including metadata, reference data, and transactional data.
Govern the complete lifecycle of critical data elements, crafting and enforcing policies with DLP (Data Loss Prevention) and ensuring GDPR, CCPA, and SOX compliance.
Drive the adoption of data quality tools and methodologies, establishing metrics and processes for data quality assurance.
Provide expertise in the maintenance of metadata repositories to promote data lineage, data cataloging, and data dictionary maintenance.
Participate in data governance initiatives, contributing to the development of policies, standards, and procedures that enforce data integrity and consistency.
Troubleshoot and resolve master data issues, performing root cause analysis and recommending long-term improvement plans.
Mentor junior staff and serve as a subject matter expert in MDM best practices, technologies, and trends.
Engineer and support master data consolidation efforts, leveraging deduplication and matching engines, ensuring cross-system consistency via robust survivorship and reconciliation practices.
Automated MDM processes with RPA (Robotic Process Automation) solutions to enhance efficiency and reduce manual overhead.
These duties must be performed with or without reasonable accommodation.
We know experience comes in many forms and that many skills are transferable. If your experience is close to what we're looking for, consider applying. Diversity has made us the entrepreneurial and innovative company that we are today.
Requirements:
Bachelor’s degree in computer science, Information Technology, or a related field.
Minimum of 5 years of experience in master data management, data governance, or a similar role.
Proficiency in MDM software solutions and tools.
In-depth understanding of master data domains such as customers, products, vendors, materials, and financial hierarchies.
Strong understanding of data integration patterns, ETL processes, and data warehousing concepts.
Experience in financial services or a similarly regulated industry, with an understanding of compliance and regulatory requirements.
Excellent analytical and problem-solving skills.
Proven ability to collaborate with cross-functional teams and communicate complex data concepts to non-technical stakeholders.
Strong attention to detail, with an emphasis on data accuracy and integrity.
Proficient in data governance and stewardship platforms (e.g., Collibra, Alation), as well as data cataloging and lineage visualization tools.
Proficient in advanced SQL, PL/SQL, T-SQL scripting, and experience with at least one NoSQL variant.
These Qualifications Would be Nice to Have:
Master’s degree in information management, Data Science, or a related field.
Relevant professional certifications (e.g., Certified Data Management Professional (CDMP), Certified Information Systems Security Professional (CISSP).
Experience with cloud-based MDM solutions.
Knowledge of machine learning and its application to data quality and master data management.
What’s In It For You:
Medical, Dental and Vision insurance for you and your family
Relax and recharge with Paid Time Off (PTO)
6 company-observed paid holidays, plus 3 paid floating holidays
401k (after 90 days) plus employer match up to 4%
Pet Insurance for your furry family members
Wellness perks including onsite fitness equipment at both locations, EAP, and access to the Headspace App
We invest in your future through Tuition Reimbursement
Save on taxes with Flexible Spending Accounts
Peace of mind with Life and AD&D Insurance
Protect yourself with company paid Long-Term Disability and voluntary Short-Term Disability
Concora Credit provides equal employment opportunities to all Team Members and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Concora Credit Is an equal opportunity employer (EEO).
Show more
Show less","Data Management, Master Data Management, Data Governance, Data Integration, Collibra, Alation, Data Lineage, Data Catalog, Data Quality, Data Dictionary, Data Consolidation, Robotic Process Automation, Computer Science, Information Technology, ETL, Data Warehousing, SQL, PL/SQL, TSQL, NoSQL","data management, master data management, data governance, data integration, collibra, alation, data lineage, data catalog, data quality, data dictionary, data consolidation, robotic process automation, computer science, information technology, etl, data warehousing, sql, plsql, tsql, nosql","alation, collibra, computer science, data catalog, data consolidation, data dictionary, data governance, data integration, data lineage, data management, data quality, datawarehouse, etl, information technology, master data management, nosql, plsql, robotic process automation, sql, tsql"
Senior Data Engineer,Turing Labs Inc.,"Oregon City, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-turing-labs-inc-3625988633,2023-12-17,Oregon,United States,Mid senior,Hybrid,"As a Senior Data Engineer, you will own all things related to data engineering including building and maintaining data models, integrations, and pipelines for our AI software platform. This role is very hands-on and requires a structured mindset and solid implementation skills.
Responsibilities
Build data pipelines, data processing tools and scripts
Perform tasks such as writing scripts, web scraping, getting data from APIs etc, and integrate data from several data sources
Automate data pipelines using scheduling tools like Airflow
Reinvent prototypes to create production-ready data flows
Be responsible for the technical solution design, lead the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights.
Participate in the development of documentation, technical procedures and user support guides
Articulate benefit analysis of technologies in open source and proprietary products. Pilot and choose optimal tools and solutions
Support data science research by designing, developing, and maintaining all parts of the data pipeline
How You Will Lead
Communicate with stakeholders to clarify requirements. Craft technical solutions and assemble design artifacts (functional design documents, data flow diagrams, data models, etc.).
Serve the team as a subject matter expert & mentor for ETL design.
Proactively identify performance & data quality problems and drive the team to remediate them. Advocate architectural and code improvements to the team to improve execution speed and reliability.
Harness operational excellence & continuous improvement with a can do leadership demeanor.
Be prepared for changes in business direction and understand when to adjust designs.
Work effectively in an unstructured and fast-paced environment both independently and in a team setting, with a high degree of self-management with clear communication and commitment to delivery timelines.
You already have
A BS/MS degree in Computer Science, Engineering, Mathematics, Physics, or equivalent/related degree.
Built programmatic ETL pipelines with SQL-based technologies and platforms.
Solid understanding of databases, and are working with sophisticated datasets.
Shown strong problem solving with acute attention to detail and ability to meet tight deadlines and project plans.
Shown the ability to research, analyze, interpret, and produce accurate results within reasonable turnaround times with an iterative mentality with rapid prototyping designs.
Shown technical leadership with an emphasis on the data lake, data warehouse solutions, business intelligence, big data analytics, enterprise-scale custom data products.
Knowledge of data modeling techniques and high-volume ETL/ELT design.
Experience with version control systems (Github, Subversion) and deployment tools (e.g. continuous integration) required.
Experience working with Public Cloud platforms like GPC, AWS, or Snowflake.
Familiarity with scrum/agile project management methodologies and SDLC stages.
At least 3 years of expert experience with SQL.
At least 3 years of experience with the AWS ecosystem.
At least 2 years of Python development experience using Pandas.
Why work with us?
Work closely with core team building company and product strategy
Stimulating environment with the opportunity to work with industry's leading minds
Competitive compensation & perks
A culture that you would love to work in
Several professional and financial opportunities
Medical, unlimited PTO, and work remotely
Show more
Show less","Data engineering, Data modeling, Data integration, Data pipelines, Data processing, Scripting, Web scraping, Data acquisition, Data analysis, Data quality, Data lake, Data warehouse, Business intelligence, Big data analytics, Data products, SQL, Python, Pandas, Airflow, Github, AWS, Snowflake, Scrum, Agile, SDLC, Version control systems, Deployment tools","data engineering, data modeling, data integration, data pipelines, data processing, scripting, web scraping, data acquisition, data analysis, data quality, data lake, data warehouse, business intelligence, big data analytics, data products, sql, python, pandas, airflow, github, aws, snowflake, scrum, agile, sdlc, version control systems, deployment tools","agile, airflow, aws, big data analytics, business intelligence, data acquisition, data engineering, data integration, data lake, data processing, data products, data quality, dataanalytics, datamodeling, datapipeline, datawarehouse, deployment tools, github, pandas, python, scripting, scrum, sdlc, snowflake, sql, version control systems, web scraping"
Marketplace Data Analyst,US Tech Solutions,"Beaverton, OR",https://www.linkedin.com/jobs/view/marketplace-data-analyst-at-us-tech-solutions-3780241870,2023-12-17,Oregon,United States,Mid senior,Hybrid,"Marketplace Data Analyst
Location : Beaverton, Oregon, 97003
Duration : 10+ Months Contract(Strong possibility of Extension)
Top 3 Skills
:
1. Tableau: In this position we need someone capable of designing & maintaining dashboards
2. Data Management: Ensuring data quality flows according to the expectation of tools.
3. Microsoft Access: Data ingestion tools & database design/management is required as well.
Job Description:
The candidate will be responsible for Commercial/Retail data tools & solutions for business throughout the Central & South American region.
Generate, evolve & maintain data consumption tools.
Guarantee the integrity of data feeds and create sustainable databases for performance tracking.
Some of the key areas of ownership are:
1) data management (quality control, troubleshooting data issues, attributing datapoints, etc),
2) Tableau dashboards management (maintenance, enhancements required by business, new product deployments, etc) and
3) maintenance & enhancements in local databases in Microsoft Access.
About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.
Recruiter Details:
Adil Saifi - Team Lead (IT Recruitment)
Telelphone: (201) 524 9600 Ext: 7222 | Direct: (503) 207 6215
Fax: (201) 524 9601
Email: adil@ustechsolutionsinc.com
Show more
Show less","Tableau, Data management, Microsoft Access, Data ingestion tools, Database design, Database management, Data quality control, Troubleshooting, Data attribution, Data visualization, Data consumption tools, Data integrity, Sustainable databases, Performance tracking","tableau, data management, microsoft access, data ingestion tools, database design, database management, data quality control, troubleshooting, data attribution, data visualization, data consumption tools, data integrity, sustainable databases, performance tracking","data attribution, data consumption tools, data ingestion tools, data integrity, data management, data quality control, database design, database management, microsoft access, performance tracking, sustainable databases, tableau, troubleshooting, visualization"
Lead Data Analyst,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/lead-data-analyst-at-nike-3774021588,2023-12-17,Oregon,United States,Mid senior,Hybrid,"Work options:
Hybrid
Hybrid Mon-Thur Onsite
Title
: Lead Data Analyst
Location
: Beaverton, OR
Duration:
6 Month Contract
Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it’s about each person bringing skills and passion to a challenging and constantly evolving game.
What You Will Do
Seeking a Lead Data Analyst to join our data team. In this role, you will lead the generation of reports, datasets, and data visualizations to provide actionable insights to stakeholders across the business.
Develop and generate recurring reports, dashboards, and ad-hoc analyses using SQL, Python, R, Tableau, etc. to surface insights.
Mine, cleanse, and curate data from multiple sources (databases, data warehouses, etc.).
Create and document SQL queries, scripts, visualizations, and data models.
Identify and implement opportunities to improve data quality, efficiency, and analytics.
Clearly communicate insights through presentations, reports, and visualization best practices.
Manage technical projects and lead a team of junior analysts.
What You Will Need
7+ years proven work experience in Data Analyst or Business Data Analyst role.
Strong Experience in SQL and a scripting language such as Python or R.
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Experience with data warehouses/RDBMS like Snowflake & Teradata.
Experience working with BI tools like Tableau, Power BI or DOMO.
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling).
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc preferred.
Good understanding of Hadoop and Big Data processing frameworks.
Experience with source control tools such as GitHub.
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis.
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment.
Self-starter, highly organized and result oriented; and should be able to work with minimal direction.
Show more
Show less","Data Analysis, SQL, Python, R, Tableau, Snowpark, Teradata, Snowflake, Power BI, DOMO, Data Modeling, Analytical Algorithms, Forecasting, Modeling, AWS, EMR, S3, Redshift, Athena, Hadoop, Big Data, GitHub, Data Visualization, Data Storytelling, Agile","data analysis, sql, python, r, tableau, snowpark, teradata, snowflake, power bi, domo, data modeling, analytical algorithms, forecasting, modeling, aws, emr, s3, redshift, athena, hadoop, big data, github, data visualization, data storytelling, agile","agile, analytical algorithms, athena, aws, big data, data storytelling, dataanalytics, datamodeling, domo, emr, forecasting, github, hadoop, modeling, powerbi, python, r, redshift, s3, snowflake, snowpark, sql, tableau, teradata, visualization"
Lead Data Analyst,US Tech Solutions,"Beaverton, OR",https://www.linkedin.com/jobs/view/lead-data-analyst-at-us-tech-solutions-3773881528,2023-12-17,Oregon,United States,Mid senior,Hybrid,"Lead Data Analyst
Location : Beaverton, Oregon, 97003
Duration : 6+ Months Contract(Strong possibility of Extension)
Top 3 Skills
Data Analysis Cleanse, Curate, and Manipulate Data Clear Communication and Documentation
Job Description:
7+ years proven work experience in Data Analyst or Business Data Analyst role
Strong Experience in SQL and a scripting language such as Python or R
Strong problem solving and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Experience with data warehouses/RDBMS like Snowflake & Teradata
Experience working with BI tools like Tableau, Power BI or DOMO
Strong understanding of data modeling and analytical algorithms (including forecasting and modeling)
Experience working in AWS environment primarily EMR, S3, Redshift, Athena, etc preferred
Good understanding of Hadoop and Big Data processing frameworks
Experience with source control tools such as GitHub
Ability to cleanse, curate, and manipulate data; as well as visualize and story-telling the results of an analysis
Clear communication and storytelling via presentations, key performance metrics, reports, and data
Proven experience and ability to deliver results on multiple projects in a fast-paced agile environment
Self-starter, highly organized and result oriented; and should be able to work with minimal direction
About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.
Recruiter Details:
Adil Saifi - Team Lead (IT Recruitment)
Telelphone: (201) 524 9600 Ext: 7222 | Direct: (503) 207 6215
Fax: (201) 524 9601
Email: adil@ustechsolutionsinc.com
Show more
Show less","Data Analysis, Data Warehousing, SQL, Python, R, Tableau, Power BI, DOMO, Data Modeling, Analytical Algorithms, AWS (EMR S3 Redshift Athena), Hadoop, Spark, GitHub, Data Visualization, Storytelling, Agile Methodology, Data Manipulation","data analysis, data warehousing, sql, python, r, tableau, power bi, domo, data modeling, analytical algorithms, aws emr s3 redshift athena, hadoop, spark, github, data visualization, storytelling, agile methodology, data manipulation","agile methodology, analytical algorithms, aws emr s3 redshift athena, data manipulation, dataanalytics, datamodeling, datawarehouse, domo, github, hadoop, powerbi, python, r, spark, sql, storytelling, tableau, visualization"
Principal DevOps Engineer - Machine Learning Data Engineering,Mainz Brady Group,"Beaverton, OR",https://www.linkedin.com/jobs/view/principal-devops-engineer-machine-learning-data-engineering-at-mainz-brady-group-3777377693,2023-12-17,Oregon,United States,Mid senior,Hybrid,"W2 Only no C2C
We have an immediate need for a REMOTE DevOps Engineer role! The qualified candidate will join a Machine Learning and Data Science team.
5+ years of professional experience as a DevOps engineer, MLOps engineer.
AWS, Azure or GCP
SageMaker, AirFlow, Spark
Ci/Cd, Terraform
Show more
Show less","DevOps, MLOps, AWS, Azure, GCP, SageMaker, AirFlow, Spark, CI/CD, Terraform","devops, mlops, aws, azure, gcp, sagemaker, airflow, spark, cicd, terraform","airflow, aws, azure, cicd, devops, gcp, mlops, sagemaker, spark, terraform"
Senior Business Intelligence Data Analyst,City of New York,"Queens, NY",https://www.linkedin.com/jobs/view/senior-business-intelligence-data-analyst-at-city-of-new-york-3728296471,2023-12-17,Levittown,United States,Mid senior,Onsite,"Organization
The New York City Housing Authority (NYCHA) is the nation’s oldest and largest public housing authority with more than 176,000 apartments, and responsibility for administering a citywide Section 8 leased housing program that serves over 200,000 additional tenants. NYCHA’s mission is to provide quality housing for New Yorkers that is sustainable, inclusive, and safe, while fostering opportunities for economic mobility.
NYCHA’s Asset & Capital Management (A&CM) Division integrates and aligns the Authority’s existing development, modernization, and asset management work being carried out by the Real Estate Development, Capital Projects, and Comprehensive Modernization departments. This includes a $4.5 billion capital program – one of the largest in NY State – as well as a historic real estate transaction portfolio. The A&CM Division delivers comprehensive repairs to NYCHA buildings and apartments through innovative financing models, strong partnership with residents and other stakeholders, strategic, data-driven portfolio planning, and cost-effective project delivery and management. The Division also positions NYCHA’s housing portfolio for the future by incorporating innovative building materials, construction methods, and technology, improving residents’ quality of life while enhancing building performance and management systems.
Position Description
The A&CM Division’s Strategic Services Department seeks a Senior Business Intelligence Data Analyst. The BI unit has two primary areas of responsibility: providing analytical support across the division and leading execution of routine and ad hoc A&CM reports. The unit works closely with NYCHA’s CACMO to assess strategic reporting and data transparency needs for non-A&CM stakeholders and ensure fidelity of A&CM reports.
The Senior Business Intelligence Data Analyst should have exceptional proficiency with data. This proficiency should include a strong technical quantitative foundation, with understanding of various data manipulation tools and programming languages (e.g., Excel, R, Python, SQL). In addition, the Analyst should be well equipped to identify data needs across the division, taking a consultative approach to helping A&CM unit leads establish and improve upon appropriate metrics to stay abreast of their units’ work. The Analyst should be able to effectively explain complex data concepts to non-quantitative audiences, helped to foster a culture of data-driven decision making across the division.
The ideal candidate excels at working with data and being able to manipulate it through use of relational databases and programming languages to support effective decision making within NYCHA and accurate reporting to external stakeholders.
Some Key Day-to-day Responsibilities Include
Write and execute advanced SQL and python commands manipulate data for data analytics, dashboards, and reporting.
Lead, design and develop self-serve BI and visualization dashboards, providing easy access to data and insights by leveraging various BI tools and programming concepts in python.
Create and maintain interfaces within Microsoft Power Platform.
Project Management of special initiatives projects and assignments.
Work with e-Builder team to align inputs with internal and external reporting needs.
Liaise with internal stakeholders to identify opportunities to provide analysis to support decision making.
Align data quality procedures and automations for analysis and reporting with A&CM business needs.
Key Competencies
Subject Matter Expert: Expertise in applied data science, including some technical acumen and an ability to explain and present complex data to non-quantitative audiences and knowledge of effective development of data tools (e.g., dashboards).
Strong Leader & Capacity Builder: A proven leader with experience in public sector reporting, data governance, data analysis, and/or project management. Ability to work in high-performance teams through cultivating innovation, ownership, accountability, and collaboration.
Effective Analyst & Change Agent: A strategic thinker and structured problem-solver, able to drive change and innovation while effectively juggling diverse responsibilities and multiple, competing priorities.
Exemplary Communicator: Ability to communicate complex data concepts to a variety of stakeholders, including non-quantitative audiences. Communication skills should include strong data visualization acumen, including both technical skills (e.g., Dashboarding, Excel, PowerPoint) and understanding of data visualization best practices. Strong political acumen and practiced in facilitating multi-stakeholder meetings designed to achieve consensus on objectives. Experience collaborating with a diversity of cultures and sensitive to the challenges of low-income populations.
NOTE: Due to the existence of a civil service list, candidates must have civil service status in the title of Computer Specialist (Software) to be considered.
NOTE: This position is open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate in your cover letter that you would like to be considered for the position under the 55-a Program. For detailed information regarding the 55-a Program, please visit the link below:
http://www.nyc.gov/html/dcas/downloads/pdf/psb/100_1.pdf
Please read this posting carefully to make certain you meet the minimum qualification requirements before applying to this position.
Minimum Qualifications
A baccalaureate degree from an accredited college, including or supplemented by twenty-four (24) semester credits in computer science or a related computer field and two (2) years of satisfactory full-time software experience in designing, programming, debugging, maintaining, implementing, and enhancing computer software applications, systems programming, systems analysis and design, data communication software, or database design and programming, including one year in a project leader capacity or as a major contributor on a complex project; or
A four-year high school diploma or its educational equivalent and six (6) years of full-time satisfactory software experience as described in “1"" above, including one year in a project leader capacity or as a major contributor on a complex project; or
A satisfactory combination of education and experience that is equivalent to (1) or (2) above. College education may be substituted for up to two years of the required experience in (2) above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. A masters degree in computer science or a related computer field may be substituted for one year of the required experience in (1) or (2) above. However, all candidates must have a four year high school diploma or its educational equivalent, plus at least one (1) year of satisfactory full-time software experience in a project leader capacity or as a major contributor on a complex project.
NOTE: In order to have your experience accepted as Project Leader or Major Contributor experience, you must explain in detail how your experience qualifies you as a project leader or as a major contributor. Experience in computer operations, technical support, quality assurance (QA), hardware installation, help desk, or as an end user will not be accepted for meeting the minimum qualification
requirements.
Special Note
To be eligible for placement in Assignment Level IV, in addition to the Qualification Requirements stated above, individuals must have one year of satisfactory experience in a project leader capacity or as a major contributor on a complex project in data administration, database management systems, operating systems, data communications systems, capacity planning, and/or on-line applications programming.
Preferred Skills
Experience developing, implementing, and maintaining complex MS Power BI or similar tools dashboards, visualizations, and analytics. - Must possess a highly proficient understanding of relational databases and complex database structures, including hands on experience writing complex SQL queries. - Strong problem solving and troubleshooting skills which must incorporate data acquisition, transformation, and delivering information effectively utilizing modern BI tools. - Hands on experience and proficiency with MS PowerBI, Relational Databases, SQL, python, or similar programming languages. - Experience with Predictive analytics, Machine Learning and AI are highly desirable. - Experience with Microsoft Power Platform (e.g., power automate, power forms, etc.) is highly desirable.
55a Program
This position is also open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate at the top of your resume and cover letter that you would like to be considered for the position through the 55-a Program.
Public Service Loan Forgiveness
As a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/
Residency Requirement
NYCHA has no residency requirements.
Additional Information
The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.
$93,288.00 – $120,190.00
Show more
Show less","Data Analytics, SQL, Python, R, Tableau, Power BI, Microsoft Power Platform, Machine Learning, Data Visualization, Data Governance, Project Management, Communication, Reporting, Problem Solving, Stakeholder Engagement, Business Intelligence, Data Manipulation, Data Transformation, Data Quality, Data Science, DataDriven Decision Making","data analytics, sql, python, r, tableau, power bi, microsoft power platform, machine learning, data visualization, data governance, project management, communication, reporting, problem solving, stakeholder engagement, business intelligence, data manipulation, data transformation, data quality, data science, datadriven decision making","business intelligence, communication, data governance, data manipulation, data quality, data science, data transformation, dataanalytics, datadriven decision making, machine learning, microsoft power platform, powerbi, problem solving, project management, python, r, reporting, sql, stakeholder engagement, tableau, visualization"
Cyber Associate Data Engineer,New York City Office of Technology & Innovation,"Brooklyn, NY",https://www.linkedin.com/jobs/view/cyber-associate-data-engineer-at-new-york-city-office-of-technology-innovation-3747044845,2023-12-17,Levittown,United States,Mid senior,Hybrid,"The Office of Technology and Innovation (OTI) leverages technology to drive opportunity, improve public safety, and help government run better across New York City. From delivering affordable broadband to protecting against cybersecurity threats and building digital government services, OTI is at the forefront of how the City delivers for New Yorkers in the 21st century. Watch our welcome video to see our work in action, follow us on social media @NYCOfficeofTech, and visit oti.nyc.gov to learn more.
At OTI, we offer great benefits, and the chance to work on projects that have a meaningful impact on millions of people. You'll have the opportunity to work with cutting-edge technology and collaborate with other passionate professionals who share your drive and commitment to making a difference through technology.
About New York City Cyber Command
OTI / NYC Cyber Command is charged with protecting all City systems against cyber threats, including systems that deliver vital services to New Yorkers. Headed by the Chief Information Security Officer of the City of New York, we provide in-depth support to over 100 agencies and offices to protect, detect, identify, respond to, and recover from cyber threats.
The Security Sciences teams provide highly functional, available, trusted solutions that enable the City government to prevent, detect, respond, and recover from cyber threats. Security Sciences is responsible for security architecture and engineering, with an emphasis on big data and emerging technology. This includes evaluating security tools, building a highly resilient and defensible security architecture, and supporting Cyber Command’s software engineering and development lifecycle to rapidly meet the mission needs.
About The Position
We are seeking a motivated Associate Data Engineer to join our Data Science Team. Our Data Science Team strives to make security data a strategic asset by providing a platform to structure, manage, integrate, control, analyze, and support threat management activities. As an Associate Data Engineer, you will help build a secure, scalable, and cloud native data processing framework that will support OTI / Cyber Command's cybersecurity mission.
Responsibilities will include:
Develop and maintain our data pipeline using Apache Beam, Java, Python, and other data processing technologies
Identify and implement performance improvements across all pipelines
Engage with data consumers and producers to design appropriate models to suit all needs.
Maintain information exchanges through publish, subscribe, and alert functions that enable users to send and receive critical information as required.
Support incident management, service-level management, change management, release management, continuity management, and availability management for databases and data management systems.
Administer databases and/or data management systems that allow for the secure storage, query, protection, and utilization of data.
HOURS/SHIFT
Day - Due to the necessary technical duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.
WORK LOCATION
Brooklyn, NY
TO APPLY
Special Note: Taking and passing civil service exams are necessary to maintain employment with the City of New York. Please check the Department of Citywide Administrative Services (DCAS) website (http://www.nyc.gov/html/dcas/html/work/exam_monthly.shtml) for important exam filing information. Please ensure that you are either a permanent employee in the civil service title listed on this posting, or, that you file for the examination when there is an open filing period. For more information regarding the civil service process, please visit the DCAS website at: http://www.nyc.gov/html/dcas/html/work/work.shtml
Interested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration
Please go to www.cityjobs.nyc.gov and search for Job ID # 613352
SUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW
APPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL
OTI participates in E-Verify.
Minimum Qualifications
A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or
A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or
Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.
Preferred Skills
The successful candidate should possess the following: - Experience with the Agile Development Methodology - Practical knowledge of both Java and Python - Familiarity with Unix scripting, Web development, and automated testing - Familiarity with machine learning techniques and machine learning toolkits such as R, scikit-learn, etc. - Experience working with Terraform. - Familiarity with the CI/CD process - At least one year professional, academic, or personal experience with software development or data engineering experience (includes internship experience). - At least 1 year professional, academic, or personal experience with object-oriented/object function scripting languages preferably java or python. - Familiarity with or exposure to cloud application development. - Familiarity with distributed data processing frameworks.
55a Program
This position is also open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate at the top of your resume and cover letter that you would like to be considered for the position through the 55-a Program.
Public Service Loan Forgiveness
As a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/.
Residency Requirement
New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.
Additional Information
The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.
Show more
Show less","Apache Beam, CI/CD, Java, Python, Unix scripting, Web development, Automated testing, Machine learning, R, Scikitlearn, Terraform, Cloud application development, Distributed data processing frameworks, Data Science, Data engineering, Agile Development Methodology, Objectoriented programming, Object function scripting languages","apache beam, cicd, java, python, unix scripting, web development, automated testing, machine learning, r, scikitlearn, terraform, cloud application development, distributed data processing frameworks, data science, data engineering, agile development methodology, objectoriented programming, object function scripting languages","agile development methodology, apache beam, automated testing, cicd, cloud application development, data engineering, data science, distributed data processing frameworks, java, machine learning, object function scripting languages, objectoriented programming, python, r, scikitlearn, terraform, unix scripting, web development"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Addison, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742680167,2023-12-17,Rutland,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, ETL, ELT","python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, etl, elt","apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Bensenville, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742672999,2023-12-17,Rutland,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, Spark, Scala, PySpark, AWS, Azure, GCP, Apache Beam, Kafka, CI/CD, IaC, DevOps","python, machine learning, spark, scala, pyspark, aws, azure, gcp, apache beam, kafka, cicd, iac, devops","apache beam, aws, azure, cicd, devops, gcp, iac, kafka, machine learning, python, scala, spark"
Operations Data Analyst / Onsite,MISUMI,"Schaumburg, IL",https://www.linkedin.com/jobs/view/operations-data-analyst-onsite-at-misumi-3787741496,2023-12-17,Rutland,United States,Mid senior,Onsite,"MISUMI USA is the North American arm of a fast-growing $3B+ global leader in the design, production, and distribution of automated manufacturing components. We are looking for friendly individuals to join our fast-growing and highly profitable company.
The position of Operations Data Analyst in the Customer Service department will contribute to the creation and adoption of better customer service standards and the development of best practices as well as providing customer service via intensive research and analysis. Essential tasks involve identifying areas within the company’s customer services that need updating or improving, often working in cooperation with the warehouse, logistics, IT, customer service (reps and leadership), and accounting/finance/AR.
This position will be responsible for highly visible deliverables that contribute significantly to the achievement of further customer satisfaction, key performance indicators (KPI) and key success factors (KSF), cost, expense, and other business objectives.
PLEASE NOTE THIS IS AN ONSITE MONDAY - FRIDAY POSITION AT OUR SCHAUMBURG, IL OFFICE***
Job Responsibilities:
Demonstrate project management/project management leadership skills (scheduling, documenting, follow-up, etc.).
Gather data on competitors and analyze their CS operations and methods of services.
Visit/Interview individuals or businesses to obtain information on their issues with the company’s Customer Service.
Forecast and track industry and customer trends, analyzing collected data Seek and provide information to help the company’s Customer Service determine its position in the marketplace.
Prepare reports of findings from the assigned task, illustrating data graphically and translating complex findings into written text.
Provide management with information and proposals concerning the concept, design, and methods of the company’s Customer Service.
Execute every task as directed by the supervisor.
Competency Requirements:
At least 2 years of job experience (Customer Service Development, Outside Sales, or Market Research experience preferred).
Advanced skills in MS Office (Excel, Access, PowerPoint)
Tableau experience is preferred.
Maintains strong attention to detail in high-pressure situations.
Good interpersonal skills within the company.
Database knowledge and proficiency are preferred.
Bachelor's degree preferably in business or marketing.
MISUMI is an Equal Opportunity Employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, veteran status, disability, marital status, sexual orientation, citizenship status, genetic information, gender identity, or any other protected status under applicable law.
FOR CALIFORNIA RESIDENTS:
MISUMI USA, Inc. (“MISUMI”) complies with the California Consumer Privacy Act (“CCPA”), California Privacy Rights Act (“CPRA”), and other applicable privacy laws. We may collect the following categories of personal information for purposes of the application and hiring process: name and contact information (phone number; mailing address; email address(es)); education and qualifications; employment history and experience; LinkedIn profile; and other information voluntarily provided by the applicant. Under the CCPA and CPRA, California residents have the right to know, correct, delete, and/or limit the use of certain information collected by MISUMI. For further information, see our full privacy policy at https://us.misumi-ec.com/careers/privacy/california-privacy-rights/. If you have any concerns, please send an email to hr1@misumiusa.com.
Powered by JazzHR
lGsb8oCUuj
Show more
Show less","Project management, Data analysis, Tableau, MS Office (Excel Access PowerPoint), Database knowledge, Business or marketing degree, Customer service development, Outside sales, Market research, Industry and customer trends forecasting, Data visualization","project management, data analysis, tableau, ms office excel access powerpoint, database knowledge, business or marketing degree, customer service development, outside sales, market research, industry and customer trends forecasting, data visualization","business or marketing degree, customer service development, dataanalytics, database knowledge, industry and customer trends forecasting, market research, ms office excel access powerpoint, outside sales, project management, tableau, visualization"
Senior Transportation Planner - Data Scientist,AECOM,"Ocoee, FL",https://www.linkedin.com/jobs/view/senior-transportation-planner-data-scientist-at-aecom-3783530182,2023-12-17,Grand Island,United States,Mid senior,Onsite,"Company Description
Work with Us. Change the World.
At AECOM, we're delivering a better world. Whether improving your commute, keeping the lights on, providing access to clean water, or transforming skylines, our work helps people and communities thrive. We are the world's trusted infrastructure consulting firm, partnering with clients to solve the world’s most complex challenges and build legacies for future generations.
There has never been a better time to be at AECOM. With accelerating infrastructure investment worldwide, our services are in great demand. We invite you to bring your bold ideas and big dreams and become part of a global team of nearly 50,000 planners, designers, engineers, scientists, digital innovators, program and construction managers and other professionals delivering projects that create a positive and tangible impact around the world.
We're one global team driven by our common purpose to deliver a better world. Join us.
Job Description
AECOM is seeking a talented and experienced
Senior Transportation Engineer
or Transportation Planner to be part of our team supporting our client, Florida Turnpike Enterprise located in Ocoee, Florida.
This position will be responsible for the application of the recently developed statewide tour-based model along with the application of regional travel demand models. The job duties include fulfillment of scripting requests for innovative toll project evaluations while contributing to a multidisciplinary team specializing in planning, economic feasibility analysis, and design of toll roads. These projects are technology-forward and will lead travel demand analysis for toll roadway infrastructure.
The Transportation Engineer/Planner will utilize skills and experience to create and update model code for travel demand model development activities using various modeling tools including but not limited to Visum, TransCAD, QGIS, R, and Python software.
The Engineer/Planner will work to assemble and prepare data for model validation and long-range future year travel forecasts; conduct model validation to required accuracy standards; develop traffic forecasts for toll road projects; and prepare the documentation of results. As part of the model design, the Engineer/Planner will use QGIS/ArcGIS with related database development and application skills, along with SQL/Python/R or other programming languages. The position will also utilize analytical skills for research on design, data development, and statistical analyses to address perceived model shortcomings and recommend an improved modeling approach.
Qualifications
Minimum:
Bachelor's degree in Civil Engineering or Urban Planning or related field
8 plus years of transportation planning, modeling, or related experience, or demonstrated equivalency of experience and/or education
Experience with principles, practices, and objectives of transportation planning applications, specifically the 4-Step process of travel demand modeling, Activity-Based Models, micro-simulation and dynamic traffic assignment;
Preferred:
Master's degree in Civil Engineering, Transportation or Urban Planning or related field
Programming experience with at least two of the following: Microsoft Visual Studio, SQL, C++ (preferred), Python (preferred), R (preferred), JAVA
Experienced with of travel demand modeling software, such as Cube, TransCAD, TRANSIMS, and VISUM (preferred)
Experienced with choice models, development, and application to travel demand, use of econometrics in estimating demand models from survey data
Experienced with GIS and database analysis including developing scripts to process data in and out of GIS files or automated routines in QGIS / ArcGIS
Experienced with utilization of advanced techniques with modification and extension of theories, precepts, and practices of planning sciences, data sciences (preferred) and related disciplines
Experienced with technical writing skills and technical presentations to present modeling results for various stakeholders (clients, TRB or IBTTA conferences, model user groups).
Additional Information
Visa sponsorship is available for this position.
Relocation is available for this position
All your information will be kept confidential according to EEO guidelines.
The offered rate of compensation will be based on individual education, qualifications, experience, and work location. The salary range for this position typically is $100,000.00 to $140,000.00.
AECOM is proud to offer a comprehensive benefits program to meet the diverse needs of our employees. Depending on your employment status, AECOM benefits may include medical, dental, vision, life, AD&D, disability benefits, paid time off, leaves of absence, voluntary benefits, perks, U.S and global well-being programs, employee assistance programs, business travel insurance, service recognition awards, retirement savings plan, and employee stock purchase plan.
About AECOM
AECOM is the world’s trusted infrastructure consulting firm, delivering professional services throughout the project lifecycle – from advisory, planning, design and engineering to program and construction management. On projects spanning transportation, buildings, water, new energy and the environment, our public- and private-sector clients trust us to solve their most complex challenges. Our teams are driven by a common purpose to deliver a better world through our unrivaled technical and digital expertise, a culture of equity, diversity and inclusion, and a commitment to environmental, social and governance priorities. AECOM is a Fortune 500 firm and its Professional Services business had revenue of $13.1 billion in fiscal year 2022. See how we are delivering sustainable legacies for generations to come at aecom.com and @AECOM.
Freedom to Grow in a World of Opportunity
You will have the flexibility you need to do your best work with hybrid work options. Whether you’re working from an AECOM office, remote location or at a client site, you will be working in a dynamic environment where your integrity, entrepreneurial spirit and pioneering mindset are championed.
You will help us foster a culture of equity, diversity and inclusion – a safe and respectful workplace, where we invite everyone to bring their whole selves to work using their unique talents, backgrounds and expertise to create transformational outcomes for our clients.
AECOM provides a wide array of compensation and benefits programs to meet the diverse needs of our employees and their families. We also provide a robust global well-being program. We’re the world’s trusted global infrastructure firm, and we’re in this together – your growth and success are ours too.
Join us, and you’ll get all the benefits of being a part of a global, publicly traded firm – access to industry-leading technology and thinking and transformational work with big impact and work flexibility. As an Equal Opportunity Employer, we believe in each person’s potential, and we’ll help you reach yours.
All your information will be kept confidential according to EEO guidelines.
ReqID:
J10100305
Business Line:
Transportation
Business Group:
DCS
Strategic Business Unit:
East
Career Area:
Planning
Work Location Model:
On-Site
Show more
Show less","Civil Engineering, Urban Planning, Transportation planning, Travel demand modeling, ActivityBased Models, Microsimulation, Dynamic traffic assignment, GIS, Database analysis, QGIS, ArcGIS, SQL, Python, R, JAVA, Cube, TransCAD, TRANSIMS, VISUM, Choice models, Econometrics, Programming, Technical writing, Technical presentations","civil engineering, urban planning, transportation planning, travel demand modeling, activitybased models, microsimulation, dynamic traffic assignment, gis, database analysis, qgis, arcgis, sql, python, r, java, cube, transcad, transims, visum, choice models, econometrics, programming, technical writing, technical presentations","activitybased models, arcgis, choice models, civil engineering, cube, database analysis, dynamic traffic assignment, econometrics, gis, java, microsimulation, programming, python, qgis, r, sql, technical presentations, technical writing, transcad, transims, transportation planning, travel demand modeling, urban planning, visum"
SENIOR DATA BASE ANALYST - 42001753,State of Florida,"Tallahassee, FL",https://www.linkedin.com/jobs/view/senior-data-base-analyst-42001753-at-state-of-florida-3786591894,2023-12-17,Tallahassee,United States,Mid senior,Onsite,"Requisition No: 818995
Agency: Agriculture and Consumer Services
Working Title: SENIOR DATA BASE ANALYST - 42001753
Pay Plan: Career Service
Position Number: 42001753
Salary: $48,140.46 - $83,000.06
Posting Closing Date: 12/24/2023
Total Compensation Estimator
Tool
SENIOR DATA BASE ANALYST
FLORIDA DEPARTMENT OF AGRICULTURE AND CONSUMER SERVICES
OFFICE OF AGRICULTURE TECHNOLOGY SERVICES
OPEN COMPETITIVE OPPORTUNITY***
Contact
Shelley Harden, 850-245-1096
Minimum Requirements
A bachelor's degree from an accredited college or university in management information systems or in one of the computer sciences
and
four (4) years of experience in computer systems analysis and/or computer programming, two (2) years of which must have been in data base design or data base administration;
or
A bachelor’s degree from an accredited college or university
and
five (5) years of experience in computer systems analysis and/or computer programming, two (2) years of which must have been in data base design or data base administration;
or
A master's degree from an accredited college or university in management information systems or in one of the computer sciences
and
three (3) years of experience in computer systems analysis and/or computer programming, two (2) years of which must have been in data base design or data base administration;
or
A doctorate from an accredited college or university in management information systems or in one of the computer sciences and two (2) years of experience in data base design or data base administration;
or
Six (6) years of experience in computer systems analysis and/or computer programming, two years of which must have been in data base design
and
either (1) completion of a 720 classroom hour program of study from a vocational/technical school or accredited community college in an area of data processing (excluding data entry),
or
(2) 60 semester or 90 quarter hours of college course work from an accredited institution which includes four courses in computer science or management information systems.
Experience in computer systems analysis and/or computer programming can substitute on a year-for-year basis for the required college education.
Completion of a one-year program of study from a vocational/technical school in an area of data processing (excluding data entry) can substitute for one (1) year of computer systems analysis and/or computer programming experience.
ATTENTION CANDIDATES***
To be considered for a position with the Florida Department of Agriculture and Consumer Services:
All fields in the Candidate Profile must be completed (an attached resume is not a substitution for the information required on the candidate profile).
Work history, duties and responsibilities, hours worked, supervisor, and formal education fields, etc. must be filled out to determine qualifications for this position.
Responses to Qualifying Questions must be verifiable in the Candidate Profile.
The Florida Department of Agriculture and Consumer Services values
and supports employment of individuals with disabilities. Qualified
individuals with disabilities are encouraged to apply.
Additional Requirements
Perform on-call duties when assigned.
The position may be activated to support the Emergency Support Function (ESF) and respond to emergencies, including natural disasters or other types of incidents, as needed.
Notes
Successful applicant must pass a background screening, including fingerprinting, as a condition of employment.
Job Duties
The Senior DBA is responsible for the design, development, implementation, and ongoing maintenance of all database instances on multiple environments, including the release process for system and schema changes. The Senior DBA performs database query analysis, performance monitoring, optimization, develops and maintains stored procedures/functions/triggers, and maintains an appropriate database access layer. In addition, the Senior DBA is responsible for ensuring database scalability, backups, replication, reporting, and ensuring the confidentiality, integrity, and availability of data.
Lead the administration, maintenance, and support of the FDACS Microsoft SQL Server or Oracle database environment. This includes assisting users in the analysis, review, coordination, and implementation of new requests.
Work with the application development teams to ensure that the associated logical data model design adheres to agency standards and is based upon best practices and guidelines.
Coordinate and deploy all changes to production databases following industry best practices, agency policies, and procedures. This includes PL/SQL and T-SQL object and data model changes.
Write T-SQL and/or PL/SQL Stored Procedures, Functions, Packages, and Database Triggers as needed to augment security and performance requirements of application development.
Assist in bulk data loads, data cleansing, and data migration by writing and utilizing custom scripts and processes. This includes migrations to and from multiple data sources and types.
Participate in project meetings, as necessary, to understand application and database requirements.
Ensure that the testing and installation of all database hardware and software products are coordinated to ensure their compatibility with all existing products. Monitor and troubleshoot problems with the Oracle and/or SQL databases.
Run diagnostic tools to identify database performance bottlenecks and take appropriate corrective actions to improve database performance. Work with development teams to tune SQL queries.
Work with the infrastructure team to ensure that the associated hardware resources are allocated to the databases and to ensure high availability and optimum performance.
Design, implement, and maintain database maintenance scripts, including replication and monitoring.
Day-to-day database-related operational duties, including monitoring, statistical analysis, and troubleshooting.
Participate in 24/7 on-call support for database environments when required.
Design and maintain automated processes including SSIS, SQL Agent, DBA jobs, DBA scheduler.
Conduct capacity planning and configuration management studies to forecast future software and hardware requirements. Develop and maintain technical standards, procedures, and techniques for the guidance of Data Center personnel.
Consult management, application developers, and database administrators on all aspects of the database management system development and maintenance including storage and backups.
Provide support on all the stages of database implementation and/or database-related deployments.
Implement and enforce database security based on best practices, Federal and State regulations, and agency policies.
Conduct performance measurement studies and recommend system reconfiguration to affect improved performance.
Function as a technical specialist in troubleshooting problems relating to both hardware and software to determine malfunction causes and to initiate and/or recommend corrective actions. Responsible for working with vendor technical personnel in notifying vendors and/or correcting vendor-supplied software.
Review systems, operational procedures, and standards within the Data Center and recommend means to improve efficiency and quality of services to users.
Research and review computer technical development in hardware and software to provide supervisory personnel with up-to-date information in this field.
Assess future information requirements to develop long-range, comprehensive database plans.
Evaluate and make recommendations regarding proposed vendor hardware acquisitions requiring a database, and an online management software interface.
Perform related work as required, which will include, but not be limited to, adherence to the policies and procedures of the Department’s information resource security program (chapter 8 – Department’s Supervisory Manual) and allocate time as requested for disaster recovery activities.
Knowledge, Skills And Abilities
Experience in Microsoft SQL Server 2016 or higher database administration
Experience with Microsoft and/or Oracle database backup and recovery
Knowledge of the principles, practices, and techniques of SQL software administration, installation, and upgrade
Knowledge of Windows or Unix scripting.
Knowledge of database server and storage management.
Ability to write PL/SQL, T-SQL, stored procedures, functions, packages, and database triggers.
Operational support skills, including experience with system installations, migrations, and upgrades.
Ability to process information logically and solve problems.
Ability to monitor and resolve problems with database system components.
Ability to effectively communicate technical data processing information verbally and in writing.
Ability to work well in a team environment, and to establish and maintain effective working relationships with others.
Ability to receive and give constructive criticism and maintain effective working relationships with others.
Strong interpersonal skills to resolve problems in a professional manner.
Knowledge of Cherwell Administration
Benefits
The Benefits of Working for the State of Florida
Working for the State of Florida is more than a paycheck. The State’s total compensation package for employees features a highly competitive set of employee benefits including:
Annual and Sick Leave benefits;
Nine paid holidays and one Personal holiday each year;
State Group Insurance coverage options, including health, life, dental, vision and other supplemental insurance options;
Retirement plan options, including employer contributions (For more information, please visit www.myfrs.com ;)
Flexible Spending Accounts;
Tuition waivers;
And more!
For a complete list of benefits, visit www.mybenefits.myflorida.com .
For an estimate of the total compensation package for this position, please visit the
“Total Compensation Estimator Tool”
located above under the “Posting Closing Date.”
Special Notes
The State of Florida is an Equal Opportunity Employer/Affirmative Action Employer, and does not tolerate discrimination or violence in the workplace.
Candidates requiring a reasonable accommodation, as defined by the Americans with Disabilities Act, must notify the agency hiring authority and/or People First Service Center (1-866-663-4735). Notification to the hiring authority must be made in advance to allow sufficient time to provide the accommodation.
The State of Florida supports a Drug-Free workplace. All employees are subject to reasonable suspicion drug testing in accordance with Section 112.0455, F.S., Drug-Free Workplace Act.
VETERANS’ PREFERENCE.
Pursuant to Chapter 295, Florida Statutes, candidates eligible for Veterans’ Preference will receive preference in employment for Career Service vacancies and are encouraged to apply. Certain service members may be eligible to receive waivers for postsecondary educational requirements. Candidates claiming Veterans’ Preference must attach supporting documentation with each submission that includes character of service (for example, DD Form 214 Member Copy #4) along with any other documentation as required by Rule 55A-7, Florida Administrative Code. Veterans’ Preference documentation requirements are available by clicking here . All documentation is due by the close of the vacancy announcement.
Show more
Show less","Microsoft SQL Server 2016, Oracle database, Microsoft SQL Server, Windows or Unix scripting, PL/SQL, TSQL, SQL, Database administration, Data Center","microsoft sql server 2016, oracle database, microsoft sql server, windows or unix scripting, plsql, tsql, sql, database administration, data center","data center, database administration, microsoft sql server, microsoft sql server 2016, oracle database, plsql, sql, tsql, windows or unix scripting"
GIS Data Analyst,TekIntegral,"Tallahassee, FL",https://www.linkedin.com/jobs/view/gis-data-analyst-at-tekintegral-3677446593,2023-12-17,Tallahassee,United States,Mid senior,Onsite,"Title: GIS Data Analyst
Work authorization: NO H1b, CPT, OPT
Location: Tallahassee, FL (Onsite)
Remote or onsite: 100% on Client's site
Duration: 12+ months with possible extensions
Must-have Skills
EDU : BS/equiv
2+ y with GIS
Oracle Spatial
Preferred
State Client exp.
Requirements
Availability to work 100% at the Client's site in Tallahassee, FL (required);
Experience as GIS Data Analyst (2+ years);
Experience with ArcGIS;
Experience with Oracle Spatial;
Experience with environmental regulatory business processes and practices;
Experience Data analysis and resolution of system issues related to Geodatabase creation;
Experience working with GIS and non-GIS data conflicts in FL-SOLARIS, and BTLDS data related to Geodatabase adaptation of JAVA based systems;
Experience with GIS analysis of spatial data to verify accuracy of converted Oracle Spatial data to ESRI Geodatabase features;
Experience with land survey and cadastral GIS mapping and mapping applications;
Experience with projects related to cadastral mapping, property tax records, and/or land
Ownership;
Experience working with geospatial data;
Government experience (preferred);
Bachelor's Degree in Computer Science, Information Systems or other Information
Technology major, or equivalent work experience
Responsibilities Include But Are Not Limited To The Following
Assisting in the preparation and documentation of program requirements and specifications.
Facilitate coordination with ESRI, DSL, and OTIS for creation and conversion to ESRI
Geodatabase from existing ORACLE Spatial database.
Provide support as needed in creation of long-term solution for cloud migration of supported
applications
Ability to be creative, to use sound judgment, and to display foresight to identify potential
problems.
Ability to establish and maintain effective working relationships with others
Ability to work independently and part of a team.
Ability to work as part of a team.
Ability to determine work priorities and ensure proper completion of work assignments.
Ability to solve problems and manage issues effectively proactively and aggressively.
Strong communications skills.
Ability to communicate effectively and efficiently, both verbally and in writing, with both technical and non-technical staff and clients.
Ability to solve problems and manage issues effectively, efficiently, and proactively.
Strong deadline and task management skills.
Sound discretionary judgment skills
Show more
Show less","GIS, Oracle Spatial, ArcGIS, Geodatabase, JAVA, ESRI, DSL, OTIS, Cloud Migration, Creative Problem Solving, Collaboration and Communication, Teamwork, Effective Communication, Problem Solving, Deadline and Task Management, Discretionary Judgment","gis, oracle spatial, arcgis, geodatabase, java, esri, dsl, otis, cloud migration, creative problem solving, collaboration and communication, teamwork, effective communication, problem solving, deadline and task management, discretionary judgment","arcgis, cloud migration, collaboration and communication, creative problem solving, deadline and task management, discretionary judgment, dsl, effective communication, esri, geodatabase, gis, java, oracle spatial, otis, problem solving, teamwork"
"Senior Data Scientist, Marketing Analytics",GoTo,"Tallahassee, FL",https://www.linkedin.com/jobs/view/senior-data-scientist-marketing-analytics-at-goto-3786375719,2023-12-17,Tallahassee,United States,Mid senior,Onsite,"Job Description**Where you’ll work:** Anywhere within Central Time Zone +/- 2 hours**Marketing at GoTo**Being direct and relatable makes work and life easier for everyone. From storytelling and content to influencer relations and user lifecycle campaigns, the Marketing team at GoTo sets the stage for how we interact with customers, prospects and employees. When you become a part of this team, you will directly be impacting the business, revenue, and image of GoTo. Come discover with us.**Your Day to Day****As a Senior Data Scientist, Marketing Analytics you would be working on** :+ Create and deploy advanced statistical models and analyses from relational data sources in areas such as campaign performance, lead quality, buyer segmentation, anomaly detection, and spend efficiency+ Build full-cycle analytics experiments, reports, and dashboards using SQL, Python, PySpark, Tableau or other scripting and statistical tools+ Present technical findings in a summarized form to non-technical audiences; translate complex quantitative data into succinct actionable insights. Measure and report on actual performance vs target; perform analysis to identify root cause drivers of variances+ Collaborate with members of multiple departments to understand and influence data infrastructures**What We’re Looking For****As a Senior Data Scientist, Marketing Analytics your background will look like**+ 5+ years of experience in data analytics, consulting, big data, and/or related quantitative roles+ Experience working with marketing measurement and attribution, lead scoring, purchase funnel optimization, cohort analyses, time series analyses, regression models, machine learning, etc.+ Experience in SQL, ETL, A/B Testing, and statistical analysis (e.g., hypothesis testing, experimentation, regressions) with statistical packages such as Matlab, R, SAS, or Python+ Proficient in analytics & visualization tools (e.g., Tableau)+ Familiarity with SQL engines (e.g., Hive, Presto) and large-scale data processing engines (e.g., PySpark). Comfortable utilizing large data sets that live across data lakes and cloud-based systems (e.g., AWS, Azure and GCP)You’ll be working towards a shared goal with an open-minded and cohesive team greater than the sum of its parts. At GoTo, we’re passionate about growing a diverse and inclusive work ecosystem because unique takes make us a stronger company, and Stronger Together. We’re committed to creating an inclusive space for everyone, no matter what. That’s how we’ll **Be Real** , **Think Big** , **Move Fast** , and **Keep Growing** along the way.Learn more (oto.com/company/corporate-responsibility) .RPJLogMeIn, Inc. is committed to providing equal opportunity in employment to all employees and applicants for employment. No employee or applicant shall be discriminated against in the terms and conditions of employment on the basis of race, color, religious creed, gender, sex, pregnancy, religion, marital or domestic partner status, age, national origin, ancestry, physical or mental disability (including AIDS/HIV), medical condition, sexual orientation, gender identity, gender expression, genetic information, military and veteran status, application for or denial of family and medical care leave and/or pregnancy disability leave, or any other basis protected by federal, state or local law or ordinance or regulation. LogMeIn, Inc. also prohibits discrimination based on the perception that anyone has one of these characteristics or is associated with a person who has or who is perceived as having any of those characteristics.Each officer, manager, and employee is expected to support, cooperate with, and carry out this policy. Any employee who believes he or she has been the victim of employment discrimination, or has witnessed discrimination in the workplace based on any of these factors should report the matter immediately to Human Resources.
Show more
Show less","Data Analytics, Consulting, Machine Learning, Data Science, Big Data, Quantitative Analysis, SQL, Python, PySpark, Tableau, R, SAS, Matlab, Statistical Analysis, A/B Testing, Regression Models, Hypothesis Testing, Experimentation, Hive, Presto, AWS, Azure, GCP, Cloud Computing, Data Lakes","data analytics, consulting, machine learning, data science, big data, quantitative analysis, sql, python, pyspark, tableau, r, sas, matlab, statistical analysis, ab testing, regression models, hypothesis testing, experimentation, hive, presto, aws, azure, gcp, cloud computing, data lakes","ab testing, aws, azure, big data, cloud computing, consulting, data lakes, data science, dataanalytics, experimentation, gcp, hive, hypothesis testing, machine learning, matlab, presto, python, quantitative analysis, r, regression models, sas, spark, sql, statistical analysis, tableau"
Data Warehouse Manager (On-site from Day One),Dice,"Tallahassee, FL",https://www.linkedin.com/jobs/view/data-warehouse-manager-on-site-from-day-one-at-dice-3787366054,2023-12-17,Tallahassee,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Kyra Solutions, is seeking the following. Apply via Dice today!
Join Our Team
Do you want to make an impact on the world around you? The work we do at Kyra Solutions directly impacts government entities and the citizens they serve. In Transportation, our solutions are designed to save lives on the roadways and in our Regulatory practice, we are designing solutions to make government digital for your accessibility. Kyra works hard to offer long-term growth potential, competitive wages, and continuous professional development for our employees.
If you are interested in furthering your career with Kyra and help us improve the way governments serve their citizens, please send your resume, and make sure to include salary expectation, availability, and contact information. You do not want to miss this opportunity!
Title
Data Warehouse Manager (On-site from Day One)
Required Skills And Experience
7 or more years of experience in a BI, Data Engineer, or ETL role.
Proven experience working with SQL databases and Microsoft SQL Server.
Proven track record of successful data warehouse implementations.
Proven experience in data warehouse design and development.
Proven experience in ETL tools, processes, and in data migration projects.
Proficient understanding of data warehousing methodologies.
Proven proficiency with Microsoft Power BI for creating dashboards and reports.
Proven experience with data modeling, data architecture, and data quality concepts.
Proven proficiency in SQL and other data languages.
Excellent troubleshooting and problem-solving abilities.
Capable of explaining complex concepts to non-technical stakeholders.
Must be able to clearly convey information and ideas in both written and verbal forms to share information, share ideas, and resolve issues.
Ability to communicate and interact professionally and tactfully with people of different levels of education, cultural backgrounds, and life experiences and to exhibit respect to others.
Strong organizational skills and attention to detail, including the ability to adequately establish time frames and deadlines, meet deadlines, and follow through on tasks and assignments.
Preferred Skills And Experience
Experience working in the corrections or public safety sector.
Experience working with financial systems.
Previous involvement in public sector IT projects.
Experience working with large datasets and data lakes.
Experience with other data visualization tools.
Familiarity with Databricks SQL.
Experience with big data technologies.
Familiarity with Machine Learning (ML) and Artificial Intelligence (AI) concepts and their application in data analysis.
Experience in implementing ML and AI solutions in data warehouse environments.
Experience with security technologies around cloud solutions, preferably Azure.
Experience managing projects using Scrum and Kanban.
Preferred Licensing or Certifications, such as Association for Supply Chain Management (ASCM) Warehousing Certification; Data Warehousing and BI Certification Training; Data Warehousing for Business Intelligence Specialization; BI Foundations with SQL, ETL, and Data Warehousing Specialization, and Enterprise Data Management.
Primary Responsibilities
Document the current state of the client Budget and Accounting Information System (BARS) application databases and the current data feeds from the client accounting system, Florida Accounting Information Resource (FLAIR), including but not limited to their structure, data dictionaries, data flow, interdependencies, and potential problem areas.
Design, develop, and implement a new Microsoft Structured Query Language (SQL) Server data warehouse, ensuring it meets the project requirements and aligns with the overall data strategy of the client (the client is under a cloud first initiative, and the new data warehouse is preferred to reside in the Azure cloud).
Develop and maintain Extract, Transform, and Load (ETL) processes to migrate data from the new States accounting system, Florida Planning, Accounting, and Ledger Management (PALM), to the new SQL Server data warehouse.
Implement data modeling and design techniques to build scalable and efficient data structures to meet the client accounting and budgeting reporting needs.
Design and create data dashboards, visualizations, and reports using Power Business Intelligence (BI), providing critical insights to stakeholders.
Troubleshoot any issues that arise in the data warehouse and implement solutions.
Collaborate with other team members, stakeholders, and data users to ensure the new data warehouse meets the needs of the client.
Continuously monitor, refine, and report on the performance and capacity of the data warehouse.
Configure and administer data warehouse security settings, working closely with the Information Security team to ensure the best practices are in place.
Stay up to date on new data warehousing technologies and approaches.
Train and mentor less experienced team members.
Coordinate with other IT teams to ensure smooth data warehouse operations.
Handle incident management and problem resolution for data warehouse issues.
Develop data mining models to identify trends and patterns within large datasets.
Ensure that data is accurate and clean, using statistical methods such as data profiling and data quality analysis to identify potential problems with data quality before using it in reports or analyses.
Develop and thoroughly documenting the processes and procedures for the warehouses day-to-day operations, including creating backup plans for disaster recovery in case of equipment failure or data loss.
Design and implementing data warehouse solutions that meet the client needs while ensuring compliance with industry regulations regarding the security of sensitive data.
Work collaboratively with stakeholder teams to compile audience profiles, insights, and measurement plans.
Create simple, easy-to-follow reports and dashboards that key department heads will utilize on a regular basis to make informed business decisions.
Work with multiple Project Managers to deliver capabilities, projects, data sets, reports that have been prioritized by the Business Product Owners.
Work with the Manager of the client Research & Data Team and other stakeholders to accurately integrate collected data into the data warehouse, using descriptive analytics to research and perform statistical analysis to identify actionable insights.
Ensure that reporting and key institutional metrics are regularly updated, available, and accurate.
Provide expertise to the Data Governance team.
Create ad-hoc reports as needed.
Stay current of technology and best practices.
Regularly attend training and proactively conducts research when available.
Oversee the work of several other individuals and must be able to motivate team members, delegate tasks, perform knowledge transfers, and mentor staff.
Must be able to handle all duties and information in a confidential manner.
Must have the ability to simplify information systems and make sound business decisions.
Perform other job-related duties as assigned.
Education
Candidates must possess either a masters or bachelors degree from an accredited college or university in Computer Science, Information Systems, or other related field or 4 years of equivalent work experience.
Location
Tallahassee, FL
Duration
Long Term
Why Kyra?
Founded in 1997, Kyra Solutions is a national leader of transportation technology and regulatory solutions in government. We specialize in the art and science of digital transformation in government. Our commitment to providing the highest level of service and tailored solutions has supported our consistent double-digit growth for over a decade. We are headquartered in the greater Tampa Bay area with other offices across Florida and an innovation center in Silicon Valley, CA.
Because of our dedication to our employees, we have won one of the Best Companies to Work for in Florida 2 years in a row by Florida Trend magazine. Kyra has won other numerous awards including the coveted INC magazine’s one of America’s Fastest Growing Companies several years in a row. Kyra’s commitment to our employees, to best practices in project management and business analysis, and to solution development has led to our achievement in becoming the first Project Management Institute certified company in Florida. Our proven successful track record has resulted in several prestigious awards including the State of Florida's Diversity Business of the Year Award. We are proud to be a sponsor for the TaxWatch Productivity Awards and partner to Florida TaxWatch. for more information.
Background & References
Verifiable professional references will be required along with the resume; however, references will be checked/contacted after the interview and before the project starts. Level two background check will be done on the selected candidate for employment, criminal (State & Federal), education, and others as mandated by the client. Please make sure your resume and all other information provided are accurate. Any misrepresentation will mean permanent disqualification by the client. Equal employment opportunity employer.
Show more
Show less","SQL databases, Microsoft SQL Server, Data warehouse design, Data warehouse development, ETL tools, Data migration, Data warehousing methodologies, Microsoft Power BI, Data modeling, Data architecture, Data quality, SQL, Troubleshooting, Problemsolving, Machine Learning (ML), Artificial Intelligence (AI), Azure, Scrum, Kanban, Data Warehousing and BI Certification Training, Data Warehousing for Business Intelligence Specialization, BI Foundations with SQL ETL and Data Warehousing Specialization, Enterprise Data Management, Oracle, Teradata, Informatica, DataStage, Talend, DataFlux, SAS","sql databases, microsoft sql server, data warehouse design, data warehouse development, etl tools, data migration, data warehousing methodologies, microsoft power bi, data modeling, data architecture, data quality, sql, troubleshooting, problemsolving, machine learning ml, artificial intelligence ai, azure, scrum, kanban, data warehousing and bi certification training, data warehousing for business intelligence specialization, bi foundations with sql etl and data warehousing specialization, enterprise data management, oracle, teradata, informatica, datastage, talend, dataflux, sas","artificial intelligence ai, azure, bi foundations with sql etl and data warehousing specialization, data architecture, data migration, data quality, data warehouse design, data warehouse development, data warehousing and bi certification training, data warehousing for business intelligence specialization, data warehousing methodologies, dataflux, datamodeling, datastage, enterprise data management, etl tools, informatica, kanban, machine learning ml, microsoft power bi, microsoft sql server, oracle, problemsolving, sas, scrum, sql, sql databases, talend, teradata, troubleshooting"
Staff Data Scientist - E2E,Walmart End to End,"Bentonville, AR",https://www.linkedin.com/jobs/view/staff-data-scientist-e2e-at-walmart-end-to-end-3731505309,2023-12-17,Arkansas,United States,Associate,Onsite,"The
Staff Data Scientist
in
Walmart Retail Intelligence
team will be core contributor in AI discovery, research, algorithmic solution design, and rapid prototyping of ML solutions to deliver efficiency in operations of Walmart US store and distribution center associates. This role will be focused on designing ML and OR models in Associate Engine that unifies associate signals to optimize workload planning and task prioritization. They will be responsible for detailed ML/AI model designs, feature engineering, model development and experimentation, and pilot testing and iterative model refinement. They will work collaboratively with Product and Tech teams to productionize and scale ML/AI solutions and establish model monitoring and management processes. This role will require partnership with other teams within the Walmart E2E Fulfillment organization (data governance, science management, retail analytics, customer engine, foundation).
The Staff Data Scientist will play a pivotal role in selection of ML/AI or Operations Research algorithms and development of them at scale, that are linked to strategic business goals, unified associate signals, efficiency of associate operations, and delivering the best associate and customer experience. They are core contributors in shaping the vision of data science work streams including prototype development cycle time, model accuracy, model reliability, model stability, coverage of use cases, decisions impact and resources cost savings.
What you'll do:
Apply broad knowledge of ML/AI capabilities to identify growth areas, new and emerging opportunities. Conducts research by studying organization goals, strategies, practices, and user projects. Recommends strategies by evaluating organization outcomes; identifying problems; evaluating trends; anticipating requirements.
Influence a team of Data Scientists focused on powering ML/AI solutions within the scope of associate work prioritization, workforce planning and retention strategies.
Partner closely with Data Science Management, Business Leaders, and ML engineering leaders to inform, drive and accelerate innovations in discovery experiences via Insights, frameworks, causal inference solutions and machine learning prototypes.
Execute full life cycle of projects through project planning, data collection, model prototyping and deployment, with responsibilities encompassing stakeholder management and communication to cross-functional partners.
Adhere to MLOps framework to create, build, and deploy data science algorithms continuously test and monitor drifts.
Shares highly complex information related to areas of expertise. Interacts with senior management to keep abreast of objectives. Interacts with peers to interpret information and improve cross-functional processes and programs. Builds and enhances key internal and external contacts.
What you'll bring:
Experience in the development of large-scale ML/AI solutions, Causal Inference and Ranking/Prioritization systems.
3 years+ of experience in Applied ML familiar with core supervised and unsupervised algorithms.
Skilled in Programming language (Python) and Large-scale training using data structures and algorithms.
1 year+ experience with Causal Inference frameworks, Bayesian Networks and Operations Research methods (Newsvendor problem, Travelling Salesman problem, Shortest Path models, etc.)
Experience with cloud environments including Azure, Google Cloud, etc.
Solid MLOps practices including good design documentation, unit testing, integration testing and source code control (git).
Experienced with agile methodologies using project planning and tracking management tools e.g., JIRA.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
About Walmart E2E:
Imagine working in
an environment where one line of code can make life easier for hundreds of
millions of people. That’s what we do at Walmart E2E. We’re a team of software
engineers, data scientists, cybersecurity expert's and service professionals
within the world’s leading retailer who make an epic impact and are at the
forefront of the next retail disruption. People are why we innovate, and people
power our innovations. We are people-led and tech-empowered. We train our team
in the skillsets of the future and bring in experts like you to help us grow.
We have roles for those chasing their first opportunity as well as those looking
for the opportunity that will define their career. Here, you can kickstart a
great career in tech, gain new skills and experience for virtually every
industry, or leverage your expertise to innovate at scale, impact millions and
reimagine the future of retail.
​​​​
Future Ways of Working:
Our company's
success can be attributed to our employees. While technology has allowed us to
be effective while working remotely, there is no substitute for being in the
office together; it helps to shape our culture, collaborate, innovate, build
relationships, and move more quickly. We strive to provide flexibility to
promote a healthy work-life balance but recognize that in-person interactions
are important to our culture and shared success. We'll meet in person on a regular
and purposeful basis.
Who We Are
Join Walmart and your work could help over 275 million global
customers live better every week. Yes, we are the Fortune #1 company. But
you’ll quickly find we’re a company who wants you to feel comfortable bringing
your whole self to work. A career at Walmart is where the world’s most complex
challenges meet a kinder way of life. Our mission spreads far beyond the walls
of our stores. Join us and you'll discover why we are a world leader in
diversity and inclusion, sustainability, and community involvement. From
day one, you’ll be empowered and equipped to do the best work of your life. careers.walmart.com
Benefits:
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer:
Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas, and opinions – while being inclusive of all people.
Minimum Qualifications...
Outlined below are the required
minimum qualifications for this position. If none are listed, there are no
minimum qualifications.
Option 1: Bachelors
degree in Statistics, Economics, Analytics, Mathematics, Computer Science,
Information Technology or related field and 4 years' experience in an analytics
related field. Option 2: Masters degree in Statistics, Economics, Analytics,
Mathematics, Computer Science, Information Technology or related field and 2
years' experience in an analytics related field. Option 3: 6 years' experience
in an analytics or related field
Preferred Qualifications...
Outlined
below are the optional preferred qualifications for this position. If none are
listed, there are no preferred qualifications.
Data science,
machine learning, optimization models, PhD in Machine Learning, Computer
Science, Information Technology, Operations Research, Statistics, Applied
Mathematics, Econometrics, Successful completion of one or more assessments in
Python, Spark, Scala, or R, Using open source frameworks (for example, scikit
learn, tensorflow, torch)
Primary Location...
702 SW 8TH ST, BENTONVILLE, AR 72716, United States of America
Show more
Show less","Machine Learning, Causal Inference, Optimization models, Natural Language Processing, Data Science, Python, R, Spark, Scala, TensorFlow, Scikitlearn, Torch, Bayesian Networks, Operations Research, MLOps, Agile Methodologies, Jira, Git, Unit testing, Integration testing, Source code control, Cloud environments (Azure Google Cloud etc.), Associate Engine, Workload planning, Task prioritization","machine learning, causal inference, optimization models, natural language processing, data science, python, r, spark, scala, tensorflow, scikitlearn, torch, bayesian networks, operations research, mlops, agile methodologies, jira, git, unit testing, integration testing, source code control, cloud environments azure google cloud etc, associate engine, workload planning, task prioritization","agile methodologies, associate engine, bayesian networks, causal inference, cloud environments azure google cloud etc, data science, git, integration testing, jira, machine learning, mlops, natural language processing, operations research, optimization models, python, r, scala, scikitlearn, source code control, spark, task prioritization, tensorflow, torch, unit testing, workload planning"
Data Engineer (GCP/ Spark/Pyspark & Scala functional),"The Dignify Solutions, LLC","Bentonville, AR",https://www.linkedin.com/jobs/view/data-engineer-gcp-spark-pyspark-scala-functional-at-the-dignify-solutions-llc-3768008994,2023-12-17,Arkansas,United States,Mid senior,Onsite,"7 years exp
Technical/Functional Skills -
Tech Skills - Scala, Spark, GCP, Dataproc, Hadoop, Airflow, SBT, Maven, Docker, Kubernetes, pyspark, Jenkins, Bigquery
Experience with workflow management tools such as Jenkins, Airflow
Experience running spark/hadoop workloads using Dataproc, Dataflow, Cloud composer, EMR, HD Insights or similar.
Proven working expertise with Big Data Technologies Spark, PySpark, hive, and SQL.
Demonstrates expertise in writing complex, highly optimized queries across large data sets Knowledge and experience in Kafka, Storm, Druid and Presto
Show more
Show less","Scala, Spark, GCP, Dataproc, Hadoop, Airflow, SBT, Maven, Docker, Kubernetes, Pyspark, Jenkins, Bigquery, Workflow management tools, Dataflow, Cloud composer, EMR, HD Insights, Big Data Technologies, Hive, SQL, Kafka, Storm, Druid, Presto","scala, spark, gcp, dataproc, hadoop, airflow, sbt, maven, docker, kubernetes, pyspark, jenkins, bigquery, workflow management tools, dataflow, cloud composer, emr, hd insights, big data technologies, hive, sql, kafka, storm, druid, presto","airflow, big data technologies, bigquery, cloud composer, dataflow, dataproc, docker, druid, emr, gcp, hadoop, hd insights, hive, jenkins, kafka, kubernetes, maven, presto, sbt, scala, spark, sql, storm, workflow management tools"
"Senior, Data Engineer",Walmart,"Bentonville, AR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-walmart-3685758080,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
We are looking for a savvy Data Engineer to join our growing team of engineering experts in Finance Data Factory. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our data architecture to support our next generation of products and data initiatives.
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and GCP 'big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and GCP regions.
Create data tools for analytics team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
About the Team: Finance Data Factory - Engineering
Everyone has data, but the sheer volume of data at Walmart can be limitless. In the Data Engineering team, we help Walmart manage this data by building pipelines and data lakes to prepare big data for analysis and unlocking actionable insights in real-time. We also use cross-departmental data and machine learning to build a holistic view of true profitability, saving millions of dollars across item categories and geographies while assisting our leadership in making better decisions faster. Finance Data Factory - Engineering
What You'll Bring
Experience in Managing Team of 3-5 Engineers
Advanced working SQL knowledge and experience working with relational databases, Big Query, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Big Query, Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Airflow, etc.
Experience with GCP cloud services: GCS, Dataproc, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Scala, etc.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in
software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related
field.
2 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering
Primary Location...
805 SE MOBERLY LN, BENTONVILLE, AR 72712, United States of America
Show more
Show less","Data Engineering, Data Pipeline Architecture, SQL, GCP, Big Data, Hadoop, Spark, Kafka, NoSQL, Python, Scala, Airflow, Postgres, Cassandra, GCS, Dataproc, Storm, SparkStreaming, Business Intelligence, Business Analytics, ETL, Cloud Computing","data engineering, data pipeline architecture, sql, gcp, big data, hadoop, spark, kafka, nosql, python, scala, airflow, postgres, cassandra, gcs, dataproc, storm, sparkstreaming, business intelligence, business analytics, etl, cloud computing","airflow, big data, business analytics, business intelligence, cassandra, cloud computing, data engineering, data pipeline architecture, dataproc, etl, gcp, gcs, hadoop, kafka, nosql, postgres, python, scala, spark, sparkstreaming, sql, storm"
Staff Data Engineer,Walmart,"Bentonville, AR",https://www.linkedin.com/jobs/view/staff-data-engineer-at-walmart-3739252963,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
What you'll do...
As part of the Intelligent Tech Data Pipeline team, you will partner collaboratively with business and technical stakeholders to maintain the organization's data, ensure polices, processes, and standards are adhered to so that consistent and trusted data drives business initiatives and powers data driven decision making in a reliable, repeatable, and scalable way. Ensure data pipeline is compliant, secure, discoverable, traceable, and of high quality.
Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance; accordingly, developing contingency plans; and demonstrating adaptability and supporting continuous learning.
Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application and ensuring compliance with them.
Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost-effectiveness.
About Team
Our team owns interfaces that allow in-store and edge solutions to send data into a centralized data store, where it can be used by the business to reduce cost, customer friction, and make better business decisions.
What You'll Do
Supporting and driving long-term and short-term strategies for achieving business outcomes, especially in Data Engineering Space.
Partnering with cross-functional teams to identify business problems and opportunities.
Acquiring data from disparate data sources using API's, SQL and NoSQL.
Transform data using SQL, NoSQL, and Python. Visualizing data using a diverse tool set including but not limited to Python and R.
Collaborating with engineering and product teams to deploy data engineering solutions in Google and Azure clouds.
Communicating effectively to all levels of the organization, including executives.
Guiding and mentoring junior associates on Technical and Non-Technical aspects in the Data Engineering Space.
Good to have experience applying and integrating with sophisticated data science and advanced analytics models such as machine learning, statistical modeling, simulation, and optimization to produce valuable insights that guide decision-making and enhance business outcomes.
You will be responsible for analyzing, designing, coding, testing and deployment of technical solutions to follow Data Engineering requirements,
Data Quality
Data Access control
Data Classification etc.
What You'll Bring
Highly proficient coding in one or more languages: Java, C++, Python, Spark, Scala, Go or R.
Highly experienced in Spark and Distributed computing frameworks.
Highly experience in programming design patterns.
Highly proficient with CI/CD tool like Git, Looper, Jenkins etc. and workflow orchestration tools like Airflow/ Automic etc.
Expertise in designing, developing and deploying big data applications in Google Cloud Platform.
Proven advanced development experience in leading data-driven projects from design to execution, driving and influencing project roadmaps.
Deep expertise in writing and debugging complex SQL queries.
Strong written, oral communication and presentation skills
Problem solving skills / Abstract thinking - ability to understand a real-life problem, make the right judgement of problem statement, and give a feasible design of solution.
Experience in interacting with business stakeholders and effectively drive conversations.
Data Modeling knowledge of: Cloud data strategy, data warehouse, data lake, and enterprise big data platforms; Data modeling techniques and tools (For example, Dimensional design and scalability), Entity Relationship diagrams, Erwin, etc. ; Query languages SQL / NoSQL; Data flows through the different systems; Tools supporting automated data loads; Artificial Intelligent - enabled metadata management tools and techniques.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field.
3 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 4 years' experience in software engineering or related field
Primary Location...
PHYLLIS ST. N OF HWY 102, WEST SIDE OF STREET, BENTONVILLE, AR 72712, United States of America
Show more
Show less","Data Engineering, Data Integration, Cloud Computing, Python, Spark, Scala, SQL, NoSQL, Git, Airflow, Kubernetes, Docker, Jenkins, Machine Learning, Artificial Intelligence, Data Analysis, Data Visualization, Communication, Problem Solving, Analytical Thinking, Data Modeling, Data Warehousing, Data Quality, Data Security, ETL Tools, Big Data, Distributed Computing, Java, C++, R, Go","data engineering, data integration, cloud computing, python, spark, scala, sql, nosql, git, airflow, kubernetes, docker, jenkins, machine learning, artificial intelligence, data analysis, data visualization, communication, problem solving, analytical thinking, data modeling, data warehousing, data quality, data security, etl tools, big data, distributed computing, java, c, r, go","airflow, analytical thinking, artificial intelligence, big data, c, cloud computing, communication, data engineering, data integration, data quality, data security, dataanalytics, datamodeling, datawarehouse, distributed computing, docker, etl tools, git, go, java, jenkins, kubernetes, machine learning, nosql, problem solving, python, r, scala, spark, sql, visualization"
GCP Data Engineer,Smart TechLink Solutions Inc.,"Bentonville, AR",https://www.linkedin.com/jobs/view/gcp-data-engineer-at-smart-techlink-solutions-inc-3744021883,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Mandatory Areas
Must Have Skills
Spark 8+ Yrs of Exp
Scala 8+ Yrs of Exp
GCP 5+ Yrs of Exp
Hive 8+Yrs of Exp
SQL - 8+ Yrs of Exp
ETL Process / Data - 8+ Years of experience
Responsibilities
As a Senior Data Engineer, you will
Design and develop big data applications using the latest open source technologies.
Desired working in offshore model and Managed outcome
Develop logical and physical data models for big data platforms.
Automate workflows using Apache Airflow.
Create data pipelines using Apache Hive, Apache Spark, Scala, Apache Kafka.
Provide ongoing maintenance and enhancements to existing systems and participate in rotational on-call support.
Learn our business domain and technology infrastructure quickly and share your knowledge freely and actively with others in the team.
Mentor junior engineers on the team
Lead daily standups and design reviews
Groom and prioritize backlog using JIRA
Act as the point of contact for your assigned business domain
Requirements
8+ years of hands-on experience with developing data warehouse solutions and data products.
4+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive,Scala, Airflow or a workflow orchestration solution are required
4 + years of experience in GCP,GCS Data proc, BIG Query
2+ years of hands-on experience in modeling(Erwin) and designing schema for data lakes or for RDBMS platforms.
Experience with programming languages: Python, Java, Scala, etc.
Experience with scripting languages: Perl, Shell, etc.
Practice working with, processing, and managing large data sets (multi TB/PB scale).
Exposure to test driven development and automated testing frameworks.
Background in Scrum/Agile development methodologies.
Capable of delivering on multiple competing priorities with little supervision.
Excellent verbal and written communication skills.
Bachelor's Degree in computer science or equivalent experience.
The most successful candidates will also have experience in the following:
Gitflow
Atlassian products BitBucket, JIRA, Confluence etc.
Continuous Integration tools such as Bamboo, Jenkins, or TFS
Show more
Show less","Apache Airflow, Apache Hive, Apache Kafka, Apache Spark, Atlassian, Bamboo, Bitbucket, Confluence, Data warehouse, ETL, GCP, Gitflow, Hadoop, Hive, JIRA, Jenkins, Java, Multi TB/PB scale, MySQL, Perl, Python, RDBMS, Scala, Shell, SQL, TFS","apache airflow, apache hive, apache kafka, apache spark, atlassian, bamboo, bitbucket, confluence, data warehouse, etl, gcp, gitflow, hadoop, hive, jira, jenkins, java, multi tbpb scale, mysql, perl, python, rdbms, scala, shell, sql, tfs","apache airflow, apache hive, apache kafka, apache spark, atlassian, bamboo, bitbucket, confluence, datawarehouse, etl, gcp, gitflow, hadoop, hive, java, jenkins, jira, multi tbpb scale, mysql, perl, python, rdbms, scala, shell, sql, tfs"
Senior BigData Engineer,"Sunrise Systems, Inc.","Bentonville, AR",https://www.linkedin.com/jobs/view/senior-bigdata-engineer-at-sunrise-systems-inc-3707368353,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Job Title: Senior BigData Engineer
Location: Bentonville, AR 72712
Duration: 4 Months contract with possible extension
Job Description
Our client is looking for a highly energetic and collaborative Senior Data Engineer with experience leading enterprise data projects around Business and IT operations. The ideal candidate should be an expert in leading projects in developing and testing data pipelines, data analytics efforts, proactive issue identification and resolution and alerting mechanism using traditional, new and emerging technologies. Excellent written and verbal communication skills and ability to liaise with technologists to executives is key to be successful in this role.
Must Have Skills
Strong 8 years of experience in Big Data/ Hadoop.
Strong 3 years of experience in Scala.
Strong 3 years of experience in Python.
Strong 4 years of experience in SQLs and Spark queries.
You Bring
8+ years in IT with min 6+ years of Data Engineering and Analyst experience.
Bachelor's degree in computer science, Computer Engineering, or a software related discipline. A master's degree in a related field is an added plus.
6 + years of experience in Data Warehouse and Hadoop/Big Data.
3+ years of experience in strategic data planning, standards, procedures, and governance.
4+ years of hands-on experience in Python or Scala.
4+ years of experience in writing and tuning SQLs, Spark queries.
3+ years of experience working as a member of an Agile team.
Experience with Kubernetes and containers is a plus.
Experience in understanding and managing Hadoop Log Files.
Experience in understanding Hadoop multiple data processing engines such as interactive SQL, real time streaming, data science and batch processing to handle data stored in a single platform in Yarn.
Experience in Data Analysis, Data Cleaning (Scrubbing), Data Validation and Verification, Data Conversion, Data Migrations and Data Mining.
Experience in all the phases of Data warehouse life cycle involving Requirement Analysis, Design, Coding, Testing, and Deployment., ETL Flow.
Experience in architecting, designing, installation, configuration, and management of Apache Hadoop Clusters.
Experience in analyzing data in HDFS through Map Reduce, Hive and Pig.
Experience building and optimizing 'big data' data pipelines, architectures, and data sets.
Strong analytic skills related to working with unstructured datasets.
Experience in Migrating Big Data Workloads.
Experience with data pipeline and workflow management tools: Airflow.
Experience with scripting languages: Python, Scala, etc.
Cloud Administration.
As a Senior Data Engineer, this is your opportunity to·
Assembling large to complex sets of data that meet non-functional and functional business requirements
Identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using GCP/Azure and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Working with stakeholders including data, design, product, and executive teams and assisting them with data-related technical issues
Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Strong background in data warehouse design
Overseeing the integration of new technologies and initiatives into data standards and structures
Strong Knowledge in Spark, PySpark, SQL, PL/SQL (Procedures, Function, Triggers, Packages and fixing the problems.)
Experience in Cloud platform (GCP/Azure) data migration – Source/Sink mapping, Build pipelines, workflow implementation, ETL and data validation processing
Strong verbal and written communication skills to effectively share findings with shareholders
Experience in Data Analytics, optimization, machine learning techniques or Python is added advantage
Good understanding of web-based application development tech stacks like Java, AngularJs, NodeJs is a plus·
Key Responsibilities
20% Requirements and design.
60% coding & testing and 10% review coding done by developers, analyze and help to solve problems.
5% deployments and release planning.
5% customer relations.
For This Role, We Value
The ability to adapt quickly to a fast-paced environment
Excellent written and oral communication skills.
A critical thinker that challenges assumptions and seeks new ideas.
Proactive sharing of accomplishments, knowledge, lessons, and updates across the organization.
Experience designing, building, testing, and releasing software solutions in a complex, large organization.
Demonstrated functional and technical leadership.
Demonstrated analytical and problem-solving skills (ability to identify, formulate, and solve engineering problems).
Show more
Show less","Big Data, Hadoop, Scala, Python, SQL, Spark, Data Warehouse, Kubernetes, Containers, Apache Hadoop, MapReduce, Hive, Pig, Airflow, GCP, Azure, Java, AngularJS, NodeJS, PL/SQL","big data, hadoop, scala, python, sql, spark, data warehouse, kubernetes, containers, apache hadoop, mapreduce, hive, pig, airflow, gcp, azure, java, angularjs, nodejs, plsql","airflow, angularjs, apache hadoop, azure, big data, containers, datawarehouse, gcp, hadoop, hive, java, kubernetes, mapreduce, nodejs, pig, plsql, python, scala, spark, sql"
Senior Data Engineer,Dice,"Bentonville, AR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-dice-3788099579,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Revature, is seeking the following. Apply via Dice today!
Job Description (JD):
Required: BIG Data and Scala
Candidate should have 5– 6 years of experience.
Problem Formulation: Identifies possible options to address the business problems within one's discipline through analytics, big data analytics and automation.
Applied Business Acumen: Supports the development of business cases and recommendations. Owns delivery of project activity and tasks assigned by others. Supports process updates and changes to solves business issues.
Data Governance: Supports the documentation of data governance processes. Supports the daimple mentation of data governance practices.
Data Strategy: Understands, articulates and applies principles of the defined strategy to routine business problems that involve a single function.
Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current analytics trends.
Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.
Data Modeling: Analyzes complex data elements, systems, data flows, dependencies and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary, foreign keys and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions.
Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Contributes code documentation, maintains playbooks, and provides timely progress updates.
Demonstrates up-to-date expertise, applies this to the development and execution.
Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders. Adapt to competing demands, organizational changes and big new responsibilities.
Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans.
You have consistently high standards,your passion for quality is inherent in everything.
Well versed with Hadoop, Hive, Spark using Scala.
You evangelize an extremely high standard of code quality, system reliability, and performance
You have a proven track record coding with at least one programming language (e.g.,Java, Python)
You’re experienced in computing platforms(e.g., Google Cloud Platform, Azure)
You’re skilled in data modeling &data migrationprotocols
Experience with Thought Spot, Druid, Big Query and Click House is added advantage.
Experience with the integration tools like Automic, Airflow
Senior Data Engineer
Show more
Show less","Big Data, Scala, Problem Formulation, Applied Business Acumen, Data Governance, Data Strategy, Data Transformation, Data Integration, Data Source Identification, Data Modeling, Code Development, Testing, Hadoop, Hive, Spark, Java, Python, Google Cloud Platform, Azure, Thought Spot, Druid, Big Query, Click House, Automic, Airflow","big data, scala, problem formulation, applied business acumen, data governance, data strategy, data transformation, data integration, data source identification, data modeling, code development, testing, hadoop, hive, spark, java, python, google cloud platform, azure, thought spot, druid, big query, click house, automic, airflow","airflow, applied business acumen, automic, azure, big data, big query, click house, code development, data governance, data integration, data source identification, data strategy, data transformation, datamodeling, druid, google cloud platform, hadoop, hive, java, problem formulation, python, scala, spark, testing, thought spot"
Lead Data Architect,Confiz,"Bentonville, AR",https://www.linkedin.com/jobs/view/lead-data-architect-at-confiz-3784208671,2023-12-17,Arkansas,United States,Mid senior,Onsite,"We are looking for a savvy Lead Data Engineer to join our growing team of analytics experts. you will be responsible for designing, developing, and maintaining our data architecture and pipelines in Snowflake, leveraging Azure services. You will play a crucial role in ensuring data availability, integrity, and accessibility for our data-driven initiatives. In addition, you will lead a team of data engineers and collaborate closely with cross-functional teams to drive data-related projects. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities
Create and maintain optimal data pipeline architecture in Snowflake
Design and implement data architecture in Snowflake, including schema design, data modeling, and optimization for performance and scalability.
Develop ELT pipelines using Azure Data Factory, and other Azure services to ingest, data into Snowflake.
Implement data quality checks and governance processes to ensure data accuracy, consistency, and compliance with industry standards and regulations.
Lead and mentor a team of data engineers, providing guidance, coaching, and support to ensure project success and skill development.
Ensure data security and access control measures are in place and adhere to company policies and standards.
Work with stakeholders including the Data Architect, Product Owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Stay up-to-date with industry trends and best practices in data engineering and propose improvements to enhance data infrastructure.
Requirements
Minimum 8 years of experience as a Data Engineer, with at least 4-5 years of experience in Snowflake.
Proficiency in working with Snowflake and Azure Data Analytics, including Azure Data Factory
Experience with data modeling, ETL/ELT processes, and data integration.
Hands-on experience in design and implement data architecture in Snowflake, including schema design, data modeling, and optimization for performance and scalability
Experience in Leading and mentoring a team of data engineers, providing guidance, coaching, and support to ensure project success and skill development.
Experience with relational SQL like SQL Server, Postgres and NoSQL databases e.g., Cosmos etc.
Experience with data pipeline and workflow management tools: Azure Data Factory, etc.
Experience with one of cloud data analytic services GCP/Azure/AWS.
Experience with stream-processing systems: Spark-Streaming, Flink etc.
Experience in ensuring data quality in big data pipelines. Unit testing of big data pipelines.
Advanced working SQL Analytical Queries knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Best Security practice in Data computing, including encryption of sensitive data, encryption at rest and encryption in flight.
Strong analytic skills related to working with unstructured datasets.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
We have an amazing team of 550+ individuals working on highly innovative enterprise projects & products. Our customer base includes fortune 5 retail and CPG companies, leading store chains, fast growth fintech and multiple Silicon Valley startups.
What makes Confiz stand out is our focus on processes and culture. Confiz is ISO 9001:2015 certified. We have a vibrant culture of learning via collaboration and making workplace fun.
People who work with us work with the cutting-edge technologies while contributing success to the company as well as to themselves.
To know more about Confiz Limited, visit: https://web.facebook.com/lifeatconfiz
Show more
Show less","Snowflake, Azure Data Services, Data Architecture, Data Pipelines, Data Modeling, Data Engineering, Data Quality, Data Governance, Data Security, Data Analytics, ETL/ELT, Big Data, SQL, NoSQL, Azure Data Factory, SparkStreaming, Flink, Cloud Data Analytic Services, Message Queuing, Stream Processing, Data Manipulation, Data Extraction, Data Visualization, Machine Learning, Artificial Intelligence, Product Development, Customer Success, Stakeholder Management, Team Leadership, Coding, Root Cause Analysis, Business Intelligence, Data Warehousing","snowflake, azure data services, data architecture, data pipelines, data modeling, data engineering, data quality, data governance, data security, data analytics, etlelt, big data, sql, nosql, azure data factory, sparkstreaming, flink, cloud data analytic services, message queuing, stream processing, data manipulation, data extraction, data visualization, machine learning, artificial intelligence, product development, customer success, stakeholder management, team leadership, coding, root cause analysis, business intelligence, data warehousing","artificial intelligence, azure data factory, azure data services, big data, business intelligence, cloud data analytic services, coding, customer success, data architecture, data engineering, data extraction, data governance, data manipulation, data quality, data security, dataanalytics, datamodeling, datapipeline, datawarehouse, etlelt, flink, machine learning, message queuing, nosql, product development, root cause analysis, snowflake, sparkstreaming, sql, stakeholder management, stream processing, team leadership, visualization"
Staff Software Engineer - Data Pipeline,Walmart,"Bentonville, AR",https://www.linkedin.com/jobs/view/staff-software-engineer-data-pipeline-at-walmart-3704281532,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
You will be working alongside a team of ML Engineers building Conversational Products for Walmart Associates and Customers. Your next team works extensively on NLP, NLU, intent and context recognition.
About Team
Building the right technology foundation for Infrastructure & platforms is vital to success at the scale of Walmart. Our team builds and maintains the foundational technologies that support the tech organization. Included in this are data platforms, enterprise architecture, DevOps, cloud computing, and infrastructure. All of these products and services are supported by scalable and powerful infrastructure, ensuring a secure and seamless employee and customer experience across stores, digital channels, and distribution centers.
What You'll Do
Work in a highly collaborative environment with a multidisciplinary team.
Work with Solution Architects, Technical Architects, Sr. Engineers to build AI/ML model and model systems.
Work with machine learning engineers to deploy, operate, and optimize scalable solutions.
Work with product managers to design user journeys and feedback loop.
Create opportunities to develop yourself with an end-to-end AI/ML product experience.
Identify right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forums
Deploy and monitor products on Cloud platforms.
Develop and implement best-in-class monitoring processes to enable data applications meet SLAs.
Guide the team technically for end to end solution lifecycle.
What You'll Bring
Demonstrates up-to-date expertise in Data Engineering, complex data pipeline development
Architectural Design, develop, implement and tune distributed data processing pipelines that process large volume of data; focusing on scalability, low -latency, and fault-tolerance in every system built.
Exposure to Data Governance (Data Quality, Metadata Management, Security, etc.)
Knowledge of Databricks, Snowflake is an added advantage.
Experience with ThoughtSpot, Druid, Big Query and ClickHouse is added advantage.
Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus.
Hands on working experience in any messaging platform like Kafka is preferred.
Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation
Demonstrating creative, critical thinking & troubleshooting skills.
Demonstrates expertise in writing complex, highly-optimized queries across large data sets
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in computer science, computer engineering, computer information systems, software engineering, or related area and 4 years' experience in software engineering or related area.Option 2: 6 years' experience in software engineering or related area.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Master's degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 2 years' experience in software engineering or related area
Primary Location...
805 SE MOBERLY LN, BENTONVILLE, AR 72712, United States of America
Show more
Show less","NLP, NLU, Intent recognition, Context recognition, Data Engineering, Data processing pipelines, Data Governance, Data Quality, Metadata Management, Databricks, Snowflake, ThoughtSpot, Druid, Big Query, ClickHouse, NoSQL, Cosmos DB, RDBMS, MySQL, Postgres, Kafka, Software Development, Requirement Intake, Effort Estimation, Creative thinking, Critical thinking, Troubleshooting, Complex queries","nlp, nlu, intent recognition, context recognition, data engineering, data processing pipelines, data governance, data quality, metadata management, databricks, snowflake, thoughtspot, druid, big query, clickhouse, nosql, cosmos db, rdbms, mysql, postgres, kafka, software development, requirement intake, effort estimation, creative thinking, critical thinking, troubleshooting, complex queries","big query, clickhouse, complex queries, context recognition, cosmos db, creative thinking, critical thinking, data engineering, data governance, data processing pipelines, data quality, databricks, druid, effort estimation, intent recognition, kafka, metadata management, mysql, nlp, nlu, nosql, postgres, rdbms, requirement intake, snowflake, software development, thoughtspot, troubleshooting"
"Senior, Data Analyst",Walmart,"Bentonville, AR",https://www.linkedin.com/jobs/view/senior-data-analyst-at-walmart-3783847393,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
Senior Data Analyst - Walmart Claims Services
Walmart Corporate - Claims Services Data - Job R-1691115
Bentonville, AR (Onsite)
Important Liability Claims Senior Data Analyst role supporting Walmart Global Business Services
About Walmart Claims Services
The Walmart Claims Services team investigates, evaluates, negotiates, and resolves customer property damage or bodily injury claims on behalf of Walmart and our family of brands. Throughout the claims process, our team engages with Walmart facilities, customers or their attorneys, and different participants both inside and outside of the organization to deliver the best possible claim outcomes while protecting the Walmart brand and maintaining shopper loyalty.
Walmart Claims Services is hiring a Senior Data Analyst. Reporting to the Senior Manager of Risk Management Analytics, you'll present data for Claims Services tasks and reporting requirements. You'll ensure the accuracy and integrity of data, implement best practices for data visualization, and work with partners across Walmart Claims Services to aid our journey to become a World Class claims organization. This Senior Data Analyst will create visually appealing comprehensive reports, indicators, and dashboards that facilitate informed decision-making. You'll present your recommendations to important team members including senior management to help strengthen our strategy for administering a large volume of customer property damage and bodily injury claims on behalf of Walmart and our many brands. We offer a supportive environment to expand your skills and develop your career within the dynamic Fortune#1 culture.
What You'll Do
Design Data Visualizations that portray insights to our customers using tools such as Power BI and Tableau
Use SQL, ETL tools, Google BigQuery, Dataiku, Tableau Prep, R, and Python to prepare data analytics, comprehensive reports, and dashboards
Integrate Machine Learning Models in dashboards to track performance
Develop Analytical Solutions to address gaps in our business process
Present Your Reports and Results to both technical and non-technical audiences
Use Data Analytics to Develop Business Strategies by proposing technical solutions that improve claims outcomes and organizational results
Align with Main Partners to help mitigate risk and ensure claims procedures always remain customer centered
What You'll Bring
4+ years' data science, data analysis, data visualization, or statistics experience with a large corporation, major retailer, major insurance company, law firm, or related organization.
We prefer you have a Master's degree in Business, Computer Science, Engineering, Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field.
You have completed assessments in data analysis and Business Intelligence tools and scripting languages such as SQL, Python, Spark, Scala, R, Power BI, or Tableau.
You can write clean, efficient, and scalable code following best practices and coding standards.
You have experience in data visualization and advanced techniques for complex data types and the use of tools such as Python, R libraries, Tableau Prep, GGplot, Matplotlib, Ploty, Tableau, PowerBI.
You have considerable soft skills and are comfortable facing clients and partners at all levels.
You are at your best while working in a team environment.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Benefits & Perks
Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
Primary Location
608 SW 8th STREET, BENTONVILLE, AR 72712, United States of America
*Comprehensive Relocation Package Available*
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Business, Engineering, Statistics, Economics, Analytics, Mathematics, Arts, Finance or related field and 2 years' experience in data analysis, data science, statistics, or related field. Option 2: Master's degree in Business, Engineering, Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field. Option 3: 4 years' experience in data analysis, data science, statistics, or related field.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data science, data analysis, statistics, or related field, Master's degree in Business, Computer Science, Engineering, Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field, Related industry experience (for example, retail, merchandising, healthcare, eCommerce), Successful completion of assessments in data analysis and Business Intelligence tools and scripting languages (for example, SQL, Python, Spark, Scala, R, Power BI, or Tableau)
Primary Location...
608 SW 8TH ST, BENTONVILLE, AR 72712-6207, United States of America
Show more
Show less","Data Visualization, Power BI, Tableau, SQL, ETL Tools, Google BigQuery, Dataiku, Tableau Prep, R, Python, Machine Learning, Data Analytics, Business Strategies, Business Intelligence, Data Science, Statistics, Analytics, Mathematics, Spark, Scala, GGplot, Matplotlib, Ploty","data visualization, power bi, tableau, sql, etl tools, google bigquery, dataiku, tableau prep, r, python, machine learning, data analytics, business strategies, business intelligence, data science, statistics, analytics, mathematics, spark, scala, ggplot, matplotlib, ploty","analytics, business intelligence, business strategies, data science, dataanalytics, dataiku, etl tools, ggplot, google bigquery, machine learning, mathematics, matplotlib, ploty, powerbi, python, r, scala, spark, sql, statistics, tableau, tableau prep, visualization"
"Senior, Data Analyst",Walmart,"Bentonville, AR",https://www.linkedin.com/jobs/view/senior-data-analyst-at-walmart-3779743328,2023-12-17,Arkansas,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
Are you a creative and driven data analyst ready to take on an exciting opportunity?
As a Senior Data analyst, you'll be part of a collaborative team that's dedicated to empowering transportation and end to end supply chain operators and leadership with actionable insights, reporting, and automation. You'll be responsible for creating and maintaining backend data pipelines, frontend interactive reports, and business logic automation for business-critical decision support. We're looking for someone who has experience with SQL, Python, Dataiku, Tableau, Tableau (Prep), Power BI, Google Big Query on cloud service and local server environments. The products managed and built by this role have a high degree of visibility with stakeholder consumers in the thousands or possess positions of significance. This is an exciting opportunity to work on projects that have a real impact on the success of the organization and to collaborate with others who share your passion for data analysis. Join our team and help us unleash the power of data to drive business success. The role is located in Bentonville, AR.
What You'll Do
Data Source Identification:
Identify and evaluate potential data sources from internal and external sources.
Implement processes for acquiring and integrating data from various sources.
Ensure that the data is accurate, complete, and accessible.
Understand Business Context
Work closely with stakeholders to understand business goals and requirements.
Translate business requirements into data requirements.
Develop an understanding of the industry and competitive landscape.
Technical Problem Formulation
Formulate technical problems and solutions for data-related challenges.
Develop and implement algorithms for data analysis and modeling.
Evaluate and select appropriate software tools, frameworks, and technologies.
Data Visualization
Design and execute effective data visualization strategies for presenting complex data.
Develop and implement effective dashboards and reports.
Collaborate with other teams to create impactful visualizations that support business decisions.
Data Quality Management
Plan and lead efforts to ensure data quality and integrity.
Develop and implement data governance policies and procedures.
Monitor data quality metrics and proactively identify and address issues.
Exploratory Data Analysis
Conduct exploratory analyses to generate insights and identify patterns in data.
Develop and implement statistical analyses and models.
Communicate findings and insights to stakeholders in a clear and impactful manner.
You'll sweep us off our feet if...
Proficiency in SQL, GCP, Tableau, Dataiku.
You have strong exploratory data analysis skills and ability to make analytical, data driven recommendations and solutions.
Project Management experience with end-to-end ownership of projects.
You have compelling, persuasive oral and written communication skills and can build strong cross-functional relationships.
Ability to thrive in ambiguity and often lack of clarity, demonstrated ability to thrive in a fast-paced environment while managing multiple projects and tight deadlines.
You'll make an impact by...
Advanced Analytics : In-depth investigation of key drivers of transportation dynamic optimization and data quality. Work with large, disparate datasets to measure business KPIs and to conduct strategic analyses to inform critical business decisions. Develop data visualization to monitor and quantify the impact of changes and unique business challenges and use data to influence strategic decision making.
Partnership and Collaboration : Engaging with team members and cross-functional partners on a consistent basis and establish credibility and influence change. Drive process optimization and collaboration across the organization. Analyze stakeholder needs and partner with them to share relevant information.
Planning : Thinking strategically to identify the key metrics and initiatives needed to drive change within the organization. Quickly translate strategic, conceptual ideas into clear action plans, roadmaps, and timelines. Ensure final deliverables are actionable and communicated in a way that drives quantifiable business benefit in terms of improved efficiencies or improved decision making.
Minimum Qualifications
Bachelor's degree in Supply Chain, Data Science, Computer Science, Analytics, Statistics or relevant field with 4 years of experience in analytics related field or Master's degree in Supply Chain, Data Science, Computer Science, Analytics, Statistics or relevant field with 2 years of experience in analytics related.
Six Sigma Certification (Green, Black) a plus
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Benefits & Perks
Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
Who We Are
Join Walmart and your work could help over 275 million global customers live better every week. Yes, we are the Fortune #1 company. But you'll quickly find we're a company who wants you to feel comfortable bringing your whole self to work. A career at Walmart is where the world's most complex challenges meet a kinder way of life. Our mission spreads far beyond the walls of our stores. Join us and you'll discover why we are a world leader in diversity and inclusion, sustainability, and community involvement. From day one, you'll be empowered and equipped to do the best work of your life. careers.walmart.com
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Business, Engineering, Statistics, Economics, Analytics, Mathematics, Arts, Finance or related field and 2 years' experience in data analysis, data science, statistics, or related field. Option 2: Master's degree in Business, Engineering, Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field. Option 3: 4 years' experience in data analysis, data science, statistics, or related field.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data science, data analysis, statistics, or related field, Master's degree in Business, Computer Science, Engineering, Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field, Related industry experience (for example, retail, merchandising, healthcare, eCommerce), Successful completion of assessments in data analysis and Business Intelligence tools and scripting languages (for example, SQL, Python, Spark, Scala, R, Power BI, or Tableau)
Primary Location...
311 NORTH WALTON BOULEVARD, BENTONVILLE, AR 72716, United States of America
Show more
Show less","SQL, Python, Google Big Query, Tableau, Dataiku, Data Analysis, Data Modeling, Data Visualization, Data Governance, Data Quality Management, Business Intelligence, Exploratory Data Analysis, Statistical Analysis, Communication Skills, Project Management, Partnership and Collaboration, Strategic Planning","sql, python, google big query, tableau, dataiku, data analysis, data modeling, data visualization, data governance, data quality management, business intelligence, exploratory data analysis, statistical analysis, communication skills, project management, partnership and collaboration, strategic planning","business intelligence, communication skills, data governance, data quality management, dataanalytics, dataiku, datamodeling, exploratory data analysis, google big query, partnership and collaboration, project management, python, sql, statistical analysis, strategic planning, tableau, visualization"
Sr Data Engineer,Crown Castle,"Cecil, AR",https://www.linkedin.com/jobs/view/sr-data-engineer-at-crown-castle-3716013089,2023-12-17,Arkansas,United States,Mid senior,Remote,"Position Title:
Senior Data Engineer (P4)
Company Summary
Crown Castle is the nation’s largest provider of shared communications infrastructure: towers, small cells and fiber. It all works together to meet unprecedented demand—connecting people and communities and transforming the way we do business. Whenever you make a call, track a workout or stream music and videos, we’re the ones providing the communications infrastructure that makes it all possible. From 5G and the internet of things to drones, autonomous vehicles and AR/VR, we enable the technologies that help people stay safe, connected and ready for the future. Crown Castle is publicly traded on the S&P 500, and one of the largest Real Estate Investment Trusts in the US.
We offer a total benefits package and professional growth development for teammates in any stage of their career. Along with caring for our teammates, we’re an active member in the communities where we live, work and do business. We have a responsibility to give back, which we do through our Connected by Good program. Giving back allows us to improve public spaces where people connect, promote public safety and advance access to education and technology.
Role
Crown Castle’s Data & Digital team has been evolving its capabilities across all technology functions, investing in the team, technology, data, and processes. The Data and Digital team is leading innovation by developing microservice and event- based solutions across products, applications, platforms, and data utilizing open source technologies in the cloud to drive Crown Castle’s technology evolution.
As a Senior Data Engineer, specializing in Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS, you will provide technical leadership in the design and advancement of our data infrastructure. You will in design, build and optimize scalable, performant data pipelines, ensuring data quality and reliability, in support of a variety of data initiatives across the organization.
Responsibilities
Collaborate closely with cross-functional teams including Data Scientists, Analysts, and Software Engineers to understand data requirements and translate them into efficient solutions using Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services.
Design and implement end-to-end data pipelines, leveraging Kafka for real-time data streaming, Snowflake for scalable data warehousing, and Databricks for advanced analytics.
Develop and maintain ETL processes using AWS Glue to facilitate seamless and reliable data extraction, transformation, and loading into Snowflake and Databricks.
Optimize data models and schema designs for improved query performance within Snowflake and Databricks, while ensuring data consistency and integrity.
Implement robust data security measures and access controls in alignment with company policies and industry best practices.
Utilize Airflow to orchestrate and schedule data workflows, ensuring timely and accurate data processing.
Monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.
Contribute to the evolution of data warehousing strategies, encompassing data partitioning, clustering, and distribution within Snowflake and Databricks.
Monitor industry trends and technological advancements in Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services, and advocate for their effective utilization within the organization.
Provide mentorship and guidance to the data engineering team to help foster growth and facilitate knowledge sharing.
Expectations
Self-motivated individual who can handle ambiguous/undefined problems and think abstractly to deliver results
Demonstrate a strong sense of ownership, urgency, and drive as well as the ability to work well across diverse teams.
Ability to effectively articulate technical challenges and solutions to business users and other technical teams
Ability to develop compelling insights and logical arguments to persuade others
Critical thinking skills (understanding of the relationship between environment, objectives, goals, and priorities that drive optimal decisions)
Demonstrates executive maturity and ability to communicate at all levels of the company
Navigates challenging interactions with candor and through constructive debate to build support and commitment for initiatives
Education/Certifications
Bachelor’s degree in Computer Science, information Systems, or related discipline
Master’s degree preferred, PhD a plus
Snowflake, Databricks, AWS, or related certifications
Experience/Minimum Requirements
8+ years data engineering experience focused on Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services.
Snowflake architecture, performance optimization techniques, and best practices.
SQL skills and experience with database design principles.
Hands-on experience in designing, building, and maintaining complex ETL pipelines using Snowflake, Databricks, Kafka, and AWS Glue.
Familiarity with data warehousing concepts and methodologies.
Real-time data streaming principles and experience with Kafka.
Cloud computing and AWS services, with a focus on data-related services such as S3, Redshift, and Lambda.
Proficiency in scripting and programming languages such as Python, Java, or similar.
Excellent problem-solving skills and the ability to troubleshoot complex data-related issues.
Working Conditions:
This is a remote role with the expectation of occasional on-site/in-person collaboration with teammates and stakeholders that may require up to 20% travel.
For New York, Colorado, California and Washington residents
- The hiring range offered for this position is $148,000-$170,000 annually. In addition to salary, employees are eligible for an annual bonus of up to 20% of annual salary and restricted stock. Employees (and their families) are eligible for medical, dental, vision, and basic life insurance. Employees are able to enroll in our company’s 401k plan. Employees will also receive 18 days of paid time off each year and 12 paid holidays throughout the calendar year.
Show more
Show less","Snowflake, Databricks, Kafka, AWS Glue, Airflow, AWS, Python, Java, SQL, ETL, Data warehousing, Data streaming, Cloud computing, Data security, Data governance, Data quality, Data modeling, Data engineering, Data analysis, Data science, Software engineering","snowflake, databricks, kafka, aws glue, airflow, aws, python, java, sql, etl, data warehousing, data streaming, cloud computing, data security, data governance, data quality, data modeling, data engineering, data analysis, data science, software engineering","airflow, aws, aws glue, cloud computing, data engineering, data governance, data quality, data science, data security, data streaming, dataanalytics, databricks, datamodeling, datawarehouse, etl, java, kafka, python, snowflake, software engineering, sql"
Data Engineer - Scala(U.S. remote),Railroad19,"Little Rock, AR",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782853663,2023-12-17,Arkansas,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, Software platform support, Troubleshooting, Oral communication, Written communication, Analytical skills, Problemsolving skills, Selfdirection, Computer science, Computer engineering","scala 212, spark 24, aws, emr, s3, relational databases, nonrelational databases, restful apis, software platform support, troubleshooting, oral communication, written communication, analytical skills, problemsolving skills, selfdirection, computer science, computer engineering","analytical skills, aws, computer engineering, computer science, emr, nonrelational databases, oral communication, problemsolving skills, relational databases, restful apis, s3, scala 212, selfdirection, software platform support, spark 24, troubleshooting, written communication"
Principal Data Engineer,Crown Castle,"Cecil, AR",https://www.linkedin.com/jobs/view/principal-data-engineer-at-crown-castle-3713006863,2023-12-17,Arkansas,United States,Mid senior,Remote,"Position Title:
Principal Data Engineer (P5)
Company Summary
Crown Castle is the nation’s largest provider of shared communications infrastructure: towers, small cells and fiber. It all works together to meet unprecedented demand—connecting people and communities and transforming the way we do business. Whenever you make a call, track a workout or stream music and videos, we’re the ones providing the communications infrastructure that makes it all possible. From 5G and the internet of things to drones, autonomous vehicles and AR/VR, we enable the technologies that help people stay safe, connected and ready for the future. Crown Castle is publicly traded on the S&P 500, and one of the largest Real Estate Investment Trusts in the US.
We offer a total benefits package and professional growth development for teammates in any stage of their career. Along with caring for our teammates, we’re an active member in the communities where we live, work and do business. We have a responsibility to give back, which we do through our Connected by Good program. Giving back allows us to improve public spaces where people connect, promote public safety and advance access to education and technology.
Role
Crown Castle’s Data & Digital team has been evolving its capabilities across all technology functions, investing in the team, technology, data, and processes. The Data and Digital team is leading innovation by developing microservice and event- based solutions across products, applications, platforms, and data utilizing open source technologies in the cloud to drive Crown Castle’s technology evolution.
As a Principal Data Engineer, specializing in Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS, you will provide technical leadership in the design and advancement of our data infrastructure. You will architect and optimize high-performing data pipelines, ensure data quality, and lead diverse data-driven initiatives across the organization.
Responsibilities
Define and execute data architecture strategies using Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services in alignment with enterprise architecture, business requirements and technology roadmaps
Lead the design and development of complex, end-to-end data pipelines, leveraging the strengths of Databricks for advanced analytics, Kafka for real-time data streaming, and Snowflake for scalable data warehousing.
Collaborate cross-functionally to ensure alignment between data engineering efforts and overarching business objectives.
Set best practices for data modeling, schema design, query optimization, and performance tuning within Snowflake and Databricks.
Champion the implementation of comprehensive data security measures and access controls, ensuring compliance with industry standards and company policies.
Establish and oversee data orchestration and scheduling processes using Airflow, guaranteeing reliable and efficient execution of data workflows.
Provide mentorship, guidance, and technical leadership to a team of data engineers, nurturing their growth and fostering technical excellence.
Proactively monitor, troubleshoot, and enhance data pipelines, identifying and resolving performance bottlenecks, data quality issues, and other challenges.
Drive the evolution of data warehousing strategies within Snowflake, integrating advanced techniques such as clustering, partitioning, and distribution.
Remain at the forefront of technological advancements in Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services, championing innovation and continuous improvement
Leverage economic thinking, planning, and leadership skills to influence critical decisions
Collaborate with cross-functional teams to ensure alignment between data engineering efforts and overarching business objectives.
Expectations
Self-motivated individual who can handle ambiguous/undefined problems and think abstractly to deliver results
Demonstrate a strong sense of ownership, urgency, and drive as well as the ability to work well across diverse teams.
Ability to effectively articulate technical challenges and solutions to business users and other technical teams
Ability to develop compelling insights and logical arguments to persuade others
Critical thinking skills (understanding of the relationship between environment, objectives, goals, and priorities that drive optimal decisions)
Demonstrates executive maturity and ability to communicate at all levels of the company
Navigates challenging interactions with candor and through constructive debate to build support and commitment for initiatives
Education/Certifications
Bachelor’s degree in Computer Science, information Systems, or related discipline
Master’s degree preferred, PhD a plus
Snowflake, Databricks, AWS, or related certifications
Experience/Minimum Requirements
10+ years data engineering experience focused on Snowflake, Databricks, Kafka, AWS Glue, Airflow, and AWS services.
Snowflake architecture, performance optimization, and advanced feature design
Demonstrated success in architecting and implementing complex data pipelines using Databricks, Kafka, AWS Glue, Snowflake, and related technologies.
SQL skills and strong understanding of cloud database design principles
Comprehensive grasp of data warehousing methodologies, practices, and strategies.
Proven experience in implementing data security measures in compliance with industry standards.
Cloud computing and AWS services, with expertise in data-related services such as Redshift, S3, Lambda, and more.
Proficiency in scripting and programming languages such as Python, Java, or similar.
Ability to rapidly come up to speed on new teams and services to make decisive organizational and technical impact
Strong interpersonal, communication, and collaboration skills including the ability to seek consensus, provide technical leadership, lead by example, exhibit patience and determination
Experience working in an Agile environment
Working Conditions:
This is a remote role with the expectation of occasional on-site/in-person collaboration with teammates and stakeholders that may require up to 20% travel.
For New York, Colorado, California and Washington residents
- The hiring range offered for this position is $175,700 - $200,000 annually. In addition to salary, employees are eligible for an annual bonus of up to 30% of annual salary and restricted stock. Employees (and their families) are eligible for medical, dental, vision, and basic life insurance. Employees are able to enroll in our company’s 401k plan. Employees will also receive 18 days of paid time off each year and 12 paid holidays throughout the calendar year.
Show more
Show less","Snowflake, Databricks, Kafka, AWS Glue, Airflow, AWS, Python, Java, SQL, Data engineering, Data architecture, Data pipelines, Data modeling, Data security, Data warehousing, Agile, Cloud computing","snowflake, databricks, kafka, aws glue, airflow, aws, python, java, sql, data engineering, data architecture, data pipelines, data modeling, data security, data warehousing, agile, cloud computing","agile, airflow, aws, aws glue, cloud computing, data architecture, data engineering, data security, databricks, datamodeling, datapipeline, datawarehouse, java, kafka, python, snowflake, sql"
HRIS Data Analyst,Tential Solutions,"Little Rock, AR",https://www.linkedin.com/jobs/view/hris-data-analyst-at-tential-solutions-3726864511,2023-12-17,Arkansas,United States,Mid senior,Hybrid,"Exciting full-time
HRIS Data Analyst
opportunity!
Our client is a prominent financial institution, recognized by Forbes as a top bank and consistently ranked as a top performer 12 times in the past eight years. With a presence in eight states, they operate through 240+ offices and boast $30.76 billion in assets.
As an integral part of our team you will oversee all aspects of the HRIS integration to include business process configuration, internal and external reporting, and providing day-to-day analytical support of the HRIS systems. You will contribute your unique talents, skills and experiences to participating in any future development activities, including system upgrades and the implementation of additional applications and functionality.
Responsibilities
Supporting daily HRIS operations.
Managing system integrations and maintenance.
Directing vendor relationships.
Participating in the design and implementation of new HRIS features.
Ensuring data integrity through routine audits and analysis.
Using HRIS proactively to address business needs.
Collaborating on HRIS documentation and procedures.
Providing oversight for HR technology strategies.
Acting as an internal HRIS consultant.
Assisting in report generation and HR staff training.
Ensuring compliance with regulatory policies and guidelines.
Required Skills
Bachelor degree in information systems, mathematics, computer science, quantitative systems, a related degree with some specialization in computer related experience and/or equivalent work experience required
2+ years’ experience as an analyst/administrator; information systems analyst/technician; or other positions with appropriate experience and/or education,
HRIS knowledge preferred
Show more
Show less","Information Systems, Mathematics, Computer Science, Quantitative Systems, HRIS, Data Analysis, System Integration, Vendor Management, System Upgrades, Data Integrity, HR Technology Strategies, HR Consulting, Report Generation, HR Training, Regulatory Compliance","information systems, mathematics, computer science, quantitative systems, hris, data analysis, system integration, vendor management, system upgrades, data integrity, hr technology strategies, hr consulting, report generation, hr training, regulatory compliance","computer science, data integrity, dataanalytics, hr consulting, hr technology strategies, hr training, hris, information systems, mathematics, quantitative systems, regulatory compliance, report generation, system integration, system upgrades, vendor management"
"Senior Data Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Gainesville, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783183887,2023-12-17,Gainesville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Senior Data Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Gainesvi-SeniorDataAnal.002
Show more
Show less","Python, JavaScript, JSON, R, OOP languages, Generative AI, Data science, Product engineering, Written and verbal English communication skills, Research, Project management, Stakeholder management, Technology, AI, Algorithms, Educational tools, Data analytics, Coaching, Remote work","python, javascript, json, r, oop languages, generative ai, data science, product engineering, written and verbal english communication skills, research, project management, stakeholder management, technology, ai, algorithms, educational tools, data analytics, coaching, remote work","ai, algorithms, coaching, data science, dataanalytics, educational tools, generative ai, javascript, json, oop languages, product engineering, project management, python, r, remote work, research, stakeholder management, technology, written and verbal english communication skills"
"Data Reporter/Analyst - Full Time, Temporary",Hearst Newspapers,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-reporter-analyst-full-time-temporary-at-hearst-newspapers-3777301310,2023-12-17,El Cerrito,United States,Associate,Hybrid,"Data Reporter/Analyst - Full Time, Temporary
The Hearst Newspapers (HNP) DevHub is hiring a Data Reporter/Analyst on a three-month contract to work on data-driven journalism projects. In this role, the successful candidate will work with our visual storytelling, engineering and data teams on stories and tools that help our readers better understand the communities we cover.
The HNP DevHub is a team of editorial engineers, data journalists, developers, designers, project managers and content strategists who collaborate with local newsrooms to create best-in-class digital experiences. We partner with newsrooms like the San Francisco Chronicle, Houston Chronicle, San Antonio Express-News, (Albany) Times Union, and the Hearst Connecticut Media Group and build projects like election results pages, searchable databases, severe weather trackers and elegantly designed alternative story forms.
The Skillset
Ideal candidates will be able to showcase some of the following. We do not expect candidates to be proficient in every area.
Experience in a modern newsroom (including student publications) or a related work environment.
Expertise in data analysis and data visualization, using tools such as R, Python, SQL, Datawrapper, D3, and Adobe Illustrator/ai2html.
Ability to use national datasets to identify stories about local communities and summarize the key findings.
Proficiency with Excel and Google Sheets.
Working knowledge of Git and Github.
Ability to self-manage and handle multiple projects and fast deadlines.
Appreciation of Slack, Zoom and other workplace tools.
Any experience with the following tools/systems is a plus:
Backend languages such as Node or PHP.
APIs and the use of web-scraping tools alongside AWS systems such as S3, EC2, Lambda.
HTML, CSS, JavaScript and relevant code libraries.
Front-end frameworks like React, Svelte or Vue.
The Responsibilities
Collaborate with team members on data-driven projects.
Analyze data and build visualizations for HNP’s core newsrooms.
Communicate with our editorial partners in HNP newsrooms.
If you’re passionate about ambitious local journalism, don’t hesitate to apply and tell us about yourself. We know there are many great candidates who may not check all of these boxes.
The Basics
This position will have a Monday-to-Friday daytime schedule but weekend and evening, work may be required as news warrants.
Remote work friendly. The team is based in San Francisco and New York City.
In accordance with applicable law, Hearst is required to include a reasonable estimate of the compensation for this role if hired in California. The reasonable estimate is between an hourly rate equivalent to $80K to $85K per year over a 12-month period. Please note this information is specific to those hired in San Francisco. A final decision on the successful candidate’s starting salary will be based on a number of permissible, non-discriminatory factors, including but not limited to skills and experience, training, certifications, and education.
About the San Francisco Chronicle & SFGATE:
The Chronicle is world-class journalism, San Francisco style. With name-brand voices and a keen editorial eye, The Chronicle is an authority that still surprises. It’s not afraid of being controversial or of doing things that haven’t been done. It’s bolder, brighter, and fearless. It’s news that is delivered to the discerning reader through multiple platforms.
SFGATE provides a 360-degree view of San Francisco, wildly reflective of right now. Provocative, energetic, and unapologetic, the Gate is in constant conversation with the world's most eclectic city.
Join our world-class team!
Show more
Show less","Data Analysis, Data Visualization, R, Python, SQL, Datawrapper, D3, Adobe Illustrator, ai2html, Excel, Google Sheets, Git, Github, Slack, Zoom, Node, PHP, APIs, AWS, S3, EC2, Lambda, HTML, CSS, JavaScript, React, Svelte, Vue","data analysis, data visualization, r, python, sql, datawrapper, d3, adobe illustrator, ai2html, excel, google sheets, git, github, slack, zoom, node, php, apis, aws, s3, ec2, lambda, html, css, javascript, react, svelte, vue","adobe illustrator, ai2html, apis, aws, css, d3, dataanalytics, datawrapper, ec2, excel, git, github, google sheets, html, javascript, lambda, node, php, python, r, react, s3, slack, sql, svelte, visualization, vue, zoom"
GCP Data Architect/Engineer,Experfy,"San Francisco, CA",https://www.linkedin.com/jobs/view/gcp-data-architect-engineer-at-experfy-3590304065,2023-12-17,El Cerrito,United States,Mid senior,Onsite,"We are looking for a GCP Data Architect/Engineer who has experience in building enterprise level solution on GCP cloud environment with Kafka, Pubsub, Snowflake, BigQuery, Cloud function, AI Platform, Dataflow, Dataproc, Cloud Compute, AppEngine, etc.
Ability to continuously learn, work independently, and make decisions with minimal supervision. You like nothing more than to watch colleagues flourish under your guidance. Be a technological cloud advocate to a wider audience inside and outside of the business. Curiosity - Bring fresh and bold ideas to the team and wider firm be it new software or containerisation of existing infrastructure and projects Job SummaryThis role is for the Infrastructure Engineering team. The teams mission is to be the best Infrastructure team ever, by using the right tools for the right job, failing fast and automating like our lives depend on it. Right from the leadership level, our team are fully committed to the DevOps culture and are consistently trying to change to ensure that culture is nurtured and grows.We operate at genuine scale taking advantage of our alliance partnerships with the industry leading public cloud providers to deliver cutting edge solutions to clients on a global stage. If the public cloud is your future, youre Google Cloud Platform practitioner, youre amazing at solving difficult, interesting and complex challenges and actual writing the code You will be working to continuously advance and standardise our clients infrastructure and pipelined deployments, whilst collaborating with colleagues to write infrastructure as code that scales and takes advantage of the technologies available in the market
Requirements
Demonstrated experience interacting with and influencing decision-making by non-analytical business audiences
Experience with data access, manipulations and statistical analysis through python and or PySpark
Experience in using Terraform and/or similar infrastructure as code to provision environment on GCP
Experience with Big Data / Analytics technologies like Hadoop, Hive, Spark, Python, Scala, R, Machine Learning
Experience in DevOP CI/CD with gitlab, Jenkins, Nexus Repo, checkmarkrx, and or other quality control or security governance tools
Show more
Show less","GCP, Kafka, Pubsub, Snowflake, BigQuery, Cloud Function, AI Platform, Dataflow, Dataproc, Cloud Compute, AppEngine, Terraform, Python, PySpark, Hadoop, Hive, Spark, Scala, R, Machine Learning, DevOps, CI/CD, GitLab, Jenkins, Nexus Repo, Checkmarx","gcp, kafka, pubsub, snowflake, bigquery, cloud function, ai platform, dataflow, dataproc, cloud compute, appengine, terraform, python, pyspark, hadoop, hive, spark, scala, r, machine learning, devops, cicd, gitlab, jenkins, nexus repo, checkmarx","ai platform, appengine, bigquery, checkmarx, cicd, cloud compute, cloud function, dataflow, dataproc, devops, gcp, gitlab, hadoop, hive, jenkins, kafka, machine learning, nexus repo, pubsub, python, r, scala, snowflake, spark, terraform"
DATA & MEDICAL ECONOMIC ANALYST,"Integrated Home Care Services, Inc.","Hollywood, FL",https://www.linkedin.com/jobs/view/data-medical-economic-analyst-at-integrated-home-care-services-inc-3779906086,2023-12-17,Pompano Beach,United States,Mid senior,Onsite,"Job Details
Description
Who we are:
IHCS provides an Integrated Delivery System in the home setting, which includes, DME, Respiratory, Home Health and Home Infusion services. IHCS has a select network of Medicare and/or Medicaid Certified and Accredited providers to respond to the needs of our patients – 24/7. We operate with the sole intent of providing the highest quality in-home care services that improve and enhance the daily living for our patients, where our patients are #1
Our delivery model is trusted by national Managed Care Organizations (MCOs), physicians and patients, positioned with over two decades of expertise as the market leader in value-based Home Health, Durable Medical Equipment, and Home Infusion Services. We currently serve over 2 million lives throughout the nation and the Commonwealth of Puerto Rico.
As we continue expand our footprint throughout the United States, we are seeking a senior level big picture professional who is passionate about leading expansions through Medical Economics and Data Analytics.
Join our team as we strive for excellence through teamwork delivering high quality care to our patients through Exceptional Customer Service, Proven Outcomes, and Seamless Care.
Offering a competitive compensation package, including but not limited to;
Medical, Vision, Dental, Short- and Long-term insurance
6+ Days of Holidays Pay
15+ days of PTO
Employer paid life insurance
401K with employer contribution
Wellness program with reward incentives
Employee recognition and reward programs
Comprehensive paid training program
What Will You Be Doing
The Medical Economics Analyst is responsible for identifying, developing, analyzing, underwriting and structuring future client partnerships while monitoring the performance of current client relationships. The incumbent will provide our health plan partners and patients the ability to deliver high quality services while reducing the overall cost of care by constructing detailed analyses, confront challenging and unfamiliar subjects and interfacing with stakeholders across portfolio of businesses (IHCS, and subsidiaries). The Medical Economics Analyst will develop strategies, find solutions, and build innovative new approaches that support the organization’s goals and future success by providing consistently and on-ongoing insight, analytics and conclusions.
What Will You Come With
Bachelor's degree, preferably with a quantitative major (e.g. Computer Science, Business Math, Economics)
Three years of claims-based healthcare analytics in a payer, provider, vendor and or consulting experience
Three to five years of hands-on Medical Economics Data Analyst experience
Five years of Healthcare Payor Portfolio Management experience
Two years of consulting, advisor, and facilitation/ presentation experience
Competencies
Medical Economics
Market Development
Project Management
Data & Analytics
Research
Specialized Knowledge
Medical Full Revenue Cycle, including claims, network analysis and cost-model development
Expert in SQL and Microsoft Office Products
Values Based Healthcare Delivery System
Skills & Abilities
Strong analytical abilities with a track record of identifying insights from quantitative and qualitative data
Strong attention to detail with the ability to multi-task and prioritize in changing requirements
Excellent organizational and time management skills
Demonstrate accountability and dependability
Ability to simplify complex scenarios into summary concepts and discussion points
Independent thinker with growth mindset and the desire to expand technical skills while understanding the broader business context, and contribute to business strategy
Strong customer service and interpersonal skills
Excellent verbal, written, and presentation skills including ability to listen and communicate with a wide variety of audiences
Works well in a team environment and collaborates effectively with others
Self-motivated and demonstrates a sense of urgency and commitment prioritizing organizational needs.
Chartered Financial Analyst (CFA) or Fundamentals of Sustainability Accounting (FSA) Credential- highly desired
IHCS is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees
Show more
Show less","SQL, Microsoft Office Products, Computer Science, Business Math, Economics, Healthcare Payor Portfolio Management, Project Management, Data & Analytics, Research, Medical Economics, Market Development, Medical Full Revenue Cycle, Claimsbased healthcare analytics, Medical Economics Data Analyst, Network analysis, Costmodel development","sql, microsoft office products, computer science, business math, economics, healthcare payor portfolio management, project management, data analytics, research, medical economics, market development, medical full revenue cycle, claimsbased healthcare analytics, medical economics data analyst, network analysis, costmodel development","business math, claimsbased healthcare analytics, computer science, costmodel development, dataanalytics, economics, healthcare payor portfolio management, market development, medical economics, medical economics data analyst, medical full revenue cycle, microsoft office products, network analysis, project management, research, sql"
Allen Plant - CMMS Database Management Specialist,Milliken & Company,"Blacksburg, SC",https://www.linkedin.com/jobs/view/allen-plant-cmms-database-management-specialist-at-milliken-company-3775657655,2023-12-17,Shelby,United States,Mid senior,Onsite,"Milliken & Company is a global manufacturing leader whose focus on materials science delivers tomorrow’s breakthroughs today. From industry-leading molecules to sustainable innovations, Milliken creates products that enhance people’s lives and deliver solutions for its customers and communities. Drawing on thousands of patents and a portfolio with applications across the textile, flooring, chemical and healthcare businesses, the company harnesses a shared sense of integrity and excellence to positively impact the world for generations. Discover more about Milliken’s curious minds and inspired solutions at Milliken.com and on Facebook, Instagram, LinkedIn and Twitter.
POSITION TITLE
Engineering Services CMMS Database Management and CAD Specialist
Position Overview
The Engineering Services Specialist will be responsible for maintaining accurate data within plant Computerized Maintenance Management System (CMMS) in accordance with company policy. Additionally, the associate will work in an industrial chemical processing setting maintaining current piping and instrument design drawings (P& ID’s) within the Milliken Database System (MDS). Employees must exercise principles of safety, environmental management, quality management, and good housekeeping in all aspects of the job and must inform supervision of any unsafe practices or conditions. This person reports directly to the Sr. Engineering Services Engineer.
The work schedule for this role is Monday – Friday, 1st shift.
Job Responsibilities
Work with necessary resources (Project Engineers, Engineering Services Managers, Milliken knowledge experts, etc.) to develop a World Class asset management program that is sustainable.
Become fully versed in CMMS functionality, to include implementing new equipment, maintenance tasks, survey points, route structuring, and owning import templates.
Become proficient in SQL and database metadata modification as needed to query and implement database changes.
Continuous improvement of proactive maintenance program with close collaboration of ESD Planning and Scheduling Facilitators as well as plant management.
Lead the CMMS utilization training and education requirements for the plant site.
Work with Engineering Services managers and facilitators to place appropriate standard time expectations on proactive maintenance tasks and points, as well as develop necessary preventative and proactive tasks.
Review completed work orders for proper documentation of parts, confirmed time and notes.
Identify recurring maintenance problems and collaborate with plant engineers/managers to prevent reoccurrence.
Analyze data and submit recommendations to reliability engineer for decisions on fit to run, end of life, repairs, and inspections.
Develop and update the CMMS database to categorize, store, and provide the relative assets and data.
Ensure testing and inspection frequencies are reviewed and set appropriately based on the standards adopted.
Assist in updating, training, and utilization for calibration database to reflect plant equipment.
Coordinate with facilitators for outage planning.
Generate P& ID’s, Elementary Wiring Diagrams, Single Line Diagrams (SLD), Loop Sheets, Flow Diagrams, General Arrangement Drawings, isometric piping diagrams, control system wiring diagrams, instrumentation specification sheets, and other required drawings from vendor documentation in AutoCAD.
Check current drawings for accuracy and create new drawings, engineering red and green line mark-ups by walking down as-built piping and equipment installed in the plant.
Generate and modify piping isometric drawings of pressure relief systems.
Communicate with department manager(s), project manager(s), and engineers to gather information required to complete the P& ID documentation.
Ensure management of change (MOC) is strictly enforced at all levels.
Be knowledgeable of the OSHA VPP Star Certification requirements and assist in obtaining and maintaining of this certification.
Assist in the plant wide enforcement of all safety rules and regulations.
Actively participate in the plant safety process, including participation on a safety team.
Perform other duties as requested by the manager.
Qualifications
Associate degree or relevant experience
Ability to communicate and interact with multiple levels within an organization as well as outside vendors.
Proficient in PC hardware components, desktop operation system software, application software, Human Resource SAP, AutoCAD or relevant drafting design software, and Microsoft Office.
Proficient in SQL databasing language or has ability to become proficient.
Ability to work alone or on a team depending on slated project(s).
Ability to access all areas of the facility and to work on elevated platforms.
Ability to travel from plant to plant as need, some overnight travel might be required as well.
Milliken is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to actual or perceived race, color, creed, religion, sex or gender (including pregnancy, childbirth or related medical condition, including but not limited to lactation), sexual orientation, gender identity or gender expression (including transgender status), ancestry, national origin, citizenship, age physical or mental disability, genetic information, marital status, veteran or military status or any other characteristic protected by applicable law.
Show more
Show less","CMMS, SQL, AutoCAD, Microsoft Office, SAP, Drafting design software, P&ID's, Elementary Wiring Diagrams, Single Line Diagrams (SLD), Loop Sheets, Flow Diagrams, General Arrangement Drawings, isometric piping diagrams, control system wiring diagrams, instrumentation specification sheets, CAD, Database management, Proactive maintenance, Preventative maintenance, Calibration database, Management of change (MOC), OSHA VPP Star Certification, Safety regulations, PC hardware components, Desktop operation system software, Application software, Human Resource, Data analysis, Troubleshooting","cmms, sql, autocad, microsoft office, sap, drafting design software, pids, elementary wiring diagrams, single line diagrams sld, loop sheets, flow diagrams, general arrangement drawings, isometric piping diagrams, control system wiring diagrams, instrumentation specification sheets, cad, database management, proactive maintenance, preventative maintenance, calibration database, management of change moc, osha vpp star certification, safety regulations, pc hardware components, desktop operation system software, application software, human resource, data analysis, troubleshooting","application software, autocad, cad, calibration database, cmms, control system wiring diagrams, dataanalytics, database management, desktop operation system software, drafting design software, elementary wiring diagrams, flow diagrams, general arrangement drawings, human resource, instrumentation specification sheets, isometric piping diagrams, loop sheets, management of change moc, microsoft office, osha vpp star certification, pc hardware components, pids, preventative maintenance, proactive maintenance, safety regulations, sap, single line diagrams sld, sql, troubleshooting"
Junior Data Engineer,Los Angeles Dodgers,"Los Angeles, CA",https://www.linkedin.com/jobs/view/junior-data-engineer-at-los-angeles-dodgers-3728707694,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Job Details
Description
Title: Junior Data Engineer
Department: Baseball Operations – Baseball Systems
Status: Full-Time
Pay Rate: $34.00-$37.00/hour*
Reports to: Director, Baseball Systems Platform
Compensation rates vary based on job-related factors, including experience, job skills, education, and training.
The Los Angeles Dodgers Baseball Systems team is committed to building and maintaining the technological platforms for baseball data, analysis, and decision-making for the Dodgers, and to providing technical expertise and advice across Dodger Baseball Operations. We focus on both the tools needed to put a winning team on the field today and those we need to ensure a winning future. The data engineering group within Baseball Systems operates the data platform used throughout Baseball Operations. We design, implement, and maintain the processes that bring in game, tracking, and scouting data, make them available to the rest of the Dodgers, and in turn re-ingest output from analytics to make that available as well.
The Junior Data Engineer will join the data engineering group within Baseball Systems to support our data operations. You will be responsible for implementing new ETL services and working with combining baseball data sources to create a cohesive view of games and plays. You will support the health of our data platform across relational databases, non-relational stores, and cloud file storage. You will collaborate with the rest of Baseball Operations to make sure that the data we provide them is complete, accurate, and meets their needs.
We work together to build and maintain a winning, industry-leading team. If you are also enthusiastic about baseball and want to see your work reflected on the field and in the box score, please contact us!
Essential Duties/Responsibilities
Implement and document Dodgers R&D database schema and ETL layer
Design, implement, and test data collection, storage, and mapping procedures
Maintain computational environments to support analytical modeling (statistics, machine learning, and optimization)
Apply statistical models for data quality testing and missing data imputation
Write, optimize, and automate data processing tasks
Deploy and maintain system and database monitoring tools
Perform other related duties as assigned
Basic Requirements/Qualifications
S. or M.S. in Computer Science, Computer Engineering, or a related field
SQL development skills and an understanding of database technologies (PostgreSQL preferred)
Experience with Python, Bash, and other scripting languages
Experience using Linux servers in a virtualized environment
Familiarity with cloud-based and distributed computing concepts (AWS and Kubernetes preferred)
Excellent analytical and problem-solving skills
Current Los Angeles Dodgers employees should apply via the internal job board in UltiPro by following these prompts:
MENU > MYSELF > MY COMPANY > VIEW OPPORTUNITIES > select the position > CONSENT > APPLY NOW
LOS ANGELES DODGERS LLC is firmly committed to providing equal opportunity for all qualified applicants from every race, creed, and background. The Organization is also firmly committed to complying with all applicable laws and governmental regulations at the state and local levels which prohibit discrimination.
LOS ANGELES DODGERS LLC considers all applicants without regard to national origin, race, color, religion, age, sex, sexual orientation, disability, military status, citizenship status, pregnancy or related medical conditions, marital status, ancestry-ethnicity, or any other characteristic protected by applicable state or federal civil rights law. The Immigration Reform and Control Act require that the Organization obtain documentation from every individual who is employed which verifies identity and authorizes their right to work in the United States.
Show more
Show less","Python, Bash, SQL, Linux, PostgresQL, Kubernetes, AWS, Databases, Data Engineering, ETL, Data Quality Testing, AWS, Cloudbased Computing, Distributed Computing","python, bash, sql, linux, postgresql, kubernetes, aws, databases, data engineering, etl, data quality testing, aws, cloudbased computing, distributed computing","aws, bash, cloudbased computing, data engineering, data quality testing, databases, distributed computing, etl, kubernetes, linux, postgresql, python, sql"
Data Engineer,REALLY,"Austin, TX",https://www.linkedin.com/jobs/view/data-engineer-at-really-3787912055,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"As a Data Engineer at REALLY, your primary responsibility is to seamlessly integrate data from internal and external sources into a unified warehouse data model. You will play a crucial part in facilitating data and analytics endeavors across REALLY’s business functions.
We’re seeking an individual with hands-on experience in data modeling, test automation, and the development of efficient data pipelines. You are comfortable with rapid-prototyping, while understanding how to build sustainable and scalable solutions. You are eager to understand our business, enabling you to shape the vision of data's impact and value at REALLY.
Responsibilities:
Create and maintain an internal database for ingesting data from various sources into one centralized platform, unlocking the ability for the business to analyze all key metrics across REALLY’s multiple business functions
Through your work, the business will be able to analyze the full customer journey from first engagement to purchase and track the full lifecycle in one single view of the customer
Partner with executives and engineering team to bring to life our data philosophy through the right processes and tools
Automate manual data processes; design and implement internal process improvements for optimizing data delivery and ensuring the highest level of data quality
Become a subject matter expert on the tools REALLY uses to track data across the business
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Build analytics tools that utilize different data pipeline sources to provide actionable insights into customer acquisition, behavior (ex. Google Analytics, Search Console, Ads, etc).
Partner with our software engineers to develop architectures that enable scalable data extraction and transformation for both predictive and prescriptive modeling.
Qualifications:
5+ years of professional (or comparable) data/analytics experience
Experience with SQL and relational databases
Experience with Data Warehouses such as Snowflake
Experience with BI platforms such as Looker and Tableau
Technical expertise in designing and creating data models
Strong attention to detail
A passion for problem solving with strong analytical capabilities associated with working on unstructured datasets
A desire to follow exceptional software engineering processes, and familiarity with common engineering process tools like Github and Gitlab
Excellent written and verbal communication skills
Ability to communicate complex information to non-technical audiences
Bonus Experience:
Experience with Snowflake, Redshift, or BigQuery
Programming experience in Python
Experience creating data visualizations using Javascript
Perks and Benefits:
Competitive compensation
Medical, dental, and vision coverage for you and your family
Life insurance paid for by REALLY
REALLY is an equal opportunity employer and we welcome everyone to our team. We believe that diversity is integral to our success, and do not discriminate based on race, color, religion, age, or any other basis protected by law.
This position is based in Austin, TX.
REALLY is an equal opportunity employer and we welcome everyone to our team. We believe that diversity is integral to our success, and do not discriminate based on race, color, religion, age, or any other basis protected by law.
Powered by JazzHR
O0te1Osi3U
Show more
Show less","Data Warehouses (Snowflake), Data Engineering, Data Modeling, SQL, Data Pipelines, Analytics, BigQuery, Looker, Tableau, RapidPrototyping, Git, Github, Gitlab, Javascript, Data Visualizations, Predictive Modeling, Prescriptive Modeling, Data Visualization","data warehouses snowflake, data engineering, data modeling, sql, data pipelines, analytics, bigquery, looker, tableau, rapidprototyping, git, github, gitlab, javascript, data visualizations, predictive modeling, prescriptive modeling, data visualization","analytics, bigquery, data engineering, data visualizations, data warehouses snowflake, datamodeling, datapipeline, git, github, gitlab, javascript, looker, predictive modeling, prescriptive modeling, rapidprototyping, sql, tableau, visualization"
Data Engineer - Chicago - 165k,Sierra ITS,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-its-3676932344,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Data Engineer
Sierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer to join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source data repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and maintain the reliability, scalability, and performance of these tools and technologies.
PAY:
150K plus 10% bonus, great benefits. No sponsorship available.
LOCATION:
Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago area applicants only - no third parties please.
Responsibilities
Design and develop scalable search applications to enable efficient data retrieval and indexing from the deep and dark web.
Work with internal and external stakeholders to optimize data infrastructure and identify cost savings, where possible.
Build and maintain data pipelines to ingest, transform, and process large volumes of open, deep, and dark web data from diverse sources.
Develop and maintain API endpoints for querying dark web data, ensuring efficient and reliable access to our systems.
Monitor and optimize search performance, address bottlenecks, and implement enhancements
Implement data quality and validation measures to ensure accuracy and integrity of indexed data.
Identify and integrate optimal database solutions.
Qualifications
Expertise in data modeling, infrastructure design, and data integration to enhance thier data
Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
Proficiency in working with large-scale data processing frameworks, especially Elasticsearch
Solid understanding of data modeling and schema design principles for efficient search and retrieval
Familiarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or processes
Experience with API development and management, including authentication, versioning, and performance optimization
Proven experience as a Data Engineer, preferably in the development and management of search engines and APIs
Demonstrated knowledge of cloud platforms, especially AWS and Azure
Excellent communication and collaboration skills to work effectively in a cross-functional team environment
Familiarity with building CI/CD pipelines to ensure the delivery of high-quality software
Knowledge of data privacy and security considerations when working with sensitive data
Strong programming skills in Python
Show more
Show less","Data engineering, Data modeling, Infrastructure design, Data integration, Data pipelines, Elasticsearch, API development, API management, Authentication, Versioning, Performance optimization, Search engine development, AWS, Azure, CI/CD, Data privacy, Data security, Sensitive data handling, Python","data engineering, data modeling, infrastructure design, data integration, data pipelines, elasticsearch, api development, api management, authentication, versioning, performance optimization, search engine development, aws, azure, cicd, data privacy, data security, sensitive data handling, python","api development, api management, authentication, aws, azure, cicd, data engineering, data integration, data privacy, data security, datamodeling, datapipeline, elasticsearch, infrastructure design, performance optimization, python, search engine development, sensitive data handling, versioning"
Data Engineer,The Walt Disney Company,"Lake Buena Vista, FL",https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-company-3764137624,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"About The Role & Team
Join the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization at The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and international, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which include Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.
We use technology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business decisions and drive business value.
As a member of the Data Engineering team you will be responsible for partnering with Decision Science Products, Decision Science, Client and Technology team members on various development and sustainment projects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering services.
As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory Optimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop, implement, improve and support our solutions. The work will involve various data engineering activities throughout the project SDLC.
What You Will Do
Work assignments may cover activities such as participation in data requirements gathering, source-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets for science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the role is responsible for understanding the business domain and processes, then applying that knowledge to the assigned work. This role communicates data engineering progress to the project leadership team, and actively participates in meetings and discussions.
Required Qualifications & Skills
Minimum 3 years of related work experience
Experience with ELT/ETL data pipeline development and maintenance
Expertise using Python and SQL
Ability to showcase an understanding of one or more business domains
Prior experience gathering data requirements and producing data design solutions
Experience with developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion
Experience crafting and building relational databases (preferably in Postgres or Snowflake)
Experience leading and deploying code using a source control product such as GitLab/GitHub
Able to formulate solutions and communicate sophisticated technical concepts to non-technical team members
Preferred Qualifications
Knowledgeable with theme park attendance, reservations and/or products
Showed strength interacting with API’s
Experience with data orchestration tools such as Apache Airflow
Knowledgeable on cloud architecture and product offerings, preferably AWS
Experience using containerization technologies such as Docker or Kubernetes
Education
Bachelor’s degree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experience
Master’s degree preferred Computer Science, Mathematics, Engineering or related field preferred
#DISNEYTECH
Show more
Show less","Data Engineering, Python, SQL, Postgres, Snowflake, Relational databases, GitLab, GitHub, Apache Airflow, AWS, Docker, Kubernetes, ELT, ETL, Data pipelines, Data quality monitoring, Business domain and processes, DevOps procedures, Cloud architecture, Containerization technologies","data engineering, python, sql, postgres, snowflake, relational databases, gitlab, github, apache airflow, aws, docker, kubernetes, elt, etl, data pipelines, data quality monitoring, business domain and processes, devops procedures, cloud architecture, containerization technologies","apache airflow, aws, business domain and processes, cloud architecture, containerization technologies, data engineering, data quality monitoring, datapipeline, devops procedures, docker, elt, etl, github, gitlab, kubernetes, postgres, python, relational databases, snowflake, sql"
Python Data Engineer,Tech Mahindra,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/python-data-engineer-at-tech-mahindra-3773980841,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Strong Python development and software design
Dremio/Presto/Trino
Snowflake (or other data warehouse systems)
Tableau
Docker & Kubernetes
SQL
MongoDB
Object store (S3) & data lake concepts
Git
Shell & CLI tools
REST APIs
Pandas
Show more
Show less","Python, Software Design, Dremio, Presto, Trino, Snowflake, Data Warehouse Systems, Tableau, Docker, Kubernetes, SQL, MongoDB, Object Store, S3, Data Lake Concepts, Git, Shell, CLI Tools, REST APIs, Pandas","python, software design, dremio, presto, trino, snowflake, data warehouse systems, tableau, docker, kubernetes, sql, mongodb, object store, s3, data lake concepts, git, shell, cli tools, rest apis, pandas","cli tools, data lake concepts, data warehouse systems, docker, dremio, git, kubernetes, mongodb, object store, pandas, presto, python, rest apis, s3, shell, snowflake, software design, sql, tableau, trino"
Data Engineer,Tech Mahindra,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-tech-mahindra-3773989267,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"JD:
Strong Python development and software design
Dremio/Presto/Trino
Snowflake (or other data warehouse systems)
Tableau
Docker & Kubernetes
SQL
MongoDB
Object store (S3) & data lake concepts
Git
Shell & CLI tools
REST APIs
Pandas
Show more
Show less","Python, Software design, Dremio, Presto, Trino, Snowflake, Tableau, Docker, Kubernetes, SQL, MongoDB, Object store, Data lake, Git, Shell, CLI tools, REST APIs, Pandas","python, software design, dremio, presto, trino, snowflake, tableau, docker, kubernetes, sql, mongodb, object store, data lake, git, shell, cli tools, rest apis, pandas","cli tools, data lake, docker, dremio, git, kubernetes, mongodb, object store, pandas, presto, python, rest apis, shell, snowflake, software design, sql, tableau, trino"
Data Engineer,257,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-257-3785871422,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Description
Join our dynamic team at 257 as a Data Engineer and help shape the future of data-driven decision making in the energy industry. As a Data Engineer, you will be responsible for building and maintaining the data infrastructure, data pipelines, and scalable backend systems that enable our data analysts and scientists to extract valuable insights from large and complex datasets. This is an exciting opportunity to work with cutting-edge technologies and contribute to solving complex challenges in the energy sector.
Responsibilities
Design, develop and maintain scalable and efficient data pipelines to capture and process large volumes of data from various sources
Implement data integration and transformation processes to ensure data consistency and accuracy
Collaborate with data analysts, scientists, and other cross-functional teams to understand data requirements and design appropriate data solutions
Optimize data processing and storage for performance and cost-efficiency
Identify and resolve issues related to data quality, data consistency, and data pipeline performance
Implement and maintain monitoring and alerting systems to ensure data pipelines are running smoothly
Stay up-to-date with the latest trends and technologies in data engineering and propose innovative solutions to enhance our data infrastructure
Participate in code reviews, provide constructive feedback, and ensure code quality and best practices are followed
Requirements
Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field.
5+ years of experience as a Data Engineer or in a similar role
Strong programming skills in Python or Java
Experience building and optimizing data pipelines using Apache Kafka, Apache Spark, or similar technologies
Familiarity with cloud-based data platforms such as AWS, Azure, or Google Cloud Platform
Knowledge of SQL and experience with relational databases such as MySQL or PostgreSQL
Knowledge of data modeling, data warehousing, and ETL processes
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration skills to work effectively in a cross-functional team
Experience in the energy or sustainability industry is a plus
Why Join Us?
Fascinating technical challenge - solving complex data engineering problems to drive data insights
Meaningful impact - contribute to the transition to renewable energy and reduce carbon emissions
Opportunity for growth - learn, develop, and advance your career in a rapidly evolving industry
Collaborative and inclusive work environment
Competitive compensation and benefits package
Show more
Show less","Data Engineering, Data Infrastructure, Data Pipelines, Backend Systems, Data Analytics, Data Science, Data Integration, Data Transformation, Data Consistency, Data Quality, Data Processing, Data Storage, Data Modeling, Data Warehousing, ETL Processes, Data Visualization, Machine Learning, Artificial Intelligence, Python, Java, Apache Kafka, Apache Spark, Cloud Computing, AWS, Azure, Google Cloud Platform, SQL, MySQL, PostgreSQL, Problem Solving, Troubleshooting, Communication, Collaboration, Renewable Energy, Sustainability","data engineering, data infrastructure, data pipelines, backend systems, data analytics, data science, data integration, data transformation, data consistency, data quality, data processing, data storage, data modeling, data warehousing, etl processes, data visualization, machine learning, artificial intelligence, python, java, apache kafka, apache spark, cloud computing, aws, azure, google cloud platform, sql, mysql, postgresql, problem solving, troubleshooting, communication, collaboration, renewable energy, sustainability","apache kafka, apache spark, artificial intelligence, aws, azure, backend systems, cloud computing, collaboration, communication, data consistency, data engineering, data infrastructure, data integration, data processing, data quality, data science, data storage, data transformation, dataanalytics, datamodeling, datapipeline, datawarehouse, etl, google cloud platform, java, machine learning, mysql, postgresql, problem solving, python, renewable energy, sql, sustainability, troubleshooting, visualization"
SQL DATA ENGINEER,Bamboo Solutions,"Washington, DC",https://www.linkedin.com/jobs/view/sql-data-engineer-at-bamboo-solutions-3787728876,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"We are looking for a
motivated, experienced SQL Data Enginee
r to act as an integral part of a government client’s program. As a member of the engineering team, the SQL Data Engineer’s knowledge share and partnership will be critically meaningful. The right candidate will have the ability to adapt to changing technology and apply it to the customer needs. Great communication, leadership and personal skills are a must.
Clearance: Must be able to attain Public Trust
Potential for Remote Work: Hybrid Remote, 2 days a week onsite in Washington, DC
Description of Work:
Design and implementation of Extract, Transform, and Load (ETL) processing routines and data feeds; create necessary data structures or models to support data at all stages, and design and implement custom data analytics, alerts and BI/reporting products
Perform extensive data profiling and analysis based on the client’s data
Work with source teams to define BI and reporting requirement
Education And Experience
Bachelor’s degree (e.g., Computer Science, Engineering, or related discipline)
5 or more years of experience in Microsoft SQL Server, SSIS, SSRS, SSAS and procedural programming
3 or more years of experience developing database ETL environments with business intelligence applications
3 or more years of experience with Cloud Engineering services
Experience working with databases and BI tools such as Power BI
Ability to work independently and as part of a team
We offer:
Competitive salary based on experience
Profit sharing distributed twice a year
15 days of paid time off and 10 paid holidays per year
401(k) with employer matching
Health and dental benefits
Opportunity to work with other talented technical professionals
SharePointXperts is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected Veteran status.
SharePointXperts participates in E-Verify. Click the following links for important information about our participation in this program and your rights.
https://www.e-verify.gov/sites/default/files/everify/posters/IER_RightToWorkPoster%20Eng_Es.pdf
https://www.e-verify.gov/sites/default/files/everify/posters/EVerifyParticipationPoster.pdf
Powered by JazzHR
bpCtW7BiN6
Show more
Show less","SQL, Data Engineering, ETL, Data Analytics, BI, Power BI, SSIS, SSRS, SSAS, Cloud Engineering, SQL Server","sql, data engineering, etl, data analytics, bi, power bi, ssis, ssrs, ssas, cloud engineering, sql server","bi, cloud engineering, data engineering, dataanalytics, etl, powerbi, sql, sql server, ssas, ssis, ssrs"
Junior Data Engineer,TickPick,"New York, NY",https://www.linkedin.com/jobs/view/junior-data-engineer-at-tickpick-3774896539,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Who We Are
Same Seats. Better Prices.
Among the fastest growing technology companies, TickPick is reshaping the ticketing industry, putting money back in the wallets of live event-goers. Since inception, we have saved our customers over $100 million in service fees. Our BestPrice Guarantee backs up our promise to deliver better prices than our competition.
For the last five years, TickPick has been named a Deloitte Technology Fast 500 award winner and has landed on lists of Inc. 5000’s and Crain's New York Business’ Fast 50.
If you are passionate about concerts, sports, theater or all of them, and want to see your skills and experience have a direct impact on a fast growing company, TickPick is the place for you. We are building a diverse team, committed to providing the most innovative, transparent, and cost-effective ticket marketplace in the industry.
Who You Are
You are: a data engineer, backend software engineer, or other data specialist with strong desire and ability to implement high-impact data movement and management within a growing, technology-first team.
In this core role, you will be responsible for building and maintaining data pipelines touching a wide array of tools from the modern data stack, including but not limited to Snowflake, Spark, Dagster, dbt, and Azure Cloud offerings. You’ll be working closely with our current Data Engineer and other Analytics-focussed team members, as well as stakeholders from across the whole business.
If you value continual learning and are looking to join a high-visibility and high-impact team at a growing company that’s making data a priority, then this role is for you.
Core Responsibilities
Discover opportunities for data acquisition and implementation solutions
Develop production processes and solutions to model, mine and surface data
Improve and ensure data reliability, quality and efficiency
Requirements
BS or above in Computer Science or a related field, or equivalent personal or professional experience
Communication ability is paramount: you understand the value of open communication and have a track record of interacting effectively with stakeholders and team members
Experience and competency with at least one cloud services provider (Azure preferred; AWS or GCP also worthwhile)
Strong Python and SQL skills, as well as at least general competency with web languages (HTML/Javascript)
Python data competency – knowledge of and experience with, eg, Pandas or other Dataframe libraries. Polars, PySpark, or DuckDB are a big plus
Experience and competency with at least one general data orchestration tool, eg Airflow, Dagster, Prefect, or similar other experience. Dagster specifically is a big plus
dbt or other data modeling experience is a plus
Kafka experience is a plus
Experience with web scraping tools is a plus, eg BeautifulSoup, Playwright, requests, or Selenium
Experience with a dashboarding tool (eg Looker, Tableau, Superset, Metabase, or Streamlit) is a plus, but not most centrally important
Experience with managed ETL tools (eg Fivetran, Hightouch) is worth mentioning if you have it, but not most centrally important
Diversity at TickPick
At TickPick, we know that diversity of all types, in an environment that pursues equity and inclusion, strengthens our organization’s culture. When our employees are representative of the communities we serve, with diversity in demographics and a broad set of backgrounds, we provide a superior experience for both our customers and our employees. Fostering an open and supportive environment where our employees are empowered and encouraged to bring their whole selves to the table enables TickPick to thrive. The diverse approaches and collaborative problem solving that result enable us to provide an innovative, nimble, and creative marketplace for our customers and sellers. This belief is central to who we are and what we do, and we are proud of it.
TickPick, LLC is proud to be an equal opportunity employer open to all qualified candidates regardless of race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, marital status, citizenship status, military status, protected veteran status or any other category protected
Benefits
Per the NYC pay transparency law, the hiring range for this position is $90,000 to $110,000.
As a Candidate For This Position, Your Salary Will Be Contingent Upon Your Work Experience, Education, Skills And Any Other Factors TickPick Considers Relevant To The Hiring Decision. In Addition To Your Salary, TickPick Believes In Providing a Competitive Benefits Package For Its Employees. TickPick Offers:
A hybrid in-office approach, enabling remote work a portion of each week
Health Care Plan (Medical, Dental & Vision)
Retirement Plan Contribution (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Holidays)
Family Leave (Maternity, Paternity)
Training & Development
$100 Monthly Stipend to Attend Live Events
Employee Outings
Free Lunch & Snacks
Show more
Show less","Python, SQL, HTML, JavaScript, Pandas, Polars, PySpark, DuckDB, Airflow, Dagster, Prefect, dbt, Kafka, BeautifulSoup, Playwright, requests, Selenium, Looker, Tableau, Superset, Metabase, Streamlit, Fivetran, Hightouch, Snowflake, Spark, Azure Cloud, AWS, GCP","python, sql, html, javascript, pandas, polars, pyspark, duckdb, airflow, dagster, prefect, dbt, kafka, beautifulsoup, playwright, requests, selenium, looker, tableau, superset, metabase, streamlit, fivetran, hightouch, snowflake, spark, azure cloud, aws, gcp","airflow, aws, azure cloud, beautifulsoup, dagster, dbt, duckdb, fivetran, gcp, hightouch, html, javascript, kafka, looker, metabase, pandas, playwright, polars, prefect, python, requests, selenium, snowflake, spark, sql, streamlit, superset, tableau"
Data Engineer,Nasdaq Private Market,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-nasdaq-private-market-3771461592,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Nasdaq Private Market (NPM) is a mission-driven company that supports the entire secondary ecosystem of private companies throughout the journey from private to public. NPM is a standalone, independent company, which received strategic investments from Nasdaq, Inc., Citi, Goldman Sachs, Allen & Co, and Morgan Stanley. NPM is dedicated to serving a large and growing market for private company secondary liquidity that is built from Nasdaq’s franchise of technology and operating marketplaces. NPM is both a financial technology provider and operator of its own private marketplace that offers an institutional-grade, centralized secondary trading venue for issuers, brokers, shareholders and prospective investors of private company stock so clients can use one platform to transact with confidence through an experience regulated operator.
NPM operates in a fast-paced environment that is constantly evolving. NPM has worked with 250+ private companies and 50,000+ employees, stakeholders and investors, which includes some of the fastest growing venture-backed companies globally.
About you
Self-starter, motivated to build and know how to prioritize for execution and on-time delivery
Problem-solver, ready to tackle all challenges with your strong analytical skills
Collaborative team player, able to work cross-functionally across different teams within the organization
Strong communications skills, detail-oriented approach with positive attitude
About the Role
As a Data Engineer, you will be building out the core Data services and systems for customer facing company data platform. You will be leading, building, and owning backend services, ETL pipelines, and performing variety of data modeling exercise to ensure the data are properly structured and stored.
A typical day will include
You are self-motivated, multi-tasker and demonstrated team player.
You are in constant communication with the Product team to ensure quality of design and delivery.
You lead design sessions and coach more junior engineers.
You help enforce code quality and best practices via peer code reviews.
Coordinate cross-functionally to insure project meets business objectives and compliance standards
Support test and deployment of new products and features
Qualifications include demonstration of skills/experience in
Data warehousing, data modeling, and data transformation for both batch and streaming
How to write complex SQL and Python
Expert at building performant data pipelines and optimizing existing workflows for new features
Big Data tech - Airflow, Spark, Presto, S3, Redshift
Experience with sourcing and modeling data from application APIs
Team-based software development tools and best practices
Backend service development experience is a big plus
Come as You Are
Nasdaq Private Market is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.
Show more
Show less","Data warehousing, Data modeling, Data transformation, SQL, Python, Airflow, Spark, Presto, S3, Redshift, Backend service development, ETL pipelines","data warehousing, data modeling, data transformation, sql, python, airflow, spark, presto, s3, redshift, backend service development, etl pipelines","airflow, backend service development, data transformation, datamodeling, datawarehouse, etl pipelines, presto, python, redshift, s3, spark, sql"
Data Engineer,Itility US,"San Diego, CA",https://www.linkedin.com/jobs/view/data-engineer-at-itility-us-3787903005,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Who We Are.
We believe in merging technology and data to drive our customers one step beyond. Itility digital consultants are experts in data, cloud, software, and IT infrastructure. Acting as the ‘digital twin’ of customers, we work shoulder-to-shoulder to exceed business goals and push the boundaries of what you thought was possible. We combine an agile way of working with proven methods and building blocks. This enables teams to act quickly and shape, deliver, and run innovative digital solutions. With a continuous focus on implementing your strategy and generating results.
About The Role.
Are you adept at coding to capture data? Are you thrilled when you dig into various data sources and transform them into valuable, user-friendly information? Are your skills at the intersection of data and software? Is your day made when data flows flawlessly based on the carefully constructed code you've created? Then we have the perfect opportunity for you! For numerous enterprise clients, we develop data connectors to streamline data flow from multiple sources to analytics platforms, such as Splunk, Databricks, Hadoop, or others. One thing these systems share is that the data will be utilized in a production environment, which means it must flow consistently and monitored for any interruptions. Ensuring data validity and quality is also crucial.
What You'll Do.
Developing data connectors with Python or other programming languages.
Setting up data validation tests within the data pipeline.
Establishing monitoring and alert systems for data flow interruptions or corruption.
Leading incident resolution efforts to minimize impact on end users.
Teaming up to build, deploy, maintain, and optimize data ingestion and connectors for data flow to the data storage system.
Creating and managing distributed systems for data extraction, ingestion, and processing of large, diverse data sets.
Developing data products in stages, integrating and managing data from various sources.
Collaborating with software engineers and data scientists to design data sets for diverse applications, from proof-of-concept to production.
Partnering with Business Analysts for requirement collection, pipeline implementation decisions, data identification, and tooling selection.
Working with ETL/data services and application teams to support data solution development.
Collaborating with software engineers and data scientists to design data sets for diverse applications, from proof-of-concept to production.
Qualifications.
Possess a bachelor’s or master’s degree.
Experience in creating data ingestion scripts.
Exhibit team spirit and good communication skills.
Good understanding of SQL and Python.
Experience with data platforms and data lakes in an enterprise setting is a plus.
Demonstrate a hands-on approach, strong customer focus, problem-solving skills, and quick results.
Prior experience with Linux is necessary.
Familiarity with continuous integration & delivery tools, e.g. Jira, Git, Jenkins, Bamboo.
3+ years’ experience with analytics platforms and tools, such as Databricks, Snowflake, Teradata, Spark, Kafka.
3+ years’ cloud experience and demonstrated proficiency working with AWS, GCP, and/or Azure Cloud DevOps Services is preferred.
Belief in scrum/agile work methodologies and software practices for professional data flow.
Benefits & Total Rewards.
100% employer paid medical, dental and vision insurance
401k with up to 4% employer match
Paid vacation and sick time
Paid company holidays
HAS Accounts
Flexible Spending Accounts
Life Insurance
Professional Training and Development Programs
Powered by JazzHR
F66ZRFyOWw
Show more
Show less","Python, Splunk, Databricks, Hadoop, Data Validation, Monitoring, Alert Systems, Data Extraction, Data Ingestion, Data Processing, Data Products, Data Sets, ProofofConcept, SQL, Linux, Continuous Integration, Delivery Tools, Jira, Git, Jenkins, Bamboo, Analytics Platforms, Snowflake, Teradata, Spark, Kafka, AWS, GCP, Azure Cloud DevOps Services, Agile, Scrum","python, splunk, databricks, hadoop, data validation, monitoring, alert systems, data extraction, data ingestion, data processing, data products, data sets, proofofconcept, sql, linux, continuous integration, delivery tools, jira, git, jenkins, bamboo, analytics platforms, snowflake, teradata, spark, kafka, aws, gcp, azure cloud devops services, agile, scrum","agile, alert systems, analytics platforms, aws, azure cloud devops services, bamboo, continuous integration, data extraction, data ingestion, data processing, data products, data sets, data validation, databricks, delivery tools, gcp, git, hadoop, jenkins, jira, kafka, linux, monitoring, proofofconcept, python, scrum, snowflake, spark, splunk, sql, teradata"
Data Engineer,Accroid Inc,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3782787357,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"IA
ETL Development:
Design, develop, and maintain robust ETL processes, ensuring efficient extraction, transformation, and loading of data into Snowflake from diverse sources.
Uphold data quality, integrity, and reliability standards throughout the ETL lifecycle.
Snowflake Optimization:
Leverage in-depth knowledge of Snowflake architecture, features, and best practices to optimize performance, scalability, and efficiency of data pipelines and warehouse operations.
Data Modeling:
Collaborate with data architects to design and implement data models in Snowflake, aligning with business requirements and ensuring scalability for future growth.
Show more
Show less","ETL, Data Extraction, Data Transformation, Data Loading, Snowflake, Data Quality, Data Integrity, Data Reliability, Data Modeling, Data Architecture, Data Warehouse","etl, data extraction, data transformation, data loading, snowflake, data quality, data integrity, data reliability, data modeling, data architecture, data warehouse","data architecture, data extraction, data integrity, data loading, data quality, data reliability, data transformation, datamodeling, datawarehouse, etl, snowflake"
Data Engineer,Accroid Inc,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3778553874,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Remote
Data Engineer
12+ Months
Develop and maintain SQL scripts, stored procedures, and queries to extract and manipulate data from various sources, including MSSQL and Postgres databases.
Work with MuleSoft (Mule) for data integration and API development, if required or willing to learn it.
Monitor and optimize data pipelines, troubleshoot issues, and ensure data integrity and performance.
Collaborate with data analysts to understand data requirements and support their data analysis and modeling efforts.
Stay up to date with industry trends, best practices, and emerging technologies related to data engineering, Snowflake, ETL, SQL, and related tools.
Solid understanding and hands-on experience with ETL tools and frameworks.
Excellent communication and collaboration skills to work effectively with cross-functional teams.
Show more
Show less","SQL, Stored procedures, MSSQL, Postgres, MuleSoft (Mule), Data integration, API development, Data pipelines, Data integrity, Data performance, Data analysis, Data modeling, Snowflake, ETL, ETL tools, ETL frameworks","sql, stored procedures, mssql, postgres, mulesoft mule, data integration, api development, data pipelines, data integrity, data performance, data analysis, data modeling, snowflake, etl, etl tools, etl frameworks","api development, data integration, data integrity, data performance, dataanalytics, datamodeling, datapipeline, etl, etl frameworks, etl tools, mssql, mulesoft mule, postgres, snowflake, sql, stored procedures"
Senior Data Engineer,Lyft,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-lyft-3707966994,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"At Lyft, community is what we are and it’s what we do. It’s what makes us different. To create the best ride for all, we start in our own community by creating an open, inclusive, and diverse organization where all team members are recognized for what they bring.
Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create a transportation experience for our customers, and providing insights into the effectiveness of our product launch & features.
As a Data Engineer at Lyft, you will be a part of an early stage team that builds the data transport, collection, and storage, and exposes services that make data a first-class citizen at Lyft. We are looking for a Data Engineer to build a scalable data platform. You’ll have ownership of our core data pipeline that powers Lyft’s top-line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support Lyft’s growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of Lyft data to fuel several teams such as Analytics, Data Science, Marketplace, and many others.
Responsibilities:
Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance
Write well-crafted, well-tested, readable, maintainable code
Participate in code reviews to ensure code quality and distribute knowledge
Unblock, support and communicate with internal & external partners to achieve results
Experience:
5+ years of relevant professional experience
Strong experience with Spark
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Strong skills in a scripting language (Python, Ruby, Bash)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft’s business goals with data engineering
Benefits:
Great medical, dental, and vision insurance options
Mental health benefits
Family building benefits
In addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off
401(k) plan to help save for your future
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Pre-tax commuter benefits
Lyft Pink - Lyft team members get an exclusive opportunity to test new benefits of our Ridership Program
Lyft is an equal opportunity/affirmative action employer committed to an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status or any other basis prohibited by law. We also consider qualified applicants with criminal histories consistent with applicable federal, state and local law.
Starting in September 2023, this role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
The expected range of pay for this position in the San Francisco Bay Area is $162,000 - $180,000. Salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. Range is not inclusive of potential equity offering, bonus or benefits. Your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.
Show more
Show less","Spark, Data pipeline management, Data engineering, Hadoop, MapReduce, Yarn, HDFS, Hive, Presto, Pig, HBase, Parquet, SQL, MySQL, PostgreSQL, SQL Server, Oracle, Airflow, Oozie, Azkaban, UC4, Python, Ruby, Bash, ETL, Data analytics","spark, data pipeline management, data engineering, hadoop, mapreduce, yarn, hdfs, hive, presto, pig, hbase, parquet, sql, mysql, postgresql, sql server, oracle, airflow, oozie, azkaban, uc4, python, ruby, bash, etl, data analytics","airflow, azkaban, bash, data engineering, data pipeline management, dataanalytics, etl, hadoop, hbase, hdfs, hive, mapreduce, mysql, oozie, oracle, parquet, pig, postgresql, presto, python, ruby, spark, sql, sql server, uc4, yarn"
Senior Data Engineer,Infosys,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-infosys-3776117867,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"In the role of
Lead Data Engineer
, you will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.
Required Qualifications:
Candidate must be located within the commuting distance of
Sunnyvale, CA
or be willing to relocate to
Sunnyvale, CA
. This position may require travel within the US and Canada.
Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education
At least 4 years of experience in Information Technology.
At least 3 years of hands on experience with Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems.
At least 2 years of experience with Spark required.
At least 2 years of experience with Scala or Python required.
U.S. citizens and GC holders are encouraged to apply.
Preferred Qualifications:
At least 1 years of AWS development experience is preferred
Ability to work within deadlines and effectively prioritize and execute on tasks.
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels.
Experience in Drive automations
DevOps Knowledge is an added advantage.
Proficient knowledge of SQL with any RDBMS.
About Us
Infosys is a global leader in next-generation digital services and consulting. We enable clients in 50 countries to navigate their digital transformation. With over three decades of experience in leading the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver outstanding levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.
Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.
To learn more about Infosys and see our perspectives in action please visit us at www.Infosys.com
Along with competitive pay, as a full-time Infosys employee you are also eligible for the following benefits :-
Medical/Dental/Vision/Life Insurance
Long-term/Short-term Disability
Health and Dependent Care Reimbursement Accounts
Insurance (Accident, Critical Illness , Hospital Indemnity, Legal)
401(k) plan and contributions dependent on salary level
Paid holidays plus Paid Time Off
Show more
Show less","Software Development Life Cycle, Data Engineering, Hadoop, Spark, Scala, Python, SQL, AWS, DevOps, RDBMS","software development life cycle, data engineering, hadoop, spark, scala, python, sql, aws, devops, rdbms","aws, data engineering, devops, hadoop, python, rdbms, scala, software development life cycle, spark, sql"
10604 - Big Data Engineer,Hyundai AutoEver America,"Fountain Valley, CA",https://www.linkedin.com/jobs/view/10604-big-data-engineer-at-hyundai-autoever-america-3758140487,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Purpose:
Design, build, and maintain the Information/Proposed Changes platform that enables large-scale data processing and analysis. Responsible for developing and maintaining data pipelines, data lakes, and other data-related platform. Work closely with data scientists, analysts, and other stakeholders to ensure that data is properly collected, stored, and processed for analysis and reporting purposes. Implement and maintain data security and access controls to ensure that data is protected. Responsible for troubleshooting and resolving technical issues related to data infrastructure and ensuring that data systems are scalable and efficient. Play a critical role in enabling organization to derive insights and value from their data assets.
Essential Functions:
This job requires experience in building and maintaining scalable data pipelines and robust data models from structured and unstructured sources for AI/ML.
The ideal candidate should have advanced SQL skills and be able to query and transform large structured/unstructured datasets using Spark/PySpark, Spark SQL/Hive and Hive/NoSQL.
They should also have experience in developing Big Data pipelines in orchestration tools such as Airflow and Oozie, designing tooling for access management, monitoring, data controls, and self-service ETL/Analytics pipelines.
Other requirements include hands-on experience with On-Prem Big Data Platform, sound knowledge of Distributed Data Processing frameworks like YARN, and proficiency in writing data pipelines using Spark, Python and Scala.
The ideal candidate should also have experience in developing frameworks/utilities in Python, working in a Dev/Ops environment, and following development best practices such as code reviews and unit testing.
Additionally, the candidate should be able to diagnose software issues and engineering workarounds, have a good understanding of BI tools such as Tableau/Power BI and MicroStrategy for Big Data, and be able to lead, guide and assist team members with project development and problem solving.
The candidate should also be flexible and able to learn and use new technologies, work well in a team environment as well as independently to achieve goals.
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Basic Requirements:
Bachelor’s degree or master’s degree preferred
8+ years of experience working with and maintaining IT Application support.
5+ years of experience with Big Data Engineer
Big Data Technologies: Knowledge of Big Data technologies such as Hadoop, Spark, Hive, Pig, Kafka, and NoSQL databases such as MongoDB, Cassandra, and HBase.
Distributed Systems: Understanding of distributed systems and distributed computing principles.
Programming Languages: Proficiency in programming languages such as Java, Python, Scala, and SQL.
Data Modeling: Knowledge of data modeling techniques and tools to design efficient data structures for Big Data systems.
Data Processing: Experience with data processing and ETL (Extract, Transform, Load) tools and techniques.
Cloud Computing: Familiarity with cloud computing platforms such as AWS, Azure, and Google Cloud.
Data Security: Knowledge of data security principles and experience implementing security measures for Big Data systems.
Data Warehousing: Understanding of data warehousing concepts and experience designing and maintaining data warehouses.
Analytics and Machine Learning: Familiarity with analytics and machine learning tools and techniques and their implementation in Big Data systems.
Performance Tuning: Experience with performance tuning and optimization techniques for Big Data systems to ensure scalability, reliability, and high availability.
Certifications
Cloudera/Hortonworks Spark/Hadoop certification
Salary Range - $91,810 - $131,285
Show more
Show less","Data Pipelines, Data Lakes, Spark, PySpark, Spark SQL, SQL, Hive, NoSQL, Airflow, Oozie, Hadoop, YARN, Scala, Python, Tableau, Power BI, MicroStrategy, MongoDB, Cassandra, HBase, Java, AWS, Azure, Google Cloud, Machine Learning","data pipelines, data lakes, spark, pyspark, spark sql, sql, hive, nosql, airflow, oozie, hadoop, yarn, scala, python, tableau, power bi, microstrategy, mongodb, cassandra, hbase, java, aws, azure, google cloud, machine learning","airflow, aws, azure, cassandra, data lakes, datapipeline, google cloud, hadoop, hbase, hive, java, machine learning, microstrategy, mongodb, nosql, oozie, powerbi, python, scala, spark, spark sql, sql, tableau, yarn"
Senior Data Engineer,"Double Line, Inc.","Austin, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-double-line-inc-3787736740,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Development team with an experienced and innovative Senior Data Engineer with impressive analytical skills. Sound interesting?
If so, we're looking for a motivated and driven person like you who has:
Successfully completed multiple projects where you designed and executed ways to solve complex problems for clients around data integration, data quality, data warehousing, and analytics
Demonstrated proficiency in deciding which ETL and data streaming technologies to use in AWS, Google Cloud, and Azure-based solutions, and propensity to pick something new when you want to push yourself and the team to innovate
Mastery of T-SQL and experience with postgreSQL or other forms of SQL
Experience in an Agile environment with Lean software development principles
Drive to amplify the skills of teammates through mentoring and training junior and mid-level data engineers
Mindset of continuous improvement and setting best practices
Deadline-driven mentality
Bonus points if you're bringing knowledge of or really want to learn the following:
A wide variety of data processing tools and approaches, from Python to Google BigQuery to SSIS to AWS Lambda to Azure Data Factory and others
Performance implications of memory and disk usage at different data volumes
Business intelligence tools and dashboard design theory
We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Bring a new idea to our team of brilliant data engineers in the first 30 days
Lead the collaborative design process of a data engineering solution in one of our projects in the first 2 months
Become a mentor to a data engineer within your first 6 months
In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that challenge state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
Direct connection to the Executive team where you can help drive the future of the company
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture
We need to know - can you make this happen? If so, we definitely need to talk to you.
Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.
Powered by JazzHR
FcLpeWsMBJ
Show more
Show less","Data Integration, Data Quality, Data Warehousing, Analytics, ETL, Data Streaming, AWS, Google Cloud, Azure, TSQL, postgreSQL, SQL, Agile, Lean Software Development, Python, Google BigQuery, SSIS, AWS Lambda, Azure Data Factory, Performance Implications, Memory Usage, Disk Usage, Data Volumes, Business Intelligence Tools, Dashboard Design Theory","data integration, data quality, data warehousing, analytics, etl, data streaming, aws, google cloud, azure, tsql, postgresql, sql, agile, lean software development, python, google bigquery, ssis, aws lambda, azure data factory, performance implications, memory usage, disk usage, data volumes, business intelligence tools, dashboard design theory","agile, analytics, aws, aws lambda, azure, azure data factory, business intelligence tools, dashboard design theory, data integration, data quality, data streaming, data volumes, datawarehouse, disk usage, etl, google bigquery, google cloud, lean software development, memory usage, performance implications, postgresql, python, sql, ssis, tsql"
Data Engineer,Accroid Inc,"Eden Prairie, MN",https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3746829021,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Onsite
Data Engineer
PH and Skype
12+ Months
Minnesota
We need candidates that have a strong SQL Server, ETL and SSIS background.
Need to have the ability to ingest data, understand data lake concepts and be able to work closely with the business.
5+ SQL development and report writing experience
ETL development with SSIS experience
Data warehouse design and data integration experience
Experience with Asur Data Factory would be a huge plus
Show more
Show less","Data Engineering, SQL Server, ETL, SSIS, Data Ingestion, Data Lake Concepts, SQL Development, Report Writing, Data Warehouse Design, Data Integration, Azure Data Factory","data engineering, sql server, etl, ssis, data ingestion, data lake concepts, sql development, report writing, data warehouse design, data integration, azure data factory","azure data factory, data engineering, data ingestion, data integration, data lake concepts, data warehouse design, etl, report writing, sql development, sql server, ssis"
Senior Data Engineer,UpClear,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-upclear-3735349222,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Description
ABOUT UPCLEAR
UpClear delivers a SaaS revenue management platform that is used by some of the most recognizable consumer goods brands in the world. Our system supports Trade Promotion Management, Trade Promotion Optimization, Integrated Business Planning and Revenue Management.
We serve more than 80 brands in over 30 countries. Our growth is substantial and consistent; we have been on the Inc 5000 list of fastest growing private companies for eight years in a row.
UpClear's global headquarters is in New York City and we have satellite offices in London, Paris, and Hong Kong.
Position Overview
We are looking for a Senior Data Engineer to join our team and help us build and maintain our data infrastructure. The ideal candidate will have a strong understanding of data engineering principles and best practices, as well as experience with a variety of data technologies. They will also have experience with a variety of data tools and technologies, and be able to work independently and as part of a team.
Responsibilities
Design, Develop, Deploy and Maintain SQL Schema objects (Tables, Indexes, Stored Procedures etc).
Optimize and refactor data architecture solutions using technology other than SQL.
Optimize database performance and troubleshoot performance issues.
Integrate data from disparate sources into a single data warehouse.
Develop, maintain, and document data models and schemas.
Work with other engineers to design and implement data governance protocols and procedures.
Ensure high availability and accuracy of data across multiple geographical regions.
Perform code reviews and give senior guidance to other developers.
Ability to use database monitoring tools to identify performance bottlenecks.
Work directly with product leadership in building functional/technical specifications for customer requests.
Deep dive into complex data issues and provide detailed analysis and solutions to client representatives.
Requirements
Bachelor’s Degree in computer science or similar
5+ Years Experience with a Database Platform (e.g., SQL Server, Azure SQL)
Ability to perform DBA tasks and operations, preferably in cloud-based database solutions (e.g., Azure SQL).
Experience with optimizing data procedures and ETL pipelines.
Experience with a source control system (e.g., GIT)
Experience with Agile Development Processes.
Experience with designing, developing and analyzing complex T-SQL (e.g. Functions, Stored Procedures)
Experience and knowledge of CI/CD principles.
Experience with managing, developing, and operating large data warehouses.
Experience in developing data-based solutions with a programming language (e.g. Python, C#)
Experience in a cloud platform operations a plus (e.g., Azure, AWS, GCP)
Experience with a scripting language or with Devops automation a plus (e.g. Bash, Powershell)
Benefits
WHY UPCLEAR ?The salary range listed is a good faith determination of potential base compensation that may be offered to a successful applicant for this position at the time of this job advertisement and may be modified in the future. When determining a team member's base salary several factors may be considered as applicable including, but not limited to, relevant education, qualifications, certifications, experience, skills, seniority.
Be part of a growing global SaaS company, with offices in NYC, London, Paris, Hong-Kong
Work on latest Cloud technology and build architecture for fast-growing Tech
Weekly happy hours, good office culture, global cross team collaboration, direct access to executive leadership for guidance.
UpClear employees have access to a range of competitive benefits, including
Various Health Care Plans you can choose from to best fits your needs (Medical, Dental & Vision)
Retirement Plan with company match (401k, IRA)
Generous Paid Time Off package that grows with seniority (Vacation, Sick, and Public Holidays)
Paid Maternity leave
Paid Parental bonding leave
One month paid sabbatical after five continuous years of work at Upclear
Hybrid work model
Competitive Salary ($140K - $190K)
Show more
Show less","Data Engineering, SQL, Data Warehousing, TSQL, ETL, GIT, Agile Development, CI/CD, Python, C#, Azure, AWS, GCP, Bash, Powershell","data engineering, sql, data warehousing, tsql, etl, git, agile development, cicd, python, c, azure, aws, gcp, bash, powershell","agile development, aws, azure, bash, c, cicd, data engineering, datawarehouse, etl, gcp, git, powershell, python, sql, tsql"
Data Engineer II,Glidewell Dental,"Irvine, CA",https://www.linkedin.com/jobs/view/data-engineer-ii-at-glidewell-dental-3769526638,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Essential Functions:
Analyze data to discover and interpret trends, patterns, and relationships
Responsible for the integrity of automated reports
Design, implement and analyze controlled experiments to assess and optimize new opportunities across business channels
Analyze and Identify best-performing customer segments to help strategize precise targeting methods
Develop and maintain reporting dashboards to support decision-making among data analysts and the Technical Solutions team.
Monitor and evaluate trends for performance and opportunities
Work cross-functionally to establish instrumentation and reporting standards
Create dashboards and reports to communicate actionable data
Review data quality and provide guidance and controls to upstream data providers and sources
Perform ad hoc data requests
Stay current on industry tools, techniques and competitor marketing strategies.
Manage delivery of scheduled and ad hoc reports, noting business trends in new customer counts, order / sales impact of marketing triggers and promoted product performance.
Analyze results and develop performance improvement opportunities
Responsibilities:
Bachelor’s degree in Mathematics, Statistics, or other quantitative field
5+ years of data analytics experience
Experience with Tableau
Tableau Qualified Associate Certification preferred
Experience with SQL, Postgres, and RedShift a plus
High proficiency in Excel modeling, data mining, and scenario analysis
Highly analytical and quantitative, with strong attention to detail
Self-starter with excellent written and verbal communication as well as interpersonal skills
Ability to thrive in a fast-paced, ambiguous, interruptive environment
Ability to work independently as well as work collaboratively toward a common goal
Pay Band: $115,000 - $135,000/ year
Glidewell Laboratories is the industry leader in dental technology due to our agility, speed, and cutting edge technology. We work in a fast-paced and highly sought-after employee-friendly work environment. Behind all of this success is an amazing group of people who are passionate about bringing innovation to the marketplace, while providing quality and affordability to better the lives of people all over the world. If you share in our passion for teamwork and a vision for excellence, let's talk about a rewarding career at Glidewell!
In addition are the following generous employee benefits: Medical, Dental, Vision, 401K with company match, company-paid life insurance, additional onsite dental services, vacation, holiday, and sick time, employee gym (with fitness classes and meditation room), employee medical/wellness center (with massage therapy and acupuncture), two company subsidized cafes, Internet cafes, employee lounges with big screen TVs, game tables, fun company sponsored events, a diverse work environment with over forty nationalities represented, and much more!
Glidewell Laboratories is an Equal Opportunity Employer and prohibits any kind of unlawful discrimination and harassment. We are committed to the principle of equal employment opportunity for all employees and to provide employees with a work environment free of discrimination and harassment on the basis of race, color, religion, national origin, sex, age, physical or mental disability, veteran status, sexual orientation, gender identity, genetic information, or any other status protected by the statutes, rules, and regulations in the locations where it operates. If you are an individual with a disability and need a reasonable accommodation to assist with your job search or application for employment, please contact us at recruitment@glidewelldental.com. Please indicate the specifics of the assistance needed.
Show more
Show less","Data analysis, Data mining, Data visualization, Tableau, SQL, Postgres, RedShift, Excel, Scenario analysis, Statistical modeling, Business intelligence, Data warehousing, Data quality assurance, Data governance, Machine learning, Artificial intelligence, Big data, Cloud computing","data analysis, data mining, data visualization, tableau, sql, postgres, redshift, excel, scenario analysis, statistical modeling, business intelligence, data warehousing, data quality assurance, data governance, machine learning, artificial intelligence, big data, cloud computing","artificial intelligence, big data, business intelligence, cloud computing, data governance, data mining, data quality assurance, dataanalytics, datawarehouse, excel, machine learning, postgres, redshift, scenario analysis, sql, statistical modeling, tableau, visualization"
Data Engineer,Scipher Medicine,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/data-engineer-at-scipher-medicine-3774019742,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Scipher Medicine is a precision immunology company that uses AI and network science to match patients with the most effective therapies. Its flagship product, PrismRA, is a blood-based test predicting whether a patient with RA will likely respond to anti-TNF therapy. Scipher Medicine is also developing the Spectra platform, which can be used to discover and validate new drug targets for various autoimmune diseases. The company has made significant progress quickly, with its PrismRA test already being used by clinicians in the US and Europe, and its Spectra platform has the potential to revolutionize drug development for autoimmune diseases.
Overview
We are seeking a skilled and autonomous Data Engineer to join our team. We are interested in candidates with experience designing, developing, and maintaining our data ingestion processes into Data Warehouses and other data pipelines.
This is a full-time position based in our Salt Lake City office. As a key member of our team, you'll be at the forefront of our data infrastructure transformation. Our dynamic environment and cutting-edge technology offer a unique chance to bring your innovative data pipeline concepts to life rapidly.
Role Requirements
Ability to translate business requirements,
into effective data models in a data warehouse.
Autonomous learning.
We’ll need folks who enjoy parsing large swaths of software documentation and gain sufficient prowess quickly to turn around and work on POCs that can be veritable candidates for production-ready processes/pipelines.
Advanced SQL skills,
including schema design, CTEs, complex stored procedures, task etc. You’re familiar with standard data types but also variant types (json, xml). Extra points if you’re familiar with Snowflake-flavored syntax and features (stages, external functions, snowpipes, python UDFs, external tables, etc.).
Vast experience building DBT models
. Our workflows/pipelines are becoming more complex, and we’d like to implement the scalability, extensibility, and unit-testing features of DBT. Extra points if you have experience with both the open source & cloud versions and can help us navigate that decision.
DevOps and CI/CD experience
. Automating the process of promoting through dev/test/prod workflows. Extra points for Terraform experience.
Strong Python OOP
We're seeking individuals well-versed in class utilization, inheritance, and functional programming. Live debugging and Python package creation should be second nature to you. Your code is clean, pythonic, and readable, and you appreciate a great formatter.
Experience standing up an Orchestration Tool.
We’ve got our eye on Prefect Orchestration.
You’re great at creating
and
learning from internal documentation –
we’re great at documenting. You understand that documentation (or playbooks as we call them) are living documents, not historical relics – and you update the playbook whenever an issue arises so we’re better prepared for next time.
Experience being on call
on a rotating schedule.
Nice to have:
Understanding of HL7 -
dev
AWS Experience –
Points if you can speak to expertise in SAM, Lambda, S3, boto3, API gateway, or anything else you see as obviously missing from this list and can explain to us why.
GCP Experience –
Points if you can speak to cloud functions, GCS, Big Query, ML related workflows, or anything else you see as obviously missing from this list and can explain to us why.
Salesforce API v51+
- We do a lot of reverse ETL work with Salesforce.
Streamlit Experience
– We’d like to create some Snowflake Streamlit apps (or maybe even self-standing) at some point.
Scipher
is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex,national origin, sexual orientation, gender identity, disability status,protected veteran status, or any other characteristic protected by law.
Show more
Show less","Data Warehousing, Data Pipelines, Advanced SQL, Snowflake, DBT, DevOps, CI/CD, Python OOP, Prefect Orchestration, AWS, HL7, Salesforce API, Streamlit","data warehousing, data pipelines, advanced sql, snowflake, dbt, devops, cicd, python oop, prefect orchestration, aws, hl7, salesforce api, streamlit","advanced sql, aws, cicd, datapipeline, datawarehouse, dbt, devops, hl7, prefect orchestration, python oop, salesforce api, snowflake, streamlit"
Senior Data Engineer,Finfare,"Irvine, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-finfare-3786225924,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"About Finfare
Finfare is dedicated to empowering SMBs, by providing the ultimate expense management solution for growing businesses seeking to streamline their financial operations and maximize cash back rewards. Finfare offers easy expense tracking, personalized spend controls, seamless accounting integration, powerful analytics, and reliable charge card services. Our cardholders can tap into our vast network of reward program publishers and card-linked merchant offers, ensuring even bigger cash back rewards for their business.
We are always working on redefining the boundaries of digital financial services and aim to stay one step ahead in the ever-evolving fintech landscape.
About The Role
As a Senior Data Engineer at Finfare, you will play a pivotal role in developing data pipelines and administering relational databases. In this rapidly evolving field, you will be responsible for maintaining a robust data infrastructure and staying up-to-date with emerging techniques and technologies. We are seeking a proactive and adaptable individual who can deliver high-quality code, design data models, and ensure the security and reliability of our data systems.
Responsibilities
Define and deliver high-quality code and queries for feature development and bug fixes with minimal assistance.
Enhance and support the database/warehouse to ensure its robustness and reliability.
Design data models and implement data pipelines in accordance with established software development guidelines and data security standards.
Assist with database administration, scripting, and the development of data structures while managing tables and access controls.
Demonstrate a strong proficiency in SQL and database design principles.
Utilize extensive experience with MySQL databases and big data technologies.
Manage data warehousing and work with large data sets effectively.
Collect data and build data pipelines to ensure data availability and accuracy.
Other duties as assigned.
Qualifications And Requirements
Bachelor's degree in a related field
4-7 years of experience in data engineering or a related field.
Proficiency in Python and SQL for creating and maintaining database pipelines.
Familiarity with data platform technologies (preferred but not mandatory).
Strong database administration skills, including the ability to design and maintain databases and data models.
Experience with data partitioning and data extraction to manage and scale database size effectively.
Bonus Points
Previous experience in a fintech or data-intensive industry.
Knowledge of data security best practices and data governance.
A proactive mindset with a willingness to learn and adapt to emerging technologies and techniques.
Compensation
The salary range for this position is $150K- 180K (depending on experience).
Benefits at Finfare
Competitive health, vision, and dental benefits (covering 100% premium for employee and all dependent(s))
Unlimited PTO
401K (with employer matching)
Parental leave
Employee stock purchase plan (if applicable)
Health and wellness reimbursement
Catered lunches and weekly lunch stipend
Hybrid
Work sponsorship (if applicable)
Other employee perks
As part of our dedication to the diversity of our workforce, Finfare is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, gender, gender identity, gender expression, sexual orientation, age, physical or mental disability, medical condition, marital/domestic partner status, military and veteran status, genetic information or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances.
Show more
Show less","Python, SQL, MySQL, Data pipelines, Data modeling, Data security, Data warehousing, Big data technologies, Data partitioning, Data extraction, Data governance, Data visualization, Data analytics, Software development, Database administration, Database design, Charge card services, Analytics, Accounting integration","python, sql, mysql, data pipelines, data modeling, data security, data warehousing, big data technologies, data partitioning, data extraction, data governance, data visualization, data analytics, software development, database administration, database design, charge card services, analytics, accounting integration","accounting integration, analytics, big data technologies, charge card services, data extraction, data governance, data partitioning, data security, dataanalytics, database administration, database design, datamodeling, datapipeline, datawarehouse, mysql, python, software development, sql, visualization"
Data Engineer,Advantis Global,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-advantis-global-3777174117,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"About This Featured Opportunity
As a Data Engineer on our team, you will focus on data engineering and software engineering, with two main tentpoles of responsibility. Firstly, you will play a pivotal role in developing and maintaining data pipelines, facilitating the smooth movement of data from various systems to others, making it accessible to our data analytics team. The second tentpole involves working on a third-party project, utilizing on-premises data analytics tool to enable seamless connections between Tableau and our data lake, empowering SQL queries on extensive datasets.
THE OPPORTUNITY FOR YOU
Your day-to-day responsibilities will involve troubleshooting bugs and issues, interacting with the Data Analytics team and supporting the data warehousing environment.
KEY SUCCESS FACTORS
3-5 years of Python Development experience
Proficiency in Python development beyond scripting, showcasing the ability to create applications, systems, and implement complex software design patterns.
Experience in building data pipelines and engineering solutions.
Experience working with Data Analytics and Tools
Hands-on experience with data analytics tools, particularly Dremio, and expertise in ETL functions for moving data efficiently.
Proficiency in using Pandas for data manipulation and transformation.
Experience with Data Warehousing and Big Data Analytics:
Experience working with data warehouses and understanding how to store large amounts of data efficiently.
Familiarity with big data analytics techniques and tools like Trino and HDFS on Hive.
Benefits
Company sponsored Health, Dental and Vision
Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit, qualifications and abilities. Company policy prohibits unlawful discrimination based on race, color, religion, sex (including gender, gender identity, gender expression, pregnancy, childbirth or medical condition related to pregnancy or childbirth), sexual orientation, national origin, ancestry, age, physical or mental disability, genetic information, political affiliation, union membership, marital or registered domestic partnership status, military or veteran status or any other characteristic protected by law (“Protected Characteristic”). Additionally, Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
#AGIT
Show more
Show less","Python, Pandas, SQL, ETL, Dremio, Data Warehousing, Big Data Analytics, Trino, HDFS, Hive","python, pandas, sql, etl, dremio, data warehousing, big data analytics, trino, hdfs, hive","big data analytics, datawarehouse, dremio, etl, hdfs, hive, pandas, python, sql, trino"
Data Engineer,Accroid Inc,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3763529974,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Ohio
JAVA 8/11, Jenkins, GitHub, SQL, Embedded Tomcat, Spring boot, Swagger, Micro Services Kafka, Mortgage Servicing Platform
Cobol, LoanServ, Snowflake
Strong knowledge of data warehousing solutions and relational SQL and NoSQL databases, Snowflake, MS SQL Server a plus
Experience with AWS cloud services: EC2, RDS, MSK, Lambda
Experience with object-oriented/object function scripting languages a plus
Solid analytical skills and the ability to understand complex business requirements.
Show more
Show less","Java 8/11, Jenkins, GitHub, SQL, Apache Tomcat, Spring Boot, Swagger, Kafka, Mortgage Servicing Platform, COBOL, LoanServ, Snowflake, Data warehousing solutions, Relational SQL, NoSQL databases, AWS, EC2, RDS, MSK, Lambda, Objectoriented scripting languages, Object function scripting languages","java 811, jenkins, github, sql, apache tomcat, spring boot, swagger, kafka, mortgage servicing platform, cobol, loanserv, snowflake, data warehousing solutions, relational sql, nosql databases, aws, ec2, rds, msk, lambda, objectoriented scripting languages, object function scripting languages","apache tomcat, aws, cobol, data warehousing solutions, ec2, github, java 811, jenkins, kafka, lambda, loanserv, mortgage servicing platform, msk, nosql databases, object function scripting languages, objectoriented scripting languages, rds, relational sql, snowflake, spring boot, sql, swagger"
AWS Data Engineer,LTI - Larsen & Toubro Infotech,"Hartford, CT",https://www.linkedin.com/jobs/view/aws-data-engineer-at-lti-larsen-toubro-infotech-3716353452,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"A little about us...
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world.
· We provide our employees with a learning environment that promotes growth and creativity.
· To learn more plase visit us at https://www.ltimindtree.com/Twitter @ https://twitter.com/ltimindtree
""Role: Senior Developer
Experience 4-6 Years
Location: USA – Hartford, CT
Must Have: AWS (S3), Python, Databricks
Good to have: Snowflake, Spark
Overview:
Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming analytics landscape.
Looking for Data Migration Engineer having an experience migrating data from on prem to cloud.
Must Have: AWS (S3), Python, Databricks
Good to have: Snowflake, Spark
Requirements:
• Strong python skills
• AWS/Cloud infrastructure knowledge (commonly used AWS services, IAM)
• Experience with building data pipelines using Databricks on AWS
• Knowledge and Hands-on Snowflake
• Experience in Agile methodologies and Atlassian tools like JIRA.
• Expertise in using version control tools like Git, Bitbucket
• Experience on CI/CD using Kubernetes, GIT and Monitoring and Alerting tools
• Experience on data migration from On-Prem databases to AWS Cloud on S3
Roles & resposibilities:
• Acts as a single point of contact for data migration to AWS projects for customer
• Provides innovative and cost-effective solution using AWS, Spark, python & databricks
• Develops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
• As a leader in the Cloud Engineering you will be responsible for the overseeing development
• Learn/adapt quickly to new Technologies as per the business need
• Develop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability
• Understand where to obtain information needed to make the appropriate decisions
• Demonstrate ability to break down a problem to manageable pieces and implement effective, timely solutions
• Identify the problem versus the symptoms
• Develop solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
Skills:
• The Candidate must have 3-5 yrs of experience in AWS, Python, Databricks
• Hands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR
• Experience on spark scripting
• Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform
• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
• Strong experience with relational databases and data access methods, especially SQL.
• Knowledge of Amazon AWS architecture and design
Minimum Qualifications
• Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.
• Four years of data engineering or equivalent experience.
""How will you grow?
· Role-based Training programs
· Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional
· We encourage you to acquire various beneficial international certifications, with costs s reimbursed
· Our role-based workshop helps us groom future leaders for LTI
What's in it for you?
· Excellent benefits plan: medical, dental, vision, life, FSA, & PTO
· Roll over vacation days
· Commuter benefits
· Excellent growth and advancement opportunities
· Certification reimbursement
· Rewards and recognition programs
· Innovative and collaborative company culture
We are an
Equal Opportunity/Affirmative Action employer
. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristic protected by law.
Show more
Show less","AWS, S3, Python, Databricks, Snowflake, Spark, Agile, JIRA, Git, Bitbucket, Kubernetes, Monitoring, Alerting, SQL, Data pipelines, ETL, Data warehouses, Relational databases, Cloud infrastructure, Version control, CI/CD","aws, s3, python, databricks, snowflake, spark, agile, jira, git, bitbucket, kubernetes, monitoring, alerting, sql, data pipelines, etl, data warehouses, relational databases, cloud infrastructure, version control, cicd","agile, alerting, aws, bitbucket, cicd, cloud infrastructure, data warehouses, databricks, datapipeline, etl, git, jira, kubernetes, monitoring, python, relational databases, s3, snowflake, spark, sql, version control"
Senior Data Engineer,Genpact,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-genpact-3784837554,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Responsibilities:
Design and development of large-scale data solutions with Python and build robust data pipelines and dynamic systems.
Configure Cloudera environment to deploy ETL pipelines.
Keep Updated with Cloudera environment.
Support different teams on deploying tools and configuring for development, testing and Production.
Collaborates with different client teams, to develop and maintain long-term relationships with key stakeholders.
Deep engagement and consultation with all business teams to understand current and future needs.
Works with onshore team in establishing design patterns and development standards. Conducts code reviews and overseas unit testing.
Collaborates between onshore and offshore teams for project plan, code reviews, QA, and deployments.
Brainstorm with development team on optimizing existing dataflow, quality, and performance tuning, and building proof-of-concepts.
Qualifications we seek in you!
Minimum Qualifications
Professional graduate/post-graduate degree in Computer Science or information technology discipline
Preferred Qualifications/ Skills
Subject Matter Expert in Python and it’s libraries, ETL pipelines, Web Services.
Expert-level programming experience in Python.
Expert level Shell Scripting.
Hands-on experience in DevOps tools – GitHub, Maven, Jenkins, Docker, and Implementation of CI/CD pipelines
Hands-on experience in Cloudera Admin activities.
Experience in any cloud platform.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
Handling entire software lifecycle: requirement gathering, project planning and status reporting to various stakeholders
End-to-end project handling, requirements gathering, designing, and implementing effective pipelines.
Show more
Show less","Python, Shell Scripting, ETL pipelines, Web Services, DevOps tools, GitHub, Maven, Jenkins, Docker, CI/CD pipelines, Cloudera Admin, Cloud platform, Large data processing, Data integration, Software lifecycle, Project planning, Status reporting, Pipeline design, Pipeline implementation","python, shell scripting, etl pipelines, web services, devops tools, github, maven, jenkins, docker, cicd pipelines, cloudera admin, cloud platform, large data processing, data integration, software lifecycle, project planning, status reporting, pipeline design, pipeline implementation","cicd pipelines, cloud platform, cloudera admin, data integration, devops tools, docker, etl pipelines, github, jenkins, large data processing, maven, pipeline design, pipeline implementation, project planning, python, shell scripting, software lifecycle, status reporting, web services"
Data Engineer(Python),HireKeyz Inc,"Wilmington, DE",https://www.linkedin.com/jobs/view/data-engineer-python-at-hirekeyz-inc-3782011117,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position: Data Engineer(Python)
Location: Wilmington, DE (Onsite)
Duration: Fulltime
Job Description
9+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
5+ years of experience migrating/developing data solutions in the AWS cloud is required.
2+ years of experience building/implementing data pipelines using Databricks or similar cloud database.
Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark and Experience in Spark.
Knowledge or experience in architectural best practices in building data lakes
Show more
Show less","Data Engineering, Python, AWS, Cloud Computing, Databricks, SQL, ObjectOriented Programming, Apache Spark, Data Lakes, Data Pipelines","data engineering, python, aws, cloud computing, databricks, sql, objectoriented programming, apache spark, data lakes, data pipelines","apache spark, aws, cloud computing, data engineering, data lakes, databricks, datapipeline, objectoriented programming, python, sql"
Senior Data Engineer,"Double Line, Inc.","Raleigh, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-double-line-inc-3787736763,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Development team with an experienced and innovative Senior Data Engineer with impressive analytical skills. Sound interesting?
If so, we're looking for a motivated and driven person like you who has:
Successfully completed multiple projects where you designed and executed ways to solve complex problems for clients around data integration, data quality, data warehousing, and analytics
Demonstrated proficiency in deciding which ETL and data streaming technologies to use in AWS, Google Cloud, and Azure-based solutions, and propensity to pick something new when you want to push yourself and the team to innovate
Mastery of T-SQL and experience with postgreSQL or other forms of SQL
Experience in an Agile environment with Lean software development principles
Drive to amplify the skills of teammates through mentoring and training junior and mid-level data engineers
Mindset of continuous improvement and setting best practices
Deadline-driven mentality
Bonus points if you're bringing knowledge of or really want to learn the following:
A wide variety of data processing tools and approaches, from Python to Google BigQuery to SSIS to AWS Lambda to Azure Data Factory and others
Performance implications of memory and disk usage at different data volumes
Business intelligence tools and dashboard design theory
We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Bring a new idea to our team of brilliant data engineers in the first 30 days
Lead the collaborative design process of a data engineering solution in one of our projects in the first 2 months
Become a mentor to a data engineer within your first 6 months
In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that challenge state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
Direct connection to the Executive team where you can help drive the future of the company
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture
We need to know - can you make this happen? If so, we definitely need to talk to you.
Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.
Double Line does not currently offer relocation assistance.
Powered by JazzHR
wb4SoyF12N
Show more
Show less","Data Integration, Data Quality, Data Warehousing, Analytics, ETL, Data Streaming, AWS, Google Cloud, Azure, TSQL, PostgreSQL, SQL, Agile, Lean Software Development, Mentoring, Training, Python, Google BigQuery, SSIS, AWS Lambda, Azure Data Factory, Business Intelligence Tools, Dashboard Design","data integration, data quality, data warehousing, analytics, etl, data streaming, aws, google cloud, azure, tsql, postgresql, sql, agile, lean software development, mentoring, training, python, google bigquery, ssis, aws lambda, azure data factory, business intelligence tools, dashboard design","agile, analytics, aws, aws lambda, azure, azure data factory, business intelligence tools, dashboard design, data integration, data quality, data streaming, datawarehouse, etl, google bigquery, google cloud, lean software development, mentoring, postgresql, python, sql, ssis, training, tsql"
Data Engineer,Accroid Inc,"Inver Grove Heights, MN",https://www.linkedin.com/jobs/view/data-engineer-at-accroid-inc-3766662568,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Onsite
Data Engineer
MN
12+ Months
Administer and configure the Alation data catalog to ensure optimal performance and functionality.
Maintain data catalogs, glossaries, and data dictionaries, ensuring they are up-to-date and accurate.
Implement metadata standards and taxonomies to ensure consistency and ease of data discovery.
Work with data stewards and subject matter experts to capture, curate, and document metadata for various datasets.
Establish and enforce metadata quality standards to enhance data reliability and trustworthiness.
Lead the planning and execution of data catalog changes, including new data sources, metadata updates, and system enhancements.
Communicate changes effectively to stakeholders, addressing concerns and ensuring a smooth transition.
Show more
Show less","Data Catalog Administration, Data Catalog Configuration, Metadata Standards, Data Taxonomy, Data Discovery, Data Stewardship, Data Curation, Data Documentation, Metadata Quality Standards, Data Reliability, Data Trustworthiness, Data Catalog Planning, Data Catalog Execution, Data Sources, Metadata Updates, System Enhancements, Stakeholder Communication","data catalog administration, data catalog configuration, metadata standards, data taxonomy, data discovery, data stewardship, data curation, data documentation, metadata quality standards, data reliability, data trustworthiness, data catalog planning, data catalog execution, data sources, metadata updates, system enhancements, stakeholder communication","data catalog administration, data catalog configuration, data catalog execution, data catalog planning, data curation, data discovery, data documentation, data reliability, data sources, data stewardship, data taxonomy, data trustworthiness, metadata quality standards, metadata standards, metadata updates, stakeholder communication, system enhancements"
Data Engineer,PA Consulting,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-pa-consulting-3752017186,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Company Description
We believe in the power of ingenuity to build a positive human future.
As strategies, technologies and innovation collide, we create opportunity from complexity.
Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results.
An innovation and transformation consultancy, we are over 4000 specialists in consumer and manufacturing, defence and security, energy and utilities, financial services, government and public services, health and life sciences, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists. We operate globally from offices across the UK, US, Netherlands and Nordics.
PA. Bringing Ingenuity to Life
Job Description
Your day to day
We’re an innovation and transformation consultancy that believes in the power of ingenuity to build a positive-human future in a technology-driven world. Our diverse teams of experts combine innovative thinking with breakthrough-technologies to progress further, faster.
Are you ready to harness the power of data to drive advancements in healthcare? Are you passionate about designing, building, and maintaining data infrastructure that plays a pivotal role in improving patient outcomes and shaping the future of medicine? If you're seeking a rewarding career at the intersection of healthcare and technology, we invite you to be part of our dynamic team. This is a unique, multi-year, project-based opportunity to build and grow a clinical data registry platform over many years working with a dedicated team of collaborators and customers. As a Data Engineer for our cutting-edge medical data registry, you'll be at the forefront of managing, optimizing, and expanding our data infrastructure, enabling critical insights that can positively impact patient outcomes. If you're excited about leveraging your data engineering skills to make a difference in the world of healthcare, we want to hear from you.
Qualifications
Minimum qualifications:
Advanced SQL and Python
Expertise in the design and construction of Big Data Lakes and Data Warehouses capable of ingesting, standardizing, and serving billions of data rows spanning diverse datasets ranging from tens to hundreds
Experience building dynamic, metadata driven pipelines and analyses
Building and managing fully automated data pipelines (ETL, ELT, ELTL) including:
Designing and building data interfaces to source systems
Combining and transforming data into the appropriate format for storage
Developing data sets for analytics purposes
Developing pipelines that can handle common issues/errors in a robust and automated way
Cloud experience in Azure, AWS or GCP
Preferred qualifications:
Spark / PySpark experience highly preferable
Working in Agile and DevOps environments
Basic Python, Bash, or PowerShell for automation
Data modelling – Kimball, Data Vault, Star/Snowflake schema, Query-first etc.
Data visualisation in Power BI, Tableau, Qlik or similar
Architecting Data Platforms - designing BI/MI/Analytics solutions using Big Data, Relational or Streaming technologies
One or more of the following certifications:
Microsoft Certified: Azure Data Engineer Associate
AWS Certified Data Analytics - Specialty
GCP Professional Data Engineers
Additional Information
Life At PA encompasses our peoples' experience at PA. It's about how we enrich peoples’ working lives by giving them access to unique people and growth opportunities and purpose led meaningful work.
We believe diversity fuels ingenuity. Diversity of thought brings exciting perspectives; diversity of experience brings a wealth of knowledge, and diversity of skills brings the tools we need. When we bring people together with diverse backgrounds, identities, and minds, embracing that difference through an inclusive culture where our people thrive; we unleash the power of diversity – bringing ingenuity to life. We are dedicated to supporting the physical, emotional, social and financial well-being of our people.
The Salary for this role is between $90,000 - $110,000
Show more
Show less","SQL, Python, Big Data Lakes, Data Warehouses, Data Pipelines, ETL, ELT, ELTL, Azure, AWS, GCP, Spark, PySpark, Agile, DevOps, Bash, PowerShell, Kimball, Data Vault, Star/Snowflake schema, Queryfirst, Power BI, Tableau, Qlik, BI/MI/Analytics, Microsoft Certified: Azure Data Engineer Associate, AWS Certified Data Analytics  Specialty, GCP Professional Data Engineers","sql, python, big data lakes, data warehouses, data pipelines, etl, elt, eltl, azure, aws, gcp, spark, pyspark, agile, devops, bash, powershell, kimball, data vault, starsnowflake schema, queryfirst, power bi, tableau, qlik, bimianalytics, microsoft certified azure data engineer associate, aws certified data analytics specialty, gcp professional data engineers","agile, aws, aws certified data analytics specialty, azure, bash, big data lakes, bimianalytics, data vault, data warehouses, datapipeline, devops, elt, eltl, etl, gcp, gcp professional data engineers, kimball, microsoft certified azure data engineer associate, powerbi, powershell, python, qlik, queryfirst, spark, sql, starsnowflake schema, tableau"
Data Engineer,Steneral Consulting,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3764173559,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Cincinnati Ohio 45227 - onsite - Local candidates preferred but we can consider remote for Extraordinary candidate
Candidate must have mortgage experience
.
Job Description
Job Overview:
We are seeking a skilled Data Engineer with a strong background in mortgage banking to join our IT team. The candidate will be instrumental in constructing, testing, and maintaining our mortgage banking data architecture. This includes optimizing data flow and collection to ultimately support data analysis and decision-making processes.
Responsibilities
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices for mortgage banking.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Translate complex functional and technical requirements into detailed architecture, design, and high-performing software.
Integrate new data management technologies and software engineering tools into existing structures.
Create data tools for analytics and line of business that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain a secure and compliant data processing environment in line with industry regulations.
Qualifications
Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.
Proven experience as a Data Engineer, preferably in the mortgage banking industry.
Strong knowledge of data warehousing solutions and relational SQL and NoSQL databases, Snowflake, MS SQL Server a plus.
Experience with AWS cloud services: EC2, RDS, MSK, Lambda.
Experience with object-oriented/object function scripting languages a plus.
Solid analytical skills and the ability to understand complex business requirements.
Familiarity with data pipeline and workflow management tools: dbt, Apache Kafka, Snowflake data pipeline/streams.
Knowledge of financial and mortgage banking principles.
Strong organizational and interpersonal skills, with the ability to manage tasks and timelines effectively.
Must Have
JAVA 8/11, Jenkins, GitHub, SQL, Embedded Tomcat, Spring boot, Swagger, Micro services kafka msp
Nice To Have
Cobol , LoanServ msp
Show more
Show less","Data Engineer, Mortgage banking, Data architecture, Data management systems, Data analysis, Predictive models, Software engineering tools, Data tools, Data warehousing solutions, Relational SQL, NoSQL databases, Snowflake, MS SQL Server, AWS cloud services: EC2 RDS MSK Lambda, Objectoriented/object function scripting languages, Analytical skills, Business requirements, Data pipeline and workflow management tools: dbt Apache Kafka Snowflake data pipeline/streams, Financial and mortgage banking principles, Organizational and interpersonal skills, Task and timeline management, JAVA 8/11, Jenkins, GitHub, SQL, Embedded Tomcat, Spring boot, Swagger, Microservices Kafka MSP, Cobol, LoanServ MSP","data engineer, mortgage banking, data architecture, data management systems, data analysis, predictive models, software engineering tools, data tools, data warehousing solutions, relational sql, nosql databases, snowflake, ms sql server, aws cloud services ec2 rds msk lambda, objectorientedobject function scripting languages, analytical skills, business requirements, data pipeline and workflow management tools dbt apache kafka snowflake data pipelinestreams, financial and mortgage banking principles, organizational and interpersonal skills, task and timeline management, java 811, jenkins, github, sql, embedded tomcat, spring boot, swagger, microservices kafka msp, cobol, loanserv msp","analytical skills, aws cloud services ec2 rds msk lambda, business requirements, cobol, data architecture, data management systems, data pipeline and workflow management tools dbt apache kafka snowflake data pipelinestreams, data tools, data warehousing solutions, dataanalytics, dataengineering, embedded tomcat, financial and mortgage banking principles, github, java 811, jenkins, loanserv msp, microservices kafka msp, mortgage banking, ms sql server, nosql databases, objectorientedobject function scripting languages, organizational and interpersonal skills, predictive models, relational sql, snowflake, software engineering tools, spring boot, sql, swagger, task and timeline management"
Data Engineer,Steneral Consulting,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3762801435,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Cincinnati Ohio 45227 - onsite - Local candidates only
Job Description
Job Overview:
We are seeking a skilled Data Engineer with a strong background in mortgage banking to join our IT team. The candidate will be instrumental in constructing, testing, and maintaining our mortgage banking data architecture. This includes optimizing data flow and collection to ultimately support data analysis and decision-making processes.
Responsibilities
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices for mortgage banking.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Translate complex functional and technical requirements into detailed architecture, design, and high-performing software.
Integrate new data management technologies and software engineering tools into existing structures.
Create data tools for analytics and line of business that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain a secure and compliant data processing environment in line with industry regulations.
Qualifications
Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.
Proven experience as a Data Engineer, preferably in the mortgage banking industry.
Strong knowledge of data warehousing solutions and relational SQL and NoSQL databases, Snowflake, MS SQL Server a plus.
Experience with AWS cloud services: EC2, RDS, MSK, Lambda.
Experience with object-oriented/object function scripting languages a plus.
Solid analytical skills and the ability to understand complex business requirements.
Familiarity with data pipeline and workflow management tools: dbt, Apache Kafka, Snowflake data pipeline/streams.
Knowledge of financial and mortgage banking principles.
Strong organizational and interpersonal skills, with the ability to manage tasks and timelines effectively.
Must Have
JAVA 8/11, Jenkins, GitHub, SQL, Embedded Tomcat, Spring boot, Swagger, Micro services kafka msp
Nice To Have
Cobol , LoanServ msp
Show more
Show less","Data Engineering, Mortgage Banking, Data Architecture, Data Analysis, Decision Making, Data Management Systems, SQL, NoSQL, Snowflake, AWS, EC2, RDS, MSK, Lambda, Apache Kafka, dbt, Spring Boot, Swagger, Microservices, Cobol, LoanServ, Jenkins, GitHub, Embedded Tomcat, Java","data engineering, mortgage banking, data architecture, data analysis, decision making, data management systems, sql, nosql, snowflake, aws, ec2, rds, msk, lambda, apache kafka, dbt, spring boot, swagger, microservices, cobol, loanserv, jenkins, github, embedded tomcat, java","apache kafka, aws, cobol, data architecture, data engineering, data management systems, dataanalytics, dbt, decision making, ec2, embedded tomcat, github, java, jenkins, lambda, loanserv, microservices, mortgage banking, msk, nosql, rds, snowflake, spring boot, sql, swagger"
Python/ Data Engineer,HireKeyz Inc,"Plano, TX",https://www.linkedin.com/jobs/view/python-data-engineer-at-hirekeyz-inc-3765083210,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Role:
Python/ Data Engineer
Type: Fulltime
Location: Plano, TX/Wilmington, DE(Day1 Onsite)
Jd
4-6+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
4+ years of experience migrating/developing data solutions in the AWS cloud is required.
2+ years of experience building/implementing data pipelines using Databricks or similar cloud database.
Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark and Experience in Spark.
Knowledge or experience in architectural best practices in building data lakes
Show more
Show less","Python, Data Pipeline, Cloud Environment, AWS, Databricks, Cloud Database, SQL, ObjectOriented Programming, Spark, Data Lake, Data Stream","python, data pipeline, cloud environment, aws, databricks, cloud database, sql, objectoriented programming, spark, data lake, data stream","aws, cloud database, cloud environment, data lake, data pipeline, data stream, databricks, objectoriented programming, python, spark, sql"
Senior Data Engineer,Infosys,"Austin, Texas Metropolitan Area",https://www.linkedin.com/jobs/view/senior-data-engineer-at-infosys-3776124015,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"In the role of
Lead Data Engineer
, you will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.
Required Qualifications:
Candidate must be located within commuting distance of
Austin, TX or
be willing to relocate to
Austin, TX
. This position may require travel within the US and Canada.
Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education
At least 4 years of experience in Information Technology.
At least 3 years of hands on experience with Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems.
At least 2 years of experience with Spark required.
At least 2 years of experience with Scala or Python required.
U.S. citizens and GC holders are encouraged to apply.
Preferred Qualifications:
At least 1 years of AWS development experience is preferred
Ability to work within deadlines and effectively prioritize and execute on tasks.
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels.
Experience in Drive automations
DevOps Knowledge is an added advantage.
Proficient knowledge of SQL with any RDBMS.
About Us
Infosys is a global leader in next-generation digital services and consulting. We enable clients in 50 countries to navigate their digital transformation. With over three decades of experience in leading the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver outstanding levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.
Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.
To learn more about Infosys and see our perspectives in action please visit us at www.Infosys.com
Show more
Show less","Software Development Life Cycle, Hadoop, Spark, Scala, Python, AWS, SQL, RDBMS, DevOps","software development life cycle, hadoop, spark, scala, python, aws, sql, rdbms, devops","aws, devops, hadoop, python, rdbms, scala, software development life cycle, spark, sql"
Junior Data Engineer,ANSER,"Washington, DC",https://www.linkedin.com/jobs/view/junior-data-engineer-at-anser-3693336033,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Company Overview
ANSER enhances national and homeland security by strengthening public institutions. We provide thought leadership for complex issues through independent analysis, and we deliver practical, useful solutions. ANSER values collaboration, integrity, and initiative and we are client focused in all that we do. Because we were established for the purpose of public service and not for profit, we measure our success in the impact of our service.
Job Description
ANSER is seeking a Junior Data Engineer (Operational Suitability Specialist) to support programs at (Joint Base Anacostia-Bolling JBAB).
Responsibilities
Designs, implements, and operates data management systems for intelligence needs. Designs how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems. Works with data users to determine, create, and populate optimal data architectures, structures, and systems. Plans, designs, and optimizes data throughput and query performance. Participates in the selection of backend database technologies (e.g., SQL, NoSQL, HPC, etc.), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.
Experience providing operational suitability test support to test programs or support DIA in testing procedure development for externally funded developments.
Possesses extensive logistics and suitability experience, particularly in operations analysis and support, logistics, and maintenance.
Possesses a minimum 4 years of academic course work w/ current logistics/suitability experience with space systems or 6 years practical knowledge of space systems ground systems, ground segments, manufacturing and maintenance, and mission planning.
Experience serving as the suitability expert for issues relating to reliability, availability, compatibility, transportability, interoperability, maintainability, safety, human factors, manpower supportability, logistics supportability, environmental effects, system documentation, and training requirements; contribute to all facets of test design and planning using in-depth knowledge of system under test, test policies, and test range capabilities.
Experience assisting the Test Director in timely preparation of high-quality suitability analysis across the varied test venues during all test phases to inform system development and operational evaluation; developing space system suitability specific data collection, data processing, data evaluation and data reporting as well as proactively identify suitability gaps, risks, or other issues across several test programs.
Qualifications
Current TS/SCI with ability to pass a polygraph.
Minimum 3 years of experience conducting analysis relevant to the specific labor category, with at least a portion of the experience within the last 2 years.
Bachelor's degree in an area related to the labor category from a college or university accredited by an agency recognized by the U.S. Department of Education.
Disclaimer
In compliance with the Americans with Disabilities Act Amendment Act (ADA), if you have a disability and would like to request an accommodation in order to apply for a position with ANSER, please call 703-416-2000 or e-mail Recruiting@anser.org.ANSER is proud to be an Equal Opportunity Employer. We seek individuals from a broad variety of backgrounds with varying levels of experience who have a desire to do meaningful work. We recruit, employ, train, compensate, and promote regardless of race, color, gender, religion, national origin, ancestry, disability, age, veteran status, sexual orientation, or any other characteristic protected by law.
Show more
Show less","Data Management, SQL, NoSQL, HPC, Data Architecture, Data Structures, Data Systems, Data Throughput, Query Performance, Backend Database Technologies, Logistics, Suitability, Operations Analysis, Maintenance, Space Systems, Ground Systems, Ground Segments, Manufacturing, Mission Planning, System Under Test, Test Policies, Test Range Capabilities, Data Collection, Data Processing, Data Evaluation, Data Reporting, Suitability Gaps, Suitability Risks","data management, sql, nosql, hpc, data architecture, data structures, data systems, data throughput, query performance, backend database technologies, logistics, suitability, operations analysis, maintenance, space systems, ground systems, ground segments, manufacturing, mission planning, system under test, test policies, test range capabilities, data collection, data processing, data evaluation, data reporting, suitability gaps, suitability risks","backend database technologies, data architecture, data collection, data evaluation, data management, data processing, data reporting, data structures, data systems, data throughput, ground segments, ground systems, hpc, logistics, maintenance, manufacturing, mission planning, nosql, operations analysis, query performance, space systems, sql, suitability, suitability gaps, suitability risks, system under test, test policies, test range capabilities"
Digital Data Engineer,Experfy,"Cambridge, MA",https://www.linkedin.com/jobs/view/digital-data-engineer-at-experfy-3686079733,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Our client is looking for:
Work with business teams to understand requirements, and translate them into technical needs
Gather/organize large & complex data assets, and perform relevant analysis
Ensure the quality of the data in coordination with Data Analysts and Data Scientists (peer validation)
Propose and implement relevant data models for each business case
Create data models and optimize queries performance
Communicate results and findings in a structured way
Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan
Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements
Leverage existing or create new ""standard data pipelines"" within the company to bring value through business use cases
Ensure best practices in data manipulation are enforced end-to-end
Actively contribute to Data governance community
Remains up to date on company's standards, industry practices and emerging technologies
Requirements
Key Functional Requirements & Qualifications:
Experience working with a variety of cross-functional teams
Good understanding of agile/scrum development processes and concepts
Able to work in a fast-paced, constantly evolving environment and manage multiple priorities
Pragmatic and capable of solving complex issues
Service-oriented, flexible team player
Attention to detail & technical intuition
Excellent written, verbal, and interpersonal skills for executive level communication and collaboration
Fluent in English (Other languages a plus)
Key Technical Requirements & Qualifications:
Bachelor's Degree or equivalent in in Computer Science, Engineering, or relevant field
Experience with AWS cloud services (Azure & GCP a plus)
Good knowledge of SQL and relational databases technologies/concepts
Experience working with data models and query tuning
Experience in Data warehousing solutions (Snowflake a plus)
Experience in Integration Services (IICS, Tibco a plus)
Working knowledge of scripting languages (Python, R a plus)
Familiarity with Source Code Management Tools (GitHub a plus)
Familiarity with Visualization Tools (PowerBI, Tableau a plus)
Familiarity with Project Management Tools (JIRA, Confluence a plus)
Familiarity with Service Management Tools (Service Now a plus)
Experience working in life sciences/pharmaceutical industry is a plus
Relevant cloud certifications (AWS, Azure, Snowflake, IICS) are a plus
Experience on working within compliance (e.g.: quality, regulatory - data privacy, GxP, SOX) and cybersecurity requirements is a plus
Mentoring and/or technology evangelism/advocacy experience
Additions For Data Engineer - Database
Strong experience in automation tools and methodologies specifically using Gitlab, Github action, Terraform, Ansible
Experience with programming languages such as Python, JSON, YAML, Shell Scripting
Experience with backup system like Netbackup & CommVault
Good knowledge of ServiceNow and monitoring tool such as Splunk, BPPM
Additions for Data Engineer - RWE
Experience with Real World Data (e,g, EHR, Claims) and standard data models (e,g, OMOP, FHIR)
Experience using frameworks to create pipelines (e.g. Apache Airflow, Kedro)
Show more
Show less","Data analysis, Data modeling, SQL, Relational databases, Data warehousing, Integration Services, Python, R, Git, Visualization Tools, Project Management Tools, Service Management Tools, Cloud services, AWS, Azure, GCP, Snowflake, IICS, Tibco, GitHub, PowerBI, Tableau, JIRA, Confluence, Service Now, Gitlab, Github action, Terraform, Ansible, JSON, YAML, Shell Scripting, Netbackup, CommVault, ServiceNow, Splunk, BPPM, Real World Data, EHR, Claims, OMOP, FHIR, Apache Airflow, Kedro","data analysis, data modeling, sql, relational databases, data warehousing, integration services, python, r, git, visualization tools, project management tools, service management tools, cloud services, aws, azure, gcp, snowflake, iics, tibco, github, powerbi, tableau, jira, confluence, service now, gitlab, github action, terraform, ansible, json, yaml, shell scripting, netbackup, commvault, servicenow, splunk, bppm, real world data, ehr, claims, omop, fhir, apache airflow, kedro","ansible, apache airflow, aws, azure, bppm, claims, cloud services, commvault, confluence, dataanalytics, datamodeling, datawarehouse, ehr, fhir, gcp, git, github, github action, gitlab, iics, integration services, jira, json, kedro, netbackup, omop, powerbi, project management tools, python, r, real world data, relational databases, service management tools, service now, servicenow, shell scripting, snowflake, splunk, sql, tableau, terraform, tibco, visualization tools, yaml"
Data Engineer,Ampstek,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-ampstek-3775449209,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Job Title: Data Engineer
Locations: Sunnyvale, CA(Onsite)
Client: Tech Mahindra
Type: Contract
Skills Required
• Strong Python development and software design
• Dremio/Presto/Trino
• Snowflake (or other data warehouse systems)
• Tableau
• Docker & Kubernetes
• SQL
• MongoDB
• Object store (S3) & data lake concepts
• Git
• Shell & CLI tools
• REST APIs
• Pandas
Regards
Aatmesh
Sr.Talent Acquisition Specialist
aatmesh.singh@ampstek.com
Show more
Show less","Python, Dremio, Presto, Trino, Snowflake, Tableau, Docker, Kubernetes, SQL, MongoDB, Object store (S3), Data lake, Git, Shell & CLI tools, REST APIs, Pandas","python, dremio, presto, trino, snowflake, tableau, docker, kubernetes, sql, mongodb, object store s3, data lake, git, shell cli tools, rest apis, pandas","data lake, docker, dremio, git, kubernetes, mongodb, object store s3, pandas, presto, python, rest apis, shell cli tools, snowflake, sql, tableau, trino"
Data Engineer,People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3782522697,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Data Engineer
Full Time with PeopleTech
Redmond, WA (onsite)
Client: Microsoft
Experience in Azure and Microsoft specifically in Azure services with expertise in Microsoft BI tools to create end-to-end solutions that harness the power of cloud computing for data integration, transformation, analysis, and visualization. This role is pivotal in helping our organization unlock the full potential of their data assets through modern cloud-based BI strategies.
Key expertise in:
Azure Services Utilization:
Extensive utilization of Azure services such as Azure Data Factory, Azure SQL Data Warehouse (now called Azure Synapse Analytics), Azure Analysis Services, Azure Databricks, and more. These services enable the creation of end-to-end data pipelines, scalable data warehousing, advanced analytics, and machine learning integration.
Data Integration:
Using Azure Data Factory or other
ETL tools
to extract data from diverse sources, transform it, and load it into Azure-based data storage solutions, ensuring data consistency, accuracy, and reliability.
Azure Synapse Analytics (formerly SQL Data Warehouse):
Designing and optimizing data warehouses using Azure Synapse Analytics to facilitate high-performance querying and reporting on large datasets.
Azure Analysis Services:
Developing multidimensional and tabular models within Azure Analysis Services, enabling efficient data modeling for enhanced querying and reporting capabilities.
Power BI Integration:
Creating interactive dashboards and reports using Power BI to visualize data from Azure-based sources, allowing business users to gain insights and make informed decisions.
Data Security and Compliance:
Implementing security measures and compliance standards within Azure, ensuring data privacy and protection in alignment with industry regulations.
Cloud Scalability:
Designing BI solutions that can dynamically scale based on workload demands, taking advantage of Azure's elastic and scalable infrastructure.
Monitoring and Optimization:
Continuously monitoring the performance of Azure-based BI solutions, identifying bottlenecks, and optimizing data processing to ensure efficient and responsive analytics.
Data Lake Storage:
Integrating Azure Data Lake Storage into BI solutions for managing and processing vast amounts of structured and unstructured data.
Collaboration and DevOps:
Collaborating with development, operations, and business teams to streamline the deployment and maintenance of BI solutions using Azure DevOps practices.
Hybrid Scenarios:
Extending BI solutions to accommodate hybrid environments, connecting on-premises data sources with Azure cloud services for a comprehensive view of data
Show more
Show less","Azure, Microsoft BI tools, Data integration, Data transformation, Data analysis, Data visualization, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse Analytics, Azure Analysis Services, Azure Databricks, ETL tools, Power BI, Data security, Compliance standards, Cloud scalability, Monitoring, Optimization, Data Lake Storage, Azure DevOps, Hybrid scenarios","azure, microsoft bi tools, data integration, data transformation, data analysis, data visualization, azure data factory, azure sql data warehouse, azure synapse analytics, azure analysis services, azure databricks, etl tools, power bi, data security, compliance standards, cloud scalability, monitoring, optimization, data lake storage, azure devops, hybrid scenarios","azure, azure analysis services, azure data factory, azure databricks, azure devops, azure sql data warehouse, azure synapse analytics, cloud scalability, compliance standards, data integration, data lake storage, data security, data transformation, dataanalytics, etl tools, hybrid scenarios, microsoft bi tools, monitoring, optimization, powerbi, visualization"
Senior Data Engineer,Sierra ITS,"Lake Forest, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-sierra-its-3672847898,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"(Hybrid on/off-site, Perm $$)
We've been asked to identify a very experienced data pro to join an established organization with a critical need. They have undertaken an enterprise data centralization project designed to simplify high-level decision making. They have built a small (4-member) team that will lead the creation of a central data repository that enables true data integrity and standard formatting. This is a full-time, permanent role that will be done remotely 3 days per week onsite in Lake Forest, IL 2 days per week.
Desired:
8+ years of experience with ETL programming, Python, SQL (in SAS), data modeling, DW and data lakes, Azure Synapse, dashboarding, API's
Compensation:
Base of 160-170k + 22-37k annual bonus (172-20k total), plus unlimited vacation days, 401k (6% match), and legitimate work/life balance.
Local (Chicago area) candidates only, no third parties, please. For more information please apply today.
Show more
Show less","ETL programming, Python, SQL, SAS, Data modeling, Data warehouses, Data lakes, Azure Synapse, Dashboarding, APIs","etl programming, python, sql, sas, data modeling, data warehouses, data lakes, azure synapse, dashboarding, apis","apis, azure synapse, dashboard, data lakes, data warehouses, datamodeling, etl programming, python, sas, sql"
Digital Data Engineer,Experfy,"Cambridge, MA",https://www.linkedin.com/jobs/view/digital-data-engineer-at-experfy-3590306088,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Our client is looking for:
Work with business teams to understand requirements, and translate them into technical needs
Gather/organize large & complex data assets, and perform relevant analysis
Ensure the quality of the data in coordination with Data Analysts and Data Scientists (peer validation)
Propose and implement relevant data models for each business case
Create data models and optimize queries performance
Communicate results and findings in a structured way
Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan
Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements
Leverage existing or create new ""standard data pipelines"" within the company to bring value through business use cases
Ensure best practices in data manipulation are enforced end-to-end
Actively contribute to Data governance community
Remains up to date on company's standards, industry practices and emerging technologies
Requirements
Key Functional Requirements & Qualifications:
Experience working with a variety of cross-functional teams
Good understanding of agile/scrum development processes and concepts
Able to work in a fast-paced, constantly evolving environment and manage multiple priorities
Pragmatic and capable of solving complex issues
Service-oriented, flexible team player
Attention to detail & technical intuition
Excellent written, verbal, and interpersonal skills for executive level communication and collaboration
Fluent in English (Other languages a plus)
Key Technical Requirements & Qualifications:
Bachelor's Degree or equivalent in in Computer Science, Engineering, or relevant field
Experience with AWS cloud services (Azure & GCP a plus)
Good knowledge of SQL and relational databases technologies/concepts
Experience working with data models and query tuning
Experience in Data warehousing solutions (Snowflake a plus)
Experience in Integration Services (IICS, Tibco a plus)
Working knowledge of scripting languages (Python, R a plus)
Familiarity with Source Code Management Tools (GitHub a plus)
Familiarity with Visualization Tools (PowerBI, Tableau a plus)
Familiarity with Project Management Tools (JIRA, Confluence a plus)
Familiarity with Service Management Tools (Service Now a plus)
Experience working in life sciences/pharmaceutical industry is a plus
Relevant cloud certifications (AWS, Azure, Snowflake, IICS) are a plus
Experience on working within compliance (e.g.: quality, regulatory - data privacy, GxP, SOX) and cybersecurity requirements is a plus
Mentoring and/or technology evangelism/advocacy experience
Additions For Data Engineer - Database
Strong experience in automation tools and methodologies specifically using Gitlab, Github action, Terraform, Ansible
Experience with programming languages such as Python, JSON, YAML, Shell Scripting.
Experience with backup system like Netbackup & CommVault
Good knowledge of ServiceNow and monitoring tool such as Splunk, BPPM
Additions for Data Engineer - RWE
Experience with Real World Data (e,g, EHR, Claims) and standard data models (e,g, OMOP, FHIR)
Experience using frameworks to create pipelines (e.g. Apache Airflow, Kedro)
Show more
Show less","Agile, Scrum, SQL, Data warehousing, Cloud services, Data models, Data governance, Data analysis, Data engineering, Python, R, Gitlab, Github, Terraform, Ansible, JSON, YAML, Shell Scripting, Netbackup, CommVault, ServiceNow, Splunk, BPPM, Apache Airflow, Kedro, ETL, Real World Data, OMOP, FHIR","agile, scrum, sql, data warehousing, cloud services, data models, data governance, data analysis, data engineering, python, r, gitlab, github, terraform, ansible, json, yaml, shell scripting, netbackup, commvault, servicenow, splunk, bppm, apache airflow, kedro, etl, real world data, omop, fhir","agile, ansible, apache airflow, bppm, cloud services, commvault, data engineering, data governance, data models, dataanalytics, datawarehouse, etl, fhir, github, gitlab, json, kedro, netbackup, omop, python, r, real world data, scrum, servicenow, shell scripting, splunk, sql, terraform, yaml"
"(USA) Senior, Data Engineer, Big Data",Walmart,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/usa-senior-data-engineer-big-data-at-walmart-3781922152,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position Summary...
What you'll do...
Job Summary
At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can't do that without the best talent - talent that is innovative, curious, and driven to create exceptional experiences for our customers.
Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart's environment comes the biggest of big data sets. As a Walmart Data Engineer in Marketplace, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' and sellers' lives.
About The Data And Customer Analytics (DCA) Organization
Our organization focuses on managing and delivering world-class data assets, including creating and maintaining data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to leverage data to fuel growth, driving revenue in our core and building new business model opportunities.
What You'll Do
Y ou will use cutting edge data engineering techniques to create critical datasets and dig into our mammoth scale of data to help unleash the power of data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on .
You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.
You will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the marketplace business model while making a positive impact on our customers' and sellers' lives.
You will l ead and participate in small to large sized projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs .
You will p rovide guidance and mentorship to junior data engineers.
You will design, develop and maintain highly scalable and fault-tolerant real time, near real time and batch data systems/pipelines that process, store, and serve large volumes of data with optimal performance.
You will ensure data ingested and processed is accurate and of high quality by implementing data quality checks, data validation, and data cleaning processes.
You will i dentif y possible options to address business problems within one's discipline through analytics, big data analytics, and automation .
You will build business domain expertise to s upport the data need for product teams, analytics , data scientists and other data consumers.
What You'll Bring
Bachelor's/master's degree in computer science or a related field
With 5+ years' experience in development of big data technologies/data pipelines
Proficiency in managing and manipulating huge datasets in the order of terabytes (TB) is essential.
Expertise in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance.
Expertise in building idempotent workflows using orchestrators like Automic , Airflow, Luigi etc.
Expertise in writing SQL to analyze, optimize , profile data preferably in BigQuery or SPARK SQL
Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets.
Ability to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution work.
Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process.
Ability to move at a rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative.
Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project.
Nice To Have From You
Experience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability, scalability and SLA adherence.
Good understanding of REST APIs - working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc.
Exposure in developing reports/dashboards using Looker/Tableau
Experience in eCommerce domain.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.
You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.
Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .
The annual salary range for this position is $117,000.00-$234,000.00
Additional Compensation Includes Annual Or Quarterly Performance Incentives.
Additional compensation for certain positions may also include:
Regional Pay Zone (RPZ) (based on location)
Stock equity incentives
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in
software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related
field.
2 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering
Primary Location...
640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America
Show more
Show less","Big data technologies, Hadoop, Apache Spark, Apache Hive, Automic, Airflow, Luigi, Apache Kafka, Spark streaming, Kafka Connect, REST APIs, Apache Druid, Redis, Elastic search, GraphQL, Looker, Tableau, eCommerce, SQL, Python, Scala, Java","big data technologies, hadoop, apache spark, apache hive, automic, airflow, luigi, apache kafka, spark streaming, kafka connect, rest apis, apache druid, redis, elastic search, graphql, looker, tableau, ecommerce, sql, python, scala, java","airflow, apache druid, apache hive, apache kafka, apache spark, automic, big data technologies, ecommerce, elastic search, graphql, hadoop, java, kafka connect, looker, luigi, python, redis, rest apis, scala, spark streaming, sql, tableau"
Data Engineer,People Tech Group Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3782581214,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Title: Sr. Data Engineer
Duration: Fulltime
Location: Seattle, WA (Onsite from day one)
Job Responsibilities:
• Need Senior Data engineer
• With Dynamo, DB, Python , Bigdata, Strong on Advance sql, AWS, Apache airflow, redshift, Glue, Informatica.
• Nice to have : Data warehousing, operation.
• Experience with EMR, Pyspark, Python, Coding, AWS
Show more
Show less","Data engineering, Dynamo, Database, Python, Big data, Advanced SQL, AWS, Apache Airflow, Redshift, Glue, Informatica, Data warehousing, Operations, EMR, Pyspark, Coding","data engineering, dynamo, database, python, big data, advanced sql, aws, apache airflow, redshift, glue, informatica, data warehousing, operations, emr, pyspark, coding","advanced sql, apache airflow, aws, big data, coding, data engineering, database, datawarehouse, dynamo, emr, glue, informatica, operations, python, redshift, spark"
Data Engineer,People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3778861366,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Data Engineer
Full Time with PeopleTech
Redmond, WA (onsite)
Experience in Azure and Microsoft specifically in Azure services with expertise in Microsoft BI tools to create end-to-end solutions that harness the power of cloud computing for data integration, transformation, analysis, and visualization. This role is pivotal in helping our organization unlock the full potential of their data assets through modern cloud-based BI strategies.
Key expertise in:
Azure Services Utilization: Extensive utilization of Azure services such as Azure Data Factory, Azure SQL Data Warehouse (now called Azure Synapse Analytics), Azure Analysis Services, Azure Databricks, and more. These services enable the creation of end-to-end data pipelines, scalable data warehousing, advanced analytics, and machine learning integration.
Data Integration: Using Azure Data Factory or other ETL tools to extract data from diverse sources, transform it, and load it into Azure-based data storage solutions, ensuring data consistency, accuracy, and reliability.
Azure Synapse Analytics (formerly SQL Data Warehouse): Designing and optimizing data warehouses using Azure Synapse Analytics to facilitate high-performance querying and reporting on large datasets.
Azure Analysis Services: Developing multidimensional and tabular models within Azure Analysis Services, enabling efficient data modeling for enhanced querying and reporting capabilities.
Power BI Integration: Creating interactive dashboards and reports using Power BI to visualize data from Azure-based sources, allowing business users to gain insights and make informed decisions.
Data Security and Compliance: Implementing security measures and compliance standards within Azure, ensuring data privacy and protection in alignment with industry regulations.
Cloud Scalability: Designing BI solutions that can dynamically scale based on workload demands, taking advantage of Azure's elastic and scalable infrastructure.
Monitoring and Optimization: Continuously monitoring the performance of Azure-based BI solutions, identifying bottlenecks, and optimizing data processing to ensure efficient and responsive analytics.
Data Lake Storage: Integrating Azure Data Lake Storage into BI solutions for managing and processing vast amounts of structured and unstructured data.
Collaboration and DevOps: Collaborating with development, operations, and business teams to streamline the deployment and maintenance of BI solutions using Azure DevOps practices.
Hybrid Scenarios: Extending BI solutions to accommodate hybrid environments, connecting on-premises data sources with Azure cloud services for a comprehensive view of data.
Show more
Show less","Azure Services, Azure Data Factory, Azure Synapse Analytics, Azure Analysis Services, Azure Databricks, ETL, Data Integration, Power BI, Data Security, Compliance, Cloud Scalability, Monitoring, Optimization, Data Lake Storage, Collaboration, DevOps, Hybrid Scenarios","azure services, azure data factory, azure synapse analytics, azure analysis services, azure databricks, etl, data integration, power bi, data security, compliance, cloud scalability, monitoring, optimization, data lake storage, collaboration, devops, hybrid scenarios","azure analysis services, azure data factory, azure databricks, azure services, azure synapse analytics, cloud scalability, collaboration, compliance, data integration, data lake storage, data security, devops, etl, hybrid scenarios, monitoring, optimization, powerbi"
Data Engineer,iBridge Solutions,Greater St. Louis,https://www.linkedin.com/jobs/view/data-engineer-at-ibridge-solutions-3778196289,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Description:
The selected candidate should be able to help support our Data and Process Governance Policies and Standards for data utilization and access.
A strong business insight mindset and analytics background will help enable data visualizations, while helping maintain the technical foundation they are built on.
Knowledge of industry standard applications and database theory will allow us to transform the business by optimizing our workflow and data processing to deliver data to the shop floor.
Skill Set Requirements:
Experience in managing and communicating complex projects and collaborating with cross functional teams to accomplish project goals within expected timelines.
Knowledgeable on data governance concepts and implementation
Data Access Management
Change Management
Data Historian/ Time Series Data
SQL, MSSQL, TSQL
Data modeling for creating/maintaining data integrity between multiple schemas
ETL experience
Denodo/Data virtualization experience preferred
Knowledgeable on deployment process flow concepts
Python experience
Query optimization
Experience working with big data sets
Tableau Server/Desktop/Prep setup and dashboard building
Strong data cleaning and wrangling skills
Education: Bachelors would be nice but is not required, associates with experience is acceptable.
Strong initiative, self-motivated, results orientated, and able to work independently with minimal direction
Workflow management
Demonstrated ability to see differing perspectives and work cross-functionally.
This is a client facing role so good communication skills, written and verbal, is required.
Additional role info:
Interconnectivity between industrial automation data, MIS, and data warehouse.
Helping influence and implement an end-to-end vision for how production field data will flow through the organization.
Merge new systems or methods with existing data structures.
Partner with Supply Chain to define, document, implement and maintain business processes and data workflows.
Implement data visualizations and drive reporting solutions based on Engineers, PCIT Developers and End Users needs and feedback.
Help our engineers and PCIT developers create data reports for business
Will work with the manager, regional automation or process engineers depending on the site.
50% Data engineering, 25% visualization 25% maintenance
Team Dynamic/Culture: Currently 1 FT and 2 contractors. The current team thats been working on this project for about 1yr, there is an existing automation team that has been stood up for a while that this team integrates with.
Candidate Submittal Requirments:
US Citizenship or Green Card
Can work on W2
**No C2C**
Show more
Show less","Data governance, Data visualization, SQL, MSSQL, TSQL, Data modeling, ETL, Denodo, Data virtualization, Python, Tableau, Data cleaning, Data wrangling, Interconnectivity, Automation data, MIS, Data warehouse, Supply chain, Data workflows, Data reports, Data engineering","data governance, data visualization, sql, mssql, tsql, data modeling, etl, denodo, data virtualization, python, tableau, data cleaning, data wrangling, interconnectivity, automation data, mis, data warehouse, supply chain, data workflows, data reports, data engineering","automation data, data cleaning, data engineering, data governance, data reports, data virtualization, data workflows, data wrangling, datamodeling, datawarehouse, denodo, etl, interconnectivity, mis, mssql, python, sql, supply chain, tableau, tsql, visualization"
Data Engineer,Synovix,Greater Boston,https://www.linkedin.com/jobs/view/data-engineer-at-synovix-3778227847,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Synovix is actively seeking Data Engineer for designing, building, and maintaining the infrastructure that supports data storage, processing, and retrieval. They work with large data sets and develop data pipelines that move data from source systems to data warehouses, data lakes, and other data storage and processing systems. Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs during the development, maintenance and sustainment of the KR data architecture and data- driven solutions
Responsibilities:
Develop, optimize, and maintain data ingest flows using Apache Kafka, Apache Nifi and MySQL/PostGreSQL
Develop within the components in the AWS cloud platform using services such as RedShift, SageMaker, API Gateway, QuickSight, and Athena
Communicate with data owners to set up and ensure configuration parameters
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement
Document details of each data ingest activity to ensure they can be understood by the rest of the team
Develop and maintain best practices in data engineering and data analytics while following Agile DevSecOps methodology
Desired Skills:
Strong analytical skills, including statistical analysis, data visualization, and machine learning techniques
Good understanding of programming languages like Python, R, and Java
Expertise in building modern data pipelines and ETL (extract, transform, load) processes using tools such as Apache Kafka and Apache Nifi
Proficient in programming languages like Java, Scala, or Python
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs
Experience in traditional database and data warehouse products such as Oracle, MySQL, etc.
Experience in modern data management technologies such as datalake, data fabric, and data mesh
Experience with creating DevSecOps pipeline using CI CD CT tools and GitLab
Excellent written and oral communication skills, including strong technical documentation skills
Strong interpersonal skills and ability to work collaboratively in dynamic team environment
Proven track record in demanding, customer service oriented environment
Ability to communicate clearly with all levels within an organization
Excellent analytical skills, organizational abilities and problem solving skills
Experience in instituting data observability solutions using tools such as Grafana, Splunk, AWS CloudWatch, Kibana, etc.
Experience in container technologies such as Docker, Kubernetes, and Amazon EKS
Required Qualifications:
Active Secret Clearance
Bachelors Degree in Computer Science, Engineering, or other Technical discipline required, OR a minimum of 8 years equivalent work experience
8+ years of experience of IT system administration experience
AWS Cloud certifications are a plus
Synovix is an equal opportunity employer.
Show more
Show less","Apache Kafka, Apache Nifi, MySQL, PostGreSQL, AWS, RedShift, SageMaker, API Gateway, QuickSight, Athena, Python, R, Java, Scala, Oracle, DevSecOps, GitLab, Grafana, Splunk, AWS CloudWatch, Kibana, Docker, Kubernetes, Amazon EKS, Data engineering, Data analytics, Statistical analysis, Data visualization, Machine learning, ETL, Data pipeline, Data lake, Data fabric, Data mesh, CI CD CT, Active Secret Clearance, Bachelor's Degree in Computer Science Engineering or Technical discipline, 8+ years IT system administration experience, AWS Cloud certifications","apache kafka, apache nifi, mysql, postgresql, aws, redshift, sagemaker, api gateway, quicksight, athena, python, r, java, scala, oracle, devsecops, gitlab, grafana, splunk, aws cloudwatch, kibana, docker, kubernetes, amazon eks, data engineering, data analytics, statistical analysis, data visualization, machine learning, etl, data pipeline, data lake, data fabric, data mesh, ci cd ct, active secret clearance, bachelors degree in computer science engineering or technical discipline, 8 years it system administration experience, aws cloud certifications","8 years it system administration experience, active secret clearance, amazon eks, apache kafka, apache nifi, api gateway, athena, aws, aws cloud certifications, aws cloudwatch, bachelors degree in computer science engineering or technical discipline, ci cd ct, data engineering, data fabric, data lake, data mesh, data pipeline, dataanalytics, devsecops, docker, etl, gitlab, grafana, java, kibana, kubernetes, machine learning, mysql, oracle, postgresql, python, quicksight, r, redshift, sagemaker, scala, splunk, statistical analysis, visualization"
Data Engineer,City of Philadelphia,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-engineer-at-city-of-philadelphia-3575823339,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Company Description
A best-in-class city that attracts best-in-class talent, Philadelphia is an incredible place to build a career. From our thriving arts scene and rich history to our culture of passion and grit, there are countless reasons to love living and working here. With a workforce of over 30,000 people, and more than 1,000 different job categories, the City of Philadelphia offers boundless opportunities to make an impact.
As an employer, the City of Philadelphia values inclusion, integrity, innovation, empowerment, and hard work above all else. We offer a vibrant work environment, comprehensive health care and benefits, and the experience you need to grow and excel. If you’re interested in working with a passionate team of people who care about the future of Philadelphia, start here.
What We Offer
Impact - The work you do here matters to millions.
Growth - Philadelphia is growing, why not grow with it?
Diversity & Inclusion - Find a career in a place where everyone belongs.
Benefits - We Care About Your Well-being.
The Office of Innovation & Technology (OIT) is the central IT agency for the City of Philadelphia headed by the Chief Information Officer (CIO). OIT oversees all major information and communications technology initiatives for the City of Philadelphia - increasing the effectiveness of the information technology infrastructure, where the services provided are advanced, optimized, and responsive to the needs of the City of Philadelphia’s businesses, residents, and visitors. OIT responsibilities include: identifying the most effective approach for implementing new information technology directions throughout city government; improving the value of the city’s technology assets and the return on the city’s technology investments; ensuring data security continuity; planning for continuing operations in the event of disruption of information technology or communications services; and supporting accountable, efficient and effective government across every city department, board, commission and agency.
The City’s Applications That Support Bill Paying, Filling Permits, And All Day To Day Transactions Are Reliant On OIT’s Ability To Effectively Integrate And Ingest Data Among Various Systems In Multiple Formats. This Work Is Done Not Just For The Benefit Of Government Operations, But For The Public. Examples Of OIT’s Commitment To Serving The Public Through The Effective And Innovative Integration Of City Open Data Include
https://Atlas.phila.gov
https://Openmaps.phila.gov
https://streetsmartphl.phila.gov/
www.phila.gov/solarmap
www.phila.gov/stressmap
OIT’s teams apply agile development techniques and are staffed by engaged, diverse, talented, and committed analysts, application developers, and engineers eager to do their best professional work to improve City processes and policies to better serve Philadelphia residents.
Job Description
Data Engineering at the City is a unique opportunity for meaningful, exciting work and professional development using state-of-the art technologies and software development best practices. The data engineer will develop an intimate understanding of the City’s diverse data and contribute to improving the City’s data engineering infrastructure, pipelines, models, and accessibility for the public.
The data engineer will be part of the Business Intelligence and GIS (BI/GIS) team working on behalf of the Department of Licenses and Inspections (L&I). The BI/GIS team are responsible for all data extraction and analysis coming out of L&I whether it be visual, spatial, or tabular data. The candidate responsible for developing and maintaining data integrations between L&I’s externally managed data warehouse, various departmental data visualization tools, and external departments and agencies through OIT’s central data warehouse (DataBridge) and managing the underlying infrastructure. The city is also in the middle of a multi-year upgrade of the city’s property and address data system: Address Information System (AIS) and the candidate will work with relevant stakeholders to ensure clean property data integrations between AIS and L&I.
Primary tools used include Python, Bash, SQL, GIS, Airflow, Postgres, AWS, GitHub, Software AG Web Methods and SAS API providers like ArcGIS Online and CARTO.
Essential Functions
Help City departments share data with other departments and with the public by:
 Working with business partners of varying technical ability to understand how their data is produced, stored, and updated
 Setting up automated extract, transform, and load (ETL) workflows, including standardization and enriching of datasets that pass through DataBridge
 Improving the City’s “platform” used to share data (reusable components, scheduling, logging, centralized storage, etc.) so that workflows are easier to write and maintain
 Write descriptive and accurate technical documentation of systems or applications
 Other duties as assigned
Ability To
Competencies, Knowledge, Skills and Abilities
 Reason about, and work, with data across a variety of potentially unfamiliar domains
 Apply creative problem-solving when considering how to best address technical challenges
 Communicate technical nuances in plain language with partners of varying technical background
 Write clearly and concisely with organized structure
 Write descriptive, technical policies and/or documentation
 Manage time effectively between various projects and responsibilities
Knowledge And Experience With
 Building and maintaining data pipelines / ETL’s
 Building software with Python (intermediate/advanced), with additional support for Bash and Docker if needed
 SQL – intermediate/advanced
 Databases: Oracle, SQL Server, or Postgres with intermediate to advanced experience
 Continuous Integration/Deployment
 Classifying data sensitivity and managing secure data environments
 Working in a hybrid environment (on-premises and Amazon Web Services)
Qualifications
At least 3 years’ experience in any of the following areas preferred:
 Developing software
 Data engineering/integration
 Architecting and maintaining relational databases
 Moving a variety of data types between a variety of systems (ie. Flat file, FTP server, AWS S3, Relational Databases, APIs)
 Transforming and enriching data (ie. De-normalizing, geocoding, re-projecting, anonymizing)
 Establishing ETL workflows, processes, and documentation for organizations
 Bachelor's degree in Computer Science related field, or minimum of 4 years of equivalent relevant experience
Additional Information
Salary Range: $75,000 - $85,000
Please provide a resume and include a cover letter
Did you know?
We are a Public Service Loan Forgiveness Program qualified employer
25% tuition discount program for City employees (and sometimes spouses and dependents as well) in partnership with area colleges and universities
We offer Comprehensive health coverage for employees and their eligible dependents
Our wellness program offers eligibility into the discounted medical plan
Employees receive paid vacation, sick leave, and holidays
Generous retirement savings options are available
The successful candidate must be a city of Philadelphia resident within six months of hire
Please note that effective September 1, 2021, the City of Philadelphia is requiring all new employees to present proof of vaccination against COVID-19.
The City of Philadelphia is an Equal Opportunity employer and does not permit discrimination based on race, ethnicity, color, sex, sexual orientation, gender identity, religion, national origin, ancestry, age, disability, marital status, source of income, familial status, genetic information or domestic or sexual violence victim status. If you believe you were discriminated against, call the Philadelphia Commission on Human Relations at 215-686-4670 or send an email to faqpchr @phila.gov. For more information, go to: Human Relations Website: http://www.phila.gov/humanrelations/Pages/default.aspx
Show more
Show less","Data engineering, ETL, Python, Bash, SQL, GIS, Airflow, Postgres, AWS, GitHub, Software AG Web Methods, SAS, ArcGIS Online, CARTO, Relational databases, Docker, Continuous integration/deployment, Data pipelines, Data sensitivity, Hybrid environment, Computer science","data engineering, etl, python, bash, sql, gis, airflow, postgres, aws, github, software ag web methods, sas, arcgis online, carto, relational databases, docker, continuous integrationdeployment, data pipelines, data sensitivity, hybrid environment, computer science","airflow, arcgis online, aws, bash, carto, computer science, continuous integrationdeployment, data engineering, data sensitivity, datapipeline, docker, etl, gis, github, hybrid environment, postgres, python, relational databases, sas, software ag web methods, sql"
Data Engineer,People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3782514948,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position : Data Engineer
FTE with PeopleTech
Location: Redmond, WA
Experience in collecting and documenting reporting requirements.
Experience in working with large datasets on variety of platforms such as ADLS (Parquet, AVRO, Delta formats), SQL, Synapse etc.
Experience with Azure technologies such as Azure Databricks, Azure Synapse Analytics, Azure Data Factory and ADLS.
Proficiency with at least one or more programming language such as Python ,Scala Or SQL
Experience with Data warehousing, and dimensional data modeling. Experience with one of data modeling tool such as ER Studio/Erwin would be nice to have.
Integrating the end to end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is maintained at all times
Good understanding of other Azure services like Azure Data Lake Analytics & U-SQL, Azure SQL DW
Experience with translating business/reporting requirements into technical design specification. This technical design document will be consumed by development/engineering teams as a guide for implementation.
Ability to work independently or with minimal supervision.
Serve as a consultant through the lifecycle of
Show more
Show less","Data Engineering, Data Warehousing, Data Modeling (Dimensional), Data Pipeline Integration, Data Quality Management, Python, Scala, SQL, Azure Databricks, Azure Synapse Analytics, Azure Data Factory, ADLS (Parquet AVRO Delta formats), ER Studio/Erwin, Azure Data Lake Analytics, USQL, Azure SQL DW","data engineering, data warehousing, data modeling dimensional, data pipeline integration, data quality management, python, scala, sql, azure databricks, azure synapse analytics, azure data factory, adls parquet avro delta formats, er studioerwin, azure data lake analytics, usql, azure sql dw","adls parquet avro delta formats, azure data factory, azure data lake analytics, azure databricks, azure sql dw, azure synapse analytics, data engineering, data modeling dimensional, data pipeline integration, data quality management, datawarehouse, er studioerwin, python, scala, sql, usql"
(USA) Data Engineer III,Walmart,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/usa-data-engineer-iii-at-walmart-3781930633,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position Summary...
What you'll do...
Job Summary
At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can't do that without the best talent - talent that is innovative, curious, and driven to create exceptional experiences for our customers.
Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart's environment comes the biggest of big data sets. As a Walmart Data Engineer in Marketplace, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' and sellers' lives.
About The Data And Customer Analytics (DCA) Organization
Our organization focuses on managing and delivering world-class data assets, including creating and maintaining data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to leverage data to fuel growth, driving revenue in our core and building new business model opportunities.
What You'll Do
Y ou will use cutting edge data engineering techniques to create critical datasets and dig into our mammoth scale of data to help unleash the power of data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on .
You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.
You will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the marketplace business model while making a positive impact on our customers' and sellers' lives.
You will participate with limited help in small to large sized projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs .
You will design, develop and maintain highly scalable and fault-tolerant real time, near real time and batch data systems/pipelines that process, store, and serve large volumes of data with optimal performance.
You will ensure data ingested and processed is accurate and of high quality by implementing data quality checks, data validation, and data cleaning processes.
You will i dentif y possible options to address business problems within one's discipline through analytics, big data analytics, and automation .
You will build business domain knowledge to s upport the data need for product teams, analytics , data scientists and other data consumers.
What You'll Bring
Bachelor's/master's degree in computer science or a related field
With 2 + years' experience in development of big data technologies/data pipelines
Proficiency in managing and manipulating huge datasets in the order of terabytes (TB) is essential.
Expertise in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance.
Expertise in building idempotent workflows using orchestrators like Automic , Airflow, Luigi etc.
Expertise in writing SQL to analyze, optimize , profile data preferably in BigQuery or SPARK SQL
Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets.
Ability to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution work.
Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process.
Ability to move at a rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative.
Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project.
Nice To Have From You
Experience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability, scalability and SLA adherence.
Good understanding of REST APIs - working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc.
Exposure in developing reports/dashboards using Looker/Tableau
Experience in eCommerce domain.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.
You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.
Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .
The annual salary range for this position is $117,000.00-$234,000.00
Additional Compensation Includes Annual Or Quarterly Performance Incentives.
Additional compensation for certain positions may also include:
Regional Pay Zone (RPZ) (based on location)
Stock equity incentives
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in
software engineering or related field. Option 3: Master's degree in Computer Science.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field
Primary Location...
640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America
Show more
Show less","Data engineering, Data pipelines, Big data technologies, Hadoop, Apache Spark, Apache Hive, AWS, Azure, Automic, Airflow, Luigi, SQL, BigQuery, SPARK SQL, Data modeling, ETL, ELT, Data integration, Schema evolution, Data quality, Data validation, Data cleaning, Analytics, Business intelligence, Automation, Realtime streaming, Apache Kafka, Spark streaming, Kafka Connect, REST APIs, Apache Druid, Redis, Elastic search, GraphQL, Looker, Tableau, eCommerce","data engineering, data pipelines, big data technologies, hadoop, apache spark, apache hive, aws, azure, automic, airflow, luigi, sql, bigquery, spark sql, data modeling, etl, elt, data integration, schema evolution, data quality, data validation, data cleaning, analytics, business intelligence, automation, realtime streaming, apache kafka, spark streaming, kafka connect, rest apis, apache druid, redis, elastic search, graphql, looker, tableau, ecommerce","airflow, analytics, apache druid, apache hive, apache kafka, apache spark, automation, automic, aws, azure, big data technologies, bigquery, business intelligence, data cleaning, data engineering, data integration, data quality, data validation, datamodeling, datapipeline, ecommerce, elastic search, elt, etl, graphql, hadoop, kafka connect, looker, luigi, realtime streaming, redis, rest apis, schema evolution, spark sql, spark streaming, sql, tableau"
Senior Python Data Engineer,Synechron,"New York, United States",https://www.linkedin.com/jobs/view/senior-python-data-engineer-at-synechron-3784834855,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Summary :
We are looking for strong Python Developer with strong background in data engineering and integration experience.
Duties :
Integration Engineer responsible for daily support and project based development of credit risk management systems.
ETL developers are responsible for designing and creating the data warehouse and all related extraction, transformation and load of data functions.
This is an opportunity to gain experience in risk management processing using new technologies.
You are :
5 years of full-time development experience using Python.
Experience building data piplines using Azure Data Factory and Databricks.
Experience with Python application frameworks (Django, Flask, Pyramid, Tornado).
Experience with Python testing and code analysis tools (Pytest, Pylint).
Strong SQL skills.
Familiarity with SSIS.
Strong troubleshooting skills.
On-point communication skills
.
Education :
Bachelor’s degree in computer science or finance.
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 44 offices in 19 countries and the possibility to work abroad
Laptop and a mobile phone
10 days of paid annual leave (plus sick leave and national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region)
Retirement savings plans
A higher education certification policy
Commuter benefits (varies by region)
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT :
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative
‘Synclusive’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Show more
Show less","Python, Data engineering, Data integration, Data piplines, Azure Data Factory, Databricks, Python frameworks, Django, Flask, Pyramid, Tornado, Python testing, Code analysis tools, Pytest, Pylint, SQL, SSIS, Troubleshooting, Communication","python, data engineering, data integration, data piplines, azure data factory, databricks, python frameworks, django, flask, pyramid, tornado, python testing, code analysis tools, pytest, pylint, sql, ssis, troubleshooting, communication","azure data factory, code analysis tools, communication, data engineering, data integration, data piplines, databricks, django, flask, pylint, pyramid, pytest, python, python frameworks, python testing, sql, ssis, tornado, troubleshooting"
Data Engineer with AI,PETADATA,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer%C2%A0with-ai-at-petadata-3784215580,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position: Data Engineer with AI
Location: San Francisco, CA and New York City, NY
Experience: 10-15 years.
PETADATA
is hiring for the position of
DATA ENGINEER with AI
for one of our clients.
Primary Responsibilities
The candidate should have to design, build, optimize, and support new and existing data models and processes based on the client’s requirements.
Need to build, deploy, and manage data infrastructure that can adequately handle the needs of a rapidly growing data-driven organization.
Should be able to establish scalable, efficient, automated processes for large dataset analysis, model development, and validation.
Must be able to support, test, deploy, and maintain the AWS ecosystem from an infrastructure standpoint (Hybrid Cloud SDK upgrade, security fixes, etc.).
Need to coordinate data access and security to enable data scientists and analysts to easily access data whenever they need to and maintain the AWS ecosystem from an infrastructure standpoint.
Have to model, build, and test AI software to ensure it can take on large swaths of data and achieve desired results.
Need to work with a team of machine-learning and data engineers to ensure seamless AI development and integration.
Works with domain experts and AI Scientists to define annotation guidelines and drive the annotation efforts towards high-quality data.
Required Technical And Professional Expertise
The candidate needs to have a solid foundation in computer science, with strong competencies in algorithms, data structures, object-oriented programming, design patterns, multi-threaded programming, and software design principles
Must have experience in AI, ML, python, and DBMs.
Should have hands-on experience in Python, Spark, Scala, and Hive, as well as database technology Source Code Control, Able to perform data validation, delivery, quality, and integrity.
Experienced in Warehouse: SQL, Amazon, Hive, etc.
Need to have knowledge of RDBMS and be able to work with databases that power API’s for front-end applications.
Strong understanding in using Python, JavaScript, and C++ to create a faster, more capable AI.
Good to be a constructive communicator and can effectively discuss difficult issues with team members and customers.
Must have updated AI knowledge, trends, and regulations.
Bachelor or Master’s degree in Computer Science or relevant degree is a must.
We offer a professional work environment and are given every opportunity to grow in the Information technology world.
Note
Candidates required to attend Phone/Video Call / In person interviews and after Selection of candidate (He/She) should go through all background checks on Education and Experience.
Please email your resume to:
swaroopb
@petadata.co
After carefully reviewing your experience and skills one of our HR team members will contact you on the next steps.
Show more
Show less","Data Engineering, AI, Algorithms, Data Structures, ObjectOriented Programming, Design Patterns, MultiThreaded Programming, Software Design Principles, Machine Learning, Python, DBMs, Spark, Scala, Hive, SQL, Amazon, RDBMS, JavaScript, C++, AWS SDK, Source Code Control","data engineering, ai, algorithms, data structures, objectoriented programming, design patterns, multithreaded programming, software design principles, machine learning, python, dbms, spark, scala, hive, sql, amazon, rdbms, javascript, c, aws sdk, source code control","ai, algorithms, amazon, aws sdk, c, data engineering, data structures, dbms, design patterns, hive, javascript, machine learning, multithreaded programming, objectoriented programming, python, rdbms, scala, software design principles, source code control, spark, sql"
Data Engineer,Agility Partners,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-engineer-at-agility-partners-3774582585,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"About this Role
Agility Partners is seeking a qualified Data Engineer to fill an open position with one of our banking clients. Join the IT team and play a pivotal role in constructing, testing, and maintaining the mortgage banking data architecture. This is a unique opportunity to optimize data flow and collection, supporting data analysis and decision-making processes.
Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices for mortgage banking.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Translate complex functional and technical requirements into detailed architecture, design, and high-performing software.
Integrate new data management technologies and software engineering tools into existing structures.
Create data tools for analytics and line of business, assisting in building and optimizing our product into an innovative industry leader.
Collaborate with data and analytics experts to enhance functionality in our data systems.
Maintain a secure and compliant data processing environment aligned with industry regulations.
Benefits and Perks
Reasons to Love It:
Work within a collaborative team environment where ideas and creativity are welcomed! Family and Work Life balance are important to this organization and valued for the employees.
Working for an organization that focuses on company culture, inclusion and diversity
50% medical coverage for you and your entire family, short/long term disability and life insurance options
401(k)
Life Insurance
Disability coverage
The Ideal Candidate
Qualifications:
Proven experience as a Data Engineer, preferably in the mortgage banking industry - but okay if not. Finance experience helps.
Strong knowledge of data warehousing solutions and relational SQL and NoSQL databases, Snowflake, MS SQL Server a plus.
Experience with AWS cloud services: EC2, RDS, MSK, Lambda.
Experience with object-oriented/object function scripting languages a plus.
Solid analytical skills and the ability to understand complex business requirements.
Familiarity with data pipeline and workflow management tools: dbt, Apache Kafka, Snowflake data pipeline/streams.
Knowledge of financial and mortgage banking principles.
Strong organizational and interpersonal skills, with the ability to manage tasks and timelines effectively.
Show more
Show less","Data Engineering, Data architecture, Data analysis, Software engineering, Data management, Relational and NoSQL databases, Snowflake, MS SQL Server, AWS cloud services, EC2, RDS, MSK, Lambda, Objectoriented scripting, Data pipeline, Workflow management, Dbt, Apache Kafka, Snowflake data pipeline, Financial principles, Mortgage banking principles","data engineering, data architecture, data analysis, software engineering, data management, relational and nosql databases, snowflake, ms sql server, aws cloud services, ec2, rds, msk, lambda, objectoriented scripting, data pipeline, workflow management, dbt, apache kafka, snowflake data pipeline, financial principles, mortgage banking principles","apache kafka, aws cloud services, data architecture, data engineering, data management, data pipeline, dataanalytics, dbt, ec2, financial principles, lambda, mortgage banking principles, ms sql server, msk, objectoriented scripting, rds, relational and nosql databases, snowflake, snowflake data pipeline, software engineering, workflow management"
Senior Data Engineer for E-Commerce,"HireIO, Inc.","Seattle, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-for-e-commerce-at-hireio-inc-3739540947,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"The Global E-Commerce team focuses on building data infrastructure and data product areas to support business engineering teams working directly.
As a data engineer in the Global E-Commerce team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.
Responsibilities - What You'll Do
Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis);
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;
Establish solid design and best engineering practice for engineers as well as non-technical people
Position Requirements：
Requirements
BS or MS degree in Computer Science or related technical field or equivalent practical experience;
Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.);
Experience with performing data analysis, data ingestion and data integration;
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems;
Experience with schema design, data modeling and SQL queries;
Passionate and self-motivated about technologies in the Big Data area
Show more
Show less","Hadoop, MapReduce, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink, Data analysis, Data ingestion, Data integration, ETL, Schema design, Data modeling, SQL","hadoop, mapreduce, hive, spark, metastore, presto, flume, kafka, clickhouse, flink, data analysis, data ingestion, data integration, etl, schema design, data modeling, sql","clickhouse, data ingestion, data integration, dataanalytics, datamodeling, etl, flink, flume, hadoop, hive, kafka, mapreduce, metastore, presto, schema design, spark, sql"
Python Data Engineer,Lorven Technologies Inc.,"New York, NY",https://www.linkedin.com/jobs/view/python-data-engineer-at-lorven-technologies-inc-3644942191,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Job Description
""Position Description:Machine Learning :
Python Pyspark , PYTorch Machine learning modeling experience .
Python scripting expertise
MLOps experience
Banking experience is a plus Data Engineer
Data Engineer having 6-10 year experience in ETL, Data Ingestion and implementation of data warehousing and Data Lake solutions
Strong PySpark/Python programming experience - Partitioning, parallel distributed computation and spark cluster concepts
Optimization for loading and processing ETL of large data sets
Strong SQL programming skills. Ability to write SQL queries Joins, Unions, Group by and Sub queries
Azure Cloud and preferable Dataiku experience in building data-preparation and model flows to optimize the code and productionize the flows
Relational database experience (experience with Snowflake is a big plus)
Having some experience in Azure Databricks
Show more
Show less","Machine Learning, Python, Pyspark, PyTorch, MLOps, Data Engineering, ETL, Data Warehousing, Data Lake, PySpark, Python programming, Partitioning, Parallel distributed computation, Spark cluster concepts, SQL, SQL queries, Joins, Unions, Group by, Subqueries, Azure Cloud, Dataiku, Datapreparation, Model flows, Relational database, Snowflake, Azure Databricks","machine learning, python, pyspark, pytorch, mlops, data engineering, etl, data warehousing, data lake, pyspark, python programming, partitioning, parallel distributed computation, spark cluster concepts, sql, sql queries, joins, unions, group by, subqueries, azure cloud, dataiku, datapreparation, model flows, relational database, snowflake, azure databricks","azure cloud, azure databricks, data engineering, data lake, dataiku, datapreparation, datawarehouse, etl, group by, joins, machine learning, mlops, model flows, parallel distributed computation, partitioning, python, python programming, pytorch, relational database, snowflake, spark, spark cluster concepts, sql, sql queries, subqueries, unions"
(USA) Data Engineer III,Walmart,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/usa-data-engineer-iii-at-walmart-3781930634,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Position Summary...
What you'll do...
Job Summary
At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can't do that without the best talent - talent that is innovative, curious, and driven to create exceptional experiences for our customers.
Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart's environment comes the biggest of big data sets. As a Walmart Data Engineer in Marketplace, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' and sellers' lives.
About The Data And Customer Analytics (DCA) Organization
Our organization focuses on managing and delivering world-class data assets, including creating and maintaining data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to leverage data to fuel growth, driving revenue in our core and building new business model opportunities.
What You'll Do
Y ou will use cutting edge data engineering techniques to create critical datasets and dig into our mammoth scale of data to help unleash the power of data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on .
You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.
You will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the marketplace business model while making a positive impact on our customers' and sellers' lives.
You will participate with limited help in small to large sized projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs .
You will design, develop and maintain highly scalable and fault-tolerant real time, near real time and batch data systems/pipelines that process, store, and serve large volumes of data with optimal performance.
You will ensure data ingested and processed is accurate and of high quality by implementing data quality checks, data validation, and data cleaning processes.
You will i dentif y possible options to address business problems within one's discipline through analytics, big data analytics, and automation .
You will build business domain knowledge to s upport the data need for product teams, analytics , data scientists and other data consumers.
What You'll Bring
Bachelor's/master's degree in computer science or a related field
With 2 + years' experience in development of big data technologies/data pipelines
Proficiency in managing and manipulating huge datasets in the order of terabytes (TB) is essential.
Expertise in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance.
Expertise in building idempotent workflows using orchestrators like Automic , Airflow, Luigi etc.
Expertise in writing SQL to analyze, optimize , profile data preferably in BigQuery or SPARK SQL
Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets.
Ability to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution work.
Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process.
Ability to move at a rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative.
Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project.
Nice To Have From You
Experience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability, scalability and SLA adherence.
Good understanding of REST APIs - working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc.
Exposure in developing reports/dashboards using Looker/Tableau
Experience in eCommerce domain.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.
You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.
Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .
The annual salary range for this position is $117,000.00-$234,000.00
Additional Compensation Includes Annual Or Quarterly Performance Incentives.
Additional compensation for certain positions may also include:
Regional Pay Zone (RPZ) (based on location)
Stock equity incentives
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in
software engineering or related field. Option 3: Master's degree in Computer Science.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field
Primary Location...
640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America
Show more
Show less","Apache Spark, Apache Hive, Apache Hadoop, Apache Kafka, Apache Druid, Redis, Elastic Search, GraphQL, REST APIs, SQL, BigQuery, Spark SQL, Looker, Tableau, AWS, Azure, GCP, Airflow, Luigi, Automic, Scala, Python, Java, SQL, NoSQL, Hadoop Distributed File System (HDFS), MapReduce, HiveQL, Pig Latin, Oozie, Flume, Sqoop, HBase, Cassandra, MongoDB, Elasticsearch, Kibana, Logstash, Kafka Connect, Prometheus, Grafana, Data modeling, Data integration, Data warehousing, Data mining, Machine learning, Artificial intelligence, Cloud computing, Software engineering, Software development, Data engineering, DevOps, Agile development, Scrum, Kanban, Continuous integration (CI), Continuous delivery (CD), Testdriven development (TDD), Behaviordriven development (BDD), Pair programming, Mob programming, Extreme programming (XP), Lean software development","apache spark, apache hive, apache hadoop, apache kafka, apache druid, redis, elastic search, graphql, rest apis, sql, bigquery, spark sql, looker, tableau, aws, azure, gcp, airflow, luigi, automic, scala, python, java, sql, nosql, hadoop distributed file system hdfs, mapreduce, hiveql, pig latin, oozie, flume, sqoop, hbase, cassandra, mongodb, elasticsearch, kibana, logstash, kafka connect, prometheus, grafana, data modeling, data integration, data warehousing, data mining, machine learning, artificial intelligence, cloud computing, software engineering, software development, data engineering, devops, agile development, scrum, kanban, continuous integration ci, continuous delivery cd, testdriven development tdd, behaviordriven development bdd, pair programming, mob programming, extreme programming xp, lean software development","agile development, airflow, apache druid, apache hadoop, apache hive, apache kafka, apache spark, artificial intelligence, automic, aws, azure, behaviordriven development bdd, bigquery, cassandra, cloud computing, continuous delivery cd, continuous integration ci, data engineering, data integration, data mining, datamodeling, datawarehouse, devops, elastic search, elasticsearch, extreme programming xp, flume, gcp, grafana, graphql, hadoop distributed file system hdfs, hbase, hiveql, java, kafka connect, kanban, kibana, lean software development, logstash, looker, luigi, machine learning, mapreduce, mob programming, mongodb, nosql, oozie, pair programming, pig latin, prometheus, python, redis, rest apis, scala, scrum, software development, software engineering, spark sql, sql, sqoop, tableau, testdriven development tdd"
Python Data Engineer,Lorven Technologies Inc.,"New York, NY",https://www.linkedin.com/jobs/view/python-data-engineer-at-lorven-technologies-inc-3652261445,2023-12-17,New Westminster, Canada,Mid senior,Onsite,"Job Description
""Position Description: Machine Learning :
Python Pyspark , PYTorch Machine learning modeling experience .
Python scripting expertise
MLOps experience
Banking experience is a plus Data Engineer
Data Engineer having 6-10 year experience in ETL, Data Ingestion and implementation of data warehousing and Data Lake solutions
Strong PySpark/Python programming experience - Partitioning, parallel distributed computation and spark cluster concepts
Optimization for loading and processing ETL of large data sets
Strong SQL programming skills. Ability to write SQL queries Joins, Unions, Group by and Sub queries
Azure Cloud and preferable Dataiku experience in building data-preparation and model flows to optimize the code and productionize the flows
Relational database experience (experience with Snowflake is a big plus)
Having some experience in Azure Databricks
Show more
Show less","Python, PySpark, PYTorch, Machine learning modeling, Python scripting, MLOps, Banking, Data Engineer, ETL, Data Ingestion, Data warehousing, Data Lake, PySpark programming, Partitioning, Parallel distributed computation, Spark cluster concepts, Optimization, SQL programming, SQL queries, Joins, Unions, Group by, Sub queries, Azure Cloud, Dataiku, Datapreparation, Model flows, Relational database, Snowflake, Azure Databricks","python, pyspark, pytorch, machine learning modeling, python scripting, mlops, banking, data engineer, etl, data ingestion, data warehousing, data lake, pyspark programming, partitioning, parallel distributed computation, spark cluster concepts, optimization, sql programming, sql queries, joins, unions, group by, sub queries, azure cloud, dataiku, datapreparation, model flows, relational database, snowflake, azure databricks","azure cloud, azure databricks, banking, data ingestion, data lake, dataengineering, dataiku, datapreparation, datawarehouse, etl, group by, joins, machine learning modeling, mlops, model flows, optimization, parallel distributed computation, partitioning, pyspark programming, python, python scripting, pytorch, relational database, snowflake, spark, spark cluster concepts, sql, sql queries, sub queries, unions"
Senior/Principal Database Engineer – CockroachDB,Wasabi Technologies,"Boston, MA",https://www.linkedin.com/jobs/view/senior-principal-database-engineer-%E2%80%93-cockroachdb-at-wasabi-technologies-3774885210,2023-12-17,Dedham,United States,Mid senior,Remote,"At Wasabi, we’re a proven collection of pioneers, visionaries and disruptive doers. We see things differently than our competitors, and we make our mark in the industry by challenging the norm and delivering the unexpected and improbable. We’re a fast-growing company taking the Cloud Storage industry by storm and recognized as one of the best places to work in Boston.
Wasabi hot cloud storage is a new class and category of cloud storage, breaking all traditional barriers and boundaries of storage with a disruptive value proposition of being 1/5th the cost of AWS S3, faster than the competition, with no fees for egress or API request and delivered as a single-tier solution. Cloud storage has never been so simple, so fast and so inexpensive. It’s all part of our vision to make cloud storage the next great global utility, just like electricity.
Role Description: Senior/Principal Software Engineer – CockroachDB
Role Purpose:
Wasabi is seeking a highly skilled and motivated Senior/Principal CockroachDB Engineer to play a key role in our engineering team. As a Senior/Principal CockroachDB Engineer, you will be responsible for leveraging your expertise in CockroachDB to design, develop, and optimize robust database solutions that align with our organizational goals. You will collaborate closely with cross-functional teams to ensure scalability, security, and high performance across our database infrastructure.
Principals Only. No Recruiters.
Responsibilities:
Spearhead the process of identifying and addressing immediate performance issues in our database systems, laying the groundwork for more comprehensive improvements.
Work collaboratively with a multidisciplinary team to drive advancements in the CockroachDB codebase, playing a pivotal role in the evolution of our metadata storage capabilities.
Apply your practical software engineering skills to thrive in our dynamic startup environment, focusing on tangible results and efficient execution.
Engage in the development and refinement of a robust, distributed, metadata storage system capable of managing our rapidly expanding object count.
Requirements:
Bachelor’s or Master’s degree in Computer Science or a related field.
Proven experience working with CockroachDB in large-scale enterprise environments.
7+ years of expertise in database architecture, design, implementation, and optimization.
Proficient in SQL and related database languages.
Strong problem-solving skills and the ability to troubleshoot complex database issues.
Familiarity with database security measures and best practices.
Excellent communication and collaboration skills to work effectively in a team environment.
Experience with cloud-based database solutions is a plus.
Ability to thrive in a fast-paced, dynamic work environment.
Wasabi Technologies is an Equal Opportunity Employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
Show more
Show less","CockroachDB, SQL, Database architecture, Database design, Database implementation, Database optimization, Database security, Cloudbased database solutions, Distributed systems, Software engineering, Troubleshooting, Problemsolving, Collaboration, Communication","cockroachdb, sql, database architecture, database design, database implementation, database optimization, database security, cloudbased database solutions, distributed systems, software engineering, troubleshooting, problemsolving, collaboration, communication","cloudbased database solutions, cockroachdb, collaboration, communication, database architecture, database design, database implementation, database optimization, database security, distributed systems, problemsolving, software engineering, sql, troubleshooting"
Staff Data Engineer,Recruiting from Scratch,"Santa Barbara, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398227,2023-12-17,Santa Barbara,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Stream processing systems, Kafka, Storm, Spark Streaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, stream processing systems, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, stream processing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Santa Barbara, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395230,2023-12-17,Santa Barbara,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, ETL, Data Management, Data Classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, etl, data management, data classification, retention","airflow, automated testing, continuous integration, data classification, data management, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, retention, snowflake, spark, spark streaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Santa Barbara, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773092102,2023-12-17,Santa Barbara,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML/DL modeling, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Management, 401K, GenderAffirming Offerings, Included Health","data engineering, mldl modeling, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, 401k, genderaffirming offerings, included health","401k, airflow, aws, azure, bash, data engineering, data management, docker, dynamodb, etl, gcp, genderaffirming offerings, git, helm, included health, java, kafka, kubeflow, kubernetes, machine learning, mldl modeling, nlp, nosql, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Santa Barbara, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707767,2023-12-17,Santa Barbara,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Pandas, Python, Java, Bash, SQL, Git, Airflow, ML Ops, KubeFlow, NLP, Machine Learning, Data Mining, Kafka, Snowflake, Kubernetes, NoSQL, ETL, Docker, Spark, Data Governance, Hadoop, Big Data, Data pipelines","data engineering, pandas, python, java, bash, sql, git, airflow, ml ops, kubeflow, nlp, machine learning, data mining, kafka, snowflake, kubernetes, nosql, etl, docker, spark, data governance, hadoop, big data, data pipelines","airflow, bash, big data, data engineering, data governance, data mining, datapipeline, docker, etl, git, hadoop, java, kafka, kubeflow, kubernetes, machine learning, ml ops, nlp, nosql, pandas, python, snowflake, spark, sql"
Data Center Engineer,World Wide Technology,"Delaware, OH",https://www.linkedin.com/jobs/view/data-center-engineer-at-world-wide-technology-3782266431,2023-12-17,Marysville,United States,Associate,Onsite,"Data Center Engineer
Company Overview
World Wide Technology (WWT), a global technology solutions provider with $17 billion in annual revenue, combines the power of strategy, execution and partnership to accelerate transformational outcomes for large public and private organizations around the world. Through its Advanced Technology Center, a collaborative ecosystem of the world's most advanced hardware and software solutions, WWT helps customers and partners conceptualize, test and validate innovative technology solutions for the best business outcomes and then deploys them at scale through its 4 million square feet of global warehousing, distribution and integration space. With over 10,000 employees and more than 55 locations around the world, WWT's culture, built on a set of core values and established leadership philosophies, has been recognized 11 years in a row by Fortune and Great Place to Work® for its unique blend of determination, innovation and leadership for diversity and inclusion. With this culture at its foundation, WWT bridges the gap between business and technology to make a new world happen for its customers, partners and communities.
World Wide Technology Holding Co, LLC. (WWT) has an opportunity available for a
Data Center Engineer
to support our client in an ongoing Data Center refresh project.
Location:
Delaware, OH
Available Shifts:
(2 Openings) 7 PM – 7 AM CST Thursday, Friday, Saturday, and Alternating Wednesdays.
MUST BE OKAY WITH 12 HOUR NIGHT SHIFTS - Expectation is to have 80 hours of work in a 2 week period.
Duration:
12 Months (Expected to renew for up to 3 years, on an ongoing 12 month renewal)
Contract Designation:
Full Time Contingent – Contractors will be eligible for WWT’s Full Time Employee Benefits Package including Medical, Vision, Dental, PTO, Paid Holidays, and more.
Responsibilities:
Installing/de-installing/relocating all distributed systems and network hardware (CSUs, DSUs, routers, switches, encryptors, firewalls, etc.) in the Americas Data Centers within the internal service level mandates
Installing/de-installing /extending/relocating/testing all carrier circuits to the network hardware
Installing/de-installing/relocating all patch cabling for systems and network hardware
Installing/de-installing/relocating all Data Center hardware
Assist with the coordination of cabinet power, circuit, and patch infrastructure installations w/various facilities, electrical and communications vendors
Assist with the coordination of network component configurations
Coordinate and Install SAN cabling infrastructure
Managing network ports and assist with the management of all consumable items (cables, labels, tie wraps, rail kits, etc.)
Maintaining the integrity of the data center facilities, systems and communications environments through general housekeeping and best operations practices
Qualifications:
Required skills include 3+ years of experience in the implementation, maintenance and analysis of data center facilities, hardware, communications infrastructure, strategies, tools and effective troubleshooting techniques.
Basic background on enterprise data center facilities and infrastructure environments such as PDUs, RPPs, network and SAN infrastructures. In depth knowledge on complex, Enterprise class inter-networked environments involving a combination of switched/routed/shared Ethernet, TwinAx (100GigE, 25GigE,10GigE, GigE, 100M, and 10M), token ring, SAN, and wide area connectivity.
Strong knowledge of WAN technologies (OC-x, DS-x), subnetting and TCP/IP protocol a must.
Excellent communication and writing skills a must.
Knowledge of trouble ticketing systems, change control, Project processes and associated tools.
Logical problem- solving techniques and associated experience in system, data center facilities, and telecommunications.
Must be Able to Lift up to 50lbs.
Equal Opportunity Employer Minorities/Women/Veterans/Disabled
Show more
Show less","Data Center Facilities, Hardware, Communications Infrastructure, Troubleshooting, PDUs, RPPs, Network Infrastructures, SAN Infrastructures, Switched Ethernet, Routed Ethernet, Shared Ethernet, TwinAx, Token Ring, Wide Area Connectivity, WAN Technologies, Subnetting, TCP/IP, Trouble Ticketing Systems, Change Control, Project Processes","data center facilities, hardware, communications infrastructure, troubleshooting, pdus, rpps, network infrastructures, san infrastructures, switched ethernet, routed ethernet, shared ethernet, twinax, token ring, wide area connectivity, wan technologies, subnetting, tcpip, trouble ticketing systems, change control, project processes","change control, communications infrastructure, data center facilities, hardware, network infrastructures, pdus, project processes, routed ethernet, rpps, san infrastructures, shared ethernet, subnetting, switched ethernet, tcpip, token ring, trouble ticketing systems, troubleshooting, twinax, wan technologies, wide area connectivity"
Data Engineer,My IT LLC,"Delaware, OH",https://www.linkedin.com/jobs/view/data-engineer-at-my-it-llc-3768023173,2023-12-17,Marysville,United States,Mid senior,Onsite,"Job title: Data Engineer
Type: C2C
Location: Cincinnati, OH, USA (Onsite)
Years of work experience: Minimum 7 year(s)
Vendor: Vajraasys Limited Dba RNXT Corporation
Qualifications
Experience required:
5+ years of proven ability in professional Data Development.
3+ years of proven ability in developing with Azure and SQL (Oracle, SQL Server).
3+ years of experience with PySpark/Spark.
2+ years of experience in Azure Data Factory and/or Azure Databricks.
Experience in working with large-scale data sets and distributed systems.
Full understanding of ETL concepts and Data Warehousing concepts.
Exposure to version control software (Git, GitHub SaaS).
Strong understanding of Agile Principles (Scrum).
Bachelor's Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM).
Show more
Show less","Data Development, Azure, SQL, Oracle, SQL Server, PySpark, Spark, Azure Data Factory, Azure Databricks, Distributed Systems, ETL Concepts, Data Warehousing Concepts, Git, GitHub, Agile Principles, Scrum","data development, azure, sql, oracle, sql server, pyspark, spark, azure data factory, azure databricks, distributed systems, etl concepts, data warehousing concepts, git, github, agile principles, scrum","agile principles, azure, azure data factory, azure databricks, data development, data warehousing concepts, distributed systems, etl concepts, git, github, oracle, scrum, spark, sql, sql server"
Data Engineer,My IT LLC,"Delaware, OH",https://www.linkedin.com/jobs/view/data-engineer-at-my-it-llc-3768022186,2023-12-17,Marysville,United States,Mid senior,Onsite,"Assignment Details
Company name: Lorven Technologies Inc
Job Title: Data Engineer
Location: Fremont, CA (Onsite) & Austin, TX (Onsite)
Duration: Long Term Contract
Skills Required
Strong Python Development Exp + Strong MSSQL Exp
Minimum education: Bachelor
Industry experience: desirable
Years of work experience: 8 year(s)
Description
5+ years of prior experience Data Engineer.
Must be proficient in SQL, Python. Automate repetitive personal tasks through Python and SQL
Experience with Tableau or any visualization tool, Data Warehousing, Data Modeling
Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus
Experience with user-defined workflows (e.g., Airflow)
Experience with writing Kafka consumers and producers.
Experience Apache Spark Streaming and Hive is plus.
Experience with Linux servers and docker is required.
Problem solver that is action-oriented with the ability to look at problems in new ways.
Working knowledge of data management software like Airflow, or other ETL tools a plus.
Strong analytical and problem-solving ability to design an effective solution.
Ability to support multiple on-going projects in a fast-paced environment.
Strong communicational skills, organizational skills, negotiation skills, and flexibility to address competing demands.
Superior business judgement ability to flex between big picture thinking, understand and distill complex ideas, and analyze data to drive strategic objectives.
Show more
Show less","Python Development, MSSQL, SQL, Tableau, Data Warehousing, Data Modeling, SQL Server, MySQL, Vertica, NoSQL, Airflow, Kafka, Spark Streaming, Hive, Linux, Docker, ETL, Data Management","python development, mssql, sql, tableau, data warehousing, data modeling, sql server, mysql, vertica, nosql, airflow, kafka, spark streaming, hive, linux, docker, etl, data management","airflow, data management, datamodeling, datawarehouse, docker, etl, hive, kafka, linux, mssql, mysql, nosql, python development, spark streaming, sql, sql server, tableau, vertica"
Data Engineer,Rohini IT Consulting LLP,"Delaware, OH",https://www.linkedin.com/jobs/view/data-engineer-at-rohini-it-consulting-llp-3780626006,2023-12-17,Marysville,United States,Mid senior,Onsite,"Job title: Data Engineer
Type: C2C
Location: Cincinnati, OH, USA (Onsite)
Years of work experience: Minimum 7 year(s)
Vendor: Vajraasys Limited Dba RNXT Corporation
Experience required:
Qualifications:
5+ years of proven ability in professional Data Development.
3+ years of proven ability in developing with Azure and SQL (Oracle, SQL Server).
3+ years of experience with PySpark/Spark.
2+ years of experience in Azure Data Factory and/or Azure Databricks.
Experience in working with large-scale data sets and distributed systems.
Full understanding of ETL concepts and Data Warehousing concepts.
Exposure to version control software (Git, GitHub SaaS).
Strong understanding of Agile Principles (Scrum).
Bachelor's Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM).
Show more
Show less","Data Development, Azure, SQL, PySpark, Spark, Azure Data Factory, Azure Databricks, Data Warehousing, ETL, Git, GitHub SaaS, Agile Principles, Scrum","data development, azure, sql, pyspark, spark, azure data factory, azure databricks, data warehousing, etl, git, github saas, agile principles, scrum","agile principles, azure, azure data factory, azure databricks, data development, datawarehouse, etl, git, github saas, scrum, spark, sql"
Data Engineer,Rohini IT Consulting LLP,"Delaware, OH",https://www.linkedin.com/jobs/view/data-engineer-at-rohini-it-consulting-llp-3780622768,2023-12-17,Marysville,United States,Mid senior,Onsite,"Assignment Details:
Company name: Lorven Technologies Inc
Job Title: Data Engineer
Location: Fremont, CA (Onsite) & Austin, TX (Onsite)
Duration: Long Term Contract
Skills Required:
Strong Python Development Exp + Strong MSSQL Exp
Minimum education: Bachelor
Industry experience: desirable
Years of work experience: 8 year(s)
Description:
· 5+ years of prior experience Data Engineer.
· Must be proficient in SQL, Python. Automate repetitive personal tasks through Python and SQL
· Experience with Tableau or any visualization tool, Data Warehousing, Data Modeling
· Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus
· Experience with user-defined workflows (e.g., Airflow)
· Experience with writing Kafka consumers and producers.
· Experience Apache Spark Streaming and Hive is plus.
· Experience with Linux servers and docker is required.
· Problem solver that is action-oriented with the ability to look at problems in new ways.
· Working knowledge of data management software like Airflow, or other ETL tools a plus.
· Strong analytical and problem-solving ability to design an effective solution.
· Ability to support multiple on-going projects in a fast-paced environment.
· Strong communicational skills, organizational skills, negotiation skills, and flexibility to address competing demands.
· Superior business judgement – ability to flex between big picture thinking, understand and distill complex ideas, and analyze data to drive strategic objectives.
Show more
Show less","Python, MSSQL, SQL, Tableau, Data Warehousing, Data Modeling, SQL Server, MySQL, Vertica, Airflow, Kafka, Apache Spark Streaming, Hive, Linux, Docker, ETL","python, mssql, sql, tableau, data warehousing, data modeling, sql server, mysql, vertica, airflow, kafka, apache spark streaming, hive, linux, docker, etl","airflow, apache spark streaming, datamodeling, datawarehouse, docker, etl, hive, kafka, linux, mssql, mysql, python, sql, sql server, tableau, vertica"
Lead Data Engineer,Quantum Health,"Dublin, OH",https://www.linkedin.com/jobs/view/lead-data-engineer-at-quantum-health-3779702746,2023-12-17,Marysville,United States,Mid senior,Onsite,"Description
Location: This position is located at our Dublin, OH campus with hybrid flexibility.
We’re on a mission to make healthcare simpler and more effective. We fight to ensure our members get the care they need, when they need it, at the most affordable cost – that’s why we call ourselves Healthcare Warriors ® . We’re committed to building diverse and inclusive team s , so if you’re excited about this position, we encourage you to apply – even if your experience doesn’t match every requirement.
The Lead Data Engineer is responsible for leading and delivering enterprise projects and initiatives for Quantum Health's Data portfolio. This role will help with planning, verifying requirements, developing an implementation plan, assisting with solution architecture, executing, and providing direction and mentorship to other engineers on the team. You will also work regularly with our Project Managers, Scrum Masters, and other stakeholders to provide status, raise potential blockers, and help remove any ambiguity on deliverables for your team. The Lead Data Engineer will work closely with the Data Architecture and Governance teams to ensure adherence to architectural and governance standards and provide inputs to develop new standards.
For success in this role, you will be experienced in ETL/ELT, Data Modeling, Data Engineering, problem solving, enterprising, and leading a delivery team to help drive fast, accurate and available data to our partners and clients.
What you’ll do
Lead, educate, and mentor a team of data engineers on Quantum's data integration practices.
Oversee technical activities associated with the assigned projects, provide estimations, review requirements, set timelines and milestones, and distribute tasks to your team.
Implement data integration capabilities and standards that are repeatable and reusable capabilities to transform how the company collects, curates and consumes data.
Help interpret and turn high level business requirements into tangible, clear tasks and requirements for the engineering team.
Collaborate with other teams across the organization to incorporate data governance, metadata management, and data quality into our delivery approach.
Build internal expertise in cloud integration tools, data lake and associated analytical tools or applications.
Provide oversight / guidance related to data issues needing technical or systems expertise.
All other duties as assigned.
What you’ll bring
Education: Bachelor’s or Master’s degree in the field of Information Technology, Computer Science, Mathematics, Data Analytics or other technical field.
Experience: 6+ years of proven progressive experience in Data Engineering and Modeling using at least three various types of data technologies (RDMBS, NoSQL, Graph, Columnar, Document). Direct experience with JavaScript and MarkLogic a plus.
Ability to analyze data, find patterns, identify issues, and enhance and improve the integrity and quality of data and associated technical processes
5+ years of SQL knowledge and experience working with relational databases, query authoring (SQL) and experience manipulating and enhancing data from a variety of sources
5+ years of practice experience in Domain-Driven Design, SOA, Data Management, Data Services and Platforms.
Working knowledge of message queuing, stream processing, and scalable big data stores.
Maintains in-depth knowledge of data best practices, technologies, architectures, and emerging data technologies along with their potential application to existing or future offerings.
Experience with Enterprise integration platforms, IPaaS and API constructs
3+ years of experience in a healthcare data engineering environment strongly preferred.
Deep understanding of standardization, integration and error handling with data integration processes.
Must be an evangelist to help Quantum Health become more sophisticated in its use of data and helping create a data-informed culture.
Strong communication, mentoring / coaching skills and influencing/leadership skills.
Strong listening and consultative skills and ability to negotiate and influence.
Lead by example by pushing innovative solutions in the data & analytics space.
Excellent analytical, problem solving and troubleshooting abilities.
Protect and take care of our company and member’s data every day by committing to work within our company ethics and policies
Strong administrative/technical skills; Comfort working on a PC using Microsoft Office (Outlook, Word, Excel, PowerPoint), IM/video conferencing (Teams & Zoom), and telephones efficiently .
Trustworthy and accountable behavior, capable of viewing and maintaining confidential information daily.
What’s in it for you
Compensation: Competitive base pay, incentive plans and employee referral bonuses.
Coverage: Health, vision and dental featuring our best-in-class healthcare navigation services, along with life insurance, legal and identity protection, adoption assistance, EAP, Tela d oc services and more.
Retirement: 401(k) plan with up to 4% employer match and full vesting on day one.
Balance: Paid Time Off (PTO), 7 paid holidays, parental leave, volunteer days, paid sabbaticals, and more.
Development: Tuition reimbursement up to $5,250 annually, certification/continuing education reimbursement, discounted higher education partnerships, paid trainings and leadership development.
Culture: Recognition as a Best Place to Work for 15+ years, dedication to diversity, philanthropy and sustainability, and people-first values that drive every decision.
Environment: A modern workplace with a casual dress code, open floor plans, full-service dining, free snacks and drinks, complimentary 24/7 fitness center with group classes, outdoor walking paths, game room, notary and dry-cleaning services and more!
What you should know
Internal Associates: Already a Healthcare Warrior? Apply internally through Jobvite.
Process: Application > Phone Screen > Online Assessment(s) > Interview(s) > Offer > Background Check
Diversity, Equity and Inclusion: Quantum Health welcomes everyone. We value our diverse team and suppliers, we’re committed to empowering our ERGs, and we’re proud to be an equal opportunity employer.
Tobacco- F ree Campus: To further enable the health and wellbeing of our associates and community, Quantum Health maintains a tobacco-free environment. The use of all types of tobacco products is prohibited in all company facilities and on all company grounds.
Compensation Ranges: Compensation details published by job boards are estimates and not verified by Quantum Health. Details surrounding compensation will be disclosed throughout the interview process. Compensation offered is based on the candidate’s unique combination of experience and qualifications related to the position.
Agencies: Quantum Health does not accept unsolicited resumes or outreach from third-parties. Absent a signed MSA and request/approval from Talent Acquisition to submit candidates for a specific requisition, we will not approve payment to any third party.
Sponsorship: Applicants must be legally authorized to work in the United States on a permanent and ongoing future basis without requiring sponsorship.
--
KT1  Hybrid
Recruiting Scams: Unfortunately, scams targeting job seekers are common. To protect our candidates, we want to remind you that authorized representatives of Quantum Health will only contact you from an email address ending in @quantum-health.com. Quantum Health will never ask for personally identifiable information such as Date of Birth (DOB), Social Security Number (SSN), banking/direct/tax details, etc. via email or any other non-secure system, nor will we instruct you to make any purchases related to your employment.
If you believe you’ve encountered a recruiting scam, report it to the Federal Trade Commission at ReportFraud.ftc.gov and your state’s Attorney General at https://www.usa.gov/state-attorney-general.
Show more
Show less","Data Engineering, Data Modeling, ETL/ELT, JavaScript, MarkLogic, SQL, Relational Database, DomainDriven Design, SOA, Data Management, Data Services, Data Platforms, Message Queuing, Stream Processing, Big Data, Data Best Practices, Data Technologies, Data Architectures, Cloud Integration Tools, Data Lake, Analytical Tools, Enterprise Integration Platforms, IPaaS, API, Standardization, Integration, Error Handling, Healthcare Data Engineering, Microsoft Office, Teams, Zoom","data engineering, data modeling, etlelt, javascript, marklogic, sql, relational database, domaindriven design, soa, data management, data services, data platforms, message queuing, stream processing, big data, data best practices, data technologies, data architectures, cloud integration tools, data lake, analytical tools, enterprise integration platforms, ipaas, api, standardization, integration, error handling, healthcare data engineering, microsoft office, teams, zoom","analytical tools, api, big data, cloud integration tools, data architectures, data best practices, data engineering, data lake, data management, data platforms, data services, data technologies, datamodeling, domaindriven design, enterprise integration platforms, error handling, etlelt, healthcare data engineering, integration, ipaas, javascript, marklogic, message queuing, microsoft office, relational database, soa, sql, standardization, stream processing, teams, zoom"
Staff Data Engineer,Recruiting from Scratch,"Hilliard, OH",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399007,2023-12-17,Marysville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data Engineering, Data Governance, Data Science, Spark, Snowflake, Airflow, Kafka, Kubernetes, Docker, Helm, SQL, TDD, CI/CD, Kafka, ETL, Data Warehouses, Agile, Pair Programming, Stream Processing, Data Modeling, Schema Design","python, data engineering, data governance, data science, spark, snowflake, airflow, kafka, kubernetes, docker, helm, sql, tdd, cicd, kafka, etl, data warehouses, agile, pair programming, stream processing, data modeling, schema design","agile, airflow, cicd, data engineering, data governance, data science, data warehouses, datamodeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sql, stream processing, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Hilliard, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748829416,2023-12-17,Marysville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Automation, Continuous Delivery, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, SQL, Agile Engineering, Pair Programming, Continuous Integration, Kafka, Storm, Data Modeling, ETL, Data Warehouses, Data Classification, Data Retention","data engineering, tdd, automation, continuous delivery, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, sql, agile engineering, pair programming, continuous integration, kafka, storm, data modeling, etl, data warehouses, data classification, data retention","agile engineering, airflow, automation, continuous delivery, continuous integration, data classification, data engineering, data retention, data science, data warehouses, datamodeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Hilliard, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395190,2023-12-17,Marysville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, SQL, Data Warehouses, ETL pipelines, Data classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, sql, data warehouses, etl pipelines, data classification, retention","airflow, continuous integration, data classification, data warehouses, docker, etl pipelines, helm, kafka, kubernetes, pair programming, python, retention, snowflake, spark, sparkstreaming, sql, storm, tdd"
Business Data Analyst II,DHL Supply Chain,"Westerville, OH",https://www.linkedin.com/jobs/view/business-data-analyst-ii-at-dhl-supply-chain-3776516602,2023-12-17,Marysville,United States,Mid senior,Onsite,"We are seeking a professional individual that supports adminstrative tasks for our executive leadership as well as continue to deploy analytic products to the business unit. This person should be well organized, professional, and have analytical mindset.
Business Data Analyst II
Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Responsible for tracking, planning, analysis, and forecasting of storage capacities, inventory levels, equipment and/or labor requirements
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
0-2 years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Business Analysis, Data Visualization, Optimization, Data Structure, Spreadsheets, Databases, Software, Cycle Time to Action (CTA), Financial Modeling, Predictive Modeling, Data Gathering, Reporting, Forecasting, Technical Problemsolving, Customer Modeling, Labor Budgeting","business analysis, data visualization, optimization, data structure, spreadsheets, databases, software, cycle time to action cta, financial modeling, predictive modeling, data gathering, reporting, forecasting, technical problemsolving, customer modeling, labor budgeting","business analysis, customer modeling, cycle time to action cta, data gathering, data structure, databases, financial modeling, forecasting, labor budgeting, optimization, predictive modeling, reporting, software, spreadsheets, technical problemsolving, visualization"
Business Intelligence and Data Visualization Lead,DHL Supply Chain,"Westerville, OH",https://www.linkedin.com/jobs/view/business-intelligence-and-data-visualization-lead-at-dhl-supply-chain-3783614513,2023-12-17,Marysville,United States,Mid senior,Onsite,"Would you like to join the Logistics Company for the World?Have you often wondered how products get from point A to point B? DHL Supply Chain does just that.
Become an essential part of everyday life, by contributing to an organization that is Connecting People and Improving Lives. If you have a passion for people, a desire to problem-solve,and eagerness to pursue continuous improvement opportunities… we look forward to exploringcareer possibilities with you!
Job Description
Leverage technical and artistic skills to design, develop, and implement data and analytics solutions. Work with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data and analytics life-cycle.
Design, code, test, and aggregate results from SQL queries
Perform semantic layer design according to established standards
Create technical documentation to capture relationships (schema) between different databases
Communicate complex technical constructs to business users
Evaluate and improve existing BI systems and models
Collaborate with various stakeholders to integrate data from multiple source systems
Streamline the delivery of information across the enterprise
Define, develop, and implement data models to deliver self-service analytical capabilities to the business users
Partner with IT to provide business support on the deployment architecture
Design and develop tabular cubes/models for optimal performance
Provide technical support in the migration of existing reporting to Power BI
Design solutions to present large amounts of information in ways that are universally understandable or easy to interpret
Establish and monitor business intelligence design principles
Ability to develop mock-ups to effectively visualize and demonstrate proposed user experience
Harness creativity to render information useful in data storytelling
Experience working with structured, semi-structured and unstructured data sets, determining the key takeaways or action items and communicating them pictorially
Required Education And Experience
Undergraduate degree in data analytics, computer science, mathematics, statistics, management information systems, or related field, required
5+ years of analytics experience, required
2+ years of experience with Microsoft Power BI, required
2+ years of experience with data architecture and warehousing, required
2+ years of experience in data visualization role, required
Experience in all stages of BI project work (requirements and logical design, physical design, implementation, testing, and deployment), required
Microsoft BI stack (cube development, SSRS, SSAS, SSIS), required
Experience with machine learning (Python, R, Azure ML, etc.), preferred
Experience in writing complex relational and multidimensional database queries, required
Knowledge of and experience with Azure platform (Data Factory, Data Lake Store, SQL Data Warehouse, ML, Stream Analytics, etc.), required
Working knowledge of logistics and/or transportation operations to be able to identify areas for improvement, preferred
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Data analytics, Data architecture, Data warehousing, Data visualization, Business intelligence, SQL, Power BI, Machine learning, Python, R, Azure ML, Azure Data Factory, Azure Data Lake Store, Azure SQL Data Warehouse, Azure ML, Azure Stream Analytics, Logistics, Transportation","data analytics, data architecture, data warehousing, data visualization, business intelligence, sql, power bi, machine learning, python, r, azure ml, azure data factory, azure data lake store, azure sql data warehouse, azure ml, azure stream analytics, logistics, transportation","azure data factory, azure data lake store, azure ml, azure sql data warehouse, azure stream analytics, business intelligence, data architecture, dataanalytics, datawarehouse, logistics, machine learning, powerbi, python, r, sql, transportation, visualization"
Sr Business Data Analyst,DHL Supply Chain,"Westerville, OH",https://www.linkedin.com/jobs/view/sr-business-data-analyst-at-dhl-supply-chain-3748331011,2023-12-17,Marysville,United States,Mid senior,Onsite,"Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
1+ years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Analytics, Data visualization, Optimization, Reporting, Databases, Spreadsheets, Data gathering, Predictive modeling, Data analysis, Financial modeling, Problemsolving","analytics, data visualization, optimization, reporting, databases, spreadsheets, data gathering, predictive modeling, data analysis, financial modeling, problemsolving","analytics, data gathering, dataanalytics, databases, financial modeling, optimization, predictive modeling, problemsolving, reporting, spreadsheets, visualization"
Sr. Data Engineer,Outcomes™,"Dublin, OH",https://www.linkedin.com/jobs/view/sr-data-engineer-at-outcomes%E2%84%A2-3780319518,2023-12-17,Marysville,United States,Mid senior,Onsite,"Job Details
Job Location
Dublin Office - Dublin, OH
Description
JOB SUMMARY
The Senior Data Engineer is responsible for modeling complex problems, building pipelines, maintaining ETL processes and troubleshooting issues within a cloud environment. They utilize cloud databases, Databricks and databases to support a robust infrastructure which drives large, revenue generating data strategies. Additionally, the Senior Data Engineer suggests architecture ideas, works closely with architects and assists junior engineers with their tasks/approaches.
Essential Duties And Responsibilities
Participate in use case feasibility discussions and translate business idea / business problems into use cases.
Provide support as needed to maintain and update models running in production environment.
Develop and maintain complex ETL processes and algorithms
Own and enhance ETL processes to move data from PMS to cloud databases
Monitor and troubleshoot processes on a daily basis
Document new and existing processes
Proactively and independently identify performance issues and recommend enhancements
Modernize legacy data models and pipelines using ETL tools and cloud database capabilities.
Work closely with Chief Architect to make improvements and larger architectural changes
Recommend or suggest improvements to the existing architecture.
Support junior team members with technical questions, code reviews and code enhancements.
Understand business problems/needs and provide proposed solutions.
Qualifications
KNOWLEDGE & REQUIREMENTS
Required Qualifications
Be able to work well with people of various backgrounds and education levels and establish cooperative working relationships with all coworkers.
Timely and effectively communicate information to and consult with others in order to complete work assignments.
Act in a responsible, trustworthy and ethical manner that considers the impact and consequences of one’s actions or decisions.
Communicate ideas, thoughts, and facts in writing through the use of proper grammar, spelling, document formatting and sentence structure.
Identify and respond to current and future clients’ needs; provide excellent client service.
Evaluate and analyze problems or tasks from multiple perspectives; adaptively employ problem solving methods to find creative or novel solutions; use logical, systematic and sequential processes to solve problems.
Complete assigned job tasks in an accurate and timely manner.
Carefully prepare for meetings and presentations; follow up with others to ensure that agreements, tasks or commitments have been fulfilled.
Demonstrate commitment to achieving Company’s core business objectives of increasing the role of pharmacy and improving patient health in America.
Desirable Qualifications
Experience working with healthcare professionals in a clinical setting.
Experience resolving issues that do not have clear answers.
Thorough experience with Databricks, Glue, Talend, Informatica or other similar ETL tools.
Experience working in cloud databases (Redshift, BigQuery)
Highly motivated and possessed excellent interpersonal, problem solving, and technical skills.
High sense of urgency and accountability
Adaptable, friendly, and ability to work with a team.
Excellent attendance
Passion for data and digging into the minutia of datasets.
Take calculated risks based on data-driven analytics
Be a self-starter
Enjoy working in a fast-paced environment.
Education & Experience
Experience working with healthcare professionals in a clinical setting
6+ years of data analysis experience
Expertise in algorithm design, machine learning, and applied statistics
Proven track record in use of SQL specifically in cloud databases and working with data including extracting information, validating data, creating and maintaining custom data structures.
Bachelor’s degree in Business, Computer Science, Information Systems or equivalent combination of education and experience.
Show more
Show less","Cloud databases, Databricks, Databases, ETL, Python, SQL, Machine Learning, Statistics, Algorithm design, Data structures, Business intelligence, Data modeling, Data mining","cloud databases, databricks, databases, etl, python, sql, machine learning, statistics, algorithm design, data structures, business intelligence, data modeling, data mining","algorithm design, business intelligence, cloud databases, data mining, data structures, databases, databricks, datamodeling, etl, machine learning, python, sql, statistics"
Data Center Engineer,World Wide Technology,"Delaware, OH",https://www.linkedin.com/jobs/view/data-center-engineer-at-world-wide-technology-3776648680,2023-12-17,Marysville,United States,Mid senior,Onsite,"Data Center Engineer
Company Overview
World Wide Technology (WWT) is a global technology integrator and supply chain solutions provider. Through our culture of innovation, we inspire, build, and deliver business results, from idea to outcome.
Based in St. Louis, WWT works closely with industry leaders such as Cisco, HPE, Dell EMC, NetApp, VMware, Intel, AWS, Microsoft, and F5, focusing on three market segments: Fortune 500 companies, service providers and the public sector. WWT is a $17 billion dollar privately held organization that employs more than 8,000 people and operates in more than 4 million square feet of state-of-the-art warehousing, distribution and integration space strategically located throughout the world. WWT is proud to announce that it has been named on the FORTUNE ""100 Best Places to Work For®"" list for the ninth consecutive year and was awarded for multiple categories on Glassdoor’s 2019 Employees' Choice Awards, honoring the Best Places to Work.
World Wide Technology Holding Co, LLC. (WWT) has an opportunity available for a
Data Center Engineer
to support our client located at
Delaware, OH – Onsite.
Title: Data Center Engineer
Location:
Delaware, OH – Onsite
Duration/Type of Job: 12 Months
Qualifications:
Required skills include 3+ years of experience in the implementation, maintenance and analysis of data center facilities, hardware, communications infrastructure, strategies, tools and effective troubleshooting techniques.
Basic background on enterprise data center facilities and infrastructure environments such as PDUs, RPPs, network and SAN infrastructures. In depth knowledge on complex, Enterprise class inter-networked environments involving a combination of switched/routed/shared Ethernet, TwinAx (100GigE, 25GigE,10GigE, GigE, 100M, and 10M), token ring, SAN, and wide area connectivity.
Strong knowledge of WAN technologies (OC-x, DS-x), subnetting and TCP/IP protocol a must.
Excellent communication and writing skills a must.
Knowledge of trouble ticketing systems, change control, Project processes and associated tools.
Logical problem- solving techniques and associated experience in system, data center facilities, and telecommunications.
Experience with project management.
Financial Services industry knowledge a plus.
Equal Opportunity Employer Minorities/Women/Veterans/Disabled
Show more
Show less","Data center facilities, Hardware, Communications infrastructure, Troubleshooting, PDUs, RPPs, Network infrastructures, SAN infrastructures, Internetworked environments, Switched/routed/shared Ethernet, TwinAx, Token ring, WAN technologies, TCP/IP protocol, Communication skills, Writing skills, Trouble ticketing systems, Change control, Project processes, Project management, Financial Services industry","data center facilities, hardware, communications infrastructure, troubleshooting, pdus, rpps, network infrastructures, san infrastructures, internetworked environments, switchedroutedshared ethernet, twinax, token ring, wan technologies, tcpip protocol, communication skills, writing skills, trouble ticketing systems, change control, project processes, project management, financial services industry","change control, communication skills, communications infrastructure, data center facilities, financial services industry, hardware, internetworked environments, network infrastructures, pdus, project management, project processes, rpps, san infrastructures, switchedroutedshared ethernet, tcpip protocol, token ring, trouble ticketing systems, troubleshooting, twinax, wan technologies, writing skills"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hilliard, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709531,2023-12-17,Marysville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Apache Airflow, Kubernetes, Apache Spark, PySpark, Python, Java, Bash, SQL, Git, Snowflake, Docker, Helm, NoSQL, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, Relational Databases, Query Authoring","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, apache airflow, kubernetes, apache spark, pyspark, python, java, bash, sql, git, snowflake, docker, helm, nosql, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, relational databases, query authoring","apache airflow, apache spark, applied machine learning, bash, data classification, data cleaning, data engineering, data mining, data normalization, data retention, datamodeling, docker, etl, git, helm, java, kafka, kubernetes, machine learning, nosql, python, query authoring, relational databases, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hilliard, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773092090,2023-12-17,Marysville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pysPark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, Complex Data Projects","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, complex data projects","airflow, applied machine learning, aws, azure, bash, complex data projects, data classification, data engineering, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Azure Data Engineer (Hybrid),"Liberty Personnel Services, Inc.","Marlton, NJ",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-hybrid-at-liberty-personnel-services-inc-3629205589,2023-12-17,Winslow,United States,Associate,Onsite,"Job Details:
Senior Azure Data Engineer
My client has hired me to find for them a Senior Azure Data Engineer. The manager would like experience in cloud-based data warehousing and data lake solutions such as Databricks, cloud orchestration, especially Azure Data Factory. This is a hybrid role.
If you are interested please forward your resume in word format to kevin@libertyjobs.com
Kevin McCarthy
#associate
#mid-senior
#libertyjobs
Show more
Show less","Azure, Data warehousing, Data lake solutions, Databricks, Cloud orchestration, Azure Data Factory","azure, data warehousing, data lake solutions, databricks, cloud orchestration, azure data factory","azure, azure data factory, cloud orchestration, data lake solutions, databricks, datawarehouse"
Principal Data Engineer,FinTech LLC,"Philadelphia, PA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-fintech-llc-3664985394,2023-12-17,Winslow,United States,Associate,Onsite,"Salary Range: $180K-$190K/Annum.
Job Description:
We are looking for professionals with these required skills to achieve our goals:
Bachelors Degree in Computer Science, Software Engineering or Data Science.
5+ years of programming experience in Java and/or Python.
5+ years of experience in distributed systems (cloud computing).
5+ years of experience in a technical lead capacity role.
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Masters Degree in Computer Science, Software Engineering or Data Science
Experience with the Azure data and analytics stack: Databricks, Data Factory, SQL DW, Cosmos DB, Power BI, etc.
Experience integrating and supporting a variety of enterprise data tools: Ataccama, Talend, Collibra, Snowflake, Stream Sets, etc.
Experience with enterprise cloud data architecture (preferably Azure) and delivering solutions at scale.
Experience with Agile delivery frameworks and tools: SAFe, Jira, Confluence, Azure DevOps, etc.
Experience applying CI/CD principles and processes to data solutions.
Highly innovative mind-set and experience with analytics in a healthcare or Client company.
Experience using Spark and/or Databricks to solve data science and machine learning business problems.
Ability to work in close partnership with groups across the IT organization (security, compliance, infrastructure, etc.) and business stakeholders in the commercial organizations.
Ability to develop and maintain productive working relationships with suppliers and specialist technology providers.
Superior communication skills and the ability to communicate inherently complicated technical concepts to non-technical stakeholders of all levels.
Proven track record of developing and executing data-driven strategies that enhance business performance.
Strong knowledge of statistical modeling, machine learning, and deep learning techniques, and their practical applications in a business context.
About ApTask:
ApTask is a dynamic workforce management solutions company dedicated to helping professionals excel in their careers. With a focus on IT, project management, and strategic consulting roles, ApTask offers tailored opportunities that align with your aspirations. Join our thriving community of skilled professionals and unlock your potential to make a meaningful impact. Client exciting career prospects with ApTask today at
www.aptask.com
.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
​​​​​​​
Show more
Show less","Java, Python, Distributed systems, Cloud computing, Azure data and analytics stack, Databricks, Data Factory, SQL DW, Cosmos DB, Power BI, Enterprise data tools, Ataccama, Talend, Collibra, Snowflake, Stream Sets, Enterprise cloud data architecture, Agile delivery frameworks, SAFe, Jira, Confluence, Azure DevOps, CI/CD principles, Data science, Machine learning, Spark, Statistical modeling, Deep learning","java, python, distributed systems, cloud computing, azure data and analytics stack, databricks, data factory, sql dw, cosmos db, power bi, enterprise data tools, ataccama, talend, collibra, snowflake, stream sets, enterprise cloud data architecture, agile delivery frameworks, safe, jira, confluence, azure devops, cicd principles, data science, machine learning, spark, statistical modeling, deep learning","agile delivery frameworks, ataccama, azure data and analytics stack, azure devops, cicd principles, cloud computing, collibra, confluence, cosmos db, data factory, data science, databricks, deep learning, distributed systems, enterprise cloud data architecture, enterprise data tools, java, jira, machine learning, powerbi, python, safe, snowflake, spark, sql dw, statistical modeling, stream sets, talend"
Future Opportunity- Data Engineering Consultant,Avanade,"Philadelphia, PA",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781782161,2023-12-17,Winslow,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, SQL, Databricks, Azure Synapse, Data warehousing, Data storage, Data services, Data mining, Information retrieval, Data security, Data pipelines, Data streams, System integration, Data modeling, Data analysis, Data interpretation, Data integrity, Data manipulation, Data error identification, Data error handling","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, sql, databricks, azure synapse, data warehousing, data storage, data services, data mining, information retrieval, data security, data pipelines, data streams, system integration, data modeling, data analysis, data interpretation, data integrity, data manipulation, data error identification, data error handling","azure databricks, azure synapse, data error handling, data error identification, data integrity, data interpretation, data manipulation, data mining, data security, data services, data storage, data streams, dataanalytics, databricks, datamodeling, datapipeline, datawarehouse, information retrieval, microsoft fabricsynapse, powerbi, purview, python, spark, sql, system integration, tsql"
StrataJazz / EPSI Data Analyst (Remote Option),"Liberty Personnel Services, Inc.","Philadelphia, PA",https://www.linkedin.com/jobs/view/stratajazz-epsi-data-analyst-remote-option-at-liberty-personnel-services-inc-3787103996,2023-12-17,Winslow,United States,Associate,Remote,"Job Details:
StrataJazz / EPSI Data Analyst (Remote Option)
My client has hired me to find for them a StrataJazz / EPSI Data Analyst. The Data Analyst functions as a primary source of clinical and financial data, prepares summaries and reports based on the data, and is the key provider of analysis of that data. The Data Analyst recommends changes in development, maintenance of system standards to accomplish a consistent implementation of decision support tools across the organization. Experience with Stratajazz or EPSI for performance management. This role is remote optional.
If you are interested, please forward your resume in word format to kevin@libertyjobs.com
Kevin McCarthy
#associate
#mid-senior
Show more
Show less","Stratajazz, EPSI, Data Analyst, Data Analysis, Clinical Data, Financial Data, Reporting, Data Visualization, Data Interpretation, Decision Support Tools, System Standards, Consistent Implementation","stratajazz, epsi, data analyst, data analysis, clinical data, financial data, reporting, data visualization, data interpretation, decision support tools, system standards, consistent implementation","clinical data, consistent implementation, data interpretation, dataanalytics, decision support tools, epsi, financial data, reporting, stratajazz, system standards, visualization"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783190137,2023-12-17,Winslow,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Philadel-DataScientist.012
Show more
Show less","Python, JavaScript, JSON, OOP, Generative AI, Education technology, Research, Data science, Product development, Strategy, Product engineering","python, javascript, json, oop, generative ai, education technology, research, data science, product development, strategy, product engineering","data science, education technology, generative ai, javascript, json, oop, product development, product engineering, python, research, strategy"
Senior Data Engineer (Remote),MMS,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782262498,2023-12-17,Winslow,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data engineering, Data science, Data modeling, Data architecture, Data warehousing, Star schemas, Data lineage, TSQL, Stored procedures, Window functions, Common table expressions, Derived tables, Dynamic TSQL, Query optimization, Query tuning, Software development, Refactoring, Design patterns, Abstraction, Encapsulation, Azure data factory, Microsoft SQL, Data lake, Data curation, Clinical trials, Pharmaceutical development, CDISC, FHIR, OMOP, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP","data engineering, data science, data modeling, data architecture, data warehousing, star schemas, data lineage, tsql, stored procedures, window functions, common table expressions, derived tables, dynamic tsql, query optimization, query tuning, software development, refactoring, design patterns, abstraction, encapsulation, azure data factory, microsoft sql, data lake, data curation, clinical trials, pharmaceutical development, cdisc, fhir, omop, iso 9001, iso 27001, 21 cfr part 11, fda, gcp","21 cfr part 11, abstraction, azure data factory, cdisc, clinical trials, common table expressions, data architecture, data curation, data engineering, data lake, data lineage, data science, datamodeling, datawarehouse, derived tables, design patterns, dynamic tsql, encapsulation, fda, fhir, gcp, iso 27001, iso 9001, microsoft sql, omop, pharmaceutical development, query optimization, query tuning, refactoring, software development, star schemas, stored procedures, tsql, window functions"
Data Analyst,International SOS,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-analyst-at-international-sos-3727336088,2023-12-17,Winslow,United States,Mid senior,Onsite,"International SOS is the world’s leading medical and security services company with over 12,000 employees working in 1,000 locations in 90 countries. We were founded on the principle of putting our clients’ employees first and this is still true today. Led by 5,200 medical professionals and 200 security specialists our teams work night and day to find solutions to protect our clients and their employees in whatever situation they may be facing; we assess, advise and assist from a medical, security and logistical perspective on a global scale to protect and save lives and thereby enable our clients to achieve their business goals. As we’ve delivered on this mission over the last 35 years, we have become the market leader in global telehealth services and digital health solutions for an extensive client base of Fortune 500 companies, NGO’s and governments around the world.
Job Purpose
The Data Analyst will develop, implement, and maintain leading-edge analytics systems, taking complicated problems and building simple frameworks. Work with data by analyzing & transforming raw data in meaningful data structures used for Business Insights and Data Analytics. Work closely with other data engineers, architects, tech leads & scrum teams in product development. Create and maintain Data Catalogue across the business line to use it for business intelligence.
Essential Job Duties And Responsibilities
Build reporting and analytics to support product adoption, business insights, etc.,
Designs and develops operational and reporting database systems utilizing the latest techniques in data modeling and ETL concepts.
Work closely with product managers and product owners to understand and maintain focus on their analytics needs, including critical metrics and KPIs, and deliver actionable insights to relevant decision-makers
Proactively analyze data to answer key questions for stakeholders or yourself, with an eye on what drives business performance, and investigate and communicate which areas need improvement in efficiency and productivity
Create and maintain rich interactive visualizations through data interpretation and analysis, with reporting components from multiple data sources
Define and implement data acquisition and integration logic, selecting an appropriate combination of methods and tools within the defined technology stack to ensure optimal scalability and performance of the solution
Develop and maintain databases by acquiring data from primary and secondary sources and build scripts that will make our data evaluation process more flexible or scalable across datasets.
Working with programmers, engineers, and management heads to identify process improvement opportunities, propose system modifications, and devise data governance strategies.
Qualifications
Required Skills & Knowledge:
Strong SQL or Excel skills, with aptitude for learning other analytics tools
Knowledge of AWS RDS solutions & Redshift is a plus.
Analytical and diligent with great attention to detail
Resilient: ability to work successfully in a fast-paced environment with shifting priorities
Strong collaborative skills
Excellent verbal and written communication
Required Work Experience
Relevant 7+ years technical experience
Experience with SQL Server, AWS RDS, Document DB, etc.,
Experience with data visualization tools like Microsoft PowerBI, Tableau, Crystal Reports, etc.,
Experience with analytics platforms like Google Analytics or similar
Technical writing experience in relevant areas, including queries, reports, and presentations
Required Education
Bachelor’s degree in Computer Science, Information Technology (or similar)
Required Languages
English to a very high standard written and oral.
Footer
International SOS is an equal opportunity employer and does not discriminate against employees or job applicants on the basis of race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability, genetic information, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state and local laws.
Show more
Show less","SQL, Excel, AWS RDS, Redshift, Data modeling, ETL, Business intelligence, Data visualization, Data integration, Data governance, PowerBI, Tableau, Crystal Reports, Google Analytics, Technical writing, Computer Science, Information Technology, English","sql, excel, aws rds, redshift, data modeling, etl, business intelligence, data visualization, data integration, data governance, powerbi, tableau, crystal reports, google analytics, technical writing, computer science, information technology, english","aws rds, business intelligence, computer science, crystal reports, data governance, data integration, datamodeling, english, etl, excel, google analytics, information technology, powerbi, redshift, sql, tableau, technical writing, visualization"
Data Engineer (Free Library of Philadelphia),City of Philadelphia,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-engineer-free-library-of-philadelphia-at-city-of-philadelphia-3751615924,2023-12-17,Winslow,United States,Mid senior,Onsite,"Company Description
A best-in-class city that attracts best-in-class talent, Philadelphia is an incredible place to build a career. From our thriving arts scene and rich history to our culture of passion and grit, there are countless reasons to love living and working here. With a workforce of over 30,000 people, and more than 1,000 different job categories, the City of Philadelphia offers boundless opportunities to make an impact.
As an employer, the City of Philadelphia values inclusion, integrity, innovation, empowerment, and hard work above all else. We offer a vibrant work environment, comprehensive health care and benefits, and the experience you need to grow and excel. If you’re interested in working with a passionate team of people who care about the future of Philadelphia, start here.
What We Offer
Impact - The work you do here matters to millions.
Growth - Philadelphia is growing, why not grow with it?
Diversity & Inclusion - Find a career in a place where everyone belongs.
Benefits - We care about your well-being.
The Office of Innovation & Technology (OIT) is the central IT agency for the City of Philadelphia headed by the Chief Information Officer (CIO). OIT oversees all major information and communications technology initiatives for the City of Philadelphia - increasing the effectiveness of the information technology infrastructure, where the services provided are advanced, optimized, and responsive to the needs of the City of Philadelphia’s businesses, residents, and visitors. OIT responsibilities include: identifying the most effective approach for implementing new information technology directions throughout city government; improving the value of the city’s technology assets and the return on the city’s technology investments; ensuring data security continuity; planning for continuing operations in the event of disruption of information technology or communications services; and supporting accountable, efficient and effective government across every city department, board, commission and agency.
The Free Library of Philadelphia (FLP) is one of the largest public library systems in the world. As an important cornerstone of the Philadelphia community, FLP has a mission to advance literacy, guide learning, and inspire curiosity. The long-term vision of the FLP is to build an enlightened community devoted to lifelong learning. For over 100 years, the FLP has championed education in and out of the classroom, providing no cost resources for literacy and learning.
Job Description
The Free Library of Philadelphia is looking for an experienced Data Engineer to join the Research & Data Analytics group. The Data Engineer will be responsible for designing, developing, and maintaining data architecture and infrastructure that enable efficient collection, storage, and analysis of data to support Performance Management objectives.
The Data Engineer will work closely with data scientists, analysts, and other team members to ensure that data is available, reliable, and ready for analysis. Expertise in data pipelines, ETL (Extract, Transform, Load) processes, and data warehousing will be essential in this role.
Beyond being a key asset to enterprise integration and open data initiatives, the Data Engineer will help develop and support the creation of Open Data initiatives for the Free Library’s data sharing objectives by integrating appropriate City, state, and federal datasets.
Essential Functions
Design, build, and maintain data pipelines to automate extraction, transformation, and loading of data from diverse sources.
Develop and maintain data models that support Performance Management objectives, including designing schema structures, data dictionaries, and defining relationships.
Integrate data from multiple sources, such as databases, APIs, logs, and third-party data providers, into a unified and accessible format.
Develop and optimize ETL processes to ensure data quality, consistency, and availability. Handle data cleansing, enrichment, and transformation as needed.
Maintain and optimize data warehousing solutions (e.g., SQL databases, NoSQL databases) to ensure efficient data storage and availability.
Monitor and fine-tune data pipelines and database systems for optimal performance, scalability, and reliability.
Implement data security and privacy measures to protect sensitive information, ensure compliance with appropriate data safety regulations, and establish access controls.
Create and maintain documentation for data engineering processes, data models, and data flow diagrams to facilitate understanding and collaboration.
Collaborate with data analysts, software engineers, and FLP Leadership to understand data requirements and provide support for data-driven decision making.
Implement data governance practices, including data lineage, metadata management, and data cataloging.
Required
Competencies, Knowledge, Skills and Abilities
Proficiency in programming languages (R, Python) and database query languages (SQL).
Knowledge of ETL tools and data integration techniques.
Experience with data warehousing solutions.
Familiarity with big data technologies (Hadoop, Spark) is a plus.
Experience with Linux server administration.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.
Preferred
Experience with cloud platforms (e.g., AWS, Azure, Google Cloud).
Experience handling regulatory requirements related to PII.
Knowledge of containerization and orchestration tools (e.g., Docker, Kubernetes).
Understanding of data streaming technologies (e.g., Kafka, Apache Flink).
Familiarity with data governance and data cataloging tools.
Qualifications
Bachelor's or higher degree in Computer Science, Information Technology, Information Science, or a related field.
3 years experience as a Data Engineer or similar role.
Additional Information
Salary Range: $75,000 ‐ $85,000
Starting salary to be determined based on experience and qualifications.
Important: To apply, candidates must provide a cover letter and a resume.
Discover the Perks of Being a City of Philadelphia Employee
:
We offer Comprehensive health coverage for employees and their eligible dependents
Our wellness program offers eligibility into the discounted medical plan
Employees receive paid vacation, sick leave, and holidays
Generous retirement savings options are available
Pay off your student loans faster - As a qualifying employer, City of Philadelphia employees are eligible to participate in the Public Service Loan Forgiveness program. Join the ranks of hundreds of employees who have already benefited from this program and achieved student loan forgiveness.
Enjoy a Free Commute on SEPTA - Starting September 1, 2023, eligible City employees will no longer have to worry about paying for SEPTA public transportation. Whether you're a full-time, part-time, or provisional employee, you can seize the opportunity to sign up for the SEPTA Key Advantage Program and receive free Key cards for free rides on SEPTA buses, trains, trolleys, and regional rails.
Unlock Tuition Discounts and Scholarships - The City of Philadelphia has forged partnerships with over a dozen esteemed colleges and universities in the area, ensuring that our employees have access to a wide range of tuition discounts and scholarships. Experience savings of 10% to 40% on your educational expenses, extending not only to City employees but in some cases, spouse and dependents too!
Join the City of Philadelphia team today and seize these incredible benefits designed to enhance your financial well-being and personal growth!
The successful candidate must be a city of Philadelphia resident within six months of hire
Effective May 22, 2023, vaccinations are no longer required for new employees that work in non-medical, non-emergency or patient facing positions with the City of Philadelphia. As a result, only employees in positions providing services that are patient-facing medical care (ex: Nurses, doctors, emergency medical personnel), must be fully vaccinated.
The City of Philadelphia is an Equal Opportunity employer and does not permit discrimination based on race, ethnicity, color, sex, sexual orientation, gender identity, religion, national origin, ancestry, age, disability, marital status, source of income, familial status, genetic information or domestic or sexual violence victim status. If you believe you were discriminated against, call the Philadelphia Commission on Human Relations at 215-686-4670 or send an email to faqpchr@phila.gov.
Show more
Show less","Data Engineering, Data Warehousing, SQL, Python, R, Big Data, Hadoop, Spark, Linux, Docker, Kubernetes, Kafka, AWS, Azure, Google Cloud, PII, ETL, ETL Tools, Data Integration, Data Lineage, Metadata Management, Data Catalog, Data Pipelines","data engineering, data warehousing, sql, python, r, big data, hadoop, spark, linux, docker, kubernetes, kafka, aws, azure, google cloud, pii, etl, etl tools, data integration, data lineage, metadata management, data catalog, data pipelines","aws, azure, big data, data catalog, data engineering, data integration, data lineage, datapipeline, datawarehouse, docker, etl, etl tools, google cloud, hadoop, kafka, kubernetes, linux, metadata management, pii, python, r, spark, sql"
"Big Data Developer @ Philadelphia,PA (onsite)",Smart IT Frame LLC,"Philadelphia, PA",https://www.linkedin.com/jobs/view/big-data-developer-%40-philadelphia-pa-onsite-at-smart-it-frame-llc-3781754753,2023-12-17,Winslow,United States,Mid senior,Onsite,"Job Title: Bigdata Developer
Location: Philadelphia,PA (onsite)
Mode of Hiring
–
Contract
Big Data Developer (Skillset: Kafka + Spark + Splunk)
• Expertise in Kafka, Splunk as Primary. Proven skills in creating splunk dashboards. Experience in understanding the business requirements.
• Experience 7-9yrs
• Required Skills Technical Skills- Kafka, Splunk Domain Skills-
• Nice to have skills Techincal Skills- Spark & AWS Domain Skills-
• Technology Data Management
• Roles & Responsibilities Expertise in Kafka, Splunk as Primary. Proven skills in creating splunk dashboards. Experience in understanding the business requirements. Excellent communication skills. Able to work in a fast-paced development environment
• Should possess knowledge and hands on experience in, Kafka, Splunk as Primary and Spark and AWS as secondary.
• Experience with Apache Kafka(Kafka Connectors, Kafka Streams)
• Experience in Splunk in creating splunk dashboards including splunk performance optimization.
• Experience with Apache Spark streaming and batch framework
• Experience in any one of the cloud service providers, preferably AWS
• Knowledge of Hadoop (HDFS, Hive)
• Knowledge in Programming language Scala/Java/Python
• Agile/Scrum methodology experience is required.
• Demonstrate ownership and initiative taking
• Prior experience in real time streaming applications.
Regards,
Sanjeev
sanjeev@smartitframe.com
Show more
Show less","Kafka, AWS, Spark, Splunk, Hadoop, HDFS, Hive, Scala, Java, Python, Agile, Scrum, Apache Kafka Connectors, Apache Kafka Streams, Apache Spark streaming, Apache Spark batch framework, Cloud service providers","kafka, aws, spark, splunk, hadoop, hdfs, hive, scala, java, python, agile, scrum, apache kafka connectors, apache kafka streams, apache spark streaming, apache spark batch framework, cloud service providers","agile, apache kafka connectors, apache kafka streams, apache spark batch framework, apache spark streaming, aws, cloud service providers, hadoop, hdfs, hive, java, kafka, python, scala, scrum, spark, splunk"
Senior Data Engineer,Elder Research,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-elder-research-3693043556,2023-12-17,Winslow,United States,Mid senior,Onsite,"Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel Required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education And Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.
Show more
Show less","Data Engineering, Software Architecture, Data Visualization, Cloud Platforms (AWS Azure Google Cloud), Continuous Integration and Continuous Deployment (CI/CD), Agile Development, Git, SQL, NoSQL, Python, Java, REST APIs, Power BI, Tableau, Modeling, Data Analysis, Machine Learning, Artificial Intelligence","data engineering, software architecture, data visualization, cloud platforms aws azure google cloud, continuous integration and continuous deployment cicd, agile development, git, sql, nosql, python, java, rest apis, power bi, tableau, modeling, data analysis, machine learning, artificial intelligence","agile development, artificial intelligence, cloud platforms aws azure google cloud, continuous integration and continuous deployment cicd, data engineering, dataanalytics, git, java, machine learning, modeling, nosql, powerbi, python, rest apis, software architecture, sql, tableau, visualization"
Senior Data Engineer,Extend Information Systems Inc.,"Mount Laurel, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-extend-information-systems-inc-3659636711,2023-12-17,Winslow,United States,Mid senior,Onsite,"Hi, Jobseekers,
I hope you are doing well!
We have an opportunity for
Senior Data Engineer
with one of our clients for
Mount Laurel, NJ
.
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title: Senior Data Engineer-
Azure Cloud based Data/ETL
Location: Mount Laurel, NJ / Charlotte, NC (First preference will be NJ)
Terms: Direct Hire
Job Details
Keys Skills:
Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre or SQL Server SSIS or DataStage
Must Have
More than 8-12+ years of IT experience in Datawarehouse
Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
Ability to understand Design, Source to target mapping (STTM) and create specifications documents
Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any relevant certifications
Thanks & Regards
Amit Jha
Extend Information System Inc
Phone:571-800-1707
Email:
ajha@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","Data Engineering, Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre, SQL Server SSIS, DataStage, ETL, Cloud Technologies, Design, Source to Target Mapping (STTM), Banking Experience, Risk, Regulatory, Commercial, Credit Cards, Retail","data engineering, azure, synapse, adf, databricks, pyspark, informatica power centre, sql server ssis, datastage, etl, cloud technologies, design, source to target mapping sttm, banking experience, risk, regulatory, commercial, credit cards, retail","adf, azure, banking experience, cloud technologies, commercial, credit cards, data engineering, databricks, datastage, design, etl, informatica power centre, regulatory, retail, risk, source to target mapping sttm, spark, sql server ssis, synapse"
Senior Data Engineer with PySpark only NJ Location,iTvorks Inc,"Mt. Laurel, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-with-pyspark-only-nj-location-at-itvorks-inc-3780692183,2023-12-17,Winslow,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, iTvorks Inc, is seeking the following. Apply via Dice today!
Senior Data Engineer with PySpark – Day 1 onsite
Locations – Mt Laurel, NJ
Experience: 10-12+ Years
Primary Tech Skills – Databricks, Pyspark
Secondary Tech skills – Azure, SAS, Synapse,
Experience in building end-to-end architecture for Data Lakes, Data Warehouses, and Data Marts
Experience of DWH, Data Integration, Cloud, Architecture, Design, Data Modelling
Hands-on experience in Pyspark is a must.
Experience working with structured and unstructured data .
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements.
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience with Orchestration tools and GitHub
Experience to own end-to-end development, including coding, testing, debugging, and deployment.
Must be team-oriented with strong communication, collaboration, prioritization, and adaptability skill
Need to be an individual performer as well as a Lead.
Senior Data Engineer with PySpark only NJ Location
Show more
Show less","Databricks, Apache Spark, Python, Azure, SAS, Synapse, Data Lakes, Data Warehouses, Data Marts, Data Integration, Cloud, Architecture, Data Modelling, ETL, Data Warehousing, Data Ingestion, Data Preparation, Data Integration, Data Operationalization, MS SQL, Delta Lake, Spark SQL, SQL Server, Orchestration Tools, Git","databricks, apache spark, python, azure, sas, synapse, data lakes, data warehouses, data marts, data integration, cloud, architecture, data modelling, etl, data warehousing, data ingestion, data preparation, data integration, data operationalization, ms sql, delta lake, spark sql, sql server, orchestration tools, git","apache spark, architecture, azure, cloud, data ingestion, data integration, data lakes, data marts, data modelling, data operationalization, data preparation, data warehouses, databricks, datawarehouse, delta lake, etl, git, ms sql, orchestration tools, python, sas, spark sql, sql server, synapse"
Senior Data Engineer,Cognizant,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cognizant-3780598592,2023-12-17,Winslow,United States,Mid senior,Onsite,"Location: Pennsylvania, Philadelphia (Hybrid/ Onsite)
You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future*
Practice - AIA - Artificial Intelligence and Analytics
About AI & Analytics:
Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future—a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies.
By applying AI and data science, we help leading companies to prototype, refine, validate, and scale their AI and analytics products and delivery models. Cognizant’s AIA practice takes insights that are buried in data, and provides businesses a clear way to transform how they source, interpret and consume their information. Our clients need flexible data structures and a streamlined data architecture that quickly turns data resources into informative, meaningful intelligence.
Qualification
Experience building data pipelines
Experience with Big Data and DevOps technologies
Any exposure to cloud computing (GCP, Azure, AWS)
Responsibility
Data engineering activities in CCP and on prem
Build generic ELT frameworks
Experience in designing frameworks, platforms and improving the existing design
Perform POCs and assessments on new tools and technologies
Design and Develop generic framework which will be used by platforms
Design and develop data pipelines
Work with stakeholder, understand the business goal and help in achieving the same
Understand Comcast ecosystem to recommend design ideas
Must Have Skills
Databricks
Python
Spark/PySpark
Good To Have Skills
Kubernetes
Docker
Snowflake
Salary And Other Compensation
This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.
Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
#CB #Ind123
Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Dec 06 2023
About Cognizant
Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.
Show more
Show less","Python, Databricks, Spark, Kubernetes, Hadoop, Dataflow, Big Data, AWS, Azure, GCP, DevOps, Snowflake, Kafka, Clustering, Machine Learning, BI, Data Analytics, Data Platforms, Data Pipelines, Data Structures, Data Architecture, Data Engineering","python, databricks, spark, kubernetes, hadoop, dataflow, big data, aws, azure, gcp, devops, snowflake, kafka, clustering, machine learning, bi, data analytics, data platforms, data pipelines, data structures, data architecture, data engineering","aws, azure, bi, big data, clustering, data architecture, data engineering, data platforms, data structures, dataanalytics, databricks, dataflow, datapipeline, devops, gcp, hadoop, kafka, kubernetes, machine learning, python, snowflake, spark"
"Lead Data Engineer - Mt. Laurel, NJ (Onsite) - Full Time/ Permanent - Lorven Technologies",Lorven Technologies Inc.,"Mount Laurel, NJ",https://www.linkedin.com/jobs/view/lead-data-engineer-mt-laurel-nj-onsite-full-time-permanent-lorven-technologies-at-lorven-technologies-inc-3659214360,2023-12-17,Winslow,United States,Mid senior,Onsite,"Our client is looking for
Lead Data Engineer
for a
Full-Time/Permanent
project in
Mt.Laurel, NJ (Onsite).
Below are the detailed requirements.
Job Title: Lead Data Engineer
Location: Mt. Laurel, NJ (Onsite)
Duration: Full Time
Mandatory Skills
Databricks, Python, RDBMS, Powershell scripting, Datawarehouse
Job Description
Bachelor's degree in Computer science or equivalent, with a minimum of 10+ Years of relevant experience.
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure.
Data Factory with development expertise in batch and real-time data integration.
Experience in programming using Python.
RDBMS knowledge and experience in writing Store Procedures.
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing data requirements.
Experience in Cloud data warehouses like Azure Synapse, and Snowflake analytical warehouse.
Experience with Orchestration tools, Azure DevOps, and GitHub.
Experience in building end-to-end architecture for Data Lakes, Data Warehouses, and Data Marts.
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, and SQL Server.
Experience in own end-to-end development, including coding, testing, debugging, and deployment.
Extensive knowledge of ETL and Data Warehousing concepts, strategies, and methodologies.
Experience working with structured and unstructured data.
Familiarity with Azure services like Azure functions, Azure Data Lake Store, and Azure Cosmos.
Ability to provide solutions that are forward-thinking in data and analytics.
Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and a high level of focus and attention to detail.
Strong work ethic with good time management and with the ability to work with diverse teams and lead meetings.
Show more
Show less","Databricks, Python, RDBMS, Powershell, Datawarehouse, Data Lakes, Data Marts, Apache Spark, SQL, Azure Databricks, Azure, Data Factory, Bash, Cloud data warehouses, Azure Synapse, Snowflake, Azure DevOps, GitHub, MS SQL, Delta Lake, Spark SQL, SQL Server, Azure functions, Azure Data Lake Store, Azure Cosmos","databricks, python, rdbms, powershell, datawarehouse, data lakes, data marts, apache spark, sql, azure databricks, azure, data factory, bash, cloud data warehouses, azure synapse, snowflake, azure devops, github, ms sql, delta lake, spark sql, sql server, azure functions, azure data lake store, azure cosmos","apache spark, azure, azure cosmos, azure data lake store, azure databricks, azure devops, azure functions, azure synapse, bash, cloud data warehouses, data factory, data lakes, data marts, databricks, datawarehouse, delta lake, github, ms sql, powershell, python, rdbms, snowflake, spark sql, sql, sql server"
Senior Database Engineer / Desginer,Arthur Grand Technologies,"Mount Laurel, NJ",https://www.linkedin.com/jobs/view/senior-database-engineer-desginer-at-arthur-grand-technologies-3780687532,2023-12-17,Winslow,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Arthur Grand Technologies Inc, is seeking the following. Apply via Dice today!
Arthur Grand Technologies (
We are a minority owned staff augmentation and technology consulting company
To keep our valued employees, we need to keep them engaged in challenging, interesting work, offer market-relevant benefits, and provide continued opportunities for professional growth. Please send your resume to for immediate consideration
Senior Database Designer / Data Modeler (On-site)
Location – Mt. Laurel, NJ
Experience: 10-12+ Years
Extensive experience with Oracle, complex SQL, and PL/SQL.
Excellent Relational database design experience and skills.
Proficient with data modeling tools, e.g. Power Designer or ERWIN.
Excellent analytical and problem solving skills.
Quick learner, self-starter and independent with limited supervision.
Prefer to have Azure knowledge and experience
If you are interested with the above opportunity, please send us your resume to for immediate consideration.
Arthur Grand Technologies
Arthur Grand Technologies is an Equal Opportunity Employer (including disability/vets)
Senior Database Engineer / Desginer
Show more
Show less","Oracle, SQL, PL/SQL, Relational database design, Data modeling, Power Designer, ERWIN, Azure","oracle, sql, plsql, relational database design, data modeling, power designer, erwin, azure","azure, datamodeling, erwin, oracle, plsql, power designer, relational database design, sql"
Senior Data Engineer,Valorem Reply,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-valorem-reply-3754700789,2023-12-17,Winslow,United States,Mid senior,Onsite,"Note
This role requires candidates to be US Citizens.
This role requires individuals to be within driving distance from our office locations or willing to relocate (Atlanta, Chicago, Detroit, Kansas City and Philadelphia).
Valorem Reply is an award-winning digital transformation firm focused on delivering data-driven enterprise, IT modernization, customer experience, product transformation and digital workplace. Through the expertise of their people and power of Microsoft technologies, they provide hyper-scale and agile delivery of unique digital business services, strategic business models and design-led user experiences. Their innovative strategies and solutions securely and rapidly transform the way their clients do business.
The Senior Data Engineer will lead the creation of high-value data-driven solutions, leveraging Valorem Reply's proven implementation methodology and solutions for enterprise projects. They will also contribute to technical pre-sales activities as required. The responsibilities include designing solution architecture, defining requirements, and leading the project delivery team. There will be an opportunity to work with and learn about the latest cloud solutions in an exciting work environment. This position will work collaboratively across all of Valorem Reply's sales, service delivery, and account management organizations to serve Valorem Reply's customers.
This position will represent Valorem Reply's approach to advanced data engineering solutions and, as such, must demonstrate proficiency at the architecture level. It will require an understanding of how advanced analytics are positioned to meet business objectives and how data translates to business and enterprise value. The role will also involve implementing the Data Lakehouse solution through people, processes, and technology. This is a hands-on role, leading, coding, and delivering on the most advanced cloud data analytics platforms available. Projects will span from workshops to full enterprise production end-to-end solutions.
The ideal candidate will have extensive experience with Microsoft/Azure data services and Databricks technology. Proficiency with the Databricks platform and the implementation of enterprise Data Lakehouse solutions will be required. Candidates will be expected to contribute to all stages of the data lifecycle, including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation. The candidate should be familiar with market challenges in multiple industry verticals and have experience with both traditional and modern technologies across the Microsoft technology stack.
Responsibilities
Leading the development of data-driven solutions using Valorem Reply's methodology and enterprise project solutions.
Designing solution architecture and defining project requirements.
Staying up to date with the latest cloud solutions and technologies.
Collaborating across different teams to provide exceptional service to customers.
Demonstrating expertise in data engineering and understanding how it aligns with business objectives.
Managing the entire data lifecycle, from data ingestion to curation, and proficiency in Microsoft/Azure data services and Databricks technology.
Minimum Requirements
Bachelor's/master’s degree in computer science or equivalent with a focus on Azure data engineering solutions
6+ years of data engineering delivery experience
3+ years of Databricks engineering development experience
2+ years of technical team leadership or technical management experience
Candidates must be US Citizens
Show more
Show less","Microsoft/Azure, Databricks, Data Lakehouse, Data governance, Data quality, Data transformation, Data ingestion, Data modeling, Data curation, Data lifecyle management, Big Data, Cloud Computing, Cloud Solutions, Solution Architecture, Software Development, Leadership, Data Management","microsoftazure, databricks, data lakehouse, data governance, data quality, data transformation, data ingestion, data modeling, data curation, data lifecyle management, big data, cloud computing, cloud solutions, solution architecture, software development, leadership, data management","big data, cloud computing, cloud solutions, data curation, data governance, data ingestion, data lakehouse, data lifecyle management, data management, data quality, data transformation, databricks, datamodeling, leadership, microsoftazure, software development, solution architecture"
Data Analyst (Free Library of Philadelphia),City of Philadelphia,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-analyst-free-library-of-philadelphia-at-city-of-philadelphia-3751617417,2023-12-17,Winslow,United States,Mid senior,Onsite,"Company Description
A best-in-class city that attracts best-in-class talent, Philadelphia is an incredible place to build a career. From our thriving arts scene and rich history to our culture of passion and grit, there are countless reasons to love living and working here. With a workforce of over 30,000 people, and more than 1,000 different job categories, the City of Philadelphia offers boundless opportunities to make an impact.
As an employer, the City of Philadelphia values inclusion, integrity, innovation, empowerment, and hard work above all else. We offer a vibrant work environment, comprehensive health care and benefits, and the experience you need to grow and excel. If you’re interested in working with a passionate team of people who care about the future of Philadelphia, start here.
What We Offer
Impact - The work you do here matters to millions.
Growth - Philadelphia is growing, why not grow with it?
Diversity & Inclusion - Find a career in a place where everyone belongs.
Benefits - We care about your well-being.
The Office of Innovation & Technology (OIT) is the central IT agency for the City of Philadelphia headed by the Chief Information Officer (CIO). OIT oversees all major information and communications technology initiatives for the City of Philadelphia - increasing the effectiveness of the information technology infrastructure, where the services provided are advanced, optimized, and responsive to the needs of the City of Philadelphia’s businesses, residents, and visitors. OIT responsibilities include: identifying the most effective approach for implementing new information technology directions throughout city government; improving the value of the city’s technology assets and the return on the city’s technology investments; ensuring data security continuity; planning for continuing operations in the event of disruption of information technology or communications services; and supporting accountable, efficient and effective government across every city department, board, commission and agency.
The Free Library of Philadelphia (FLP) is one of the largest public library systems in the world. As an important cornerstone of the Philadelphia community, FLP has a mission to advance literacy, guide learning, and inspire curiosity. The long-term vision of the FLP is to build an enlightened community devoted to lifelong learning. For over 100 years, the FLP has championed education in and out of the classroom, providing no cost resources for literacy and learning.
Job Description
The Free Library of Philadelphia is looking for a talented Data Analyst to join the Research & Data Analytics group within the Performance Management and Technology Division. The successful candidate will be experienced in all aspects of data analytics, including data acquisition and cleaning, descriptive, prescriptive and predictive analytics, and data visualization.
The Data Analyst will create organizational value by combining exceptional analytical skills with strong functional domain knowledge to deliver actionable data products, analyses and reports aligned to Performance Management objectives.
The Data Analyst will work cooperatively with Performance Management and Technology staff to understand program objectives and manage complex projects. The Data Analyst may supervise the day to day work of associate data coordinators; supervisory tasks may include mentoring, evaluation, and ensuring compliance with standard operating procedures.
Essential Functions
Acquire data from multiple sources, both internal and external. Ensure data accuracy and integrity by cleaning and preprocessing data.
Use appropriate statistical analysis techniques to explore and identify patterns, trends, and correlations.
Create data visualizations using industry standard tools (Tableau, Power BI, Excel) to present findings effectively.
Prepare and present reports, summaries, and recommendations to both technical and non-technical audiences.
Develop and test data-driven hypotheses to answer specific questions.
Build predictive models, including regression, AI, machine learning, or time series models to forecast trends and outcomes.
Work with database systems to retrieve, manipulate, and analyze data efficiently.
Write SQL queries to extract relevant data.
Collaborate with cross-functional teams to understand data needs to deliver actionable insights.
Ensure data security and compliance with data privacy regulations.
Prepare standard operating procedures (SOPs) and ensure complete documentation of datasets, scripting routines, and workflows.
Supervise subordinate or junior staff, or interns, identify mentoring opportunities and provide assistance as needed.
Serve as Subject Matter Expert for Library agencies to ensure compliance with appropriate data management, integration, and sharing protocols.
Participate in meetings and events within department, city and the greater data analytics community.
Present reports at departmental meetings, informational sessions, and select conferences.
Required
Competencies, Knowledge, Skills and Abilities
Proficiency with R and/or Python, and SQL programming languages.
Experience developing statistical methods and analyses including predictive analytics, and correlation and regression models using appropriate statistical software tools.
Experience using industry standard data visualization and graphics tools for creation of professional reports and presentations, web content, and education materials.
Proficient in developing database query strategies from spatial and non-spatial relational databases.
Able to effectively communicate complex technical information to diverse audiences.
Excellent written and oral communication skills, strong project planning, organization, and time management skills.
Ability to create and maintain good working relationships and collaborations while working independently and taking the initiative.
Willing to expand professional compentency by taking advantage of appropriate continuing education and training opportunities.
Preferred
Experience with cloud services and data management.
Experience with Tableau and/or PowerBI applications.
Experience handling regulatory requirements related to PII.
Experience with parcel mapping, editing, and parcel management.
Experience using ESRI ArcGIS software suite to publish data and produce GIS applications.
Proficiency in spatial data creation, editing and maintenance.
Multi-user versioned and distributed editing, replication in ArcGIS Server/ArcGIS SDE 10.x environment.
Qualifications
Five or more years of relevant professional experience required. Baccalaureate degree from an accredited college or university which has included major course work in Information Science, Statistics, Data Science, Information Management, Geographic Information Systems (GIS), Urban Planning or a related field. Master’s degree is preferred.
Additional Information
Salary Range: $75,000 ‐ $85,000
Starting salary to be determined based on experience and qualifications.
Important: To apply, candidates must provide a cover letter and a resume.
Discover the Perks of Being a City of Philadelphia Employee
:
We offer Comprehensive health coverage for employees and their eligible dependents
Our wellness program offers eligibility into the discounted medical plan
Employees receive paid vacation, sick leave, and holidays
Generous retirement savings options are available
Pay off your student loans faster - As a qualifying employer, City of Philadelphia employees are eligible to participate in the Public Service Loan Forgiveness program. Join the ranks of hundreds of employees who have already benefited from this program and achieved student loan forgiveness.
Enjoy a Free Commute on SEPTA - Starting September 1, 2023, eligible City employees will no longer have to worry about paying for SEPTA public transportation. Whether you're a full-time, part-time, or provisional employee, you can seize the opportunity to sign up for the SEPTA Key Advantage Program and receive free Key cards for free rides on SEPTA buses, trains, trolleys, and regional rails.
Unlock Tuition Discounts and Scholarships - The City of Philadelphia has forged partnerships with over a dozen esteemed colleges and universities in the area, ensuring that our employees have access to a wide range of tuition discounts and scholarships. Experience savings of 10% to 40% on your educational expenses, extending not only to City employees but in some cases, spouse and dependents too!
Join the City of Philadelphia team today and seize these incredible benefits designed to enhance your financial well-being and personal growth!
The successful candidate must be a city of Philadelphia resident within six months of hire
Effective May 22, 2023, vaccinations are no longer required for new employees that work in non-medical, non-emergency or patient facing positions with the City of Philadelphia. As a result, only employees in positions providing services that are patient-facing medical care (ex: Nurses, doctors, emergency medical personnel), must be fully vaccinated.
The City of Philadelphia is an Equal Opportunity employer and does not permit discrimination based on race, ethnicity, color, sex, sexual orientation, gender identity, religion, national origin, ancestry, age, disability, marital status, source of income, familial status, genetic information or domestic or sexual violence victim status. If you believe you were discriminated against, call the Philadelphia Commission on Human Relations at 215-686-4670 or send an email to faqpchr@phila.gov.
Show more
Show less","Data Analysis, Data Acquisition, Data Cleaning, Descriptive Analytics, Prescriptive Analytics, Predictive Analytics, Data Visualization, Tableau, Power BI, Excel, SQL, Statistical Analysis, Regression Analysis, AI, Machine Learning, Time Series Analysis, Database Systems, Data Security, Data Privacy, Standard Operating Procedures, Mentoring, Communication, Collaboration, Project Planning, Time Management, R, Python, ArcGIS, GIS, Spatial Data, MultiUser Editing, Replication, Information Science, Statistics, Data Science, Information Management, Geographic Information Systems, Urban Planning","data analysis, data acquisition, data cleaning, descriptive analytics, prescriptive analytics, predictive analytics, data visualization, tableau, power bi, excel, sql, statistical analysis, regression analysis, ai, machine learning, time series analysis, database systems, data security, data privacy, standard operating procedures, mentoring, communication, collaboration, project planning, time management, r, python, arcgis, gis, spatial data, multiuser editing, replication, information science, statistics, data science, information management, geographic information systems, urban planning","ai, arcgis, collaboration, communication, data acquisition, data cleaning, data privacy, data science, data security, dataanalytics, database systems, descriptive analytics, excel, geographic information systems, gis, information management, information science, machine learning, mentoring, multiuser editing, powerbi, predictive analytics, prescriptive analytics, project planning, python, r, regression analysis, replication, spatial data, sql, standard operating procedures, statistical analysis, statistics, tableau, time management, time series analysis, urban planning, visualization"
"DBA - DATABASE ENGINEER - DIRECT HIRE - HYBRID ONSITE, PHILADELPHIA AREA - W-2, NO C2C",System Soft Technologies,Greater Philadelphia,https://www.linkedin.com/jobs/view/dba-database-engineer-direct-hire-hybrid-onsite-philadelphia-area-w-2-no-c2c-at-system-soft-technologies-3784208218,2023-12-17,Winslow,United States,Mid senior,Onsite,"Bachelor’s degree in Computer Science, Computer Engineering, Information Technology or a related field
Minimum of 5 years of experience as a MySQL/MariaDB (operating on Linux) database administrator
Experience with large databases with tens of millions of rows and terabytes of data.
Experience with multiple database systems, including MongoDB, Oracle, Postgres, and Microsoft SQL Server
Experience with Linux and Windows operating systems
Ability to learn and maintain a legacy code base
Hands on experience with SQL development, shell scripting, and Python.
Experience with ETL tools such as Informatica a plus
Experience with DevOps and CI/CD tools such as Perforce, GitLab, TeamCity, Jenkins, or Ansible a plus
Show more
Show less","MySQL, MariaDB, Linux, MongoDB, Oracle, Postgres, Microsoft SQL Server, Windows, Shell scripting, Python, Informatica, Perforce, GitLab, TeamCity, Jenkins, Ansible","mysql, mariadb, linux, mongodb, oracle, postgres, microsoft sql server, windows, shell scripting, python, informatica, perforce, gitlab, teamcity, jenkins, ansible","ansible, gitlab, informatica, jenkins, linux, mariadb, microsoft sql server, mongodb, mysql, oracle, perforce, postgres, python, shell scripting, teamcity, windows"
"Lead Data Engineer - Full Time/ Permanent - Mount Laurel, NJ (Onsite) - Lorven Technologies Inc.",Lorven Technologies Inc.,"Mount Laurel, NJ",https://www.linkedin.com/jobs/view/lead-data-engineer-full-time-permanent-mount-laurel-nj-onsite-lorven-technologies-inc-at-lorven-technologies-inc-3736300550,2023-12-17,Winslow,United States,Mid senior,Onsite,"Our client is looking
Senior/Lead Data Engineer
for Long term
Mount Laurel, NJ/Charlotte, NC
below is the detailed requirements.
Role: Senior/Lead Data Engineer
Location: Mount Laurel, NJ/Charlotte, NC
Duration: Full time
The rate is Open
Immediate Hiring - Single-level interview process
Job Description
Bachelor's degree in Computer science or equivalent, with a minimum of 10+ Years of relevant experience.
More than 8-12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience with Autosys, Unix, and scripting knowledge on Python, Shell Scripts
Experience with Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM), and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Excellent verbal and written communication skills as well as presentation skills Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail.
Strong work ethic with good time management with ability to work with diverse teams and lead meetings
Show more
Show less","Data Engineering, Informatica, Software Development, ETL, Oracle Database, Unix, Shell Scripting, Python, SQL, Azure, AWS, Informatica Cloud Connector, Risk & Regulatory, Commercial, Credit Cards, Retail","data engineering, informatica, software development, etl, oracle database, unix, shell scripting, python, sql, azure, aws, informatica cloud connector, risk regulatory, commercial, credit cards, retail","aws, azure, commercial, credit cards, data engineering, etl, informatica, informatica cloud connector, oracle database, python, retail, risk regulatory, shell scripting, software development, sql, unix"
Data - Engineer/Scientist/Architect | $130K-$180K | Hybrid AND Remote Work Options,IT Pros,"Cherry Hill, NJ",https://www.linkedin.com/jobs/view/data-engineer-scientist-architect-%24130k-%24180k-hybrid-and-remote-work-options-at-it-pros-3785377333,2023-12-17,Winslow,United States,Mid senior,Remote,"Company Description
Access opportunities with thousands of US-based companies seeking
Data - Engineers/Scientists/Architects
via IT Pros, a tech recruitment firm with a decade of experience since 2011.
Work Location
: Hybrid + 100% USA-Remote Work Schedule Options
100% USA-Remote
1-2 Days In-Office
2-3 Days In-Office
3-4 Days In-Office
5 Days In-Office
Company Locations
:
New York City, New York
Los Angeles, California
Chicago, Illinois
Houston, Texas
Phoenix, Arizona
Philadelphia, Pennsylvania
San Antonio, Texas
San Diego, California
Dallas, Texas
San Jose, California
Austin, Texas
Jacksonville, Florida
San Francisco, California
Columbus, Ohio
Indianapolis, Indiana
Fort Worth, Texas
Charlotte, North Carolina
Seattle, Washington
Denver, Colorado
Washington, D.C.
Boston, Massachusetts
Raleigh-Durham, North Carolina
Boulder, Colorado
Portland, Oregon
Atlanta, Georgia
Compensation
: Based on Years of Experience and Accomplishment
3-5 Years = $130,000 - $150,000
6-8 Years = $150,000 to $170,000
9+ Years = $170,000 to $180,000+
Benefits
:
Medical, Dental, and Vision Insurance
Life insurance, Long Term Disability, and Short Term Disability
Paid Time Off (PTO)
Plus more...
Job Description
Job descriptions for
Data - Engineers/Scientists/Architects
positions will be provided upon a successful match.
Qualifications
Successful
Data - Engineers/Scientists/Architects
will meet the following criteria:
Must be located and authorized to work in the United States without any work restrictions, now or in the future.
Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse.
Must have a solid work track record of delivering results
Must have excellent communication skills
Additional Information
Your application will be reviewed by a real human. Feedback will be provided. Your patience is appreciated. We look forward to having the opportunity to work with you.
Show more
Show less","Data Engineering, Data Science, Data Architecture, ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse","data engineering, data science, data architecture, etl, sql, nosql, hadoop, spark, kafka, apache nifi, talend, informatica, aws, azure, google cloud gcp, python, pandas, r, matplotlib, seaborn, tableau, tensorflow, pytorch, aws redshift, azure synapse","apache nifi, aws, aws redshift, azure, azure synapse, data architecture, data engineering, data science, etl, google cloud gcp, hadoop, informatica, kafka, matplotlib, nosql, pandas, python, pytorch, r, seaborn, spark, sql, tableau, talend, tensorflow"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Philadelphia, PA",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751642559,2023-12-17,Winslow,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Cloud Computing, Data Engineering, Data Pipelines, Data Warehousing, Data Lakes, SQL, NoSQL, Python, Java, R, C / C# / C++, ETL, Data Orchestration, DevOps, Git, Jenkins, CI/CD, Jira, Big Data, Open Source, Data Streaming, Snowflake, Redshift, Databricks, AWS, Azure, GCP","cloud computing, data engineering, data pipelines, data warehousing, data lakes, sql, nosql, python, java, r, c c c, etl, data orchestration, devops, git, jenkins, cicd, jira, big data, open source, data streaming, snowflake, redshift, databricks, aws, azure, gcp","aws, azure, big data, c c c, cicd, cloud computing, data engineering, data lakes, data orchestration, data streaming, databricks, datapipeline, datawarehouse, devops, etl, gcp, git, java, jenkins, jira, nosql, open source, python, r, redshift, snowflake, sql"
Data Analytics Engineer,Motion Recruitment,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-motion-recruitment-3762986218,2023-12-17,Winslow,United States,Mid senior,Remote,"We are working with a company who run a people search engine that utilizes deep web crawlers to aggregate data. Their main goal is to reconnect friends, reunite families, prevent fraud, and more. They are looking to hire Data Engineer, Analytics.
This candidate can be primarily remote but must be located close enough to the city of Philadelphia to travel to the office 2-3 times a month.
Required Skills & Experience
2 + yrs of experience in Data Engineering
Software Development background
SQL (Can do simple code commands)
NoSQL Databases
Python
AWS - EMR, RDS, Redshift, Kinesis, etc
What You Will Be Doing
Tech Breakdown
40% SQL/NoSQL
40% AWS
20% Python
Daily Responsibilities
100% Data Engineering/Analytics
The Offer
Bonus eligible
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Caroline Stranieri
Show more
Show less","Data Engineering, Software Development, SQL, NoSQL Databases, Python, AWS, EMR, RDS, Redshift, Kinesis","data engineering, software development, sql, nosql databases, python, aws, emr, rds, redshift, kinesis","aws, data engineering, emr, kinesis, nosql databases, python, rds, redshift, software development, sql"
Business/Data Analyst - Tableau | $75K-$80K | 2 Days Remote / 3 In-Office,IT Pros,"Philadelphia, PA",https://www.linkedin.com/jobs/view/business-data-analyst-tableau-%2475k-%2480k-2-days-remote-3-in-office-at-it-pros-3785373400,2023-12-17,Winslow,United States,Mid senior,Remote,"Company Description
Join a Financial Services company as a Data Analyst, located in the Center City area of Philadelphia, PA office.
Overview
: 85+ Employees
Work Location
: Hybrid (3 days in office) located in Center City Philadelphia, PA
Benefits
:
Health
Dental
Vision
Generous PTO
401k with match
Commuter Benefits
Flexible spending
Life Insurance
Short Term and Long-Term Disability Insurance
Discounts on Auto/Home/Pet Insurance
Plus More…
Job Description
The role of the Data Analyst entails using data analytics to interpret and advise leadership to improve operational effectiveness, including:
Creating and deploying report models and self-service dashboards to evaluate customer KPIs.
Working with leadership to compile data on key performance areas and make recommendations based on objective and interpretative analysis of business data.
Qualifications
PLEASE READ
✅
Location
: Must be located in the Philadelphia region and be able to commute to the Center City area of Philadelphia, PA office 3 days per week (no relocation is being offered)
✅
Education:
Bachelor’s Degree in Data Analytics, Data Science, Math or Statistics
✅
Skills:
Tableau AND Python or R
✅
Experience:
1-2 years of experience in data analytics and interpretation
✅
Work History
: Must have a steady work history with the ability to show results
Additional Information
Your application will be reviewed by a real human. Feedback will be provided. Your patience is appreciated. We look forward to having the opportunity to work with you.
Show more
Show less","Data Analytics, Tableau, Python, R, Data Science, Math, Statistics, KPI, Dashboards, Business Data, Data Interpretation, Data Analysis, Work History","data analytics, tableau, python, r, data science, math, statistics, kpi, dashboards, business data, data interpretation, data analysis, work history","business data, dashboard, data interpretation, data science, dataanalytics, kpi, math, python, r, statistics, tableau, work history"
Data Scientist,Idea Evolver,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-scientist-at-idea-evolver-3787731661,2023-12-17,Winslow,United States,Mid senior,Remote,"Company Overview
Idea Evolver specializes in audience research and consumer insights as well as designing and engineering Software as a Medical Device applications.
Our primary verticals are healthcare, food, and retail. Our client list includes American Express, Dannon, AstraZeneca, United Therapeutics, Blue Origin, and K. Hovnanian Homes.
Position Overview
We are looking for a highly skilled Data Scientist to join our Consumer Insights team. This position will be responsible for utilizing data to provide valuable insights that inform business decisions and drive growth for our company and our clients.
Responsibilities
Collect and clean large data sets from various sources such as customer databases, social media, and production applications.
Analyze and interpret data using statistical techniques and machine learning algorithms.
Build predictive models to identify patterns and trends in the data and make predictions.
Communicate insights and findings to stakeholders through data visualizations and reports.
Collaborate with cross-functional teams, including product managers and marketers, to understand business problems and develop data-driven solutions.
Continuously monitor the performance of models and identify opportunities for improvement.
Stay current with the latest data science techniques and technologies to continuously improve the quality of our data-driven insights.
Qualifications
Undergraduate degree in a quantitative field (computer science, engineering, mathematics, physics, machine learning, statistics)
3+ years of industry experience designing, developing, and deploying machine learning models
Experience with cloud-based ecosystems (GCP preferred)
Experience with Python and relevant libraries (NumPy, Pandas, Scikit-learn, etc.)
Willingness to learn Go
Solid understanding of machine learning model development life cycles and performance assessments
Experience with Git
Experience using common machine learning and deep learning libraries and techniques, including TensorFlow, PyTorch, and big data platforms
Fluency in both structured and unstructured data (SQL, NOSQL)
Experience with data visualization tools such as Looker, Tableau, or Power BI
General knowledge of Docker, Jenkins, Kubernetes, and other DevOps tools
Excellent verbal and written communication skills
Compensation and Benefits:
Competitive Salary
Health and wellness benefits
401k with company matching
20 paid days off plus holidays
Powered by JazzHR
DhSbTUSYHX
Show more
Show less","Python, NumPy, Pandas, Scikitlearn, Go, Big data platforms, TensorFlow, PyTorch, SQL, NOSQL, Looker, Tableau, Power BI, Docker, Jenkins, Kubernetes, DevOps","python, numpy, pandas, scikitlearn, go, big data platforms, tensorflow, pytorch, sql, nosql, looker, tableau, power bi, docker, jenkins, kubernetes, devops","big data platforms, devops, docker, go, jenkins, kubernetes, looker, nosql, numpy, pandas, powerbi, python, pytorch, scikitlearn, sql, tableau, tensorflow"
Data Scientist,Experfy,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-scientist-at-experfy-3646112719,2023-12-17,Winslow,United States,Mid senior,Remote,"The successful candidate will have a strong background in remote sensing, with experience in Google Earth Engine using remotely-sensed imagery for mapping applications and analysis. The successful candidate will have strong programming skills, including proficiency with Python, R and Google Earth Engine (GEE). Candidate is required to have proficiency with GIS and image processing software ArcGIS.
Requirements
Acquire and pre-process HLS data. UseClient's available R code for automating HLS geotiff extraction from Client's Land Processes Distributed Active Archive Center (LP DAAC) archives, based on region of interest and specified temporal range
Generate HLS NDVI time series to identify peak greenness from cloud-masked data
Further identify dates of peak greenness across various elevation bands of interest. These NDVI time series will be compared to concurrent 250m daily MODIS NDVI time series for additional validation of ESI phenological peaks. Generate code to automate the extraction of HLS image granule representing peak greenness for regions of interest. Similarly run process to identify optimal late-season imagery
Show more
Show less","Remote sensing, Google Earth Engine, Python, R, ArcGIS, GIS, Image processing, HLS data, NDVI time series, Peak greenness, MODIS, ESI phenological peaks","remote sensing, google earth engine, python, r, arcgis, gis, image processing, hls data, ndvi time series, peak greenness, modis, esi phenological peaks","arcgis, esi phenological peaks, gis, google earth engine, hls data, image processing, modis, ndvi time series, peak greenness, python, r, remote sensing"
Expression of Interest: Data Scientist,Fingerprint for Success (F4S),"Philadelphia, PA",https://www.linkedin.com/jobs/view/expression-of-interest-data-scientist-at-fingerprint-for-success-f4s-3787780553,2023-12-17,Winslow,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
1utrRiDyN2
Show more
Show less","Talent Pool, F4S work style assessment, Predictive analytics, Personal motivations, Team motivations, Behaviors, Performance, Technical skills, JazzHR","talent pool, f4s work style assessment, predictive analytics, personal motivations, team motivations, behaviors, performance, technical skills, jazzhr","behaviors, f4s work style assessment, jazzhr, performance, personal motivations, predictive analytics, talent pool, team motivations, technical skills"
Data Scientist/ Reinforcement Learning/ Quant,Motion Recruitment,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-scientist-reinforcement-learning-quant-at-motion-recruitment-3762980644,2023-12-17,Winslow,United States,Mid senior,Remote,"We are working with one of the top energy companies in the country looking to expand yet again! This is a long-term contract position, looking for a data scientist experienced in reinforcement learning. The ideal candidate will have 4+ years of professional experience working with technologies such as Python, AWS, and OpenAI. Candidates coming from a heavy quantitative background are a huge plus!
This role is fully remote with a preference for candidates based in the Northeast. This position cannot provide sponsorship, nor is it eligible for C2C under any basis. Required Skills & Experience
4+ years of experience
Reinforcement learning
Python
SQL
AWS
OpenAI
Desired Skills & Experience
AWS Sagemaker
Quantitative trading
Concept deployment
You Will Receive The Following Benefits
Medical & Dental Insurance
Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit
Potential For Conversion to Full-time
*
Contract Duration:
12 Months Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.
Posted By:
Caroline Stranieri
Show more
Show less","Reinforcement learning, Python, SQL, AWS, OpenAI, AWS Sagemaker, Quantitative trading, Concept deployment","reinforcement learning, python, sql, aws, openai, aws sagemaker, quantitative trading, concept deployment","aws, aws sagemaker, concept deployment, openai, python, quantitative trading, reinforcement learning, sql"
Lead Data Statistician,Jackson Lewis P.C.,"Philadelphia, PA",https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3755008543,2023-12-17,Winslow,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Wage & Hour law, Employment Law, Labor Law, Litigation, Data Analytics, Class Action, Collective Action, DOL Audit, Client Exposure Analysis, Timekeeping Data, Payroll Data, Excel, VBA, R, SQL, Python, Statistical Programs, Data Manipulation, Critical Thinking, ProblemSolving, Communication, Organization, MultiTasking, Project Management, Applied Statistics, Mathematics, Econometrics, Data Integration","wage hour law, employment law, labor law, litigation, data analytics, class action, collective action, dol audit, client exposure analysis, timekeeping data, payroll data, excel, vba, r, sql, python, statistical programs, data manipulation, critical thinking, problemsolving, communication, organization, multitasking, project management, applied statistics, mathematics, econometrics, data integration","applied statistics, class action, client exposure analysis, collective action, communication, critical thinking, data integration, data manipulation, dataanalytics, dol audit, econometrics, employment law, excel, labor law, litigation, mathematics, multitasking, organization, payroll data, problemsolving, project management, python, r, sql, statistical programs, timekeeping data, vba, wage hour law"
Sr. Data Engineer,Chubb,"Philadelphia, PA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-chubb-3756333790,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Description
We are looking for an experienced and motivated Senior ETL Developer to join our dynamic team. In this role, you will lead the delivery of key projects in support of North America financial reporting from the enterprise data warehouse and associated data marts, including the Business Analytics Repository (BAR). BAR is a strategic application within the business, feeding into multiple systems and applications both up and downstream with data that directly supports business decisions being made each and every day. You will be responsible for leading ETL development projects, coordinating with cross-functional teams to ensure project success, and creating and maintaining data integration solutions to meet business requirements. The ideal candidate will have experience with ETL development solutions such as AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS and be able to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems.
Responsibilities
Lead ETL development projects and coordinate with cross-functional teams to ensure project success
Create and maintain data integration solutions to meet business requirements
Identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems
Design and develop scalable ETL workflows and data pipelines using ETL tools such as Informatica/IICS
Ensure compliance with data governance and security policies
Develop and maintain documentation such as technical design documents, data lineage, and ETL runbooks
Mentor junior ETL developers and provide technical guidance to the team
Evaluate modern technologies and tools, and recommend solutions to improve ETL processes and performance
Contribute to the architecture, design, and development of data warehousing and business intelligence solutions
Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.
Participate in analysis, design, and ETL development as part of Agile development methodologies and provide status updates to the management
Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate current trends and developments in current and future solutions
Qualifications
5 Year/bachelor’s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience
At least 5+ years of Strong understanding of ETL development concepts and tools such as ETL development solutions (e.g., AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS)
Experience with Data Warehousing and Business Intelligence concepts and technologies
Strong knowledge of SQL and advanced programming languages such as Python and Java
Demonstrated critical thinking skills and the ability to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems
Experience with Agile methodologies and project-management skills
Excellent communication and interpersonal skills
Ability to mentor and provide technical guidance to junior ETL developers
Experience with cloud based environment required.
2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)
3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.
3+ years of experience in creating complex technical specifications from business requirements/specifications
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","ETL development, Data integration, Data quality, Performance bottlenecks, Data warehousing, Business intelligence, SQL, Python, Java, Agile methodologies, Project management, Unix/Linux, Windows Scripts, PERL, Shell script, Business requirements, Data lineage, Autosys, Hadoop, Snowflake, Informatica/IICS, AWS Glue, Google Dataflow, Azure Data Factory","etl development, data integration, data quality, performance bottlenecks, data warehousing, business intelligence, sql, python, java, agile methodologies, project management, unixlinux, windows scripts, perl, shell script, business requirements, data lineage, autosys, hadoop, snowflake, informaticaiics, aws glue, google dataflow, azure data factory","agile methodologies, autosys, aws glue, azure data factory, business intelligence, business requirements, data integration, data lineage, data quality, datawarehouse, etl development, google dataflow, hadoop, informaticaiics, java, performance bottlenecks, perl, project management, python, shell script, snowflake, sql, unixlinux, windows scripts"
Principal Data Engineer,Harnham,"Philadelphia, PA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-harnham-3699997789,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Principal Data Engineer
Technology
Philadephia, PA
$160,000-$180,000 + Bonus
The Company:
This company develops cancer medicines with life-changing potential for patients including ovarian cancer, endometrial cancer, and multiple myeloma. They are a multinational biopharmaceutical business that aims to combine talent, research, and technology to advance the fight against diseases. They are looking for a Data Engineer to help build out their new team to support across all aspects of the company.
The Role:
As a Principal Data Engineer, you will be tech savvy and hands-on as well as help lead projects in the company across multiple systems and platforms helping them migrate from on-prem to the Azure cloud. Other responsibilities include:
You will code using Python on their Azure cloud platform to help build the infrastructure and scale it as needed
You will use Databricks and other big data tools to ensure migration runs smoothly across the platform.
You will build microservices at a high level
You will lead projects in the company, lead a team of two Senior Data Engineers and work directly with stakeholders
Your Skills and Experience:
Experience using the Azure cloud
Experience using Databricks and building microservices
Experience leading small teams and mentoring engineers
Programming Experience in Python and SQL
The Benefits:
As a Principal Data Engineer, you will receive a $160,000- $180,000 base plus bonus. This role also includes health, dental, and vision care.
How to Apply:
Please register your interest by sending your resume to Kyle Margolies via the Apply link on this page.
Key Words:
Python, Azure, SQL, Databricks, Spark, Docker, Microservices, ETL, Engineering, Data Engineering
Show more
Show less","Azure, Databricks, Spark, Docker, Microservices, ETL, Engineering, Data Engineering, Python, SQL","azure, databricks, spark, docker, microservices, etl, engineering, data engineering, python, sql","azure, data engineering, databricks, docker, engineering, etl, microservices, python, spark, sql"
Database Engineer & DBA,Frank Recruitment Group,"Philadelphia, PA",https://www.linkedin.com/jobs/view/database-engineer-dba-at-frank-recruitment-group-3772491324,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Description
You'll collaborate with experts in database management, software development, and production engineering to maintain reliable platforms for our trading systems, including databases, messaging, data distribution, and alerting solutions.
Role & Responsibilities
• Become part of a skilled team of Data Engineers.
• Contribute to the administration, configuration, maintenance, and deployment of our extensive environment consisting of over 150 servers.
• Use your expertise to diagnose complex issues across the entire technology stack, spanning multiple systems.
• Collaborate closely with a diverse group of professionals, including systems engineers, software developers, and database administrators.
• Engage in discussions and share your insights and recommendations to drive improvements and expansion within our database environment.
Skills & Qualifications
5+ years of MySQLdatabase admin experience.
Proficiency in managing large databases.
Familiarity with various database systems.
Linux and Windows OS knowledge.
Scripting (shell, Python).
ETL experience (preferably Informatica).
Familiarity with DevOps and CI/CD tools.
Benefits
401K
Medical
Dental
MORE
Show more
Show less","MySQL, Database administration, Linux, Windows, Shell scripting, Python, Informatica, ETL, DevOps, CI/CD","mysql, database administration, linux, windows, shell scripting, python, informatica, etl, devops, cicd","cicd, database administration, devops, etl, informatica, linux, mysql, python, shell scripting, windows"
Data Engineer/ Snowflake/ Databricks/ Azure,Motion Recruitment,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-engineer-snowflake-databricks-azure-at-motion-recruitment-3759647168,2023-12-17,Winslow,United States,Mid senior,Hybrid,"This Data Engineering position is for a well established advertising company.
This position is hybrid for candidates currently living in the Greater Philadelphia Area
. Their tech stack includes: Python, SQL, Snowflake, Databricks, and Azure
In this role you will be doing traditional Data Engineering responsibilities across the different teams and projects across the company. You will get the opportunity to work with other engineering teams within the company allowing the opportunity to touch on and get exposure to various technologies.
Required Skills & Experience
5+ years of experience in Data Engineering
Strong SQL
Python
Azure
Snowflake
Desired Skills & Experience
Databricks
What You Will Be Doing
Tech Breakdown
100% Data Engineering
Daily Responsibilities
80% Hands On
20% Team Collaboration
The Offer
Bonus eligible
Stock Options available
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k) matching
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Caroline Stranieri
Show more
Show less","Data Engineering, Python, SQL, Snowflake, Azure, Databricks","data engineering, python, sql, snowflake, azure, databricks","azure, data engineering, databricks, python, snowflake, sql"
Senior Data Engineer,Soni,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-soni-3767090750,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Soni is currently looking for a Senior Data Engineer to join a client based in Pennsylvania. The ideal candidate will have 5+ years of experience in a similar role with extensive knowledge of data warehousing concepts and best practices.
Qualifications:
5+ years of experience performing data engineering in a data warehouse environment.
Bachelor's degree in Computer Science, Information Technology or related field.
Data manipulation + Data Analysis
Extensive SQL experience
Agile
Compensation:
$100,000-130,000
Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications
.
Show more
Show less","Data engineering, Data warehousing, SQL, Agile, Data manipulation, Data analysis, Computer Science, Information Technology","data engineering, data warehousing, sql, agile, data manipulation, data analysis, computer science, information technology","agile, computer science, data engineering, data manipulation, dataanalytics, datawarehouse, information technology, sql"
Senior Cloud Data Engineer,BDO USA,"Cherry Hill, NJ",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471243,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, SQL, Data Definition Language, Data Manipulation Language, Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, Data Pipeline, Glue, Star Schema, Data Modeling, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, artificial intelligence, application development, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, sql, data definition language, data manipulation language, views, functions, stored procedures, performance tuning, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, data pipeline, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithms, alteryx, application development, artificial intelligence, automation tools, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data definition language, data lake, data lake medallion architecture, data manipulation language, data ops, data pipeline, dataanalytics, datamodeling, datawarehouse, dbt, delta, devops, functions, git, glue, java, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, streaming data ingestion, tabular modeling, terraform, uipath, views"
Data Analyst,a2c IT Consulting,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-analyst-at-a2c-it-consulting-3785581236,2023-12-17,Winslow,United States,Mid senior,Hybrid,"We are hiring a
Senior Data Warehouse Business Systems Analyst
for a long term role.
Candidates must be local to DE, PA or NJ - applicants outside of this area will not be considered
The desired candidate will have advanced SQL skills that include the ability to write complex SQL queries. This is a data focused role in which the analyst will be transforming data (data mapping) and created business analysis documentation (interviewing stakeholders, gathering and formulating business requirements then translating the require their Chicago, IL
Job Description
The ideal candidate will be proficient with Python development and have practical Cloud Experience (Amazon Web Services and Azure).
Duties
Primary duties include:
Cloud Resource Maintenance and Reporting
Migration from On-Prem to SaaS Orchestration Tool
Establishing and maintaining CI/CD Azure DevOps Pipelines
Creating Infrastructure as Code using Terraform
Building frameworks for programmatic interfaces with BMC Helix
Software Requirements
Python (90% of code)
Amazon Web Services (AWS)
Azure Cloud Services & DevOps
Control-M and/or Helix
Networking and Windows OS experience is preferred
PowerShell is preferred
Show more
Show less","Python, AWS, Azure Cloud Services, DevOps, CI/CD, Terraform, BMC Helix, ControlM, PowerShell, Networking, Windows OS","python, aws, azure cloud services, devops, cicd, terraform, bmc helix, controlm, powershell, networking, windows os","aws, azure cloud services, bmc helix, cicd, controlm, devops, networking, powershell, python, terraform, windows os"
Sr. Data Engineer,Experfy,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3633149722,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibiliti
es
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Show more
Show less","Data warehouse architecture/modeling, ETL processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, Python, PySpark, SQL, Stored procedures, Data profiling, Process flow, Metric logging, Error handling, System architectural decisions, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data","data warehouse architecturemodeling, etl processing, confluent kafka, kinesis, glue, lambda, snowflake, sql server, python, pyspark, sql, stored procedures, data profiling, process flow, metric logging, error handling, system architectural decisions, java spring framework, spring boot, spring cloud, spring data","confluent kafka, data profiling, data warehouse architecturemodeling, error handling, etl processing, glue, java spring framework, kinesis, lambda, metric logging, process flow, python, snowflake, spark, spring boot, spring cloud, spring data, sql, sql server, stored procedures, system architectural decisions"
Senior Data Engineer,Quantexa,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-quantexa-3610847580,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 7 years, and 600+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data. Quantexa empowers organisations to drive better decisions from their data. Using the latest advancements in big data and AI, Quantexa uncovers hidden customer connections and behaviours to solve major challenges in financial crime, customer insight and data analytics.
Quantexa has accomplished rapid global expansion and achieved a valuation of $1.8 billion in April 2023 making us the first UK Unicorn business for 2023. Due to our continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀
What does a Senior Data Engineer role at Quantexa look like?
In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.
Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service. 🥇
We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.
Requirements
We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
Good experience with testing libraries of common programming languages and can talk about how they used them – for example scalatest (or equivalent packages for other languages)
Knows the difference between different test types ( unit test vs integration test for example) – can provide specific examples of one they have written themselves or oversaw when asked
Can explain unit testing and how they have been automated – can provide specific examples relevant to their own code or project when asked
Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
Passion and drive to grow within one of the UK’s fastest growing scale-ups.
Consulting or business facing skills and a desire to work with customers
Benefits
Why join Quantexa?
We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.
We offer:
Competitive Salary range of $130-141k
Company Bonus
401(k) match up to 5%
Competitive PTO Allowance + Paid US Federal Holidays + Your Birthday Off!
Medical, Dental, and Vision coverage
Short-term and Long-term Disability, Life, and AD&D insurance
Access to One Medical - primary care practice that offers 24/7 on-demand virtual care
Access to Teladoc - on-demand healthcare via phone or video
Access to Health Advocate - the nation’s leading healthcare advocacy and assistance company
Access to Calm App Subscription - the #1 app for meditation, relaxation, and sleep
Access to Talk Space - the #1 rated, HIPAA-compliant app for online counseling and therapy services
Continuous Training and Development, including access to Udemy Business
Access to WeWork offices & Company-wide socials
Our mission
We have one mission. To help businesses grow. To make data easier. And to make the world a better place. We’re not a start-up. Not anymore. But we’ve not been around that long either. What we are is a collection of bright, passionate minds harnessing complexities and helping our clients and their communities. One culture, made of many. Heading in one direction – the future.
It's All About You
Quantexa is proud to be an Equal Opportunity Employer. We’re dedicated to creating an inclusive and diverse work environment, where everyone feels welcome, valued, and respected. We want to hear from people who are passionate about their work and align with our values. Qualified applications will receive consideration for employment without regard to their race, colour, ancestry, religion, national origin, sex, sexual orientation, gender identity, age, citizenship, marital, disability, or veteran status. Whoever you are, if you’re a curious, caring, and authentic human being who wants to help push the boundaries of what’s possible, we want to hear from you.
Internal pay equity across departments is crucial to our global compensation philosophy. Grade level and salary ranges are determined through interviews and a review of experience, education, training, knowledge, skills, and abilities of the applicant, equity with other team members, and alignment with market data.
Quantexa is committed to providing reasonable accommodations in our talent acquisition processes
.
If you require support, please inform our Talent Acquisition Team.
Applicants must be authorized to work for any employer in the United States. We are unable to sponsor or take over the sponsorship of an employment visa at this time.
Show more
Show less","Scala, Java, Python, Hadoop, Spark, Data Fusion, Elasticsearch, Google Cloud Platform, Big data, Agile, Scrum, Data Science, Software engineering, Data processing, ETL, Analytics, Unit testing, Integration testing, Automation, Consulting, Business facing skills","scala, java, python, hadoop, spark, data fusion, elasticsearch, google cloud platform, big data, agile, scrum, data science, software engineering, data processing, etl, analytics, unit testing, integration testing, automation, consulting, business facing skills","agile, analytics, automation, big data, business facing skills, consulting, data fusion, data processing, data science, elasticsearch, etl, google cloud platform, hadoop, integration testing, java, python, scala, scrum, software engineering, spark, unit testing"
Senior/Staff Data Engineer,"HireIO, Inc.","Seattle, WA",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-hireio-inc-3754851535,2023-12-17,Dahlonega,United States,Mid senior,Onsite,"Location:
Usa/Usa/California/Sf Bay Area, Seattle
Sponsor Visa?
Yes
Language Requirements:
English, Mandarin
Requirements
Hands-on experience in building scalable data solutions with big data processing and distribute storage engines, such as Hadoop, Spark, Flink, Elastic Search, Druid, ClickHouse, etc
4+ years of engineering leadership experience in a cross-functional and fast-paced environment
Experience in managing a diverse team. (especially with members based in different countries)
Build strong collaboration with product and data science partners, define clear team vision, mission and strategy and explore new opportunities to empower business with scalable data solutions
Good understanding of ads technology/product is helpful but not required
Show more
Show less","Hadoop, Spark, Flink, Elastic Search, Druid, ClickHouse, Agile, Data Science, Leadership","hadoop, spark, flink, elastic search, druid, clickhouse, agile, data science, leadership","agile, clickhouse, data science, druid, elastic search, flink, hadoop, leadership, spark"
Senior Informaticist/ Data Engineer - Remote Eligible,Presbyterian Healthcare Services,"Albuquerque, NM",https://www.linkedin.com/jobs/view/senior-informaticist-data-engineer-remote-eligible-at-presbyterian-healthcare-services-3751180447,2023-12-17,New Mexico,United States,Mid senior,Onsite,"Overview
The Senior Informaticist will develop analytic tools & self-service solutions in partnership with IT for real time business decision making. Support Informaticist leadership in development and implementation of reporting/analytical approaches to evaluate health-related technological solutions and services post-implementation. Lead the development and implementation of programming standards and conventions. Responsible for supporting new products identification for customers and business stakeholders to meet their needs as well as educating the end users of available tools and technology options. Responsible for guiding, mentoring and supporting fellow junior Informaticist
Qualifications
Preferred Qualifications:
4+ years of experience in Base & Advanced SAS, preferably with one or more SAS certifications and preferably with extensive macro utilization
Proven track record in SAS usage of designing, developing, and maintaining data extract/transformation jobs, analytical reporting, and automated programming with reusable and SAS macro driven components
4+ years of experience in SQL, preferably with multiple Data Base Management systems and understanding of indexed and efficient table joins
Flexibility – projects on this team often do not have detailed requirements and usually require getting into the data to determine possibilities and then propose/build solutions that make the most of the data available – the ideal candidate can serve in both problem research/resolution and then switch hats to building sustainable process to solve that problem in an efficient manner ongoing
Experience with both Clinical and Claims data is desired but not required
Experience with HEDIS and other healthcare quality initiatives is strongly desired but not required
Other information:
SKILLS: Bachelors degree in a quantitative, IT, or related subject. A Masters is preferred. Six or more years of related business Intelligence, analytics and reporting work experience. Demonstrate working knowledge of sophisticated and varied analytics/reporting tools & solutions as well as platforms & products. Experience working with healthcare data (e.g., profiling), statistical analysis experience, and understanding of health care and delivery system processes. Good knowledge of overall program evaluation lifecycle (i.e., planning-implementation-completion/analysis-reporting), predictive modeling, data mining, and clinical best practices preferred. Grasp of a variety of server operating systems, storage systems, databases, scripting languages (e.g., Perl, Ruby, and Python), monitoring and job scheduling tools. Must have requisite content knowledge related BI tools (e.g., Business Objects/BO, Cognos), data visualizations tools (e.g., Tableau), querying language (e.g., SQL), statistical software such as SAS and Modeling techniques, as well as general health service research and outcomes reporting/analytics. Work with IT to provide analytical support in analysis and maintenance of standardized clinical content for clinical documentation, clinical decision support, and quality reporting efforts. Strong IT skills set along with business acumen to help the business stakeholders understand the output and facilitate the development of technology solutions to meet business needs.
Responsibilities
Ability to collaborate and develop working relationships across various cross functional areas of the AO as well as possess effective communication skills (oral and written) to communicate across the organization and at different levels
Collaborate with Informaticist leadership to understand complex workflow requirements and work on building analytical/reporting solution options that meet business needs
Lead small to medium complexity new software installations and enhancement requests such as installing SQL Server Integration Services (SSIS) packages as well as routine software upgrade initiatives
Lead and maintain up-to-date project documents for all initiatives that include technical details, user expectations, project goals, work effort, accountability, and deliverables
Conduct risk identification and mitigation planning and alert analytical senior leadership of any issues/escalations & risks
Continually identity areas of opportunity for improvement in systems and applications and work with appropriate teams to fully automate analytics and reporting
Identify system optimization and enhancements and collaborate with vendors and/or partner with IT to design, build and deploy effective analytical/reporting solutions
Review system configuration and design options in order to make appropriate recommendations for system maintenance requests
Design and implement sophisticated SAS queries/macro applications to meet the challenges of advanced data manipulation; design and develop interactive dashboards (e.g., using Tableau) and reports
Apply expertise in statistical inference, data mining, and visual presentation of data to help inform and support business and product decisions
Develop statistical/predictive models, reports and interactive dashboards to monitor the validity of the data across various data store(s)
Assist in developing, implementing, and maintaining predictive statistical models using techniques such as logistic regression, survival analysis, neural networks, collaborative filtering, decision trees and other machine learning
Identify opportunities to reduce manual efforts through introduction/integration of new reporting/analytic tools & automation such as SAS, BO, SQL Server Reporting Services, etc.
Build & test pilots of innovative analytics/reporting tools & solutions based on end business user needs as well as provide ongoing troubleshooting, support, and maintenance of application
Create, enhance, and maintain advanced reporting, analytics, dashboards and other BI solutions using new and available tools in the landscape including and not limited to Tableau, Sales Force, custom Microsoft based reporting solutions, SQL, etc.
Perform data analysis, data validation, data mapping/design, produce data samples/prototypes and ad hoc reports as well as obtain and analyze data by accessing multiple sources, including BO and data warehouses
Manage data validation checks/discrepancies, complicated programming logic, and large data volumes
Participate in development of tools & solutions through System Development Life Cycle (SDLC) model; participate in the software design, software development, and package implementation using standard platforms
Manage multiple tasks, milestones, deadlines, and customer relationships
Build/design tools to assure, maintain and improve overall efficiency and quality as well as validate work product of peers & of junior Informaticists
Assist in training analytical talent and business users on new tools & solutions and facilitate the implementation of tools & solutions and their integration into existing work processes
Participate in developing presentations/publications with Informaticist leadership within the AO
Contribute towards and assist Informaticists leadership in developing a year-end value story to demonstrate value of the team and their contribution to progress towards the AOs and PHSs goals.
Benefits
We offer more than the standard benefits!
Presbyterian employees gain access to a robust wellness program, including free access to our on-site and community-based gyms, nutrition coaching and classes, wellness challenges and more!
Learn more about our employee benefits:
https://www.phs.org/careers/why-work-with-us/benefits
Why work at Presbyterian?
As an organization, we are committed to improving the health of our communities. From hosting growers' markets to partnering with local communities, Presbyterian is taking active steps to improve the health of New Mexicans. For our employees, we offer a robust wellness program, including free access to our on-site and community-based gyms, nutrition coaching and classes, wellness challenges and more.
Presbyterian's story is really the story of the remarkable people who choose to work here. The hard work of our physicians, nurses, employees, board members and volunteers grew Presbyterian from a tiny tuberculosis sanatorium to a statewide healthcare system that serves more than 875,000 New Mexicans.
AA/EOE/VET/DISABLED. PHS is a drug-free and tobacco-free employer with smoke free campuses.
Offer Disclaimer
The compensation range for this role takes into account a wide range of factors, including but not limited to experience and training, internal equity, and other business and organizational needs. Compensation decisions are dependent upon the facts and circumstances of each offer.
Show more
Show less","SAS, SQL, Perl, Ruby, Python, Business Objects, Cognos, Tableau, Statistical software, Hadoop, Spark, Data mining, Machine learning, Predictive modeling, Data visualization, Data analysis, Data validation, Data mapping, Data engineering, Software development, System development life cycle, Project management, Communication skills, Teamwork skills, Problemsolving skills, Analytical skills, Business acumen, Health care knowledge","sas, sql, perl, ruby, python, business objects, cognos, tableau, statistical software, hadoop, spark, data mining, machine learning, predictive modeling, data visualization, data analysis, data validation, data mapping, data engineering, software development, system development life cycle, project management, communication skills, teamwork skills, problemsolving skills, analytical skills, business acumen, health care knowledge","analytical skills, business acumen, business objects, cognos, communication skills, data engineering, data mapping, data mining, data validation, dataanalytics, hadoop, health care knowledge, machine learning, perl, predictive modeling, problemsolving skills, project management, python, ruby, sas, software development, spark, sql, statistical software, system development life cycle, tableau, teamwork skills, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Albuquerque, NM",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744393341,2023-12-17,New Mexico,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, data science, business intelligence, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","agile engineering practices, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Real Property Data Entry Analyst,"GenTech Associates, Inc.","Albuquerque, NM",https://www.linkedin.com/jobs/view/real-property-data-entry-analyst-at-gentech-associates-inc-3787793146,2023-12-17,New Mexico,United States,Mid senior,Onsite,"Real Property Data Entry Analyst/Junior Real Property Support Specialist
All candidates must be US Citizens as a federal clearance in required to be obtained for this role.**
Kirtland Air Force Base – Full-time, in-person with situational telework at Kirtland Air Force
Base in Albuquerque, NM
Required clearance level: None, but potential for Public Trust or Secret Clearance
Educational Requirements: Two years combined experience providing data entry and/or real property support; education levels at the Associate level or above can be substituted for experience
Qualifications: None
Experience working with Air Force and DoD Standard Forms
Populate forms such as AF 123, AF 813, DD 914, DD 1354, and AF 332
Experience with Air Force or DoD real property a plus, but not required
Ability to manage a large data set of forms and appropriately use time to generate forms in the system of record, NexGenIT
Time permitting, assist the Kirtland Air Force Base team with Real Property support. This includes existence and completeness, instruments, grants, asset accountability, real estate transactions (leases, licenses, easements, permits), backlogged items, and general support and data request
A willingness to fulfill a junior level role that will be mentored by the current team
Ability to maintain confidentiality.
Ability to assess and analyze complex issues
Ability to work both independently and within a team
Ability to be self-motivated with strong interpersonal and organizational skills
Ability to multi-task in a fast-paced environment
Job Type: Full-time
Salary: $40,000.00 - $55,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Work Location: In person
Powered by JazzHR
SLP0eTX4Dz
Show more
Show less","Data Entry, NexGenIT, Air Force and DoD Standard Forms, AF 123, AF 813, DD 914, DD 1354, AF 332, Microsoft Office Suite, Existence and Completeness, Instruments, Grants, Asset Accountability, Real Estate Transactions, Leases, Licenses, Easements, Permits, Backlogged Items, General Support, Data Request, Confidentiality, Complex Issue Assessment and Analysis, Independent and Team Work, SelfMotivation, Interpersonal Skills, Organizational Skills, MultiTasking, FastPaced Environment, Microsoft Office Suite","data entry, nexgenit, air force and dod standard forms, af 123, af 813, dd 914, dd 1354, af 332, microsoft office suite, existence and completeness, instruments, grants, asset accountability, real estate transactions, leases, licenses, easements, permits, backlogged items, general support, data request, confidentiality, complex issue assessment and analysis, independent and team work, selfmotivation, interpersonal skills, organizational skills, multitasking, fastpaced environment, microsoft office suite","af 123, af 332, af 813, air force and dod standard forms, asset accountability, backlogged items, complex issue assessment and analysis, confidentiality, data entry, data request, dd 1354, dd 914, easements, existence and completeness, fastpaced environment, general support, grants, independent and team work, instruments, interpersonal skills, leases, licenses, microsoft office suite, multitasking, nexgenit, organizational skills, permits, real estate transactions, selfmotivation"
Data Analytics Consultant,"Technomics, Inc.","Albuquerque, NM",https://www.linkedin.com/jobs/view/data-analytics-consultant-at-technomics-inc-3779159708,2023-12-17,New Mexico,United States,Mid senior,Hybrid,"Technomics is seeking an experienced Data Analytics Consultant to join our Albuquerque team. The ideal candidate will possess a strong background in data analytics, with previous experience working with federal clients. This position requires strong client relationship building skills, and comfort working alongside clients in government facilities. Additionally, experience with policy development or quality assurance is considered a plus.
Responsibilities
Build and maintain strong relationships with clients, serving as their primary point of contact. Understand clients' business needs, provide timely updates, and ensure client satisfaction throughout the engagement.
Use analytic tools and techniques to perform data visualization, modeling, or statistical analysis to assist client in developing data-driven policies.
Collaborate with clients to understand their objectives, challenges, and requirements. Develop tailored data analytics strategies and solutions to address specific business problems and enhance decision-making processes.
Lead and oversee data analytics projects from initiation to completion. Define project scope, timelines, and deliverables. Coordinate with cross-functional teams to ensure smooth execution and adherence to project objectives.
Present analytical findings and recommendations to clients in a clear, concise, and compelling manner. Prepare comprehensive reports, visualizations, and dashboards to communicate complex data insights effectively.
Required Qualifications
Bachelor's degree in a relevant field such as Engineering, Economics, Statistics, or a related discipline.
At least 3 years of experience successfully managing client relationships and delivering high-quality results.
Excellent communication and presentation skills, with the ability to routinely interact with senior government officials.
Proactive problem-solving and critical-thinking abilities, with a strong attention to detail.
Project management skills, including the ability to work independently to prioritize tasks, manage timelines, and meet deadlines.
Applied experience in data analytics, data visualization tools, or statistical analysis.
Active Q or Top Secret clearance.
Preferred Qualifications
Prior experience working within Nuclear Security Enterprise.
Familiarity with policy development processes and quality assurance principles is also a plus.
This position requires the applicant to be on client site in Albuquerque three days per week. In addition the position requires the ability to travel (up to 10%) domestically within the US to provide support to the program.
We are an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, age, marital status, disability or veteran status.
Show more
Show less","Data Analytics, Data Visualization, Statistical Analysis, Policy Development, Quality Assurance, Client Relationship Building, Communication, Presentation, Problem Solving, Critical Thinking, Project Management, Data Visualization Tools, Statistical Analysis Tools, Q Clearance, Top Secret Clearance, Security","data analytics, data visualization, statistical analysis, policy development, quality assurance, client relationship building, communication, presentation, problem solving, critical thinking, project management, data visualization tools, statistical analysis tools, q clearance, top secret clearance, security","client relationship building, communication, critical thinking, data visualization tools, dataanalytics, policy development, presentation, problem solving, project management, q clearance, quality assurance, security, statistical analysis, statistical analysis tools, top secret clearance, visualization"
Sr. Data Engineer with Security Clearance,ClearanceJobs,"New Mexico, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-with-security-clearance-at-clearancejobs-3777108728,2023-12-17,New Mexico,United States,Mid senior,Hybrid,"Marathon TS has a need for a Sr. Data Engineer to support our government customer located in Albuquerque, NM. The role of the Sr. Data Engineer is collaborate with data scientists, architects, researchers, users, model and sim engineers, user experience designers, software engineers, digitizers, cataloguers, and system administrators to build a system to collect, manage, and convert raw data from the DTRIAC archive into usable information for U.S. deterrent missions. Duties include, but are not limited to:
Design tools and methodologies to process the digital collection in a production mode.
Develop and implement methods for, and configurations of, the Data Lake to support navigation, search, insertion, or extraction of information or files by the government or other performers without requiring proprietary software, tools, or data other than widely available commercial-off-the-shelf (COTS) tools, and software that can be authorized for use on government IT systems.
Develop, maintain, and improve capabilities, such as scripting, to efficiently perform maintenance, synchronization, and production processing of data in the Data Lake on Windows- and Linux-based IT systems, including HPCMP clusters.
Implement, configure, perform functional testing, and operate the data and applications of the ASD environment as a hosted capability on government IT systems.
Leverage the collection, capabilities, and team to perform targeted analyses and studies and to provide dedicated support to missions and end users.
Create documentation or training materials for Project Products.
Support integration or hosting of capabilities or products on government IT systems.
Hold and participate in Gate Reviews.
Other duties as assigned. Required Skills, Experience and Education:
5 years relevant experience.
Experience building and maintaining secure, end to end systems and services.
Experience building and working with data pipelines and large data sets).
Experience with schema design and data modeling.
Deep understanding of algorithms and efficient data structures.
Current Security Certification or equivalent required.
Proficiency with Python programming language, C, SQL, and C# required.
Experience with OCR and Machine Learning technologies and methodologies required.
Experience and demonstrable proficiency with OpenCV and PostgresDB is desirable.
Experience/Proficiency with utilizing Tesseract OCR with Python is desirable.
Experience in developing and implementing Recurrent Neural Networks (RNN) algorithms and integrating Long Short Term Memory (LSTM) highly desirable.
Experience with Academy of Color Encoding System (ACES) Developer Tools for integrating data specifications into software and hardware a plus.
Secret Clearance eligibility required
Experience with the following types of tools a plus: SAS, Apache Hadoop, Tableau, TensorFlow, BigML, Knime, RapidMiner, Apache Flink, DataRobot, Apache Spark, MongoDB, Trifacta, Minitab, Apache Kafka, QlikView, Julia, SPSS, Keras, Matplotlib, Pytorch, scikit-learn, Weka, Domino Data Science Platform, IBM Watson Studio, and Google Cloud AI Platform Marathon TS is committed to the development of a creative, diverse and inclusive work environment. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Marathon TS will be based on merit, qualifications, and abilities. Marathon TS does not discriminate against any person because of race, color, creed, religion, sex, national origin, disability, age or any other characteristic protected by law (referred to as ""protected status""). #CJJobs
Show more
Show less","Python, C, SQL, C#, OCR, Machine Learning, OpenCV, PostgresDB, Tesseract OCR, RNN, LSTM, ACES Developer Tools, SAS, Apache Hadoop, Tableau, TensorFlow, BigML, Knime, RapidMiner, Apache Flink, DataRobot, Apache Spark, MongoDB, Trifacta, Minitab, Apache Kafka, QlikView, Julia, SPSS, Keras, Matplotlib, Pytorch, scikitlearn, Weka, Domino Data Science Platform, IBM Watson Studio, Google Cloud AI Platform, Data Lake, Data Pipelines, Schema Design, Data Modeling, Algorithms, Data Structures","python, c, sql, c, ocr, machine learning, opencv, postgresdb, tesseract ocr, rnn, lstm, aces developer tools, sas, apache hadoop, tableau, tensorflow, bigml, knime, rapidminer, apache flink, datarobot, apache spark, mongodb, trifacta, minitab, apache kafka, qlikview, julia, spss, keras, matplotlib, pytorch, scikitlearn, weka, domino data science platform, ibm watson studio, google cloud ai platform, data lake, data pipelines, schema design, data modeling, algorithms, data structures","aces developer tools, algorithms, apache flink, apache hadoop, apache kafka, apache spark, bigml, c, data lake, data structures, datamodeling, datapipeline, datarobot, domino data science platform, google cloud ai platform, ibm watson studio, julia, keras, knime, lstm, machine learning, matplotlib, minitab, mongodb, ocr, opencv, postgresdb, python, pytorch, qlikview, rapidminer, rnn, sas, schema design, scikitlearn, spss, sql, tableau, tensorflow, tesseract ocr, trifacta, weka"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Albuquerque, NM",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711060,2023-12-17,New Mexico,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML Data OPs, Data stacks, Pipelines, Automation workflows, Data enrichment, Monitoring, Realtime data processing, Batch data processing, LLMs, Big data technologies, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","data engineering, ml data ops, data stacks, pipelines, automation workflows, data enrichment, monitoring, realtime data processing, batch data processing, llms, big data technologies, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, automation workflows, aws, azure, bash, batch data processing, big data technologies, data classification, data engineering, data enrichment, data management, data retention, data stacks, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, llms, machine learning, ml data ops, monitoring, nlp, pipelines, python, realtime data processing, snowflake, spark, sparkstreaming, sql, storm"
Data and Analytics Senior Analyst - Consumer Card Pricing,Lloyds Banking Group,"House, NM",https://www.linkedin.com/jobs/view/data-and-analytics-senior-analyst-consumer-card-pricing-at-lloyds-banking-group-3778849486,2023-12-17,New Mexico,United States,Mid senior,Hybrid,"JOB TITLE:
Data and Analytics Senior Analyst - Consumer Card Pricing
SALARY:
£57,546 - £63,940
LOCATION(S):
Chester/Cardiff
HOURS:
Full-time
WORKING PATTERN:
Our work style is hybrid, which involves spending at least
two days per week, or 40% of our time, at one of our office sites
Lloyds Banking Group is the largest consumer credit card lender in the UK. Our Credit Cards Pricing and Performance Team use a data-driven approach to make sure our products are always compelling and relevant, and we're excited to be making another delightful addition to the team!
About this opportunity
We're looking for someone who is great at working with large volumes of data, teasing out insights, then bringing these to life when presenting to others. You're proud of the quality of the code you write, with experience writing sophisticated SQL queries, and the ability to write well-documented code in SAS, Python, or another similar language. Your day-to-day will be a mix of data science / engineering, and analysis work: you might be optimizing an SQL query, automating a data pipeline, or developing and presenting a piece of bespoke analysis.
You'll primarily be involved in analysing Balance Transfers and Money Transfers in existing credit card customers - assessing financial performance and customer behaviour. The valuation of BTs and MTs is important in understanding the profitability of the credit card portfolio, and provides commercial insights into all aspects of customer engagement.
Day to day, you will:
Data provision - writing and optimising SQL queries to support you and the team in performing customized analysis
Writing code to automate data processing: currently in SAS but transitioning to GCP
Create analysis to support BT/MT valuation: developing and improving forecasting methodologies, and analysis techniques to provide insights into customer behaviour and performance
Drive the Promotional BT/MT process from a valuation perspective, developing meaningful insights into behavioural segment performance by price point and ensuring sufficient control over data processes.
Use our partnerships with colleagues in Finance, Credit Risk and Product Design to build, monitor and update financial valuations for new and existing offers to customers by customer behaviour segment.
Why Lloyds Banking Group
Join us and, as well as making a difference to customers, you'll enjoy a fulfilling career where you're free to be yourself. Great colleagues, transforming workspaces, hybrid working and a wide variety of career opportunities - you'll find them all here.
What you'll need
Excellent analytical ability with highly developed attention to detail and numerical skills
Experience handling and simplifying large volumes of data, writing advanced SQL and optimizing existing queries for performance
A good level of coding experience in one or more programming languages, particularly experience in manipulating data and performing analysis; SAS coding experience is not required but you must be willing and able to pick this up fairly quickly
Proficient in excel and understanding complex formulas to ensure spreadsheets are efficient and future proofed
Excellent communicating skills and be able to present succinct summaries of sophisticated and data heavy pieces of work to senior management
Experience of working independently and setting key priorities for yourself
And any experience of these would be really useful
Financial services experience including profit and loss knowledge
Experience with SQL on distributed platforms (e.g., Teradata, Apache Hive) and/or cloud SQL platforms (e.g., GCP BigQuery, Azure, AWS)
SAS coding experience
About working for us
Our focus is to ensure we're inclusive every day, building an organisation that reflects modern society and celebrates diversity in all its forms. We want our people to feel that they belong and can be their best, regardless of background, identity or culture. We were one of the first major organisations to set goals on diversity in senior roles, create a menopause health package, and a dedicated Working with Cancer initiative. And it's why we especially welcome applications from under-represented groups. We're disability confident. So if you'd like reasonable adjustments to be made to our recruitment processes, just let us know
We also offer a wide-ranging benefits package, which includes:
* A generous pension contribution of up to 15%
* An annual performance-related bonus
* Share schemes including free shares
* Benefits you can adapt to your lifestyle, such as discounted shopping
* 30 days' holiday, with bank holidays on top
* A range of wellbeing initiatives and generous parental leave policies
Want to do amazing work, that's interesting and makes a difference to millions of people? Join our journey.
Show more
Show less","SQL, Python, SAS, GCP, Apache Hive, Teradata, Azure, AWS, Data science, Analysis, Profit and loss, Spreadsheets, Financial services, Excel, Communication, Presentation","sql, python, sas, gcp, apache hive, teradata, azure, aws, data science, analysis, profit and loss, spreadsheets, financial services, excel, communication, presentation","analysis, apache hive, aws, azure, communication, data science, excel, financial services, gcp, presentation, profit and loss, python, sas, spreadsheets, sql, teradata"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Albuquerque, NM",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090511,2023-12-17,New Mexico,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Data management tools, Data classification, Retention","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management tools, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, retention, snowflake, spark, sparkstreaming, sql, storm"
Snowflake Data Engineer,Confie,United States,https://www.linkedin.com/jobs/view/snowflake-data-engineer-at-confie-3756664356,2023-12-17,Pinehurst,United States,Associate,Remote,"Purpose
Work under the guidance and supervision of the Data Warehouse Tech Manager to build Confie's next-generation Enterprise Data Warehouse. Help develop data-centric solutions to meet the company's business needs - specifically, integrating existing silo data and providing data solutions that reflect the integrated landscape
Essential Duties & Responsibilities
Gather business requirements and functional specifications.
Perform data analysis and data modeling as required to design tables and processes to fulfill the business requirements
Transform complex customer business requirements into effective, efficient, and maintainable enterprise-level solutions
Research, analyze, support, and implement database solutions and changes on Snowflake Cloud data warehouse platform
Design and develop robust and scalable data pipelines to support data integrations using Fivetran,
Snowflake, Airflow, and dbt.
Perform data cleaning, analysis and feature engineering using python.
Develop data ingestion and integrations (REST, SOAP, SFTP, etc.) processes
Ability to work with multiple data sources and types (structured/semi-structured/unstructured)
Implement efficient data integration and transformation logic using Snowflake SQL, stored procedures, or external tools
Optimization and Performance tuning of data processing workflows and SQL queries for improved performance and efficiency.
Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Monitor data pipelines for timely and accurate completion
Stay up-to-date with industry trends and advancements in data engineering, continuously improving the team's technical knowledge and skills.
On Call Support.
Qualifications and Education Requirements
5+ year’s professional experience in data engineering, designing and implementing data pipelines, and building data infrastructure.
4+ years strong experience in Snowflake data cloud and ETL development including snowflake procedures, udf’s in python and SQL, streams, tasks, snowpipe, and working with semi-structured data etc.
4+ years of strong experience with python programing, and extensively used packages like pandas, numpy, matplotlib, seaborn and scikit-learn for Data Analysis, classification and visualization.
Bachelor's degree in Computer Science, Engineering, or a related field.
Snowflake Certification is a plus
Additional certifications related to data platforms are a plus
Solid understanding of data warehousing concepts, dimensional modeling, and data integration techniques.
Strong experience with Data Integration & transformation tools like Wherescape, dbt, Matillion,
Azure Data Factory or experience with similar ETL tools is acceptable; experience with maintaining and enhancing data pipelines
Experience with databricks, Google BigQuery, Aws Redshift is a plus
Experience with data quality and observability concepts is a plus
Experience with cloud platforms (e.g., AWS, Azure, GCP) and cloud-based data technologies is a plus
Skills
A Love for All Things Data! The backbone of a good data engineer is to understand the life cycle of data movements from source to final interpretations on a report
A Passion to Learn! Strong desire and ability to learn new tools, skills, and acquire knowledge.
Confidence communicating & translating data driven insights and technical concepts into simple terminology for business clients of various levels
The following skills are important to the success of a skilled Confie technical professional:
Listening skills - the ability to understand what people say
Analytical skills- the ability to critically evaluate the information from multiple sources and breakdown high-level information into details
Critical thinking and problem solving skills
Observation skills - the ability to validate data obtained via other techniques and expose new areas for elicitation
Organizational skills - the ability to work with the vast array of information gathered during analysis and to cope with rapidly changing information
Interpersonal skills - the ability to help set priorities
Oral and written skills – excellent written and verbal communication with little or no supervision
Self-motivated
Ability to work and communicate effectively with any levels of the user community
Must be willing and able to work off hours or remote support as needed
Other Duties
This job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
Show more
Show less","Snowflake, Airflow, Fivetran, dbt, Python, SQL, ETL, Data pipelines, Data integration, Data transformation, Data quality, Data warehousing, Dimensional modeling, Data engineering, Cloud platforms, AWS, Azure, GCP, Databricks, Google BigQuery, Aws Redshift, Data observability, Data governance, Data lifecycle management, Data analysis, Data modeling, Data visualization, Machine learning, Artificial intelligence, Communication, Teamwork, Problem solving, Critical thinking, Analytical skills, Organizational skills","snowflake, airflow, fivetran, dbt, python, sql, etl, data pipelines, data integration, data transformation, data quality, data warehousing, dimensional modeling, data engineering, cloud platforms, aws, azure, gcp, databricks, google bigquery, aws redshift, data observability, data governance, data lifecycle management, data analysis, data modeling, data visualization, machine learning, artificial intelligence, communication, teamwork, problem solving, critical thinking, analytical skills, organizational skills","airflow, analytical skills, artificial intelligence, aws, aws redshift, azure, cloud platforms, communication, critical thinking, data engineering, data governance, data integration, data lifecycle management, data observability, data quality, data transformation, dataanalytics, databricks, datamodeling, datapipeline, datawarehouse, dbt, dimensional modeling, etl, fivetran, gcp, google bigquery, machine learning, organizational skills, problem solving, python, snowflake, sql, teamwork, visualization"
Data Analyst,OmniForce Solutions,"Houston, TX",https://www.linkedin.com/jobs/view/data-analyst-at-omniforce-solutions-3787361162,2023-12-17,Pinehurst,United States,Associate,Remote,"Must be located in one of the following states:
Texas
Colorado
Washington
New Jersey
Pennsylvania
Our client is a Fortune 200 leading retail energy company focused on bringing the power of energy to people and organizations. Putting customers at the center of everything they do, they provide energy solutions to millions of people through their diverse portfolio of retail brands across North America. Take your career to the next level by working on a dynamic, innovative team that moves our world towards a sustainable energy future.
Position Overview:
As a Data Analyst specializing in Power BI, you will play a pivotal role in transforming raw data into actionable insights that drive informed decision-making within our supply chain operations. The ideal candidate will possess a strong analytical mindset, attention to detail, and the ability to work independently. This is a project-based role with the potential for long-term engagement based on performance.
Key Responsibilities:
Data Analysis: Analyze large datasets to extract meaningful trends, patterns, and insights.
Utilize Power BI to create interactive and visually appealing dashboards for reporting purposes.
Report Generation: Develop and maintain reports that provide key performance indicators (KPIs) to support supply chain decision-making.
Collaborate with cross-functional teams to understand reporting requirements and deliver timely, accurate reports.
Power BI Expertise: Demonstrate proficiency in Power BI, including data modeling, DAX calculations, and dashboard design.
Extract, transform, and load (ETL) data from various sources into Power BI for analysis and reporting purposes.
Design and implement robust data models and relationships to ensure accurate and efficient data analysis.
Perform data cleansing, validation, and manipulation to maintain data accuracy and consistency.
Stay current with industry best practices and advancements in Power BI functionality.
Data Visualization: Design visually compelling and user-friendly dashboards that effectively communicate complex data insights to stakeholders.
Ensure data visualizations align with business objectives and enhance overall decision-making processes.
Power BI Asset Management: Manage Power BI assets, including reports, dashboards, workspaces, semantic models, and other components.
Oversee the sharing and distribution of Power BI items, ensuring accessibility and collaboration among team members.
Implement and maintain security measures to safeguard Power BI assets and sensitive data.
Qualifications:
Proven experience as a Data Analyst focusing on Power BI, including Power Query and Power Pivot.
Proficiency in SQL for data manipulation, extraction, and querying.
Proficiency in a range of other sources of data ingestion.
Ability to translate complex business requirements into technical specifications for data visualization.
Proficiency in data visualization techniques and best practices.
Strong analytical and problem-solving skills.
Excellent communication skills, with the ability to convey complex findings to non-technical stakeholders.
Demonstrated ability to work independently and manage multiple tasks and priorities effectively.
Proficiency in Python is preferred.
Education and Experience:
Bachelor's degree in Data Science, Business Analytics, or related field.
Minimum of 2-4 years of relevant experience in data analysis with a focus on Power BI.
Show more
Show less","Power BI, DAX, Data modeling, ETL, Data cleansing, Data visualization, Python, SQL, Power Query, Power Pivot, Data analysis, Business Analytics, Communication skills","power bi, dax, data modeling, etl, data cleansing, data visualization, python, sql, power query, power pivot, data analysis, business analytics, communication skills","business analytics, communication skills, dataanalytics, datacleaning, datamodeling, dax, etl, power pivot, power query, powerbi, python, sql, visualization"
Data Analyst,ALTOUR,United States,https://www.linkedin.com/jobs/view/data-analyst-at-altour-3780249422,2023-12-17,Pinehurst,United States,Associate,Remote,"Overview
The primary role for this position is to create & manage travel data report extracts in a variety of applications to external and internal clients at ALTOUR. This position is responsible for reviewing the data results and research methods to reduce manual labor of correcting data errors. This person would work with the departments of Automation, Accounting, and Operations to determine solutions to further streamline internal processes for accurate client data reports. Create complex reporting data solutions that might not be available to users with standard data applications. The most regularly used applications are Grasp data, Prime Analytics and Excel.
This is a work at home role with a preferred scheduele of 8:30am-5pm CST.
The salary range on this job posting/advertising has been developed to give applicants a wide range to comply with pay transparency laws in all states and geographical areas. Many factors, such as years of experience, geographical location, budget etc. are considered when determining the starting rate of pay.
This role may be eligible for an incentive, commission, bonus, or a discretionary bonus program based on the company’s financial goal achievement and individual performance.
Responsibilities
Provide support for Ad Hoc, reoccurring scheduled setup and creation of custom reports
Global data consolidation from ALTOUR locations and global agency partners
Data normalization & solutions for data integrity
Maintain and update clients and settings in data applications
Offline data manipulation within Excel for custom solutions limited in standard data applications
Hotel rate audits and manage preferred hotel property rates for reporting
Audits of reporting data to validate against data rules by client
Quality control of scheduled reports
Aspects of Credit Card reconciliation
Identify common errors and collaborate with Operations to fix data issues from reoccurring
Develop advanced custom reporting for clients outside specifications of normal reporting platform
Work with account managers to create quarterly/yearly account reviews
Provide support for requested data analysis
Create custom extraction, cleansing models and reloading data into reporting tools
Maintain support ticket queue according to defined service level
Maintain updated documentation of data procedures
PowerBI creation of data reports
Special projects as they arise
Qualifications
College degree preferred or equivalent experience.
1-3 years of experience in a data analysis role.
Microsoft skills (Word, Outlook and Advanced Excel)
Great organization skills & detail oriented
Self-starter, able to work with little direction
Independently research data issues to solve challenges, analytical problem-solving skills
Travel industry data knowledge desired
Sabre or Worldspan GDS skills desired
Travel reporting software such as Grasp Data and Prime Analytics preferred
Regular and dependable attendance and punctuality are required
PowerBI & Travcom experience a plus
Physical Requirements
Occasional travel may be required.
Job requires employee to sit for extended periods of time
Repetitive motion, substantial movements (motions) of the wrists, hands, and/ or fingers.
Typing or otherwise working primarily with fingers.
Must be able to comprehend instructions, interpret documents, and apply abstract principles to a wide range of complex tasks.
Ability to understand the meanings of words and effectively respond, analyze information and write reports, comprehend complex issues, and communicate effectively to diverse groups.
Job requires employee to perform arithmetic accurately and compute rates and percentages.
Must be able to communicate effectively and professionally (verbally and in writing) as appropriate for the needs of the audience.
Internova Travel Group is an Equal Opportunity Employer. We make employment decisions without regard to age, race, religion, national origin, gender, disability, veteran status, genetic information, sexual orientation and gender identity or any other protected class.
Our benefit offerings include choice of two medical plans and two dental plans, vision insurance, flexible spending accounts (FSAs), company-paid life insurance and AD&D, optional additional life insurance and AD&D, disability insurance, paid parental leave, paid time off, 401k Plan with company match, discounted employee travel options, access to LinkedIn Learning webinars and courses. Discounted pet insurance and auto, home, & renters insurance.
Perspective Employee Privacy Policy
Show more
Show less","Data Analysis, Microsoft Word, Microsoft Outlook, Microsoft Excel, Data Reporting, Data Normalization, Data Integrity, Data Applications, Data Manipulation, Data Audits, Quality Control, PowerBI, Travel Industry Data, Sabre, Worldspan GDS, Travel Reporting Software, Grasp Data, Prime Analytics, Travcom","data analysis, microsoft word, microsoft outlook, microsoft excel, data reporting, data normalization, data integrity, data applications, data manipulation, data audits, quality control, powerbi, travel industry data, sabre, worldspan gds, travel reporting software, grasp data, prime analytics, travcom","data applications, data audits, data integrity, data manipulation, data normalization, data reporting, dataanalytics, grasp data, microsoft excel, microsoft outlook, microsoft word, powerbi, prime analytics, quality control, sabre, travcom, travel industry data, travel reporting software, worldspan gds"
"Software Engineer 2, Data",MyFitnessPal,United States,https://www.linkedin.com/jobs/view/software-engineer-2-data-at-myfitnesspal-3758971008,2023-12-17,Pinehurst,United States,Associate,Remote,"At MyFitnessPal, our vision is to be the most trusted brand for improving your health through better food choices. We believe good health starts with what you eat. We provide the tools and resources to reach your weight management goals.
We are looking for a Software Engineer 2, Data to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you’ll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you’ll find that your teammates value collaboration, mentorship, and inclusive environments.
What you’ll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms
Travel Requirement: Approximately 10% travel will be required for this position
Job Location: Austin, TX preferred but open to remote
Please consider applying even if you don’t meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we’re building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
The reasonably estimated salary for this role at MyFitnessPal ranges from $86,700 - $125,000. Actual compensation is based on factors such as the candidate’s skills, qualifications, and experience. In addition, MyFitnessPal offers a wide range of comprehensive and inclusive employee benefits for this role including healthcare, parental planning, mental health benefits, annual performance bonus, a 401(k) plan and match, responsible time off, monthly wellness and technology allowances, and others.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom:
Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes:
If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections:
We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best:
Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back:
Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program:
Take control of your career through our mentorship program where, if you’d like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support:
Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First:
Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness:
Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness:
Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential:
Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion:
Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters:
Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future:
Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal’s competitive employer match.
At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.
MyFitnessPal participates in E-Verify.
Show more
Show less","Python, SQL, Scala, Snowflake, MySQL, MongoDB, DynamoDB, Airflow, Data Factory, REST, AWS, Data pipelines, Data modeling, Data engineering, Data orchestration, API design, Data integrity, Data validation, Data at scale, Cloud computing","python, sql, scala, snowflake, mysql, mongodb, dynamodb, airflow, data factory, rest, aws, data pipelines, data modeling, data engineering, data orchestration, api design, data integrity, data validation, data at scale, cloud computing","airflow, api design, aws, cloud computing, data at scale, data engineering, data factory, data integrity, data orchestration, data validation, datamodeling, datapipeline, dynamodb, mongodb, mysql, python, rest, scala, snowflake, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Albany, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712039,2023-12-17,Vallejo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Pipelines, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Retention, Industrycompetitive compensation, Company bonus, Equity programs, QueerInclusive Benefits, Genderaffirming offerings, 401K plan, 6% match, Immediate vest in the US, Flexible vacation policy, Monthly stipends, Onetime homeoffice setup stipend, Companysponsored events","data engineering, machine learning, data pipelines, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, retention, industrycompetitive compensation, company bonus, equity programs, queerinclusive benefits, genderaffirming offerings, 401k plan, 6 match, immediate vest in the us, flexible vacation policy, monthly stipends, onetime homeoffice setup stipend, companysponsored events","401k plan, 6 match, airflow, applied machine learning, aws, azure, bash, company bonus, companysponsored events, data classification, data engineering, datapipeline, docker, dynamodb, equity programs, etl, flexible vacation policy, gcp, genderaffirming offerings, git, helm, immediate vest in the us, industrycompetitive compensation, java, kafka, kubernetes, machine learning, monthly stipends, onetime homeoffice setup stipend, python, queerinclusive benefits, retention, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Berkeley, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708645,2023-12-17,Vallejo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Git, Pandas, R, Airflow, KubeFlow, Pipeline tools, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data classification, Data retention, 401K plan, Company bonus, Equity programs, Genderaffirming offerings, Included Health, HRT stipends, Flexible vacation policy, Cell phone stipends, Internet stipends, Wellness stipends, Food stipends, Homeoffice setup stipend","python, java, bash, sql, git, pandas, r, airflow, kubeflow, pipeline tools, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data classification, data retention, 401k plan, company bonus, equity programs, genderaffirming offerings, included health, hrt stipends, flexible vacation policy, cell phone stipends, internet stipends, wellness stipends, food stipends, homeoffice setup stipend","401k plan, airflow, aws, azure, bash, cell phone stipends, company bonus, data classification, data retention, docker, dynamodb, equity programs, etl, flexible vacation policy, food stipends, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, hrt stipends, included health, internet stipends, java, kafka, kubeflow, kubernetes, machine learning, pandas, pipeline tools, python, r, snowflake, spark, sparkstreaming, sql, storm, wellness stipends"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Emeryville, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087753,2023-12-17,Vallejo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Machine learning, Data governance, Data management tools, Data classification, Data retention, LGBTQ social networking, Queerinclusive benefits","python, java, bash, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, machine learning, data governance, data management tools, data classification, data retention, lgbtq social networking, queerinclusive benefits","airflow, aws, azure, bash, data classification, data governance, data management tools, data retention, docker, dynamodb, gcp, helm, java, kafka, kubernetes, lgbtq social networking, machine learning, python, queerinclusive benefits, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Emeryville, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707772,2023-12-17,Vallejo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data OPs Infrastructure, Data Pipelines, Automation Workflows, Data Enrichment, Monitoring Tools, AI Models, Data Platforms, Data Pre/Post Processing, ML Models, Statistical Analysis, Data Visualization, Pandas, R, Orchestration Frameworks, Airflow, KubeFlow, SQL, Python, Java, bash, Git, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Legal Compliance, Data Management Tools, Data Classification, Retention, 401K","data engineering, ml data ops infrastructure, data pipelines, automation workflows, data enrichment, monitoring tools, ai models, data platforms, data prepost processing, ml models, statistical analysis, data visualization, pandas, r, orchestration frameworks, airflow, kubeflow, sql, python, java, bash, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, legal compliance, data management tools, data classification, retention, 401k","401k, ai models, airflow, automation workflows, aws, azure, bash, data classification, data engineering, data enrichment, data management tools, data platforms, data prepost processing, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, ml data ops infrastructure, ml models, monitoring tools, nosql databases, orchestration frameworks, pandas, python, r, relational databases, retention, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Data Engineer - ETL Pipelines,"Kaygen, Inc.","Princeton, NJ",https://www.linkedin.com/jobs/view/data-engineer-etl-pipelines-at-kaygen-inc-3728496777,2023-12-17,Northampton,United States,Associate,Onsite,"KAYGEN is an emerging leader in providing top talent for technology based staffing services. We specialize in providing high-volume contingent staffing, direct hire staffing and project based solutions to companies worldwide ranging from startups to Fortune 500 and Managed Service Providers (MSP) across a wide variety of industries
Key Responsibilities:
Develop and maintain ETL pipelines using PySpark to process and transform data from various sources into our data warehouse.
Collaborate with data scientists and analysts to understand data requirements and ensure data quality and accuracy.
Optimize and performance-tune Spark jobs to handle large volumes of data efficiently.
Create and maintain documentation for ETL processes, data schemas, and data dictionaries.
Collaborate with the data infrastructure team to design and implement data storage solutions that meet performance and scalability requirements.
Participate in on-call rotation for monitoring and maintaining data pipelines.
Stay up-to-date with the latest trends and technologies in data engineering and Spark ecosystem.
Requirements:
Advanced proficiency in Python and SQL.
Strong experience with ETL pipeline development using PySpark.
Familiarity with one of the orchestration tools such as Apache Airflow, DBT, or Delta live tables.
Experience with distributed data processing and storage technologies (e.g., Hadoop, Spark, HDFS, AWS S3).
Solid understanding of data modeling and database design principles.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills.
Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent work experience).
Nice to Have:
Experience with cloud platforms such as AWS, Azure, or Google Cloud.
Knowledge of data warehousing concepts and technologies (e.g., Redshift, BigQuery).
Experience with version control systems (e.g., Git).
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes).
Previous experience in a data engineering or similar role.
At KAYGEN, we are always looking for dynamic, talented and experienced individuals. We invite you to join our team of talented IT professionals, consulting at client locations across the globe. Our culture is team-orientated; we strive to stand by our core values of respect, honesty and integrity. Our team of experienced staffing experts will work with you to find you the best opportunity. For more information please visit us at  www.kaygen.com.
Benefits:
Healthcare Insurance
Vision and Dental Insurance
401(k) Retirement Plan
Free Life Insurance
Vacation Time Off
Sick Time Off
Family Medical Leave (FMLA)
Achieve your Kaizen by clicking here. A unique and exclusive talent community supported by Kaygen, that includes programs like:
Certifications
Mentorship Program
Referrals
Family and Wellness benefits
Continuous Growth and Career Development
Show more
Show less","PySpark, ETL, Data Warehousing, Data modeling, Database design, Hadoop, Spark, HDFS, AWS S3, AWS, Azure, Google Cloud, Redshift, BigQuery, Git, Docker, Kubernetes, Apache Airflow, DBT, Delta live tables, Python, SQL","pyspark, etl, data warehousing, data modeling, database design, hadoop, spark, hdfs, aws s3, aws, azure, google cloud, redshift, bigquery, git, docker, kubernetes, apache airflow, dbt, delta live tables, python, sql","apache airflow, aws, aws s3, azure, bigquery, database design, datamodeling, datawarehouse, dbt, delta live tables, docker, etl, git, google cloud, hadoop, hdfs, kubernetes, python, redshift, spark, sql"
"Analyst, Data Analytics",NRG Energy,"Princeton, NJ",https://www.linkedin.com/jobs/view/analyst-data-analytics-at-nrg-energy-3780807518,2023-12-17,Northampton,United States,Associate,Onsite,"As an NRG employee, we encourage you to think creatively and proactively about your career choices. Our work environment is dynamic and the career opportunities across our businesses offer variety and challenge. Providing career growth to our own employees is critical to our ongoing success--take charge of your career goals and empower your future!
Job Summary
NRG is seeking an Analyst with excellent analytics skills to join our NRG Home East Lifecycle Optimization team. The position will be responsible for supporting the analytical and data needs of all the functional areas within the NRG Home East organization. The role will be highly visible and work with a wide range of reporting levels. The candidate must possess strong interpersonal and communications skills along with the ability to apply statistical rigor to data and uncover actionable insights. The nature of the work leads to an environment of shifting priorities and requires the candidate to be able to adapt to change. This position requires continual learning and skills development as well as a strong problem-solving base. The position will report to the Manger, Customer Lifecycle Optimization.
Essential Duties/Responsibilities
Support the analytics needs of all the functional areas of NRG Home – East including but not limited to Sales, Marketing, Go-to Market, Operations, Pricing and Retention and our internal Analytics departments.
Build, maintain and collaborate on predictive models and/or financial forecast models using various methodologies.
Apply statistical principles to assist in identifying proper control groups and post-campaign analysis.
Contribute to results presentations with audiences of differing technical acumen in both presentations and dashboards.
Follow established procedures and processes to perform routine analytical reporting or data mining.
Anticipate stakeholder’s needs, operate with a sense of urgency in response to ad-hoc requests and adapt to changes quickly.
Other duties as required.
Working Conditions
100% Remote.
Some overtime required as special projects arise.
Minimum Requirements
A minimum of a bachelor’s degree is required in either a Business Intelligence (or related) or STEM (Science, Technology, Engineering, Mathematics) field.
Experience with a least one analytical language: Python, R, SAS
Experience working with large data sets.
Experience with SQL queries, including knowledge of relational databases and joins.
Preferred Qualifications
Advanced degree or professional certification in an Analytical field is a plus.
Additional Knowledge, Skills And Abilities
Must be able to collaborate efficiently and effectively with individuals and teams distributed across multiple locales.
Familiarity with statistical testing techniques such as t-tests, chi-sq tests and calculation of confidence intervals of means.
Dashboarding experience is a plus (Power BI, Tableau, Spotfire, Dash, etc.)
Familiarity with financial calculations (NPV, IRR, ROI, etc.) is a plus.
Ability to manage heavy workload and shifting priorities.
Self-starter with an inquisitive mindset and an interest in understanding the ‘why’ of actions.
Physical Requirements
N/A
Why NRG Is a Great Place To Work
Great company culture!! Voted as a BEST employer by Forbes
A competitive total compensation package, including annual incentive and/or commission
Stock Purchase Plan
Benefits on the first day of employment - Medical, Dental, Vision, Life Insurance, and Short Term Disability, Wellness program, etc.
Company-paid life insurance and disability insurance
401 (k) plan to help save for retirement
Numerous discounts, including electricity discounts on NRG brands
If you reside in or intend to work remotely from California, Colorado, New York or Washington State, you may contact Careers@nrg.com for compensation information related to this position and other information as required by applicable law. Please include the job title in your request.
#CB-SS
No Outside Recruiters or Agencies **
NRG Energy is committed to a drug and alcohol free workplace. To the extent permitted by law and any applicable collective bargaining agreement, employees are subject to periodic random drug testing, and post-accident and reasonable suspicion drug and alcohol testing. EOE AA M/F/Vet/Disability Level, Title and/or Salary may be adjusted based on the applicant's experience or skills. Official description on file with Talent.
Show more
Show less","Python, R, SAS, SQL, Power BI, Tableau, Spotfire, Dash, Relational databases, NPV, IRR, ROI, Statistical testing techniques, Ttests, Chisq tests, Confidence intervals of means, Dashboarding experience, Financial calculations","python, r, sas, sql, power bi, tableau, spotfire, dash, relational databases, npv, irr, roi, statistical testing techniques, ttests, chisq tests, confidence intervals of means, dashboarding experience, financial calculations","chisq tests, confidence intervals of means, dash, dashboarding experience, financial calculations, irr, npv, powerbi, python, r, relational databases, roi, sas, spotfire, sql, statistical testing techniques, tableau, ttests"
"Data Analyst/ Processor - Hamilton, NJ (Only Local Candiadtes)",Biogensys,"Hamilton, NJ",https://www.linkedin.com/jobs/view/data-analyst-processor-hamilton-nj-only-local-candiadtes-at-biogensys-3742851654,2023-12-17,Northampton,United States,Associate,Onsite,"We are hiring
Data Analyst/ Processor
for one of our clients in
Hamilton, NJ.
Job Description
Collects and organizes appeal requests for denied member fair hearings.
Sorts and processes incoming mail regarding member redetermination documentation, and all relevant.
According to postal laws and mail unit regulations, manages necessarily outgoing mail operations in correspondence with relevant offices and Trenton Postal Office.
Manual work involving mail sorting, opening, and scanning machinery and perform clerical detail work in regard to ensure accuracy of mechanical input.
Sorts interdepartmental mail through use of call numbers and/or zip codes for distribution purposes.
Maintains simple inventory records and stocks supplies of various types of envelopes and cards and other materials used by the unit; notifies a superior official of the need for further ordering.
Appropriately distributes mail, including confidential or sensitive documentation as needed..
Work Hours
9-5
Lunch Duration: 1 hr unpaid
Additional Information
Work from office or remote: Work from office
PayRate: $19/hour on W2
Show more
Show less","Data Analysis, Data Processing, Mail Sorting, Mail Processing, Machinery Operation, Clerical Work, Inventory Management, Envelope and Card Stock Management, Mail Distribution, Confidential Documentation Handling","data analysis, data processing, mail sorting, mail processing, machinery operation, clerical work, inventory management, envelope and card stock management, mail distribution, confidential documentation handling","clerical work, confidential documentation handling, data processing, dataanalytics, envelope and card stock management, inventory management, machinery operation, mail distribution, mail processing, mail sorting"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783184753,2023-12-17,Northampton,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-NewJerse-DataResearchAn.019
Show more
Show less","Python, JavaScript, JSON, R, OOP, Generative AI, Data Science, Data Analytics, Natural Language Processing, Machine Learning, Algorithms, Research, Product Development, Technical Writing, Verbal Communication, Project Management, Stakeholder Management, EdTech, Remote Work","python, javascript, json, r, oop, generative ai, data science, data analytics, natural language processing, machine learning, algorithms, research, product development, technical writing, verbal communication, project management, stakeholder management, edtech, remote work","algorithms, data science, dataanalytics, edtech, generative ai, javascript, json, machine learning, natural language processing, oop, product development, project management, python, r, remote work, research, stakeholder management, technical writing, verbal communication"
Data Analyst - Customer Complaints,Kenvue,"Skillman, NJ",https://www.linkedin.com/jobs/view/data-analyst-customer-complaints-at-kenvue-3781681306,2023-12-17,Northampton,United States,Associate,Hybrid,"Job Description
Kenvue is currently recruiting for-
Data Analyst II, Global Complaint Vigilance
This position will be reporting to Sr. Manager, Global Complaint Vigilance Systems & Trending; based at the Skillman, NJ location.
Who We Are
At Kenvue, we realize the extraordinary power of everyday care. Built on over a century of heritage and rooted in science, we’re the house of iconic brands - including NEUTROGENA®, AVEENO®, TYLENOL®, LISTERINE®, JOHNSON’S® and BAND-AID® that you already know and love. Science is our passion; care is our talent. Our global team is made by 22,000 diverse and brilliant people, passionate about insights, innovation and committed to deliver the best products to our customers. With expertise and empathy, being a Kenvuer means to have the power to impact life of millions of people every day. We put people first, care fiercely, earn trust with science and solve with courage – and have brilliant opportunities waiting for you! Join us in shaping our future–and yours.
What You Will Do
The Analyst II participates in complex analytics activities and develops solutions for global business partners. Analyst II will be part of team that leads in areas of Data Governance, Database Infrastructure and Data Analytics development for Global Complaint platform.
Key Responsibilities
Support the data analytics team on data discovery in CDL (data lake) and data ingestions from the data source system in the CDL (data lake)
Work in partnership with system owners to understand the process and the data input in the data source
Lead the data foundation strategy in CDL (data lake) to guarantee the data availability and curation for our key quality data and additional data needed to support our data analytics strategy.
Develop methods of analysis of data to support global metrics, trend analysis and risk management planning using innovative data science methodology.
Participate in various projects within the department and communicate effectively with all levels of management. This role will analyze data and perform periodic complaint trend analysis.
Analyst II collaborates with global partners in Consumer Care Centers, Medical Safety, Business Quality, Marketing, Internal affiliates, External sites, and other business partners to ensure the integrity, consistency, compliance, and alignment of the end-to-end complaint vigilance process.
Analyst II must have the ability to assist with various projects within the department, communicate effectively with management. This role will query and analyze data and other trends from complaint systems.
Assists with developing training presentations for complaint vigilance process and systems and creation and updates of controlled documents.
Required Qualifications
A minimum of a bachelor’s degree with a focus in Science, Engineering, or related field
A minimum of 2 years of related experience including Quality Assurance and/or Quality Control .
Experience working in pharmaceutical, medical devices or other related or highly regulated industry
Demonstrated knowledge and expertise in quality processes and regulatory requirements
Experience working in an FDA regulated environment
Desired Skills-
Experience supporting manufacturing, packaging, and development operations is preferred.
Experience supporting aspects of the complaint vigilance life cycle for pharmaceutical, device, and /or cosmetic products is preferred.
Experience with one or more technologies supporting complaint handling preferred.
Experience in the creation, update, and management of complaint files from complaint identification through closure preferred.
Expertise with root cause analysis techniques including but not limited to- Brainstorming, data analysis and collection tools, 5 Whys, Fishbone (Cause and Effect), FMEA, and DMAIC preferred.
Experience with tools and techniques supporting qualitative risk analysis including but not limited to- Probability and impact assessment (likelihood of recurrence and potential effect), probability and impact matrix (risk ratings, rating rules), and risk categorization (by root causes, other qualifiers) is preferred.
Experience with systems and tools supporting analysis and reporting preferred.
What’s In It For You
Competitive Benefit Package
Paid Company Holidays, Paid Vacation, Volunteer Time, Summer Fridays & More!
Learning & Development Opportunities
Employee Resource Groups
The anticipated base pay range for this role is 60,000 to 96,600
Kenvue is proud to be an Equal Opportunity Employer.  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identify, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
Primary Location
NA-US-New Jersey-Skillman
Job Function
Quality Assurance
Organization
Johnson & Johnson Consumer Inc.
Job Qualifications
What We Are Looking For
Show more
Show less","Data Analytics, Data Governance, Database Infrastructure, Data Strategy, Root Cause Analysis, Risk Management Planning, Complaint Vigilance Life Cycle, Quality Assurance, Quality Control, Brainstorming, Data Analysis, Fishbone (Cause and Effect), FMEA, DMAIC, Probability and Impact Assessment, Risk Categorization, Analysis and Reporting","data analytics, data governance, database infrastructure, data strategy, root cause analysis, risk management planning, complaint vigilance life cycle, quality assurance, quality control, brainstorming, data analysis, fishbone cause and effect, fmea, dmaic, probability and impact assessment, risk categorization, analysis and reporting","analysis and reporting, brainstorming, complaint vigilance life cycle, data governance, data strategy, dataanalytics, database infrastructure, dmaic, fishbone cause and effect, fmea, probability and impact assessment, quality assurance, quality control, risk categorization, risk management planning, root cause analysis"
Data Engineer - ETL Pipelines,"Kaygen, Inc.","Princeton, NJ",https://www.linkedin.com/jobs/view/data-engineer-etl-pipelines-at-kaygen-inc-3728498520,2023-12-17,Northampton,United States,Mid senior,Onsite,"KAYGEN is an emerging leader in providing top talent for technology based staffing services. We specialize in providing high-volume contingent staffing, direct hire staffing and project based solutions to companies worldwide ranging from startups to Fortune 500 and Managed Service Providers (MSP) across a wide variety of industries
Key Responsibilities:
Develop and maintain ETL pipelines using PySpark to process and transform data from various sources into our data warehouse.
Collaborate with data scientists and analysts to understand data requirements and ensure data quality and accuracy.
Optimize and performance-tune Spark jobs to handle large volumes of data efficiently.
Create and maintain documentation for ETL processes, data schemas, and data dictionaries.
Collaborate with the data infrastructure team to design and implement data storage solutions that meet performance and scalability requirements.
Participate in on-call rotation for monitoring and maintaining data pipelines.
Stay up-to-date with the latest trends and technologies in data engineering and Spark ecosystem.
Requirements:
Advanced proficiency in Python and SQL.
Strong experience with ETL pipeline development using PySpark.
Familiarity with one of the orchestration tools such as Apache Airflow, DBT, or Delta live tables.
Experience with distributed data processing and storage technologies (e.g., Hadoop, Spark, HDFS, AWS S3).
Solid understanding of data modeling and database design principles.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills.
Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent work experience).
Nice to Have:
Experience with cloud platforms such as AWS, Azure, or Google Cloud.
Knowledge of data warehousing concepts and technologies (e.g., Redshift, BigQuery).
Experience with version control systems (e.g., Git).
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes).
Previous experience in a data engineering or similar role.
At KAYGEN, we are always looking for dynamic, talented and experienced individuals. We invite you to join our team of talented IT professionals, consulting at client locations across the globe. Our culture is team-orientated; we strive to stand by our core values of respect, honesty and integrity. Our team of experienced staffing experts will work with you to find you the best opportunity. For more information please visit us at  www.kaygen.com.
Benefits:
Healthcare Insurance
Vision and Dental Insurance
401(k) Retirement Plan
Free Life Insurance
Vacation Time Off
Sick Time Off
Family Medical Leave (FMLA)
Achieve your Kaizen by clicking here. A unique and exclusive talent community supported by Kaygen, that includes programs like:
Certifications
Mentorship Program
Referrals
Family and Wellness benefits
Continuous Growth and Career Development
Show more
Show less","Python, SQL, PySpark, Apache Airflow, DBT, Delta live tables, Hadoop, Spark, HDFS, AWS S3, Data modeling, Database design, AWS, Azure, Google Cloud, Redshift, BigQuery, Git, Docker, Kubernetes, Containerization, Orchestration","python, sql, pyspark, apache airflow, dbt, delta live tables, hadoop, spark, hdfs, aws s3, data modeling, database design, aws, azure, google cloud, redshift, bigquery, git, docker, kubernetes, containerization, orchestration","apache airflow, aws, aws s3, azure, bigquery, containerization, database design, datamodeling, dbt, delta live tables, docker, git, google cloud, hadoop, hdfs, kubernetes, orchestration, python, redshift, spark, sql"
Senior Data Engineer,"Enterra Solutions, LLC","Princeton, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-enterra-solutions-llc-3671401231,2023-12-17,Northampton,United States,Mid senior,Remote,"LOCATION:
U.S. Eastern Time Zone
Must reside in the US – preferably in the Eastern Time Zone. Remote working permitted. Must be eligible to work in the US without sponsorship now or in the future. This is a full-time position with benefits. Contractors will not be considered for this position.
Who we are:
Enterra provides solutions that leverage sophisticated machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing to provide highly actionable insights and recommendations to business users. Today, our solutions impact just about every aspect of the products you buy at your local store – from what is available to how it is priced and even where it is placed on the shelf. Our SolaaS (Solution as a Service) solutions are deployed within private clouds – principally on Azure.
We help transform market-leading companies into true data-driven digital enterprises.
What you will do:
The ideal candidate must be collaborative, and deadline driven. Because of the nature of our work and our technology, successful candidates must take a growth mindset and be comfortable with ambiguity, with the ability to take a proactive, structured approach to achieve results. Results-orientation and deadline driven are critical in our fast-paced environment.
The successful candidate will join a diverse team to:
Build unique high-impact business solutions utilizing advanced technologies for use by world class clients.
Create and maintain the underlying data pipeline architecture for the solution offerings from raw client data to final solution output.
Create, populate, and maintain data structures for machine learning and other analytics.
Use quantitative and statistical methods to derive insights from data.
Guide the data technology stack used to build Enterra's solution offerings.
Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses — across multiple functions and processes.
Responsibilities Include:
Work with other Enterra personnel to develop and enhance commercial quality solution offerings
Design, create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows.
Assemble large, complex data sets to meet analytical requirements – analytics tables, feature-engineering etc.
Design and build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other 'big data' technologies such as Databricks.
Design and build automated analytics tools that utilize the data pipeline to derive actionable insights.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Design and develop data integrations and data quality framework
Develop appropriate testing strategies and reports for the solution as well as data from external sources.
Evaluate new technology for use within Enterra.
Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offerings
Configure the data pipelines to accommodate client-specific requirements to onboard new clients.
Perform regular operations tasks to ingest new and changing data – implement automation where possible.
Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations.
May provide guidance and oversight to fellow data engineers
Requirements:
Bachelor's degree in Computer Science or a STEM (Science, Technology, Engineering or Math) field required
Minimum of 7 years hands on experience as a data engineer or similar position.
Minimum of 7 years commercial experience with Python or Scala Programming Language
Minimum of 7 years SQL and experience working with relational databases (Postgres preferred).
Experience with at least one of the following – Databricks, Spark, Hadoop or Kafka
Demonstratable knowledge and experience developing data pipelines to automate data processing workflows
Demonstratable experience in data modeling
Demonstratable knowledge of data warehousing, business intelligence, and application data integration solutions
Demonstratable experience in developing applications and services that run on a cloud infrastructure Azure preferred
Excellent problem-solving and communication skills
Ability to thrive in a fast-paced, remote environment.
Comfortable with ambiguity with the ability to build structure and take a proactive approach to drive results.
Attention to detail – quality and accuracy in work is essential.
The following additional skills would be beneficial:
Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization.
Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.).
Experience with container management and deployment, e.g., Docker and Kubernetes
Show more
Show less","Data Engineering, Machine Learning, Artificial Intelligence, Natural Language Processing, Python, Scala, SQL, PostgreSQL, Databricks, Spark, Hadoop, Kafka, Data Pipelines, Data Modeling, Data Warehousing, Business Intelligence, Application Data Integration, Cloud Infrastructure, Azure, Data Science, Data Visualization, Statistics, Excel, R, Power BI, Tableau, Docker, Kubernetes","data engineering, machine learning, artificial intelligence, natural language processing, python, scala, sql, postgresql, databricks, spark, hadoop, kafka, data pipelines, data modeling, data warehousing, business intelligence, application data integration, cloud infrastructure, azure, data science, data visualization, statistics, excel, r, power bi, tableau, docker, kubernetes","application data integration, artificial intelligence, azure, business intelligence, cloud infrastructure, data engineering, data science, databricks, datamodeling, datapipeline, datawarehouse, docker, excel, hadoop, kafka, kubernetes, machine learning, natural language processing, postgresql, powerbi, python, r, scala, spark, sql, statistics, tableau, visualization"
Senior Marketing Data Analyst,Oxygen,"Princeton, NJ",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-oxygen-3755968612,2023-12-17,Northampton,United States,Mid senior,Remote,"About Oxygen:
Oxygen is a modern financial platform designed for the 21st-century economy, tailored to meet the unique needs of today's consumers and small businesses (freelancers, solopreneurs and modern SMBs). We believe beauty lies in simplicity, from design to utility, and build our ever-growing product suite with this in mind. And we’re just getting started. At Oxygen, we’re focused on lifting up those who dare to push boundaries and redefine what's possible by delivering on our promise to be a more fair financial partner. The next generation building the future is writing their own rules and shattering expectations. So are we. For those who reject the status quo, welcome to Banking for the Extraordinary. Let's create the future!
We are looking for a highly specialized data analyst with experience in working with marketing teams. Responsibilities include data extraction, segmentation, process automation, A/B testing and ensuring data accuracy. The role contributes to revenue growth and customer retention through insights and optimized targeting strategies for the marketing team. We seek innovation, strong communication with senior managers, and a deep understanding of business operations and industry trends. You will need data analysis skillset and expertise to deliver insights and solutions that enhance operational efficiency and drive growth.
What You'll Do:
Oversee data cleaning, processing, insight generation, and reporting. Drafting strategic and tactical recommendations across all marketing channels to help synthesize findings and align on strategy.
Standardize reporting to support marketing campaigns and implement dashboards for tracking and creating dynamic visualizations of performance against target for all levels of granularity of tactics, creatives, audiences, and sites in the market.
Analyze large data sets to develop critical insights, such as identifying target audience segments for lead generation and/or marketing campaigns, and collaborate with business units on long-term planning, policy development, and problem resolution .
Drive cross functional analytics projects from beginning to end: build relationships with partner teams, collect and analyze data, and present key insights and recommendations to drive business improvements
Build and support source-of-truth ‘Reverse ETL’ data pipelines, to foster and maintain business clarity in the go-to-market organization
Design, assemble and maintain data models that go-to-market teams can use to build dashboards for reporting and deriving insights
Have the ability to design, organize, and coordinate A/B and multivariate tests and run AB tests across multiple digital properties, business units, target users, and global markets.
About You:
Minimum of 5 years experience in marketing data analysis
Strong experience with marketing principles, including customer segmentation strategies, channel attribution, LTV vs. CAC, and campaign optimization.
Advanced proficiency in SQL and database querying techniques, including window functions and CTEs.
Solid experience in Python for automating data processing and conducting data analysis.
Expert-level proficiency in Business Intelligence tools, such as Quicksight, Tableau, or Power BI.
High proficiency with spreadsheet (Google Sheets / Excel).
Knowledge in statistical analysis and mathematical modeling is beneficial, particularly for advanced data analysis tasks.
Exceptional analytical skills with the capability to interpret complex data and deliver results to the team and stakeholders.
Major Pluses:
Experience in Tag Management (e.g., Segment or Google Tag Manager) is advantageous.
Familiarity with A/B testing methodologies.
Compensation and Benefits:
The final compensation package for this role will be determined during the interview process and is based on a variety of factors, including, but not limited to, geographic location, internal equity, education, skill set, experience and training.
Oxygen also offers comprehensive benefits, including stock options, medical, dental, and vision plans, flexible spending accounts, pre-tax commuter benefits, a retirement plan, flex time off, paid parental leave, employer paid disability coverage, and additional health and wellbeing perks and benefits. Oxygen reserves the right to amend or modify employee perks and benefits at any time.
Not sure you qualify?
Don’t meet every single requirement? At Oxygen we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Oxygen provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Show more
Show less","Data analysis, Data extraction, Segmentation, Process automation, A/B testing, Revenue growth, Customer retention, Business operations, Industry trends, Data cleaning, Insight generation, Reporting, Marketing channels, Dashboards, Data visualization, Performance tracking, Data modeling, SQL, Database querying, Python, Business Intelligence tools, Statistical analysis, Mathematical modeling, Tag Management, Google Analytics","data analysis, data extraction, segmentation, process automation, ab testing, revenue growth, customer retention, business operations, industry trends, data cleaning, insight generation, reporting, marketing channels, dashboards, data visualization, performance tracking, data modeling, sql, database querying, python, business intelligence tools, statistical analysis, mathematical modeling, tag management, google analytics","ab testing, business intelligence tools, business operations, customer retention, dashboard, data cleaning, data extraction, dataanalytics, database querying, datamodeling, google analytics, industry trends, insight generation, marketing channels, mathematical modeling, performance tracking, process automation, python, reporting, revenue growth, segmentation, sql, statistical analysis, tag management, visualization"
Senior Data Analyst - Remote | WFH,Get It Recruit - Information Technology,"Raritan, NJ",https://www.linkedin.com/jobs/view/senior-data-analyst-remote-wfh-at-get-it-recruit-information-technology-3781168726,2023-12-17,Northampton,United States,Mid senior,Remote,"We are a dynamic and growing IT professional services company based in New England, dedicated to assisting mid-to-large enterprises in implementing secure IT environments across on-premises and public cloud platforms.
Position: Senior Data Analyst Consultant (Remote - 6-Month Opportunity)
Responsibilities
Collaborate with data owners and process stakeholders to define reporting requirements.
Work closely with data owners and process stakeholders to map essential data fields for reporting purposes.
Facilitate collaborative efforts among reporting stakeholders to develop reporting logic aligned with business objectives.
Identify and document opportunities for automating reporting processes.
Required Skills
5+ years of experience as a data analyst in a technical setting, with a background in data science.
Proficiency in working with data and process owners to identify, collect, document, and review reporting requirements.
Ability to document complex reporting needs and related data fields, demonstrating a strong understanding of business needs for a large enterprise.
Proficient in documenting data flows with diagrams.
Ability to navigate the organizational structure of a large enterprise and facilitate discussions across domains.
Practical experience using business intelligence to address business problems.
Knowledge of machine learning, data visualization, data mining, statistics, and complex reporting algorithms.
Proficiency in query structures and languages (e.g., Python, datahub, SQL).
Hands-on experience with Tableau.
Ability to interpret and leverage existing documentation.
Excellent analytical skills.
Collaborative team worker, both in person and virtually using tools such as MS Teams.
Excellent documentation skills, with demonstrated proficiency in Microsoft Office, including Word, Excel, and PowerPoint.
Ability to work as a liaison between business and information security/information technology.
Flexibility to accommodate working across different time zones.
Excellent interpersonal communication skills with strong spoken and written English.
Business outcomes mindset.
Solid balance of strategic thinking with attention to detail.
Self-starter with the ability to take initiative.
Project management and organizational skills with attention to detail.
Education
BS in Computer Science, Mathematics, or a related field of study, or equivalent work experience.
About Us
Join us in a fast-paced and growing organization focused on delivering exceptional projects for outstanding people.
EEO Statement
We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, genetics, disability, age, or veteran status.
Employment Type: Full-Time
Show more
Show less","Data Analysis, Data Science, Data Mining, Statistics, Reporting, Tableau, Microsoft Office, SQL, Python, Datahub, Business Intelligence, Machine Learning, Data Visualization, Project Management, Organizational Skills, Attention to Detail, Strategic Thinking, SelfStarter, Communication Skills, English, Business Outcomes Mindset, Computer Science, Mathematics","data analysis, data science, data mining, statistics, reporting, tableau, microsoft office, sql, python, datahub, business intelligence, machine learning, data visualization, project management, organizational skills, attention to detail, strategic thinking, selfstarter, communication skills, english, business outcomes mindset, computer science, mathematics","attention to detail, business intelligence, business outcomes mindset, communication skills, computer science, data mining, data science, dataanalytics, datahub, english, machine learning, mathematics, microsoft office, organizational skills, project management, python, reporting, selfstarter, sql, statistics, strategic thinking, tableau, visualization"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Las Vegas, NV",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783184774,2023-12-17,Nevada,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-LasVegas-DataResearchAn.013
Show more
Show less","Python, JavaScript, JSON, Generative AI, OOP languages, R, Data analysis, Data science, Product development, Product engineering, Research, Communication, Project management, Stakeholder management, EdTech, AI, Machine learning, Data analytics, Coaching","python, javascript, json, generative ai, oop languages, r, data analysis, data science, product development, product engineering, research, communication, project management, stakeholder management, edtech, ai, machine learning, data analytics, coaching","ai, coaching, communication, data science, dataanalytics, edtech, generative ai, javascript, json, machine learning, oop languages, product development, product engineering, project management, python, r, research, stakeholder management"
"Sr. Engineer, Database Infrastructure - Slack",Slack,"Nevada, United States",https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760627774,2023-12-17,Nevada,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","MySQL, Vitess, Go, PHP/Hack, Python, Ruby, Java, Kafka, Cassandra, ElasticSearch, Linux, AWS, Chef, Ansible, Puppet, Terraform, Cloud infrastructure, Deployment automation, Configuration management","mysql, vitess, go, phphack, python, ruby, java, kafka, cassandra, elasticsearch, linux, aws, chef, ansible, puppet, terraform, cloud infrastructure, deployment automation, configuration management","ansible, aws, cassandra, chef, cloud infrastructure, configuration management, deployment automation, elasticsearch, go, java, kafka, linux, mysql, phphack, puppet, python, ruby, terraform, vitess"
Staff Cybersecurity Data Platform Engineer,Adobe,"Nevada, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767923168,2023-12-17,Nevada,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, Cloud Computing, Orchestration, Data Ingestion, Medallion Architectures, Unity Catalog, Autoloader Jobs, Delta Lakehouse, Spark, Pyspark, AWS Tools/Technology, Kafka, Flink, Security Operations Center (SOC), Threat Management, Incident Response, Enterprise Security, Data Modelling, Schema Normalization","databricks, cloud computing, orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, delta lakehouse, spark, pyspark, aws toolstechnology, kafka, flink, security operations center soc, threat management, incident response, enterprise security, data modelling, schema normalization","autoloader jobs, aws toolstechnology, cloud computing, data ingestion, data modelling, databricks, delta lakehouse, enterprise security, flink, incident response, kafka, medallion architectures, orchestration, schema normalization, security operations center soc, spark, threat management, unity catalog"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Nevada, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762877418,2023-12-17,Nevada,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, SSIS, C#, Airflow, Python, AWS, MongoDB, DBT, ETL, OLAP, Snowflake","sql, ssis, c, airflow, python, aws, mongodb, dbt, etl, olap, snowflake","airflow, aws, c, dbt, etl, mongodb, olap, python, snowflake, sql, ssis"
Data Engineer,SurveyMonkey,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-surveymonkey-3782563680,2023-12-17,Ottawa, Canada,Associate,Remote,"SurveyMonkey
is a global leader in online surveys and forms that empowers people with the insights they need to make decisions with speed and confidence.
Our fast, intuitive feedback management platform connects millions of users worldwide with real-time AI-powered insights that drive meaningful decisions. We provide answers to more than 20 million questions every day so that people and organizations can attract new audiences, delight customers, create advocates, and extend their competitive advantage in the marketplace. Our vision is to raise the bar for human experiences by amplifying individual voices. Learn more at surveymonkey.com.
What We’re Looking For
We are looking for an experienced data Engineer to join a team of highly skilled data engineers and data architects to build and manage the end-to-end data pipelines (batch and near real-time) using modern cloud technologies. This is a role in the central data organization at SurveyMonkey that provides actionable insights into all key business functions of the organization
As a Data Engineer, you will work on data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.
What You’ll Be Working On
Design, architect and build data pipelines to support existing data models
Data quality: Build quality checks in the end-to-end data pipelines
Build new Data models (Fact vs Dimension). Write performant/idempotent transformations in Snowflake
Build data pipeline using Python scripting (in a modular/loop context) Write well-tested, production-ready code in Python and Snowflake SQL
Hands-on experience implementing ETL (or ELT) best practices
Translate business requirements, to technical specifications, form project scope, and deliver deployable code.
Write complex data engineering Snowflake - SQL jobs that perform sophisticated queries on the entirety of our datasets
Collaborate closely with stakeholders, upstream and downstream partners
Document our systems for internal and external stakeholders
Monitor and debug data pipelines running on Airflow
Participate in code reviews
We’d love to hear from people with
4+ years experience in data engineering and Data warehousing technologies
2+ years experience in Snowflake/ETL or similar technologies like Redshift
Experience with AWS cloud services: S3, EC2, RDS, Spark, EMR etc
Experience with object-oriented/object function scripting languages: Python (preferred), Java, Scala, etc.
Experience in orchestrating, automating, and deploying production data pipelines using Airflow/Luigi, etc
Experience with DevOPS: Git, Github actions, CI/CD pipelines, Terraform, etc
Experience building Data infrastructure or Data Platform Framework Preferred
Experience with tools such as DBT or other similar technologies
Experience with transforming, and developing data structures, metadata, dependency, and data workflows to support an Analytics function
In-depth knowledge of Data lakes, EDW concepts, and data modeling (Star, Snowflake, and Galaxy schemas)
Why SurveyMonkey? We’re glad you asked
SurveyMonkey is a place where the curious come to grow. We’re building an inclusive workplace where people of every background can excel no matter their time zone. At SurveyMonkey, we weave employee feedback into everything we do to create forward-looking benefits policies, employee programs, and an award-winning culture, including best workplace for parents, our annual holiday refresh, our annual week of service, and our C.H.O.I.C.E Fund. In addition, we’ve reimagined the way we work to allow employees to choose what works best for them -- working in-person, fully remote, or a hybrid model that combines the two through our Choice Model.
Our commitment to an inclusive workplace
SurveyMonkey is an equal opportunity employer committed to providing a workplace free from harassment and discrimination. We celebrate the unique differences of our employees because that is what drives curiosity, innovation, and the success of our business. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, gender identity or expression, age, marital status, veteran status, disability status, pregnancy, parental status, genetic information, political affiliation, or any other status protected by the laws or regulations in the locations where we operate. Accommodations are available for applicants with disabilities.
Show more
Show less","Data Engineering, Cloud Technologies, Data Warehousing, ETL/ELT, Snowflake, Python, SQL, Airflow, AWS, Spark, EMR, Java, Scala, Git, Github, CI/CD Pipelines, Terraform, Data Lakes, EDW, Data Modeling, Star Schemas, Snowflake Schemas, Galaxy Schemas, DBT","data engineering, cloud technologies, data warehousing, etlelt, snowflake, python, sql, airflow, aws, spark, emr, java, scala, git, github, cicd pipelines, terraform, data lakes, edw, data modeling, star schemas, snowflake schemas, galaxy schemas, dbt","airflow, aws, cicd pipelines, cloud technologies, data engineering, data lakes, datamodeling, datawarehouse, dbt, edw, emr, etlelt, galaxy schemas, git, github, java, python, scala, snowflake, snowflake schemas, spark, sql, star schemas, terraform"
Staff Data Engineer,Recruiting from Scratch,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398398,2023-12-17,Ottawa, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Information Technology Security Engineer [NTT DATA],CareerBeacon,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-information-technology-security-engineer-ntt-data-at-careerbeacon-3778501646,2023-12-17,Ottawa, Canada,Mid senior,Onsite,"Req ID:
261093
NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Senior Information Technology Security Engineer to join our team in Ottawa, Ontario (CA-ON), Canada (CA).
Must Perform The Following
Review, analyze and/or apply:
Networking Protocols such as HTTP, FTP, and Telnet
Secure IT architectures fundamentals, standards, communications and security protocols such as IPSec, IPv6, SSL, and SSH
IT Security protocols at all layers of the Open Systems Interconnection (OSI) and Transmission Control
Protocol/Internet Protocol (TCP/IP) stacks
Domain Name Services (DNS) and Network Time Protocols (NTP)
Network routers, multiplexers and switches
Application, host and/or Network hardening and security best practices such as shell scripting, service identification, and access control
Intrusion detection/prevention systems, malicious code defence, file integrity, Enterprise Security Management and/or firewalls;
Identify the technical threats to, and vulnerabilities of networks and cloud technologies
Manage the IT Security configuration;
Analyze IT Security tools and techniques;
Analyze the security data and provide advisories and reports;
Prepare technical reports such as IT Security Solutions option analysis and implementation plans in support of cloud email migration; and
Perform any other Work related to this category.
Mandatory Requirements
Must have at least ten (10) years of experience in the role of Information Technology Security Engineer.
Must have seven (7) years of experience within the last fifteen (15) years analyzing security of IT systems for the federal government.
Must have experience delivering at least four (4) projects within the last ten (10) years where they analyzed IT system designs to identify technical threats to, and vulnerabilities of those systems and solutions to minimize the security risk.
Must have experience delivering at least four (4) projects within the last ten (10) years where they analyzed security related to networking or telecommunications.
Other Requirements
Candidates located in the Ottawa region are preferred; remote work locations may also be considered.
Must have a valid Canadian Federal Government - Secret Clearance.
#Launchjobs
#LaunchEngineering
About NTT DATA Services
NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
Show more
Show less","HTTP, FTP, Telnet, IPSec, IPv6, SSL, SSH, DNS, NTP, Network routers, Multiplexers, Switches, Shell scripting, Service identification, Access control, Intrusion detection, Prevention systems, Malicious code defense, File integrity, Enterprise security management, Firewalls, Cloud technologies, IT Security configuration, IT Security tools, IT Security techniques, Security data, IT Security Solutions, Implementation plans, Cloud email migration, Information Technology Security Engineer, Federal government, IT system designs, Technical threats, Vulnerabilities, Security risk, Networking, Telecommunications","http, ftp, telnet, ipsec, ipv6, ssl, ssh, dns, ntp, network routers, multiplexers, switches, shell scripting, service identification, access control, intrusion detection, prevention systems, malicious code defense, file integrity, enterprise security management, firewalls, cloud technologies, it security configuration, it security tools, it security techniques, security data, it security solutions, implementation plans, cloud email migration, information technology security engineer, federal government, it system designs, technical threats, vulnerabilities, security risk, networking, telecommunications","access control, cloud email migration, cloud technologies, dns, enterprise security management, federal government, file integrity, firewalls, ftp, http, implementation plans, information technology security engineer, intrusion detection, ipsec, ipv6, it security configuration, it security solutions, it security techniques, it security tools, it system designs, malicious code defense, multiplexers, network routers, networking, ntp, prevention systems, security data, security risk, service identification, shell scripting, ssh, ssl, switches, technical threats, telecommunications, telnet, vulnerabilities"
Co-op Technical Marketing Data Analyst,Renesas Electronics,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/co-op-technical-marketing-data-analyst-at-renesas-electronics-3781730664,2023-12-17,Ottawa, Canada,Mid senior,Onsite,"Job Type
: Temporary - Full Time
Travel Required
: 0%
Remote Work Available
: Yes
Renesas is one of the top global semiconductor companies in the world. We strive to develop a safer, healthier, greener, and smarter world, and our goal is to make every endpoint intelligent by offering product solutions in the automotive, industrial, infrastructure and IoT markets. Our robust product portolio includes world-leading MCUs, SoCs, analog and power products, plus Winning Combination solutions that curate these complementary products. We are a key supplier to the world’s leading manufacturers of the electronics you rely on every day; you may not see our products, but they are all around you.
Renesas employs roughly 21,000 people in more than 30 countries worldwide. As a global team, our employees actively embody the Renesas Culture, our guiding principles based on five key elements: Transparent, Agile, Global, Innovative, and Entrepreneurial. Renesas believes in, and has a commitment to, diversity and inclusion, with initiatives and a leadership team dedicated to its resources and values. At Renesas, we want to build a sustainable future where technology helps make our lives easier. Join us and build your future by being part of what’s next in electronics and the world.
Job Description
Renesas is a market leader in timing products. We are seeking a motivated and analytical individual to join our product line team. This entry-level position is designed for students or recent graduates who have a strong interest in marketing analytics and data-driven decision-making. You will play a crucial role in supporting our marketing efforts by extracting actionable insights from data and contributing to strategic business decision. During this process, you will also learn about the hardware industry development.
What You'll Learn –
Top level view of hardware industry development from initial concept all the way to end of life.
Semiconductor product development and real-world analog design trade-offs.
In-depth understanding of timing products and next generation phase lock loop (PLL) products.
Market intelligence of the industry trends in the world of semiconductor electronics.
How to work with engineering team on trade-offs to achieve a realizable technical specification and system solution.
Day-day business operations for semiconductor business at scale.
What You'll Do –
Develop scripts to automate financial report generation.
Create and maintain BI dashboards and reports to track marketing performance.
Lead in data cleaning, transformation, and integration processes to maintain data quality and ensure the accuracy and completeness of data.
Collaborate with IT and other teams to optimize data flow and collection for marketing purposes.
Track and respond to day-day sample, market opportunities and revenue status.
Collaborate with production team on each customer priorities, developments, and shipments.
Participate and listen in feature/schedule roadmap discussions.
Collaborate with corporate marketing team on product landing pages, sell-sheet, blogs and other form of sales enablement tools.
Who You Are –
Bachelor’s or Master’s degree in a relevant field (e.g., Data Science, Data Analytic, Statistics, Mathematics, Electrical Engineering , Computer Science).
0-3 years of tactical marketing role, preferably in semiconductor industry
Expert in data analysis tools and languages such as Python, Excel, SQL, or R
Proficient with data visualization tools (e.g., Tableau, Power BI).
Strong analytical and problem-solving skills with a keen attention to detail.
Excellent communication skills and the ability to translate technical findings into actionable insights.
Eagerness to learn and adapt to new technologies and methodologies.
Renesas Electronics Canada is an Equal Opportunity employer. All qualified applicants will receive consideration for employment regardless of race, national or ethnic origin, colour, creed or religion, age, sex (including gender and pregnancy), sexual orientation, marital or civil status, same-sex partnership status, family status, disability and any other characteristic protected by law. Renesas is committed to providing accommodations for people with disabilities. If you require an accommodation, we will work with you to meet your needs.
Show more
Show less","Data Analysis, Data Visualization, Data Cleaning, Data Transformation, Data Integration, Data Management, Datadriven DecisionMaking, Financial Report Generation, Business Intelligence (BI) Dashboards, Python, Excel, SQL, R, Tableau, Power BI, Statistics, Mathematics, Electrical Engineering, Computer Science, Semiconductor Industry, Marketing Analytics, Marketing Performance","data analysis, data visualization, data cleaning, data transformation, data integration, data management, datadriven decisionmaking, financial report generation, business intelligence bi dashboards, python, excel, sql, r, tableau, power bi, statistics, mathematics, electrical engineering, computer science, semiconductor industry, marketing analytics, marketing performance","business intelligence bi dashboards, computer science, data cleaning, data integration, data management, data transformation, dataanalytics, datadriven decisionmaking, electrical engineering, excel, financial report generation, marketing analytics, marketing performance, mathematics, powerbi, python, r, semiconductor industry, sql, statistics, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Louisvuitton,"Gatineau, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-louisvuitton-3751468511,2023-12-17,Ottawa, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, Data visualization, SQL, R, Python, Tableau, Power BI, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL processes","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Power BI/Data Analyst,Synechron,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/power-bi-data-analyst-at-synechron-3766097855,2023-12-17,Cap-de-la-Madeleine, Canada,Mid senior,Hybrid,"We are
At Synechron, we believe in the power of digital to transform businesses for the better. Our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. Synechron’s progressive technologies and optimization strategies span end-to-end Artificial Intelligence, Consulting, Digital, Cloud & DevOps, Data, and Software Engineering, servicing an array of noteworthy financial services and technology firms. Through research and development initiatives in our FinLabs we develop solutions for modernization, from Artificial Intelligence and Blockchain to Data Science models, Digital Underwriting, mobile-first applications and more. Over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. With top clients to boast about, Synechron has a global workforce of 14,500+, and has 44 offices in 19 countries within key global markets.
Our challenge
We are seeking a skilled and experienced Power BI/Data Analyst to join our dynamic team. The ideal candidate will have a strong background in data analysis and visualization, with a focus on utilizing Power BI to deliver actionable insights to drive business decisions. Should have experience in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX.
The Role: Power BI Developer
Responsibilities:
·To understand business requirements in the BI context and design data models to convert raw data to meaningful insights.
· To create dashboards and visual interactive reports using Power BI.
· To identify KPIs with clear objectives and monitor them consistently.
· To analyze data and present it through reports that can help in decision-making.
· To be able to convert business requirements into technical specifications and decide the timeline to accomplish tasks.
· To design, develop, and deploy Power BI scripts and perform efficient detailed analysis.
· To perform DAX queries and functions in Power BI.
·To create charts and document data with algorithms, parameters, models, and relations explanations.
· To conduct data warehouse development (if required).
· To perform SQL querying for best results.
· To use filters and graphs for a better understanding of the data.
· To define and design new systems by analyzing current ETL processes.
· To make technical changes to existing BI systems in order to enhance their working.
Requirements:
You are:
· Understand business requirements to set functional and technical specifications
· Build automated reports and dashboards with the help of Power BI and other reporting tools
·Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX
· Be able to quickly shape data into reporting and analytics solutions
· Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more
· Years of experience: 8 years
· The candidate must be a graduate in Engineering or Computer Science
It would be great if you also had:
•Detail-oriented, ability to turn deliverables around quickly with a high degree of accuracy
•Strong analytical skills, ability to interpret business requirements and produce functional and technical design documents
•Good time management skills – Ability to prioritize and multi-task, handling multiple efforts at once
• Strong desire to understand and learn domain.
• Experience in a financial services/banking industry
• Ability to work in a fast paced environment; to be flexible and learn quickly.
• Ability to multi-task with attention to detail/ prioritize tasks.
We can offer you:
· A highly competitive compensation and benefits package
· A multinational organization with 44 offices in 19 countries and the possibility to work abroad
· Laptop and a mobile phone
· 15 days of paid annual leave (plus national holidays)
· Maternity & Paternity leave plans
· A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability
·RRSP with employer’s contribution
· A higher education certification policy
· Comprehensive Relocation Expense Coverage
. Commuter benefits
· Extensive training opportunities, focused on skills, substantive knowledge, and personal development
· On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
· Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
· Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
· A flat and approachable organization
· An excellent working atmosphere: regular drinks, sports activities, offsite weekends with a young, dynamic team
· A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Show more
Show less","Power BI, Data Analysis, Visualization, Microsoft SQL Server, SSRS, TSQL, Power Query, MDX, DAX, Business Intelligence, Data Modeling, Dashboards, Reports, KPIs, ETL, Data Warehousing, SQL, Data Mining, Algorithms, Parameters, Models, Relations, Database Design, Multidimensional Database Design, Relational Database Design, Engineering, Computer Science, Accuracy, Analytical Skills, Multitasking, Time Management, Domain Knowledge, Financial Services, Banking, FastPaced Environment, Flexibility, Attention to Detail, Udemy, Coaching, FinLabs, Center of Excellence, Diversity, Inclusion, Equality, Affirmative Action, Global Workforce, Mentoring, Learning and Development","power bi, data analysis, visualization, microsoft sql server, ssrs, tsql, power query, mdx, dax, business intelligence, data modeling, dashboards, reports, kpis, etl, data warehousing, sql, data mining, algorithms, parameters, models, relations, database design, multidimensional database design, relational database design, engineering, computer science, accuracy, analytical skills, multitasking, time management, domain knowledge, financial services, banking, fastpaced environment, flexibility, attention to detail, udemy, coaching, finlabs, center of excellence, diversity, inclusion, equality, affirmative action, global workforce, mentoring, learning and development","accuracy, affirmative action, algorithms, analytical skills, attention to detail, banking, business intelligence, center of excellence, coaching, computer science, dashboard, data mining, dataanalytics, database design, datamodeling, datawarehouse, dax, diversity, domain knowledge, engineering, equality, etl, fastpaced environment, financial services, finlabs, flexibility, global workforce, inclusion, kpis, learning and development, mdx, mentoring, microsoft sql server, models, multidimensional database design, multitasking, parameters, power query, powerbi, relational database design, relations, reports, sql, ssrs, time management, tsql, udemy, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392228,2023-12-17,Truro,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830436,2023-12-17,Truro,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data Management tools, Data Classification","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification","airflow, business intelligence, data classification, data engineering, data management tools, data science, data warehouses, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Ithaca, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830432,2023-12-17,Ithaca,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, SQL, Data Warehouses, ETL, Data Science, Data Governance, Data Engineering, TDD, Pair Programming, Continuous Integration, Continuous Delivery, Automated Testing, Software Design, Software Maintenance, Data Security, High Performance Computing, Data Analytics","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, sql, data warehouses, etl, data science, data governance, data engineering, tdd, pair programming, continuous integration, continuous delivery, automated testing, software design, software maintenance, data security, high performance computing, data analytics","airflow, automated testing, continuous delivery, continuous integration, data engineering, data governance, data science, data security, data warehouses, dataanalytics, docker, etl, helm, high performance computing, kafka, kubernetes, pair programming, python, snowflake, software design, software maintenance, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Ithaca, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744390703,2023-12-17,Ithaca,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Ithaca, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707750,2023-12-17,Ithaca,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, Machine Learning, Artificial Intelligence, Data Science, Python, Java, bash, SQL, Airflow, KubeFlow, Snowflake, Docker, Helm, Spark, Kafka, Storm, SparkStreaming","data engineer, machine learning, artificial intelligence, data science, python, java, bash, sql, airflow, kubeflow, snowflake, docker, helm, spark, kafka, storm, sparkstreaming","airflow, artificial intelligence, bash, data science, dataengineering, docker, helm, java, kafka, kubeflow, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Ithaca, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773092082,2023-12-17,Ithaca,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Natural language processing, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Data classification, Data retention, LGBTQ, 401K, Bonus, Equity, Genderaffirming offerings, Included Health, HRT, Flexible vacation, Cell phone stipend, Internet stipend, Wellness stipend, Food stipend, Homeoffice setup stipend","data engineering, machine learning, natural language processing, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, lgbtq, 401k, bonus, equity, genderaffirming offerings, included health, hrt, flexible vacation, cell phone stipend, internet stipend, wellness stipend, food stipend, homeoffice setup stipend","401k, airflow, applied machine learning, aws, azure, bash, bonus, cell phone stipend, data classification, data engineering, data retention, docker, dynamodb, equity, etl, flexible vacation, food stipend, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, hrt, included health, internet stipend, java, kafka, kubernetes, lgbtq, machine learning, natural language processing, nosql, python, snowflake, spark, sparkstreaming, sql, storm, wellness stipend"
Data Engineering Consultant,Nigel Frank International,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3740005230,2023-12-17,Oxford, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Azure Data Factory, Synapse, Azure Data Lake, ETL, SQL, Python, Databricks, Data ingestion, Data transformation, Communication, Data engineering, Machine learning, AI, Data science","azure, azure data factory, synapse, azure data lake, etl, sql, python, databricks, data ingestion, data transformation, communication, data engineering, machine learning, ai, data science","ai, azure, azure data factory, azure data lake, communication, data engineering, data ingestion, data science, data transformation, databricks, etl, machine learning, python, sql, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728584222,2023-12-17,Oxford, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Apache Spark, Python, PySpark, Azure Data Factory, Azure","databricks, sql, apache spark, python, pyspark, azure data factory, azure","apache spark, azure, azure data factory, databricks, python, spark, sql"
Principal Data Engineer (Viator),Tripadvisor,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-viator-at-tripadvisor-3786855443,2023-12-17,Oxford, United Kingdom,Mid senior,Hybrid,"At Viator, a leading travel platform, we believe that the most memorable experiences are made possible through data-driven insights. As a Principal Data Engineer on the Data Platform team, you will be at the forefront of creating innovative solutions that enable our company to harness the power of data to deliver personalized and unforgettable travel experiences to our customers. Join us in shaping the future of travel through cutting-edge data engineering and advanced analytics!
Role Overview:
As a Principal Data Engineer on the Viator Data Platform team, you will be responsible for designing, building, and maintaining the core data infrastructure and architecture that supports data-driven decision-making across our organization. You will collaborate closely with data analysts, software engineers, and other stakeholders to ensure that data is effectively and efficiently collected, processed, and made accessible for analytical purposes. You will play a crucial role in guiding the team in adopting best practices, exploring new technologies, and driving innovation in data engineering.
Key Responsibilities:
Data Pipeline Architecture: Design and develop scalable, reliable, and efficient data pipelines that facilitate the collection, ingestion, and processing of diverse data from various sources, ensuring high data quality and availability.
Data Modeling and Warehousing: Create and optimize data models that enable seamless integration of data from multiple sources into a centralized data warehouse or data lake, enabling data access and analysis for different business units.
Data Governance and Security: Establish data governance frameworks and implement robust data security measures to ensure compliance with privacy regulations and protect sensitive information.
Performance Optimization: Continuously monitor and fine-tune the data platform's performance to meet SLAs and optimize resource utilization, ensuring smooth and fast data access for analytical needs.
Technology Evaluation: Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering. Assess and recommend new tools and frameworks to improve data processing capabilities.
Team Leadership and Mentorship: Provide technical leadership and mentorship to a team of data engineers, fostering a collaborative and innovative work environment.
Cross-Functional Collaboration: Collaborate with data analysts, data scientists, and software engineers to understand their data requirements, offer data engineering support, and contribute to the development of data-driven products and solutions.
Data Quality and Monitoring: Implement robust data quality checks and monitoring systems to ensure the accuracy, consistency, and reliability of data.
Documentation and Knowledge Sharing: Create and maintain comprehensive documentation of data engineering processes, best practices, and technical standards. Facilitate knowledge sharing sessions with the broader team.
Qualifications and Experience:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Minimum of 8 years of hands-on experience in data engineering, with a focus on building large-scale data platforms and data processing pipelines.
Proficiency in programming languages such as Python, Java, or Scala, and experience with big data technologies like Hadoop, Spark, and Kafka.
Strong experience with cloud-based data platforms, such as AWS, GCP, or Azure, including services like S3, MSK, ECS, or equivalent.
In-depth knowledge of data modeling, data warehousing, and ETL/ELT processes.
Familiarity with data governance and security best practices and compliance standards.
Proven experience in performance optimization and tuning of data pipelines and database queries.
Excellent leadership and communication skills, with the ability to collaborate effectively with cross-functional teams.
Prior experience in mentoring and guiding junior data engineers is a plus.
Experience in the travel industry or related domains is desirable but not mandatory.
Join the Viator Data Platform team and be part of an exciting journey to revolutionize the travel industry with data-driven insights and personalized experiences!
#Viator
Show more
Show less","Data Pipeline Architecture, Data Modeling, Data Warehousing, Data Governance, Data Security, Performance Optimization, Technology Evaluation, Team Leadership, Mentorship, CrossFunctional Collaboration, Data Quality, Monitoring, Documentation, Knowledge Sharing, Python, Java, Scala, Hadoop, Spark, Kafka, AWS, GCP, Azure, S3, MSK, ECS, ETL, ELT, Data Governance, Security Standards, Performance Optimization, Tuning, Databases, Leadership, Communication, Collaboration, Mentoring, Travel Industry","data pipeline architecture, data modeling, data warehousing, data governance, data security, performance optimization, technology evaluation, team leadership, mentorship, crossfunctional collaboration, data quality, monitoring, documentation, knowledge sharing, python, java, scala, hadoop, spark, kafka, aws, gcp, azure, s3, msk, ecs, etl, elt, data governance, security standards, performance optimization, tuning, databases, leadership, communication, collaboration, mentoring, travel industry","aws, azure, collaboration, communication, crossfunctional collaboration, data governance, data pipeline architecture, data quality, data security, databases, datamodeling, datawarehouse, documentation, ecs, elt, etl, gcp, hadoop, java, kafka, knowledge sharing, leadership, mentoring, mentorship, monitoring, msk, performance optimization, python, s3, scala, security standards, spark, team leadership, technology evaluation, travel industry, tuning"
Principal Data Engineer (Viator),Tripadvisor,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-viator-at-tripadvisor-3784637663,2023-12-17,Oxford, United Kingdom,Mid senior,Hybrid,"Viator is a Tripadvisor company that makes it easy to find and book something you'll love to do. With an industry leading selection of high-quality experiences, Viator gives millions of travellers a month something new to discover, both near and far from home.
We believe that the most memorable experiences are made possible through data-driven insights. As a Principal Data Engineer on the Martech Engineering team, you will be at the forefront of creating innovative solutions that enable our company to harness the power of data to deliver personalized and unforgettable travel experiences to our customers. Join us in shaping the future of travel through cutting-edge data engineering and advanced analytics!
As a Principal Data Engineer on the Viator Martech Engineering team, you will be responsible for designing, building and maintaining the core Martech platform and architecture that supports data-driven decision-making across our digital marketing space. You will collaborate closely with product managers, technical managers, software engineers, and other stakeholders to ensure that data is effectively and efficiently collected, processed, and made usable for marketing purposes. You will play a crucial role in guiding the team in adopting best practices, exploring new technologies, and driving innovation in Martech data engineering. This role is a hybrid position based in either our Oxford or London offices.
What you'll do:
Work across multiple pods and squads to align and set up a marketing platform architecture supporting the digital marketing ecosystem.
Take full ownership of architecture and design for the Martech engineering stack.
Identify gaps in our current designs and lead redesigns of the platform to improve developer velocity and Viator growth.
Refine and govern our logical and physical data models across the core data warehouse and the Customer Data platforms.
Work with cross-functional stakeholders in defining and documenting requirements for building high quality and impactful data products.
Lead the evaluation, implementation and deployment of emerging tools and technologies to improve our productivity as a team and as a business.
Develop and deliver communication and education plans on Martech data/software engineering capabilities, standards, and processes.
Be the go-to-person for developers on complex problems for specific engineering solutions.
Code, test, and document new or modified data systems to create robust and scalable applications for reporting and data analytics.
What you'll need:
Strong experience around building scalable and distributed software and data systems.
Strong experience of data and software architecture/designs, especially real time distributed data streaming designs.
Fair understanding of the Martech ecosystem, primarily around the Customer Data Platform/Customer Relationship management and paid marketing space.
5+ years of Data Engineering experience focused on building modern day data pipelines catering to both batch and real time data needs.
Strong Python, Java or Scala Experience.
Strong experience in SQL; can write complex, performant, fail-safe queries.
Experience with Restful APIs.
Experience with CI/CD processes and platforms.
Experienced with setting up end to end data platforms in an enterprise environment.
Hands-on with Cloud computing technology like GCP, AWS, etc.
Ability to effectively operate both independently and as part of a team.
Self-motivated with strong problem-solving and self-learning skills.
What would be nice to have:
Exposure to CDP/ CRM tools like Braze, Segment.
BigQuery or Google Cloud Experience.
Understanding of link tracking, pixels/tags, campaign orchestration across multi-channel setup, Google Analytics, SEM/SEO ecosystem, marketing attribution, etc.
Exposure to DBT.
Exposure to Apache Airflow or other DAG frameworks is nice to have.
Exposure to Looker, Tableau, or similar visualisation / business intelligence platform.
Expertise designing and implementing data pipelines using modern data engineering approach and tools: Spark, PySpark, Java, Docker, cloud native DWH (Snowflake, Redshift), Kafka/Confluence etc.
What you'll get:
Highly competitive salary along with the following:
Annual performance related bonus
Generous stock (RSU) award upon joining, with additional awards annually
Regular salary reviews and excellent career growth opportunities
Very flexible working hours
Free meals in the office
Full family private healthcare and dental
Excellent contributory pension
Critical illness and full life cover
Standard 28 days paid holiday (not including bank holidays), increasing with tenure
£1050 annual stipend for costs outside the office such as gym membership or home office set up
“Summer Fridays” scheme allowing extra days off during the summer
Application process
30 minute call with a recruiter to learn more about the role
30 minute technical/functional interview with the hiring manager
Two one-hour interviews with members of the team, covering technical topics - including some coding - and what you would bring to Viator
#Viator
Show more
Show less","Data Engineering, Martech Engineering, Data Architecture, Data Analysis, Data Warehousing, Python, Java, Scala, SQL, Restful APIs, CI/CD, Cloud Computing, GCP, AWS, Braze, Segment, BigQuery, Google Cloud, Link Tracking, Pixels/Tags, Campaign Orchestration, Google Analytics, SEM/SEO, Marketing Attribution, DBT, Apache Airflow, DAG Frameworks, Looker, Tableau, Spark, PySpark, Docker, Cloud Native DWH, Snowflake, Redshift, Kafka, Confluence","data engineering, martech engineering, data architecture, data analysis, data warehousing, python, java, scala, sql, restful apis, cicd, cloud computing, gcp, aws, braze, segment, bigquery, google cloud, link tracking, pixelstags, campaign orchestration, google analytics, semseo, marketing attribution, dbt, apache airflow, dag frameworks, looker, tableau, spark, pyspark, docker, cloud native dwh, snowflake, redshift, kafka, confluence","apache airflow, aws, bigquery, braze, campaign orchestration, cicd, cloud computing, cloud native dwh, confluence, dag frameworks, data architecture, data engineering, dataanalytics, datawarehouse, dbt, docker, gcp, google analytics, google cloud, java, kafka, link tracking, looker, marketing attribution, martech engineering, pixelstags, python, redshift, restful apis, scala, segment, semseo, snowflake, spark, sql, tableau"
Senior Data Engineer (Viator),Viator,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-viator-at-viator-3783820482,2023-12-17,Oxford, United Kingdom,Mid senior,Hybrid,"Viator is a Tripadvisor company that makes it easy to find and book something you'll love to do. With an industry leading selection of high-quality experiences, Viator gives millions of travellers a month something new to discover, both near and far from home.
We believe that we are better together, and at Viator we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Viator, we want you to bring your unique identities, abilities and experiences, so we can collectively revolutionise travel and together find the good out there.
Viator’s MarTech and Customer Data Platform Engineering & Operations team creates and maintains the tooling used to bring millions of visitors to viator.com every month. The team owns all Marketing data from end-to-end, including data ingestion, transformation, visualization, and data architecture to ensure best-in-class operational reliability and modularity. The team is looking for a new Data Engineer to support Viator’s growing traffic and deliver start-of-the-art MarTech solutions. This is a hybrid role is based in our Oxford, London or Lisbon office.
What You'll Do
Implement modern, data pipelines for marketing data.
Monitor data pipelines for accuracy, missing data, enhancements, changes, SLA, and billing volumes to ensure all data is captured and processed accurately and when needed.
Work with cross-functional stakeholders in defining and documenting requirements for building high quality and impactful data products.
Reconcile data issues and alerts between various systems, finding opportunities to innovate and drive improvements.
Lead the evaluation, implementation and deployment of emerging tools and technologies to improve our productivity as a team.
Develop and deliver communication and education plans on data engineering capabilities, standards, and processes.
Code, test, and document new or modified data systems to create robust and scalable applications for reporting and data analytics.
Own and document data pipelines, monitoring, data accuracy, SLAs and data lineage.
What You'll Need
Bachelor of Science degree in Computer Science, Information Systems, or related field.
5+ years of Data Engineering experience focused on Data Modeling, Data Architecture and Developing cloud-based, modern data pipelines and applications for reporting and analytics.
Strong communication skills.
Strong understanding of data modelling concepts, e.g. normal forms, keys.
Strong experience in SQL;
Experience with advanced SQL features e.g. dynamic SQL, Time Travel, Schema Metadata.
Can work effectively with complex queries.
Experience working with Restful APIs.
Hands-on experience with cloud computing technologies (e.g. Google cloud storage, Amazon S3, Athena, Google Bigquery , etc.)
Ability to effectively operate both independently and as part of a team.
Ability to effectively operate in a fast paced environment.
Self-motivated with strong problem-solving and self-learning skills.
Bonus
MarTech Knowledge is a plus.
Experience in working with marketing data is a huge plus.
Understanding of link tracking, pixels/tags, Google Analytics, and marketing attribution.
Hands-on experience with Media Mix Modelling.
Strong Java Experience.
Hands on BigQuery Experience.
Exposure to DBT or DataForm.
Experience operating data pipeline infrastructure, e.g. to Apache Airflow or other DAG frameworks is nice to have.
Experience with CI/CD processes and platforms.
E-commerce, or startup experience is a plus.
What You’ll Get
Highly competitive salary along with the following
Annual performance related bonus
Generous stock (RSU) award upon joining, with additional awards annually
Regular salary reviews and excellent career growth opportunities
Very flexible working hours
Free meals in the office
Full family private healthcare and dental
Excellent contributory pension
Critical illness and full life cover
Standard 28 days paid holiday (not including bank holidays), increasing with tenure
£1050 annual stipend for costs outside the office such as gym membership or home office set up
“Summer Fridays” scheme allowing extra days off during the summer
Application process
30 minute call with a recruiter to learn more about the role
30 minute technical interview with someone from the Viator Engineering team
Four one-hour interviews with members of the team, covering technical topics - including some coding - and what you would bring to Viator
#Viator
Show more
Show less","Data Engineering, Data Modeling, Data Architecture, Cloud Computing, SQL, Restful APIs, Java, BigQuery, DBT, DataForm, Apache Airflow, CI/CD, Ecommerce, Startup, MarTech Knowledge, Google Analytics, Media Mix Modelling","data engineering, data modeling, data architecture, cloud computing, sql, restful apis, java, bigquery, dbt, dataform, apache airflow, cicd, ecommerce, startup, martech knowledge, google analytics, media mix modelling","apache airflow, bigquery, cicd, cloud computing, data architecture, data engineering, dataform, datamodeling, dbt, ecommerce, google analytics, java, martech knowledge, media mix modelling, restful apis, sql, startup"
Healthcare Data Analyst (CMH Health),Milliman,"Brookfield, CT",https://www.linkedin.com/jobs/view/healthcare-data-analyst-cmh-health-at-milliman-3780777746,2023-12-17,Ansonia,United States,Mid senior,Onsite,"Milliman's health IT consulting group is looking for driven, entrepreneurial, and self-learning data analysts to join their team. The ideal candidate will be passionate about providing the best healthcare consulting services to our clients while making meaningful contributions to projects and the firm. Our analysts are independent problem solvers who care about the quality of the work that is delivered to clients and to each other.
Who We Are
Independent for 75 years, Milliman delivers market-leading services and solutions to clients worldwide. Today, we are helping companies take on some of the world’s most critical and complex issues, including retirement funding and healthcare financing, risk management and regulatory compliance, data analytics and business transformation.
Job Responsibilities
Participate in analytics and technology consulting projects, engagements, and research efforts
Communicate and coordinate amongst Milliman Consultants, project staff, and our clients to meet milestones and timelines
Contribute to technical deliverables using analytics software (for example, SQL, SAS, Python, Excel, etc.)
Take ownership of project deadlines and deliverables
Design and develop a modeling approach to actuarial problems by utilizing large health claim datasets
Oversee and/or perform Extract, Transform, Load (ETL) tasks on healthcare data, including intake, manipulation, and quality assessment of enrollment, medical claims, and pharmacy claims
Develop analytical and project roadmaps, planning out the “data to deliverable” runway and identifying potential roadblocks ahead of time
Participate in client meetings, including occasional travel to client sites
Minimum Requirements
Bachelor’s degree required (Computer Science, Management Information Systems, Computer Engineering, Data Analytics, Math, or related degree is preferred)
0-2 years of health data analytics or similar experience
Basic proficiency in SQL, Python or PowerBI
Able to interpret, scrub, process, and explain data
Proficiency with Microsoft Office Suite, especially Excel
Competencies and Behaviors that Support Success in this Role
Proficiency in database architecture, healthcare claims data, and information technology
Knowledge and experience in the CMS VRDC environment
Strong IT background
Analytical thinker, able to assess problems logically
Strong communication skills and experience working with a team
Detailed and organized
Fast learner
Passionate about providing the best healthcare consulting services to our clients
Results oriented leader that thrives on delivering high quality products and services to our end users
Determined to solve challenging technical problems with an eye towards quality results
Take personal responsibility for their work and are motivated to succeed
The Team
The Healthcare Data Analyst will join a team that thrives on leveraging data, analytics, and technology to deliver meaningful business value. This is a team with technical aptitude and analytical prowess that enjoys building efficient and scalable products and processes. Ultimately, we are passionate about effecting change in healthcare. We also believe that collaboration and communication are cornerstones of success.
The Healthcare Data Analyst will also join a mix of Healthcare Analysts, Leads, Consultants, and Principals. In addition, as part of the broader Milliman landscape, they will work alongside Healthcare Actuaries, Pharmacists, Clinicians, and Physicians. We aim to provide everyone a supportive environment, where we foster learning and growth through rewarding challenges.
Salary
: The salary range is $­­­­­46,000 to $106,000, depending on a combination of factors, including but not limited to education, relevant work experience, qualifications, skills, certifications, location, etc.
If overall experience is less than 2 years the range would be $46,000 to $87,000; for experience of greater than 2 years, the range would be $54,000 to $106,000.
Location
:
It is preferred that candidates work on-site at our Chicago, IL, Milwaukee, WI, or Hartford, CT office. Remote candidates will be considered.
Benefits
At Milliman, we focus on creating an environment that recognizes – and meets – the personal and professional needs of the individual and their family. We offer competitive benefits which include the following based on plan eligibility:
Medical, dental and vision coverage for employees and their dependents, including domestic partners
A 401(k) plan with matching program, and profit sharing contribution
Employee Assistance Program (EAP)
A discretionary bonus program
Paid Time Off (PTO) starts accruing on the first day of work and can be used for any reason; full-time employees will accrue 15 days of PTO per year, and employees working less than a full-time schedule will accrue PTO at a prorated amount based on hours worked
Family building benefits, including adoption and fertility assistance and paid parental leave up to 12 weeks for employees who have worked for Milliman for at least 12 months and have worked at least 1,250 hours in the preceding 12-month period
A minimum of 8 paid holidays
Milliman covers 100% of the premiums for life insurance, AD&D, and both short-term and long-term disability coverage
Flexible spending accounts allow employees to set aside pre-tax dollars to pay for dependent care, transportation, and applicable medical needs
All qualified applicants will receive consideration for employment, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Show more
Show less","Health IT, Data analytics, SQL, SAS, Python, Excel, PowerBI, Database architecture, Healthcare claims data, CMS VRDC environment, Microsoft Office Suite","health it, data analytics, sql, sas, python, excel, powerbi, database architecture, healthcare claims data, cms vrdc environment, microsoft office suite","cms vrdc environment, dataanalytics, database architecture, excel, health it, healthcare claims data, microsoft office suite, powerbi, python, sas, sql"
Staff Data Engineer,Recruiting from Scratch,"Fairfield, CT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395250,2023-12-17,Ansonia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Movement Engineer,Experis,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-movement-engineer-at-experis-3785895443,2023-12-17,Ansonia,United States,Mid senior,Onsite,"Job information
Located in New Haven, CT
Temp to hire (1/2 month contract then perm hire)
Pay range: $60 - $67/hr.
Key Job Responsibilities
Experience in Data Engineering in AWS/Azure Cloud
Experience with API data integration
Experience with Talend or other data movement tools Informatica, Azure Data Factory
Snowflake or other relational database experience such as Oracle or SQL Server
CI/CD Experience either using Git or Azure Dev Ops
Team Lead responsibility Looking for the resource to become the overall Data Engineering lead
Overview
Job Description
The Knights of Columbus is embarking on the modernization of its core data platforms. We are currently seeking a Senior Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will possess experience in the data management domain, and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes.
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Essential Competencies
Professionalism/Personal Accountability, Collaboration and Teamwork, Communication, Flexible and Adapts to Change, Service to Customers and Clients
Required:
Skill Qualifications:
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Preferred:
Some experience with cloud database technologies preferred
Some experience with data visualization tools (e.g. Tableau) desirable
Required:
Education and Experience Qualifications
BA or BS in Computer Science, Information Systems or related field.
10+ years experience
Physical Demands
Must be able to remain in a stationary position for a majority of the workday.
About ManpowerGroup, Parent Company of:
Manpower, Experis, Talent Solutions, and Jefferson Wells
ManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –
Manpower, Experis, Talent Solutions, and Jefferson Wells
–
creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent.
Show more
Show less","AWS, Azure, API, Talend, Informatica, Azure Data Factory, Snowflake, Oracle, SQL Server, Git, Azure Dev Ops, ETL, ELT, Data Warehousing, SQL, Dimensional, Star Schema, AQT, MS Query, Tableau, Cloud Database, Data Visualization, Computer Science, Information Systems","aws, azure, api, talend, informatica, azure data factory, snowflake, oracle, sql server, git, azure dev ops, etl, elt, data warehousing, sql, dimensional, star schema, aqt, ms query, tableau, cloud database, data visualization, computer science, information systems","api, aqt, aws, azure, azure data factory, azure dev ops, cloud database, computer science, datawarehouse, dimensional, elt, etl, git, informatica, information systems, ms query, oracle, snowflake, sql, sql server, star schema, tableau, talend, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833029,2023-12-17,Ansonia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Big Data Technologies, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, Streamprocessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Data Management Tools, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, big data technologies, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data management tools, data classification, data retention","agile engineering practices, airflow, automated testing, automation, big data technologies, continuous delivery, continuous integration, data classification, data engineering, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, pair programming, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744387780,2023-12-17,Ansonia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Data Warehouses, ETL Pipelines, Legal Compliance, TDD, Pair Programming, Continuous Integration, Automated Testing, Dimensional Data Modeling, Schema Design","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, data warehouses, etl pipelines, legal compliance, tdd, pair programming, continuous integration, automated testing, dimensional data modeling, schema design","airflow, automated testing, continuous integration, data warehouses, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Fairfield, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744391850,2023-12-17,Ansonia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention","data engineering, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","airflow, automated testing, continuous integration, data classification, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Engineer,ClickJobs.io,"Cheshire, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-clickjobs-io-3781329336,2023-12-17,Ansonia,United States,Mid senior,Onsite,"Salary 35,000 - 65,000 GBP per year
Requirements:
Proven experience with Cloud using AWS or similar.
Strong Java Development experience
Dynamodb
Docker (containerisation)
Terraform
AWS Lambda
AWS S3
Python
Redis / Elasticache
Agile delivery / methodologies
Elastic Stack
Strong analytical and problem-solving skills.
Responsibilities:
We are looking for a Senior Data Engineer to join our fast-paced Data Services area. Acting as a Technical Lead within the multi-functional, agile based squad you'll be developing some of our new sophisticated solutions for deployment in a cloud-based environment. We are creating an environment and culture of continual learning and we are looking for a Senior Data Engineer who has some experience of coaching and mentoring other members of the team.
Taking technical ownership, you will maintain a keen awareness of industry and technology breakthroughs, ensuring we are always benefiting from best practice. As a Senior Data Engineer, you'll be comfortable talking about system design and architecture and providing mentoring to other team members.
Technologies:
AWS
Java
Docker
Terraform
Python
More:
CDL has established its position as a market leader in the highly competitive retail insurance sector by consistently creating powerful software solutions that deliver competitive and innovative advantage to our customers. We've custom-built our own modern, open offices set in our own landscaped campus in Stockport, employing over 600 staff.
As part of the Data Services team, you will work on delivering and supporting CDL's high performance platforms and solutions.
We have built a collaborative and creative culture where we employ agile techniques to deliver our pace-setting applications. We are also proud to have achieved the Top Employee 2020. We like to work in small, focused groups so you are always learning from people around you where you feel a part of the success of each project you are involved with.
CDL are currently working on a fully remote basis until we believe it is safe to return to our campus in Stockport. When we return, we plan to adopt a hybrid working model of a minimum of two days per fortnight in the office
Show more
Show less","Cloud, AWS, Java, DynamoDB, Docker, Terraform, AWS Lambda, AWS S3, Python, Redis, Elasticache, Agile, Elastic Stack, Analytical skills, Problemsolving skills","cloud, aws, java, dynamodb, docker, terraform, aws lambda, aws s3, python, redis, elasticache, agile, elastic stack, analytical skills, problemsolving skills","agile, analytical skills, aws, aws lambda, aws s3, cloud, docker, dynamodb, elastic stack, elasticache, java, problemsolving skills, python, redis, terraform"
Assistant Marketing Data Analyst,HomeServe USA,"Norwalk, CT",https://www.linkedin.com/jobs/view/assistant-marketing-data-analyst-at-homeserve-usa-3748695659,2023-12-17,Ansonia,United States,Mid senior,Onsite,"Company Overview
HomeServe USA Corp. is a Great Place to Work, and while we’re biased, we’re not just saying that. We’re proud to have been certified as a Great place to Work the last four years. What do we do and what makes it so great? Well, we’re glad you asked!
We put people at the heart of everything we do. That’s priority number one for all of us. For the 5 million Customers we serve, that means being there when they have an emergency home repair need, such as getting their a/c working, clearing their clogged pipes, or fixing broken electrical systems. With over 1,000 municipal and utility Partners, that means providing their customers with access to affordable home repair plans, making it easier, faster and less expensive to have their home repair needs met. And for the nearly 3,000 People working alongside us, it means fostering a rewarding, inclusive and challenging career experience that we think is second to none. At HomeServe USA Corp., everyone is welcome. We know that having diverse teams has a positive impact on our work and ultimately helps us better serve our customers.
Position Overview/ Responsibilities
To assist and support the Marketing Database team by carrying out data selections and analysis for direct mail, email, and outbound telemarketing campaigns.
Timely and accurate production of counts and data files for acquisition, cross-sell and retention campaigns using SAS programming.
Cleansing and standardizing partner files for use in marketing campaigns using specialist data cleansing software.
Assisting with guidance on data briefs/matrices through a consultative process with the strategy and campaign management teams.
Gathering and supporting the responses to customer and partner complaints about data use through data mining and investigations.
Supporting the data infrastructure of the Marketing Database team through table updates, database maintenance and recommending opportunities for data process improvements. –
Managing all data work with due skill, care and diligence, including effective quality controls and risk management/mitigation.
Inquires and suggests implementing ways to automate the production of data and counts for campaign activity.
Job Requirements
Bachelor’s degree (mathematics/statistics/economics/computer science or other numerical-based degree preferred) or 1-2 years’ relevant work experience.
Highly numerate and analytical
Some work experience in a commercially data driven/analysis role.
SAS, R, SQL or other programming language experience
Intermediate to Advanced Microsoft Excel skills
Strong written communication skills
Attention to detail and accuracy.
What We Are Looking For
At HomeServe USA Corp., we put our people and our customers at the heart of everything we do, and we’re looking for someone who loves the work of compliance to join our team. A compliant organization is good for everyone so join us in supporting a culture that creates great employee experiences at an organization that cares. The right people for our team (1) care about what we do, the people on our teams, and the customers we serve, (2) are open and honest when communicating, (3) engage actively in work and office life, (4) are committed to growth, learning, and improving both self and the organization, (5) take ownership and collaborate well with to get to great outcomes, (6) share in the full team’s success. We need people who challenge convention, think differently, solve problems, and strive for continuous growth.
This job description is intended to provide guidelines for job expectations and the employee's ability to perform the position described. It is not intended to be construed as an exhaustive list of all functions, responsibilities, skills and abilities. Additional functions and requirements may be assigned by supervisors as deemed appropriate.
In Return, We Offer
Competitive compensation
Career development and advancement opportunities
Friendly, open and team-oriented work atmosphere
Excellent benefits including generous medical, vision, dental and life & disability insurance.
401(k) plan with a company match
Eligibility to enroll in up to two HomeServe coverage plans paid for by the company.
The salary range for this position is $60,290 - $80,386 + 5% annual incentive bonus eligibility.
We are an equal opportunity employer.
Show more
Show less","Data Analysis, Data Mining, Data Cleansing, SAS, SQL, R, Mathematical Modeling, Statistical Analysis, Database Maintenance, Microsoft Excel, Written Communication, Attention to Detail, Data Management, Data Infrastructure, Data Process Improvement, Quality Control, Risk Management, Data Automation","data analysis, data mining, data cleansing, sas, sql, r, mathematical modeling, statistical analysis, database maintenance, microsoft excel, written communication, attention to detail, data management, data infrastructure, data process improvement, quality control, risk management, data automation","attention to detail, data automation, data infrastructure, data management, data mining, data process improvement, dataanalytics, database maintenance, datacleaning, mathematical modeling, microsoft excel, quality control, r, risk management, sas, sql, statistical analysis, written communication"
Senior Staff AI Data Engineer,Recruiting from Scratch,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090508,2023-12-17,Ansonia,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Datasets, ML Models, AIcentric Features, Data Mining, Data Cleaning, Data Normalizing, Data Modeling, Data Platforms, Data Frameworks, Data Pre/Post Processing, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Pipeline Tools, NLP, Large Language Models, Git, Python, Java, Bash, SQL, Snowflake, Kubernetes, Docker, Helm, Spark, Pyspark, AWS, GCP, Azure, NoSQL, ETL, Conversational AI, Recommender Systems, Kafka, Storm, Spark Streaming, Applied Machine Learning","data engineering, ml datasets, ml models, aicentric features, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, data prepost processing, statistical analysis, data visualization, pandas, r, airflow, kubeflow, pipeline tools, nlp, large language models, git, python, java, bash, sql, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, conversational ai, recommender systems, kafka, storm, spark streaming, applied machine learning","aicentric features, airflow, applied machine learning, aws, azure, bash, conversational ai, data cleaning, data engineering, data frameworks, data mining, data normalizing, data platforms, data prepost processing, datamodeling, docker, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, ml datasets, ml models, nlp, nosql, pandas, pipeline tools, python, r, recommender systems, snowflake, spark, spark streaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Fairfield, CT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086806,2023-12-17,Ansonia,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, ML Datasets, ML Models, AIcentric Features, Recommendations, LLMs, Ads, Visual Search, Growth/Notifications, Trust and Safety, Data Ops, ML Data Engine, Agile ML Data OPs, Data Pipeline, Data Enrichment, Data Monitoring, Data Pre/Post Processing, Data Mining, Data Cleaning, Data Normalizing, Data Modeling, Data Governance, Data Compliance, Automated Test Suites, Technical Documentation, Operational Strategy, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, NoSQL, DynamoDB, Kafka, Storm, SparkStreaming, Machine Learning, Legal Compliance, Data Classification, Data Management Tools, Data Retention","data engineer, ml datasets, ml models, aicentric features, recommendations, llms, ads, visual search, growthnotifications, trust and safety, data ops, ml data engine, agile ml data ops, data pipeline, data enrichment, data monitoring, data prepost processing, data mining, data cleaning, data normalizing, data modeling, data governance, data compliance, automated test suites, technical documentation, operational strategy, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, kafka, storm, sparkstreaming, machine learning, legal compliance, data classification, data management tools, data retention","ads, agile ml data ops, aicentric features, airflow, automated test suites, aws, azure, bash, data classification, data cleaning, data compliance, data enrichment, data governance, data management tools, data mining, data monitoring, data normalizing, data ops, data pipeline, data prepost processing, data retention, dataengineering, datamodeling, docker, dynamodb, gcp, git, growthnotifications, helm, java, kafka, kubernetes, legal compliance, llms, machine learning, ml data engine, ml datasets, ml models, nosql, operational strategy, python, recommendations, snowflake, spark, sparkstreaming, sql, storm, technical documentation, trust and safety, visual search"
Staff Data Engineer,Recruiting from Scratch,"Kennewick, WA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393544,2023-12-17,Kennewick,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, ETL, Agile Engineering, TDD, Pair Programming, Continuous Integration, Automated Testing, StreamProcessing Systems, Dimensional Data Modeling, Schema Design, Data Warehouses","data engineering, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, etl, agile engineering, tdd, pair programming, continuous integration, automated testing, streamprocessing systems, dimensional data modeling, schema design, data warehouses","agile engineering, airflow, automated testing, continuous integration, data engineering, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Kennewick, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392488,2023-12-17,Kennewick,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, data management tools, data classification, data retention","airflow, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Kennewick, WA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773085817,2023-12-17,Kennewick,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Pipelines, Data Mining, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Data Classification, Data Retention, Data Management","data engineering, machine learning, data pipelines, data mining, statistical analysis, data visualization, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, data classification, data retention, data management","airflow, aws, azure, bash, data classification, data engineering, data management, data mining, data retention, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Kennewick, WA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712032,2023-12-17,Kennewick,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Pandas, R, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, Spark Streaming, Data Mining, Cleaning, Normalizing, Modeling, NoSQL, Applied Machine Learning, Data Classification, Data Retention","data engineering, machine learning, python, java, bash, sql, git, pandas, r, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, spark streaming, data mining, cleaning, normalizing, modeling, nosql, applied machine learning, data classification, data retention","airflow, applied machine learning, aws, azure, bash, cleaning, data classification, data engineering, data mining, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, modeling, normalizing, nosql, pandas, python, r, snowflake, spark, spark streaming, sql, storm"
Junior Data Analyst Electronic Devices,Ernest Gordon Recruitment,"Llanelli, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-analyst-electronic-devices-at-ernest-gordon-recruitment-3768961537,2023-12-17,Swansea, United Kingdom,Mid senior,Onsite,"Junior Data Analyst (Electronic Devices)
£25,000 - £30,000 + Flexitime + Training + Progression + Benefits
Llanelli, Wales
Are you a Junior Data Analyst looking to progress to Digital Forensics in a role offering mentoring on highly specialised software for a prestigious company working very closely with police forces nationwide?
In this role you will be forensically analysing digital electronic devices to aid either corporate, police or private investigations. Your responsibility will be to work very closely with other technician to uncover evidence and testify findings in court or at tribunals.
With ambitious growth and plans to diversify into consulting, this is a great opportunity to play a pivotal role in this companies success. They pride themselves with being the best in the business and are the constabularies preferred supplier for digital forensic services.
This role would suit a Junior Data Analyst looking to progress to Digital Forensics and is looking for a challenging but rewarding role helping to resolve criminal investigation.
This Role
Preserving, processing and analysing data using specialist forensic tools
Producing written reports and presenting findings in court when required
Review data sets against a remit
The Person
1 year + experience as a Data Analyst
Wants to Progress Digital Forensics
Job Refence
: BBBH10737
Digital Forensics, Digital, Forensics, Analyst, Forensic Analyst, Lab, Laboratory, Technician, Expert, ISO17025, Crime, Criminal, Computer Science, Gorseinon, Swansea, Dyfed, Llanelli
Electronics, Electrical, Mechanical, Engineer, Electronic, Trainee, Junior, Apprentice, Technician, Service, Mobile, Training, Development, Printers, Birmingham, West MidlandsIf you're interested in this role, click '
apply now
' to forward an up-to-date copy of your CV.We are an equal opportunities employer and welcome applications from all suitable candidates. The salary advertised is a guideline for this position. The offered renumeration will be dependent on the extent of your experience, qualifications, and skill set.Ernest Gordon Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job, you accept the T&C's, Privacy Policy and Disclaimers which can be found at our website.
Show more
Show less","Data Analyst, Digital Forensics, Forensic Analysis, Electronic Devices, Forensics Tools, Data Reports, Data Presentation, Data Review, ISO 17025, Crime Investigation, Computer Science, Electronics, Electrical, Mechanical","data analyst, digital forensics, forensic analysis, electronic devices, forensics tools, data reports, data presentation, data review, iso 17025, crime investigation, computer science, electronics, electrical, mechanical","computer science, crime investigation, data presentation, data reports, data review, dataanalytics, digital forensics, electrical, electronic devices, electronics, forensic analysis, forensics tools, iso 17025, mechanical"
Data Analysis Specialist - Mental Health 135,Telecare Corporation,"Modesto, CA",https://www.linkedin.com/jobs/view/data-analysis-specialist-mental-health-135-at-telecare-corporation-3760900603,2023-12-17,Modesto,United States,Mid senior,Onsite,"""They made it easier for me to live, breathe, eat, and stay clean. Without them, I'd be waiting somewhere, waiting for someone to give me a chance to live..."" - Client from Telecare
What You Will Do To Change Lives
The Data Analyst is responsible for coordinating all Caminar, Avatar and Electronic Health Record (EHR) related tasks, including collection and entry of members served data (e.g. demographics, diagnosis, service codes, billing codes), running reports, ensuring accuracy of data and reporting back to the program and the payer. The position requires using outcomes data to promote program improvement with a focus on positive outcomes for the members served.
Shifts Available:
Full Time; DAYS 8:00 am - 5:00 pm; Monday - Friday
Expected starting wage range $21.94 - $27.11 - Telecare applies geographic differentials to its pay ranges. The pay range assigned to this role will be based on the geographic location from which the role is performed. Starting pay is commensurate with relevant experience above the minimum requirements.
What You Bring To The Table (Must Have)
Required:
One (1) year of database experience
Must be able to communicate effectively with staff and payer representatives
What's In It for You*
Paid Time Off: For Full Time Employee it is 16.7 days in your first year
Nine Paid Holidays & Shift differentials for hourly staff (6% for PM Shift, 10% for Overnight Shift). Weekend Shift differentials for hourly staff (5% for Weekend AM Shift, 11% for Weekend PM Shift, 15% for Weekend Overnight Shift)
Free CEUs, free Supervision for BBS Associate License, coaching, and mentorship
Online University Tuition Discount and Company Scholarships
Medical, Vision, Dental Insurance, 401K, Employee Stock Ownership Plan
For more information visit: https://www.telecarecorp.com/benefits
Join Our Compassionate Team
Telecare's mission is to deliver excellent and effective behavioral health services that engage individuals in recovering their health, hopes, and dreams. Telecare continues to advance cultural diversity, humility, equity, and inclusion at all levels of our organization by hiring mental health peers, BIPOC, LGBTQIA+, veterans, and all belief systems.
The Telecare programs operate under contract with the Stanislaus County Department of Mental Health and are funded by the Mental Health Services Act (MHSA) and Stanislaus County Behavioral Health and Recovery Services (BHRS). There are three levels of care including Assertive Community Treatment (ACT), Intensive Community Supports (ICS), and Wellness. These levels of care provide specialty mental health services to a mix of populations including adults 25+ individuals. Some individuals are experiencing co-occurring disorders, are uninsured or underinsured, and are persons representing underserved groups in Stanislaus County. Treatment provided is client-centered and will meet the clients where their needs are which can include in the field.
EOE AA M/F/V/Disability
May vary by location and position type
Full Job Description will be provided if selected for an interview.
Data Specialist, Entry Level, Medical Records, MRT
If job posting references any sign-on bonus internal applicants and applicants employed with Telecare in the previous 12 months would not be eligible.
Copyright 2022 Jobelephant.com Inc. All rights reserved.
Posted by the FREE value-added recruitment advertising agency
jeid-91d642a0cd547547b47fa52a911d9915
Show more
Show less","Data Analysis, Electronic Health Records (EHR), Demographic Data, Diagnosis, Service Codes, Billing Codes, Reporting, Outcomes Data, Program Improvement, Database Experience, Communication Skills, Behavioral Health Services, Assertive Community Treatment (ACT), Intensive Community Supports (ICS), Wellness, Cooccurring Disorders, Clientcentered Treatment","data analysis, electronic health records ehr, demographic data, diagnosis, service codes, billing codes, reporting, outcomes data, program improvement, database experience, communication skills, behavioral health services, assertive community treatment act, intensive community supports ics, wellness, cooccurring disorders, clientcentered treatment","assertive community treatment act, behavioral health services, billing codes, clientcentered treatment, communication skills, cooccurring disorders, dataanalytics, database experience, demographic data, diagnosis, electronic health records ehr, intensive community supports ics, outcomes data, program improvement, reporting, service codes, wellness"
MRT Data Analysis Specialist - Mental Health 627,Telecare Corporation,"Ceres, CA",https://www.linkedin.com/jobs/view/mrt-data-analysis-specialist-mental-health-627-at-telecare-corporation-3781993848,2023-12-17,Modesto,United States,Mid senior,Onsite,"""They made it easier for me to live, breathe, eat, and stay clean. Without them, I'd be waiting somewhere, waiting for someone to give me a chance to live..."" - Client from Telecare
What You Will Do To Change Lives
The Medical Records Technician (MRT)/Data Specialty is responsible for the maintenance, storage, and processing of all medical records and EHR data. This involves performing a variety of clerical and technical duties associated with the management and oversight of a program's medical records including, but not limited to assembly, analysis, release of information, data processing, collection, outcome report production, reconciliation and preparation of reports in a manner consistent with medical, administrative, ethical, legal and regulatory requirements of a mental health care system. The MRT/Data Specialist must be able to work independently and have the ability to interact and communicate both verbally and in writing with members served, family members, the public, physicians and staff.
Full Time; AM Shift; Monday - Friday
Expected starting wage range $17.85 - $21.51 - Telecare applies geographic differentials to its pay ranges. The pay range assigned to this role will be based on the geographic location from which the role is performed. Starting pay is commensurate with relevant experience above the minimum requirements.
What You Must Bring To The Table (Must Have)
A high school diploma or a G.E.D.
One (1) year of database experience
One (1) year of EHR or healthcare information system experience in a psychiatric inpatient or outpatient program or acute hospital setting
Working knowledge of managing a hybrid medical record system
EHR System experience, Caminar, Avatar (desired)
Knowledge of local, state, and federal regulations, survey process, accreditation standards, and psychiatric requirements
Knowledge and application of appropriate coding systems; ICD-10 CM, DSM V
Knowledge of documentation and legal issues pertaining to HIPAA, PHI, and other health information
What's In It for You*
Paid Time Off: For Full Time Employee it is 16.7 days in your first year
Nine Paid Holidays & Shift differentials for hourly staff (6% for PM Shift, 10% for Overnight Shift). Weekend Shift differentials for hourly staff (5% for Weekend AM Shift, 11% for Weekend PM Shift, 15% for Weekend Overnight Shift)
Free CEUs, free Supervision for BBS Associate License, coaching, and mentorship
Online University Tuition Discount and Company Scholarships
Medical, Vision, Dental Insurance, 401K, Employee Stock Ownership Plan
For more information visit: https://www.telecarecorp.com/benefits
Join Our Compassionate Team
Telecare's mission is to deliver excellent and effective behavioral health services that engage individuals in recovering their health, hopes, and dreams. Telecare continues to advance cultural diversity, humility, equity, and inclusion at all levels of our organization by hiring mental health peers, BIPOC, LGBTQIA+, veterans, and all belief systems.
The new Telecare Mobile Crisis Response Team (MCRT) will bring crisis services to people in the community delivered by a dedicated team of mental health professionals with law enforcement involvement only where necessary. It's an exciting opportunity to expand much-needed crisis care in Stanislaus County — in a way that is safe, accessible, and compassionate. Our goals are to help people to get the support they need and reduce unnecessary law enforcement involvement, ER visits, and hospitalizations.
EOE AA M/F/V/Disability
May vary by location and position type
Full Job Description will be provided if selected for an interview.
Data Specialist, Entry Level, Medical Records, MRT
If job posting references any sign-on bonus internal applicants and applicants employed with Telecare in the previous 12 months would not be eligible.
Copyright 2022 Jobelephant.com Inc. All rights reserved.
Posted by the FREE value-added recruitment advertising agency
jeid-57e347a5e4be124a8195a1071bc0e6ab
Show more
Show less","Medical, Healthcare, EHR, Data Processing, Data Management, Database Management, HIPAA, DSM V, ICD10 CM, PHI, Caminar, Avatar","medical, healthcare, ehr, data processing, data management, database management, hipaa, dsm v, icd10 cm, phi, caminar, avatar","avatar, caminar, data management, data processing, database management, dsm v, ehr, healthcare, hipaa, icd10 cm, medical, phi"
Provider Data Configuration Analyst IV,Central California Alliance for Health,Modesto-Merced Area,https://www.linkedin.com/jobs/view/provider-data-configuration-analyst-iv-at-central-california-alliance-for-health-3784910161,2023-12-17,Modesto,United States,Mid senior,Remote,"We have an opportunity to join the Alliance as the Provider Data Configuration Analyst IV, in the Provider Services Department.
What You'll Be Responsible For
Reporting to the Provider Data Manager, you will:
Lead and perform initial and complex configuration, maintenance and input of provider data for claims adjudication, member linkage and physician reimbursement and reporting
Act as a subject matter expert and lead to subordinate analysts
Research, analyze and resolve complex issues escalated by internal and external stakeholders
Lead cross-divisional projects and collaborate with internal and external stakeholders
Lead, coordinate and assist with the design of process improvements and monitoring of data quality
The Ideal Candidate Will
Have experience using SQL to develop audits
Have experience documenting business requirements and executing test cases
Provide subject matter expertise and provide mentorship and training to other team members
Balance working both independently and collaboratively
Be comfortable leading and facilitating problem-solving discussions
Have experience leading complex system configuration and maintenance projects
What You'll Need To Be Successful
To read the full position description, and list of requirements click here.
Knowledge of:
SQL (knowledge of and proficiency in)
The principles and practices of software configuration
Healthcare terminology, provider types and classifications and provider database tables and relationships
Methods and techniques of business analysis and reporting
Analytical and research techniques
Ability to:
Lead, train, mentor and motivate staff, and promote an atmosphere of teamwork and cooperation
Act as a subject matter expert, provide guidance regarding the most complex application configuration tasks, and explain regulations, processes, and programs related to area of responsibility
Identify and troubleshoot issues, collect, analyze and interpret complex data, identify alternative solutions, project consequences of recommendations, make recommendations for action and prepare written reports and other program documents
Report data and statistics to support process improvement, data quality, and projects
Create, run, coordinate and manage complex audits
Education and Experience:
Bachelor’s degree in Computer Science, Healthcare, Business Administration or a related field
Eight years of experience performing application configuration in a Medi-Cal or Managed Care environment (a Master’s degree may substitute for two years of the required experience); or an equivalent combination of education and experience may be qualifying
Other Details
While this position is connected to one of our Alliance offices, we are in hybrid remote/in-office work environment right now and we anticipate that the interview process will take place remotely
Our Alliance office locations have officially re-opened after the pandemic, and while some employees may work in full-time telecommute schedules, attendance at quarterly company-wide events or department meetings will be expected
The full compensation range for this position is listed by location below.
The actual compensation for this role will be determined by our compensation philosophy, analysis of the selected candidate's qualifications (direct or transferrable experience related to the position, education or training), as well as other factors (internal equity, market factors, and geographic location).
Scotts Valley pay range
$82,050—$131,290 USD
Merced pay range
$74,666—$119,475 USD
Salinas pay range
$82,050—$131,290 USD
Additionally, all positions at the Alliance are required to meet these minimum qualifications.
Our Benefits
Medical, Dental and Vision Plans
Ample Paid Time Off
12 Paid Holidays per year
401(a) Retirement Plan
457 Deferred Compensation Plan
Robust Health and Wellness Program
Onsite EV Charging Stations
About Us
We are a group of over 500 dedicated employees, committed to our mission of providing accessible, quality health care that is guided by local innovation. We feel that our work is bigger than ourselves. We leave work each day knowing that we made a difference in the community around us.
Join us at Central California Alliance for Health (the Alliance), where you will be part of a culture that is respectful, diverse, professional and fun, and where you are empowered to do your best work. As a regional non-profit health plan, we serve members in Merced, Monterey and Santa Cruz counties. To learn more about us, take a look at our
Fact Sheet
.
The Alliance is an equal employment opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. We are an E-Verify participating employer
At this time the Alliance does not provide any type of sponsorship. Applicants must be currently authorized to work in the United States on a full-time, ongoing basis without current or future needs for any type of employer supported or provided sponsorship.
Show more
Show less","SQL, healthcare terminology, provider types and classifications, provider database tables and relationships, business analysis and reporting, data quality, leading teams, mentoring, troubleshooting, analyzing data, complex software configuration, Bachelor's degree, eight years of experience","sql, healthcare terminology, provider types and classifications, provider database tables and relationships, business analysis and reporting, data quality, leading teams, mentoring, troubleshooting, analyzing data, complex software configuration, bachelors degree, eight years of experience","analyzing data, bachelors degree, business analysis and reporting, complex software configuration, data quality, eight years of experience, healthcare terminology, leading teams, mentoring, provider database tables and relationships, provider types and classifications, sql, troubleshooting"
QC Data Analyst,Steneral Consulting,"Sellersville, PA",https://www.linkedin.com/jobs/view/qc-data-analyst-at-steneral-consulting-3717079959,2023-12-17,Abington,United States,Associate,Onsite,"Share only 2 profiles
Need local candidates only
LinkedIn is must here
Job Title: QC Data Analyst
Location: Sellersville, PA (On-Site)
Duration: 4 months + extensions(Temp to hire available)
Target Start Date: 9/5 or 9/12
Client Description
CDMO providing a complete end to end solution for the development and manufacture of oral dosage and topical pharmaceuticals.
Consultant's Day To Day Responsibilities
Bench testing of pharma products using Empower HPLC and Malvern Particle Size Analyzer
Performing wet chemistry testing including viscosity, pH and dissolution.
Documentation of results into LabVantage LIMS.
Developing trend reports and presenting to QC and Manufacturing stakeholders
Writing and reviewing analytical testing SOP's for individual products
Writing deviation investigations/reports in partnership with QC leadership
Test instrumentation and methods include HPLC, GC, Dissolutions, Particle Size Analyzer, FTIR (Fourier Transform Infrared) Spectroscopy, pH, Dissolution Testing, Viscosity and TOC (Total Organic Carbon) Testing.
Required Skills
Bachelor's degree in Chemistry, Biochemistry or related field.
3 years previous experience reviewing analytical testing data in a non-academic setting.
3 years prior bench testing experience in a non-academic setting
Previous experience performing product analysis using HPLC, GC, Dissolution Testing and Particle Size Analysis
Ability to analyze and interpret a chromatogram
Show more
Show less","Chemistry, Biochemistry, HPLC, Malvern Particle Size Analyzer, Wet Chemistry, Viscosity, pH, Dissolution, LabVantage LIMS, FTIR Spectroscopy, TOC Testing, Analytical Testing, SOPs, Deviation Investigations, GC, Dissolution Testing, Particle Size Analysis, Chromatogram Analysis","chemistry, biochemistry, hplc, malvern particle size analyzer, wet chemistry, viscosity, ph, dissolution, labvantage lims, ftir spectroscopy, toc testing, analytical testing, sops, deviation investigations, gc, dissolution testing, particle size analysis, chromatogram analysis","analytical testing, biochemistry, chemistry, chromatogram analysis, deviation investigations, dissolution, dissolution testing, ftir spectroscopy, gc, hplc, labvantage lims, malvern particle size analyzer, particle size analysis, ph, sops, toc testing, viscosity, wet chemistry"
QC Data Analysts / Reviewers,United Consulting Hub,"Bucks County, PA",https://www.linkedin.com/jobs/view/qc-data-analysts-reviewers-at-united-consulting-hub-3727074573,2023-12-17,Abington,United States,Associate,Onsite,"Consultant’s Day-to-day Responsibilities
Bench testing of pharma products using Empower HPLC and Malvern Particle Size Analyzer.
Performing wet chemistry testing including viscosity, pH, and dissolution.
Documentation of results into LabVantage LIMS.
Developing trend reports and presenting them to QC and Manufacturing stakeholders.
Writing and reviewing analytical testing SOPs for individual products.
Writing deviation investigations/reports in partnership with QC leadership.
Required Skills
Bachelor’s degree in Chemistry, Biochemistry, or related field.
3 years of previous experience reviewing analytical testing data in a non-academic setting.
3 years prior bench testing experience in a non-academic setting.
Previous experience performing product analysis using HPLC, GC, Dissolution Testing, and Particle Size Analysis.
Ability to analyze and interpret chromatograms.
Prior experience tracking and trending data in MS Excel or similar products.
Show more
Show less","Chemistry, Biochemistry, HPLC, Malvern Particle Size Analyzer, Viscosity, pH, Dissolution, LabVantage LIMS, Trend reports, SOPs, Analytical testing, Deviation investigations, Chromatograms, Microsoft Excel","chemistry, biochemistry, hplc, malvern particle size analyzer, viscosity, ph, dissolution, labvantage lims, trend reports, sops, analytical testing, deviation investigations, chromatograms, microsoft excel","analytical testing, biochemistry, chemistry, chromatograms, deviation investigations, dissolution, hplc, labvantage lims, malvern particle size analyzer, microsoft excel, ph, sops, trend reports, viscosity"
"Job Opening for QC Data Analysts / Reviewers - Bucks County, PA",Steneral Consulting,"Bucks County, PA",https://www.linkedin.com/jobs/view/job-opening-for-qc-data-analysts-reviewers-bucks-county-pa-at-steneral-consulting-3727076323,2023-12-17,Abington,United States,Associate,Onsite,"Hi,
Please find attached Job Description. If you are interested please do share with me your updated resume or call me on “3025492448”.
Title:- QC Data Analysts / Reviewers
Location:- Bucks County, PA
Duration:- 6+ Months
Visa:- Citizen, GC, GC-EAD, EAD-H4
Interview Mode:- Video
Description
QC Data Analysts / Reviewers
Location – Bucks County, PA (on site Monday – Friday )- Local Candidates under 60min commute
Consultant’s Day To Day Responsibilities
Bench testing of pharma products using Empower HPLC and Malvern Particle Size Analyzer
Performing wet chemistry testing including viscosity, pH and dissolution.
Documentation of results into LabVantage LIMS.
Developing trend reports and presenting to QC and Manufacturing stakeholders
Writing and reviewing analytical testing SOP’s for individual products
Writing deviation investigations/reports in partnership with QC leadership
Required Skills
Bachelor’s degree in Chemistry, Biochemistry or related field.
3 years previous experience reviewing analytical testing data in a non-academic setting.
3 years prior bench testing experience in a non-academic setting
Previous experience performing product analysis using HPLC, GC, Dissolution Testing and Particle Size Analysis
Ability to analyze and interpret a chromatogram
Prior experience tracking and trending data in MS excel or similar product.
Gaurav Verma
Talent Acquisition -North America
Direct:+1 3025492448
gaurav.verma@steneral.com
In my absence please reach out to Mr. Harish Sharma at harish@steneral.com &
302-721-6151
Show more
Show less","HPLC, Empower, Malvern Particle Size Analyzer, LabVantage LIMS, Chemistry, Biochemistry, Analytical testing, Chromatogram, MS Excel, GC, Dissolution Testing, Particle Size Analysis","hplc, empower, malvern particle size analyzer, labvantage lims, chemistry, biochemistry, analytical testing, chromatogram, ms excel, gc, dissolution testing, particle size analysis","analytical testing, biochemistry, chemistry, chromatogram, dissolution testing, empower, gc, hplc, labvantage lims, malvern particle size analyzer, ms excel, particle size analysis"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783186500,2023-12-17,Abington,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-NewJerse-DataScientist.005
Show more
Show less","Python, JavaScript, JSON, OOP, Generative AI, Algorithms, Research, Product Development, Data Science, English Communication, R","python, javascript, json, oop, generative ai, algorithms, research, product development, data science, english communication, r","algorithms, data science, english communication, generative ai, javascript, json, oop, product development, python, r, research"
eCommerce Sales Data Analyst,The Placement Gurus,"New Jersey, United States",https://www.linkedin.com/jobs/view/ecommerce-sales-data-analyst-at-the-placement-gurus-3776639168,2023-12-17,Abington,United States,Associate,Hybrid,"Ecommerce Sales Data Analyst
Job Summary:
We are seeking a highly skilled and detail-oriented Ecommerce Sales Data Analyst to join our client’s team. They are a $100M+ manufacturer and eCommerce wholesaler in the soft home goods category. As a key member of their analytics team, you will play a crucial role in extracting valuable insights from ecommerce sales data to drive informed decision-making and enhance the overall performance of their product line. The ideal candidate will have a strong background in data analysis, particularly in the context of ecommerce sales, and a passion for translating data into actionable strategies.
The company is headquartered in East Brunswick, NJ. Ideal candidate must be available to work in office on a hybrid schedule.
Key Responsibilities:
Data Extraction and Analysis:
Extract, clean, and analyze large datasets from multiple sources related to ecommerce sales.
Identify trends, patterns, and anomalies in sales data to provide actionable insights for business optimization.
Performance Metrics Tracking:
· Develop and maintain key performance indicators (KPIs) for ecommerce sales, tracking and reporting on metrics such as conversion rates, customer acquisition, and product performance.
Sales Forecasting:
· Collaborate with cross-functional teams to develop accurate sales forecasts based on historical data, market trends, and promotional activities.
Competitor Analysis:
· Conduct regular assessments of competitor ecommerce strategies, pricing, and product positioning to inform our competitive positioning in the market.
Customer Behavior Analysis:
· Analyze customer behavior and purchasing patterns to understand preferences, improve product recommendations, and enhance the overall customer experience.
A/B Testing and Experimentation:
· Design and execute A/B tests to evaluate the impact of changes to product listings, pricing, or promotions on ecommerce sales performance.
Data Visualization:
· Create clear and compelling visualizations of data insights, presenting findings to both technical and non-technical stakeholders.
Collaboration and Communication:
· Work closely with the marketing, product development, and sales teams to provide data-driven recommendations and support strategic decision-making.
Qualifications:
Bachelor's degree in Data Science, Statistics, Business Analytics, or a related field.
Proven experience as a data analyst, preferably with a focus on ecommerce sales data.
Proficiency in data analysis and visualization tools such as SQL and Power BI.
Strong analytical and problem-solving skills, with the ability to translate complex data into actionable insights.
Familiarity with ecommerce platforms and understanding of online consumer behavior.
Excellent communication skills, with the ability to convey technical findings to a non-technical audience.
Personal Traits:
Detail-oriented with a passion for accuracy in data analysis.
Proactive and self-motivated, with the ability to prioritize and manage multiple tasks.
Collaborative team player with strong interpersonal skills.
Enthusiastic about the soft home goods industry and the intersection of data analytics and ecommerce.
Compensation Package includes competitive base salary and robust benefits including matching 401K.
Show more
Show less","Data Analysis, Data Extraction, Data Visualization, SQL, Power BI, Ecommerce Analytics, Sales Data Analysis, Forecasting, Competitor Analysis, Customer Behavior Analysis, A/B Testing, DataDriven Decision Making, Business Analytics, Data Science","data analysis, data extraction, data visualization, sql, power bi, ecommerce analytics, sales data analysis, forecasting, competitor analysis, customer behavior analysis, ab testing, datadriven decision making, business analytics, data science","ab testing, business analytics, competitor analysis, customer behavior analysis, data extraction, data science, dataanalytics, datadriven decision making, ecommerce analytics, forecasting, powerbi, sales data analysis, sql, visualization"
Lead Data Engineer,Zelis,"New Jersey, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zelis-3767526728,2023-12-17,Abington,United States,Mid senior,Onsite,"Summary
Build High level technical design both for Streaming and batch processing systems
Design and build reusable components, frameworks and libraries at scale to support analytics data products
Perform POCs on new technology, architecture patterns
Design and implement product features in collaboration with business and Technology stakeholders
Anticipate, identify, and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Drive the implementation of new data management projects and re-structure of the current data architecture
Implement complex automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development and production deployment frameworks
Drive collaborative reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues
Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products.
Partner closely with product management to understand business requirements, breakdown Epics,
Partner with Engineering Managers to define technology roadmaps, align on design, architecture, and enterprise strategy
Requirements
Minimum of 8+ years experience with the following:
Snowflake (Columnar MPP Cloud data warehouse)
DBT (ETL tool)
Python
Experience designing and implementing Data Warehouse
Preferred Skills
Azure/AWS cloud technology
SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations
Understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Design and development of Azure/AWS Data Factory Pipelines preferred.
Design and development of data marts in Snowflake preferred
Working knowledge of Azure/AWS Architecture, Data Lake, Data Factory
Business analysis experience to analyze data to write code and drive solutions
Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management and/or business analysis skills.
Attention to detail and concern for impact is essential
As a leading payments company in healthcare, we guide, price, explain, and pay for care on behalf of insurers and their members. We’re Zelis in our pursuit to align the interests of payers, providers, and consumers to deliver a better financial experience and more affordable, transparent care for all. We partner with more than 700 payers, including the top-5 national health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, over 4 million providers, and 100 million members, enabling the healthcare industry to pay for care, with care. Zelis brings adaptive technology, a deeply ingrained service culture, and a comprehensive navigation through adjudication and payment platform to manage the complete payment process.
Commitment to Diversity, Equity, Inclusion, and Belonging
At Zelis, we champion diversity, equity, inclusion, and belonging in all aspects of our operations. We embrace the power of diversity and create an environment where people can bring their authentic and best selves to work. We know that a sense of belonging is key not only to your success at Zelis, but also to your ability to bring your best each day.
Equal Employment Opportunity
Zelis is proud to be an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
We encourage members of traditionally underrepresented communities to apply, even if you do not believe you 100% fit the qualifications of the position, including women, LGBTQIA people, people of color, and people with disabilities.
Accessibility Support
We are dedicated to ensuring our application process is accessible to all candidates. If you are a qualified individual with a disability or a disabled veteran and require a reasonable accommodation with any part of the application and/or interview process, please email TalentAcquisition@zelis.com
SCAM ALERT: There is an active nationwide employment scam which is now using Zelis to garner personal information or financial scams. This site is secure, and any applications made here are with our legitimate partner. If you’re contacted by a Zelis Recruiter, please ensure whomever is contacting you truly represents Zelis Healthcare. We will never asked for the exchange of any money or credit card details during the recruitment process. Please be aware of any suspicious email activity from people who could be pretending to be recruiters or senior professionals at Zelis.
Show more
Show less","Data engineering, Streaming, Batch processing, Data warehousing, ETL, Python, Snowflake, DBT, Azure, AWS, SQL, Data lakes, Data marts, Data Factory, Git, Azure DevOps, Agile, Jira, Confluence","data engineering, streaming, batch processing, data warehousing, etl, python, snowflake, dbt, azure, aws, sql, data lakes, data marts, data factory, git, azure devops, agile, jira, confluence","agile, aws, azure, azure devops, batch processing, confluence, data engineering, data factory, data lakes, data marts, datawarehouse, dbt, etl, git, jira, python, snowflake, sql, streaming"
Sr. Data Engineer,Hermitage Infotech,"New Jersey, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-at-hermitage-infotech-3720726987,2023-12-17,Abington,United States,Mid senior,Onsite,"Hi,
Given below is the urgent req for my client.. If you are comfortable with it, available and looking for a project please send me your profile immediately in word document along with your expected hourly salary on CTC/1099 or yearly salary on W2. Please mention your work authorization and your availability to start the project.
Looking For Candidates Regarding The Following
POSITION
Sr Data Engineer
LOCATION
Remote
DURATION
4 Months
PAY RATE
Please mention your minimum expected salary on our W2.
INTERVIEW TYPE
Video
VISA RESTRICTIONS
USC/GC/EAD Only
Required Skills
3+ years of data engineering experience
Snowflake database experience
Postgres
ETL--specifically DBT for transformation workflow
Python, Tableau
Regards
Varma
732-338-7524
Varma
Hermitage Infotech, LLC
P:732-593-8453 Extension 202
F:732-289-6103
varma@hermitageinfotech.com
www.hermitageinfotech.com
Show more
Show less","Data Engineering, Snowflake, PostgreSQL, ETL, DBT, Python, Tableau","data engineering, snowflake, postgresql, etl, dbt, python, tableau","data engineering, dbt, etl, postgresql, python, snowflake, tableau"
Senior Big Data Engineer,Diverse Lynx,"New Jersey, United States",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-diverse-lynx-3764426076,2023-12-17,Abington,United States,Mid senior,Onsite,"Role :
Senior Big Data Engineer
Location : Jersey City, NJ (Onsite)
Job Type: Contract
Interview Mode : In - Person Interview
Responsibilities
Job description:
Design, develop, and implement scalable data pipelines and ETL processes using Java, Python, and Spark.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and design efficient solutions.
Manage and optimize Spark clusters to ensure high performance and reliability.
Perform data exploration, data cleaning, and data transformation tasks to prepare data for analysis and modeling.
Develop and maintain data models and schemas to support data integration and analysis.
Implement data quality and validation checks to ensure accuracy and consistency of data.
Utilize REST API development skills to create and integrate data services and endpoints for seamless data access and consumption.
Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.
Stay updated with the latest technologies and trends in big data, data engineering, data science, and REST API development, and provide recommendations for process improvements.
Mentor and guide junior team members, providing technical leadership and sharing best practices.
Qualifications
Master's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of professional experience in data engineering, working with Java, Python, Spark, and big data technologies.
Strong programming skills in Java and Python, with expertise in building scalable and maintainable code.
Proven experience in Spark cluster management, optimization, and performance tuning.
Solid understanding of data science concepts and experience working with data scientists and analysts.
Proficiency in SQL and experience with relational databases (e.g., Snowflake, Delta Tables).
Experience in designing and developing REST APIs using frameworks such as Flask or Spring.
Familiarity with cloud-based data platforms (e.g.Azure)
Experience with data warehousing concepts and tools (e.g., Snowflake, BigQuery) is a plus.
Strong problem-solving and analytical skills, with the ability to tackle complex data engineering challenges.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Java, Python, Spark, ETL, Data Pipelines, Spark Clusters, Data Exploration, Data Cleaning, Data Transformation, Data Modeling, Data Schemas, Data Quality, Data Validation, REST APIs, Data Science, Big Data Technologies, SQL, Snowflake, Delta Tables, Flask, Spring, Azure, Data Warehousing, BigQuery, CloudBased Data Platforms","java, python, spark, etl, data pipelines, spark clusters, data exploration, data cleaning, data transformation, data modeling, data schemas, data quality, data validation, rest apis, data science, big data technologies, sql, snowflake, delta tables, flask, spring, azure, data warehousing, bigquery, cloudbased data platforms","azure, big data technologies, bigquery, cloudbased data platforms, data cleaning, data exploration, data quality, data schemas, data science, data transformation, data validation, datamodeling, datapipeline, datawarehouse, delta tables, etl, flask, java, python, rest apis, snowflake, spark, spark clusters, spring, sql"
Voice/Data Engineer 4,Laksan Technologies,"Trenton, NJ",https://www.linkedin.com/jobs/view/voice-data-engineer-4-at-laksan-technologies-3667480354,2023-12-17,Abington,United States,Mid senior,Onsite,"The Network Operations group is looking to bring in a resource to help support the configuration and implementation of network devices into the State's Solarwinds Enterprise Management system. This project will require Network Edge resources to send configuration and deployment information into the State's Solarwinds system. Additionally, this resource will be needed to build network maps and provide usage statistic reports. Candidate is expected to be in the office, with possible telework up to two days a week, if approved.
Skill
Required / Desired
Amount
of Experience
Prior professional experience supporting and implementing Solarwinds Required 3 Years
Current professional work experience with Cisco networking equipment Required 3 Years
Current work experience supporting network operations and implementation activities for both LAN and WAN. Required 3 Years
Hands on configuration experience with networking protocol Cisco IOS, IOS XE, and IOS XR Required 3 Years
Show more
Show less","Solarwinds, Cisco, Network maps, Usage statistic reports, LAN, WAN, Cisco IOS, Cisco IOS XE, Cisco IOS XR","solarwinds, cisco, network maps, usage statistic reports, lan, wan, cisco ios, cisco ios xe, cisco ios xr","cisco, cisco ios, cisco ios xe, cisco ios xr, lan, network maps, solarwinds, usage statistic reports, wan"
AWS Data Engineer,Tata Consultancy Services,"New Jersey, United States",https://www.linkedin.com/jobs/view/aws-data-engineer-at-tata-consultancy-services-3764338982,2023-12-17,Abington,United States,Mid senior,Onsite,"Role: AWS Data Engineer
Location: New Jersey, United States
Note:
Must have earned certification credentials in one or more areas.
AWS Developer Associate | AWS Solution Architect
AWS IOT and Edge
Handle and drive conversation with IT solution team executives and provide advisory services and actionable recommendations on
implementation of DevOps Strategy, Practices, Processes, Principles and Platform.
Design, code, and test Greengrass Components that enhance our products and transmit data for storing and analyzing.
Maintain & deploy Assets and Models in Sitewise as per Business Requirement.
Develop and maintain AWS CDK Code base as per the Platform Infrastructure.
Design SQL queries to obtain & reconcile Telemetry Data.
Good Knowledge on Industrial IOT and Telemetry Data Sources & Servers. ( Good to Have )
Advanced Python Programming expected.
Understanding of Major DevOps Tools related to - Source Control, Continuous Integration, Configuration Management, Deployment Automation, Containers, Orchestration
Knowledge on how to automate the entire DevOps pipeline, including CI/CD, continuous testing, app performance monitoring, infrastructure settings and configurations.
Ability to create constant flow Consulting Engagement Opportunities
Strong Cloud Technology Domain Skills in AWS backed by working knowledge of Technology Trends
Provide industry expertise in understanding and design of the proposed solution to the client
Design solution for the customer in alignment with the envisaged client’s business plan
Facilitation skills ,
Terraform in Site Wise Edge
Stakeholder (internal and external) management skills
Interested candidates please do share me your updated resume to gopinath.j3@tcs.com
Show more
Show less","AWS, AWS Developer Associate, AWS Solution Architect, AWS IOT and Edge, DevOps, Greengrass, Sitewise, AWS CDK, Python, SQL, Telemetry, Industrial IOT, Source Control, Continuous Integration, Configuration Management, Deployment Automation, Containers, Orchestration, CI/CD, Continuous Testing, App Performance Monitoring, Cloud Technology Domain, Terraform, Site Wise Edge","aws, aws developer associate, aws solution architect, aws iot and edge, devops, greengrass, sitewise, aws cdk, python, sql, telemetry, industrial iot, source control, continuous integration, configuration management, deployment automation, containers, orchestration, cicd, continuous testing, app performance monitoring, cloud technology domain, terraform, site wise edge","app performance monitoring, aws, aws cdk, aws developer associate, aws iot and edge, aws solution architect, cicd, cloud technology domain, configuration management, containers, continuous integration, continuous testing, deployment automation, devops, greengrass, industrial iot, orchestration, python, site wise edge, sitewise, source control, sql, telemetry, terraform"
Data Analyst (Pharma/Lifesciences),Tiger Analytics,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-analyst-pharma-lifesciences-at-tiger-analytics-3782586152,2023-12-17,Abington,United States,Mid senior,Remote,"Tiger Analytics is an advanced analytics consulting firm. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our consultants bring deep expertise in Data Science, Machine Learning and AI. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner.
If you are passionate to work on unstructured business problems that can be solved using data, we would like to talk to you.
As a Data Analyst, you will directly work with key client stakeholders to define the business problems, analysis requirements, and determine solution requirements. You will be responsible for ensuring business value and communicating results, etc.
Requirements
3 to 7 years of work experience in the space of data analytics. Should be able to work independently with client business and analytics stakeholders
Experience in working with Pharma data sets
Strong experience on Python, SQL and/or visualization tools such as PowerBI, or Qlik
Ability to understand client’s business problems, working as part of a global team and perform project related studies and analysis
Strong experience in implementing commercial data solutions for life sciences companies
Attention to details, hands-on deep dive analysis, and strong communication skills
Fluency with pharma data sources such as Plantrak, LAAD, PE, etc including RWD sources such as TriNetX, Flatiron, Optum, Komodo etc
Good understanding of commercial pharma processes
Adhere to Analytics Lifecycle and best practices
A strong aptitude for story telling through the analysis of data
Hands-on analytical experience with a proven track record of results
Demonstrated ability to translate strategy into action; excellent analytical skills and an ability to communicate complex issues in a simple way and to orchestrate plans to resolve issues & mitigate risks
Bachelor's or higher in data analytics or equivalent work experience
Preferred experience with cloud platforms, such as AWS or Azure
Responsibilities
Manage master data, including creation, updates, and deletion
Create documentation of analytics use cases
Commissioning and decommissioning of data sets
Helping develop reports and analysis
Manage and design the reporting environment, including data sources, security, and metadata
Generating reports from single or multiple systems
Troubleshooting the reporting database environment and reports
Evaluating changes and updates to source production systems
Training end-users on new reports and dashboards
Providing technical expertise in data storage structures, data mining, and data cleansing
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, fast-growing, challenging and entrepreneurial environment, with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Python, SQL, PowerBI, Qlik, TriNetX, Flatiron, Optum, Komodo, AWS, Azure, Master data management, Data analytics use cases, Commissioning, Decommissioning, Reporting, Data storage structures, Data mining, Data cleansing","data science, machine learning, ai, python, sql, powerbi, qlik, trinetx, flatiron, optum, komodo, aws, azure, master data management, data analytics use cases, commissioning, decommissioning, reporting, data storage structures, data mining, data cleansing","ai, aws, azure, commissioning, data analytics use cases, data mining, data science, data storage structures, datacleaning, decommissioning, flatiron, komodo, machine learning, master data management, optum, powerbi, python, qlik, reporting, sql, trinetx"
Vector Database and Knowledge Graph Engineer,EXL,"New Jersey, United States",https://www.linkedin.com/jobs/view/vector-database-and-knowledge-graph-engineer-at-exl-3768066867,2023-12-17,Abington,United States,Mid senior,Remote,"Decision Analytics
EXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. Using our proprietary, award-winning Business EXLerator Framework™, which integrates analytics, automation, benchmarking, BPO, consulting, industry best practices and technology platforms, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa.
EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients’ decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries.
Please visit www.exlservice.com for more information about EXL Analytics.
Responsibilities:
The ideal candidate will have expertise in designing, implementing, and optimizing vector databases, with a strong focus on utilizing Neo4j for knowledge graph applications. The role involves contributing to the development and maintenance of our data infrastructure, ensuring efficient handling of complex relationships and vectors.
· 6+ years of experience working with data engineering with at least 2 years of experience working on vector databases.
· Design and implement vector databases to efficiently store and retrieve high-dimensional vectors.
· Utilize Neo4j for creating and managing knowledge graphs, ensuring optimal performance and scalability.
· Develop and maintain knowledge graphs using Neo4j, incorporating domain-specific data and relationships.
· Implement graph algorithms to extract meaningful insights from the knowledge graph.
· Optimize database queries and indexing strategies for vector and graph operations.
· Identify and resolve performance bottlenecks to ensure efficient data retrieval.
· Collaborate with application developers to integrate vector databases and knowledge graphs into various software solutions.
· Provide support for query optimization and data modeling for application-specific requirements.
· Implement and maintain data security measures for vector databases and knowledge graphs.
· Ensure compliance with relevant data protection regulations and industry standards.
· Work closely with cross-functional teams, including data scientists, software engineers, and product managers.
· Communicate technical concepts and solutions effectively to both technical and non-technical stakeholders.
Technical skills:
· Knowledge of distributed database systems.
· Familiarity with machine learning and AI concepts related to vector data.
· Experience with cloud-based database solutions.
· Proven experience in designing and implementing vector databases, with a focus on Neo4j for knowledge graph applications.
· Strong proficiency in database optimization, performance tuning, and query languages.
· Familiarity with graph algorithms and data modeling for knowledge graphs.
· Experience with relevant programming languages, such as Python, Java, or Scala.
· Excellent problem-solving skills and the ability to work in a collaborative team environment.
Soft skills:
· Strong work ethic and desire to produce quality results
· Consistently and proactively communicates (verbally/written) to stakeholders (progress/roadblocks/etc.)
· Continuous Improvement mindset and approach to work product
· Ability to take complex subjects and simplify it to less technical individuals
· Provides clear documentation of processes, workflows, recommendations, etc.
· High level of critical thinking capabilities
· Organized and has the ability to manage work effectively, escalating issues as appropriate
· Takes initiative & is a self-starter
· Displays ownership of their work (quality, timeliness)
· Seeks to become an expert in their field and shares their expertise through recommendations, proactive communications/actions and peer sharing/coaching where relevant
· Should be able to communicate with stakeholders directly and independently
· Should have good problem solving skills
Candidate Profile:
· Bachelor’s/Master's degree in economics, mathematics, computer science/engineering, operations research or related analytics areas; candidates with BA/BS degrees in the same fields from the top tier academic institutions are also welcome to apply
· Outstanding written and verbal communication skills
· Superior analytical and problem solving skills
· Experience in working in dual shore engagement is preferred
· Must have experience in managing clients directly
· Strong record of achievement, solid analytical ability, and an entrepreneurial hands-on approach to work
· Able to work in fast pace continuously evolving environment and ready to take up uphill challenges
· Is able to understand cross cultural differences and can work with clients across the globe
What we offer:
EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants.
You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth
Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.
We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.
Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.
""EOE/Minorities/Females/Vets/Disabilities
Show more
Show less","Vector databases, Neo4j, Knowledge graphs, Distributed database systems, Machine learning, Cloudbased databases, Python, Java, Scala, Graph algorithms, Database optimization, Query languages","vector databases, neo4j, knowledge graphs, distributed database systems, machine learning, cloudbased databases, python, java, scala, graph algorithms, database optimization, query languages","cloudbased databases, database optimization, distributed database systems, graph algorithms, java, knowledge graphs, machine learning, neo4j, python, query languages, scala, vector databases"
Master Data Management Developer,"Kaygen, Inc.","New Jersey, United States",https://www.linkedin.com/jobs/view/master-data-management-developer-at-kaygen-inc-3762692850,2023-12-17,Abington,United States,Mid senior,Remote,"Role: Senior Master Data Management Developer
Location: Remote
Remarks
:
Proven experience as a Senior MDM Developer, with expertise in Profisee MDM SaaS platform Strong knowledge of data management principles, data governance, and data quality Proficient in MDM concepts and data modeling techniques Hands-on experience in designing and implementing MDM solutions, including data integration, data cleansing, and data validation Familiarity with data integration tools, ETL processes, and data warehousing 100% remote (EST preferred)
Contract Description
Senior Master Data Management Developer - Contractor to lend subject matter expertise
Design, develop, and implement Master Data Management solutions using Profisee MDM SaaS platform
Collaborate with business stakeholders and data governance teams to understand data requirements and define data models
Develop data integration workflows, data quality rules, and data validation processes to ensure the integrity of master data
Create and maintain data mapping, transformation, and cleansing rules within the Profisee MDM SaaS solution
Participate in data profiling, data auditing, and data stewardship activities to identify and resolve data quality issues
Qualifications
Proven experience as a Senior MDM Developer, with expertise in Profisee MDM SaaS platform
Strong knowledge of data management principles, data governance, and data quality
Proficient in MDM concepts and data modeling techniques
Hands-on experience in designing and implementing MDM solutions, including data integration, data cleansing, and data validation
Familiarity with data integration tools, ETL processes, and data warehousing
Show more
Show less","Profisee MDM SaaS platform, Data integration, Data management principles, MDM concepts, Data modeling, Data governance, Data quality, Metadata management, Data integration, Data synchronization, Data migration, Data cleansing, Data deduplication, Data standardization, Data validation, Data profiling, ETL processes, Data warehousing","profisee mdm saas platform, data integration, data management principles, mdm concepts, data modeling, data governance, data quality, metadata management, data integration, data synchronization, data migration, data cleansing, data deduplication, data standardization, data validation, data profiling, etl processes, data warehousing","data deduplication, data governance, data integration, data management principles, data migration, data profiling, data quality, data standardization, data synchronization, data validation, datacleaning, datamodeling, datawarehouse, etl, mdm concepts, metadata management, profisee mdm saas platform"
"Data Engineer - Python, ETL",Euclid Innovations,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-engineer-python-etl-at-euclid-innovations-3778227968,2023-12-17,Abington,United States,Mid senior,Hybrid,"Title: Data Engineer - Python, ETL
Location: New Jersey, Hybrid
Job Description:
Integration engineer responsible for daily support and project-based development of risk management systems.
ETL developers are responsible for designing and creating the data warehouse and all related extraction, transformation, and load of data functions.
This is an opportunity to gain experience in risk management processing using new technologies.
Skills:
5+ years of experience in IT with more than 2 years in financial projects (preferably in the area of Market Risk)
Strong experience in SQL and database programming (preferably MS SQL Server).
Solid experience with SSIS.
Good understanding of ETL/ELT and DWH concepts with hands-on experience using ETL/ELT tools
Experience in Programming languages using Python;
Experience with Azure Databricks, Data Factory and Snowflake
Strong testing and troubleshooting skills
Experience with Jira/Confluence tools and Git change management
Good communication and presentation skills
Education:
Bachelor's degree in Computer Science or Finance
Show more
Show less","Python, ETL, Data Warehouse, SQL, SSIS, Azure Databricks, Data Factory, Snowflake, Jira, Confluence, Git","python, etl, data warehouse, sql, ssis, azure databricks, data factory, snowflake, jira, confluence, git","azure databricks, confluence, data factory, datawarehouse, etl, git, jira, python, snowflake, sql, ssis"
Data Analyst,Emonics LLC,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-analyst-at-emonics-llc-3778827989,2023-12-17,Abington,United States,Mid senior,Hybrid,"The ideal candidate will use their passion for big data and analytics to provide insights to the business covering a range of topics. They will be responsible for conducting both recurring and ad hoc analysis for business users.
Responsibilities
Understand the day-to-day issues that our business faces, which can be better understood with data
Compile and analyze data related to business' issues
Develop clear visualizations to convey complicated data in a straightforward fashion
Qualifications
Bachelor's or Master's degree in Statistics or Applied Mathematics or equivalent experience
1 - 2 years' Data Analysis experience
Proficient in SQL
Show more
Show less","Data Analysis, SQL, Statistics, Applied Mathematics, Data Visualization, Business Intelligence, Data Mining, Big Data","data analysis, sql, statistics, applied mathematics, data visualization, business intelligence, data mining, big data","applied mathematics, big data, business intelligence, data mining, dataanalytics, sql, statistics, visualization"
Data Architect,Piper Companies,"Wayne, PA",https://www.linkedin.com/jobs/view/data-architect-at-piper-companies-3774128450,2023-12-17,Abington,United States,Mid senior,Hybrid,"Piper Companies is currently looking for a Data Architect located Wayne, PA to work for a rapidly growing organization in the financial sector. This individual will be expected to be on-site 2/3 days a week.
Responsibilities for the Data Architect include:
Identifying installation solutions for new databases
Determining the requirements for a new database
Publishing and/or presenting design reports
Identifying areas for improvement in current systems
Coordinating with other team members to reach project milestones and deadlines
Qualifications for the Data Architect include:
15-20+ years of Data Related experience
Snowflake experience would be a nice to have
Early January Start
Keywords:
Systems engineer, data, data warehouse systems engineering, systems administrator, systems administration, systems admin, powershell, Infrastructure, hadoop, SQL, Python, Powershell
Show more
Show less","Data Architect, Snowflake, Systems engineer, Data warehouse systems engineering, Systems administrator, Systems administration, Systems admin, Powershell, Infrastructure, Hadoop, SQL, Python","data architect, snowflake, systems engineer, data warehouse systems engineering, systems administrator, systems administration, systems admin, powershell, infrastructure, hadoop, sql, python","data architect, data warehouse systems engineering, hadoop, infrastructure, powershell, python, snowflake, sql, systems admin, systems administration, systems administrator, systems engineer"
Big Data Lead,Soho Square Solutions,"Trenton, NJ",https://www.linkedin.com/jobs/view/big-data-lead-at-soho-square-solutions-3781785593,2023-12-17,Abington,United States,Mid senior,Hybrid,"Job Description
Primary : SQL server, Snowflakes, , IICS, Data lakes and Data modeling
ADF nice to have.
Strictly no AWS
Position Summary
The Data Engineer is a key contributor to Our Data & Analytics Strategy to transform as a Data-driven organization and position Credit Union as a Technology Leader. This position is responsible for the design, development, deploy and operations of data analytics in Modern Cloud Data & Analytics platform.
You will be part of a passionate, highly collaborative team who works in a fast-paced agile environment. You will work closely with business & technology owners, product managers, data champions and cross-functional teams to support data & analytical needs and enable data-as-a-service function.
The ideal candidate will be a experienced data engineering & analytics professional, excels in delivering value, familiar with modern data architectures, offering comprehensive data & analytical technology skills in Data Pipelines, Snowflake, Cloud Data & Analytics Platforms, Data Warehouse, BI Analytics & Dashboards. Preferable to have financials services domain experience from banking, credit union, fintech and other financial sectors
Qualifications
6+ years of professional experience in data & analytics technologies/solutions as data engineer or related roles B.S. in computer science or equivalent experience.
Skills
Proficient level data engineering skills using Cloud Data Platforms - Snowflake, ELT/ETL mechanisms, Informatica IICS, Stored procedures to ingest structured, semi-structured datasets using files, and RDBMS - SQL Server.
Experience in Data lakes and Data modeling is Must
Azure Data factory(ADF) experience is nice to have
Proficient in data analysis using Tableau, SQL, and other data analysis techniques.
Expertise with data operations, sustaining data pipelines and production system processes.
Experience in applying Data management patterns to measure & monitor data quality. Applying data governance standards and creating technical data dictionary.
Experience in SDLC Processes, CI/CD and Agile Methodologies.
Ability to work with Jira, Share Point, Confluence, Git.
Production support experience will be an added advantage.
Job Description , Roles And Responsibilities
Financial services background with preference to domain experience in Credit Union, Banking, FinTech sectors. Other financial sectors and domains will be considered
Develops and delivers data & analytics solutions and projects to meet organizational priorities and timelines.
Analyzes enterprise and external data sources, creates optimized & resilient data pipelines to ingest and enriches them.
Designs analytical consume ready data models by applying modern data warehouse techniques. Creates and enriches curated data models while ensuring conformed, transformed, enriched and connected data sets
Enables dashboards/reports in Tableau; Applies analytical mindset in deriving data driven insights, designs & creates intuitive dashboards and scorecards.
Practices and adheres to enterprise data standards, guidelines, data management architecture. Finds opportunities to improve best practices, identifies repeatable solutions to underlying problems.
Works with offshore and external partner resources and owns delivery of work items.
Innovative problem solving, and Can-do mind set. Outcome oriented and always finds a way deliver commitments.
Fast learner and quickly adopts to dynamic environment. Ability to work in a fast-paced agile team environment.
Applies & demonstrates Team-First mindset. Collaborates with team for data & analytics platform maturity.
Good Communication
Able to work independently with customer
Show more
Show less","SQL Server, Snowflake, Data Lakes, Data Modeling, ADF, Data Engineering, Cloud Data Platforms, Informatica IICS, Tableau, RDBMS, Data Quality Management, Data Governance, SDLC Processes, CI/CD, Agile Methodologies, Jira, Share Point, Confluence, Git","sql server, snowflake, data lakes, data modeling, adf, data engineering, cloud data platforms, informatica iics, tableau, rdbms, data quality management, data governance, sdlc processes, cicd, agile methodologies, jira, share point, confluence, git","adf, agile methodologies, cicd, cloud data platforms, confluence, data engineering, data governance, data lakes, data quality management, datamodeling, git, informatica iics, jira, rdbms, sdlc processes, share point, snowflake, sql server, tableau"
Principal Data Engineer,Bristol Myers Squibb,"New Jersey, United States",https://www.linkedin.com/jobs/view/principal-data-engineer-at-bristol-myers-squibb-3766360942,2023-12-17,Abington,United States,Mid senior,Hybrid,"Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.
Remote Opportunity
When you join BMS, you are joining a diverse, high-achieving team united by a common mission.
The Informatics and Predictive Sciences (IPS) mission is to Pioneer, Partner and Predict to drive transformative insights for patient benefit. IPS conducts applied computational research in areas that include genomic, structural and molecular informatics, computational and systems biology, patient selection and translational biomarker research, and broader fields including knowledge science, epidemiology and machine learning—across the full lifecycle of drug discovery and development and across all therapeutic areas at BMS. We do this in close partnership with scientific and clinical experts in the field, both inside and outside the company.  We perform innovative science to empower key data-driven decisions across a rich pipeline of next-generation medicines. In doing so, our work transforms the lives of patients, as well as our own lives and careers.
Here, you’ll get the chance to grow and thrive through opportunities that are uncommon in scale and scope. You’ll pursue innovative ideas while advancing professionally alongside some of the brightest minds in biopharma.
Bristol Myers Squibb seeks a hardworking individual to contribute to groundbreaking informatics and data management initiatives within Research. This hands-on Principal Data Engineer role interfaces closely with scientists in the Chemistry, Biotherapeutics, and Informatics and Predictive Sciences groups working to identify novel therapies. We are seeking an individual with extensive experience integrating data and building data solutions to make data accessible and relevant for the research community.
Responsibilities Include, But Are Not Limited To, The Following
Work with multi-functional teams to lead design and implementation of cloud-based, integrated data platforms to facilitate the identification of novel therapeutic compounds.
Collaborate with colleagues across Informatics and Predictive Sciences to make a wide variety of data, from raw to scientific insights, readily available to researchers.
Collaborate with analysts to apply solutions for integration and delivery of complex biologic data to a wide variety of predictive modeling approaches.
Design and develop infrastructure and processes to load public and proprietary data from multiple source systems leveraging a mix of custom software and database development, open-source tools, and cloud-based services.
Collaborate with the project manager, solution architect, infrastructure team, and external vendors as needed to support successful delivery of technical solutions.
Innovate and advise on the latest technologies and standard methodologies in Data Engineering.
Basic Qualifications
Bachelor's Degree in an engineering or biology field with 8+ years of academic / industry experience
or Master’s Degree in an engineering or biology field with 6+ years of academic / industry experience
or PhD in an engineering or biology field with 4+ years of academic / industry experience in an engineering or biology field.
Preferred Qualifications
Demonstrated proficiency in processing, analyzing, and constructing complex data capture, management, and dissemination solutions for public or proprietary biological patient and assay datasets including transcriptomic, proteomic, and/or small molecule datasets.
Demonstrated proficiency with predictive modeling approaches, and/or preparing data for predictive modeling.
Demonstrated proficiency with current software engineering methodologies, such as Agile SDLC approaches, distributed source code control, project management, issue tracking, and CI/CD tools and processes.
Excellent skills in an object-oriented programming language such as Python or Java, and proficiency in SQL and R.
High degree of proficiency in cloud computing. Preference will be given to candidates with AWS experience.
Solid understanding of container strategies such as Docker, Fargate, ECS, and ECR.
Excellent skills and deep knowledge of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs. NoSQL, and database optimizations.
Experience developing web applications in frameworks like Shiny, Vue, React, etc. is a plus.
Along with programming proficiency, must have a strong capacity for independent thinking and the ability to grasp underlying scientific questions.
Must have experience leading complex scientific data management and integration initiatives in a research environment.
Must be effective in a dynamic environment while adapting to changing priorities and have excellent written and verbal communication and presentation skills.
Must have excellent time management and interpersonal skills.
The starting compensation for this job is a range from $121,000 to $167,200 plus incentive cash and stock opportunities (based on eligibility).
The starting pay rate takes into account characteristics of the job, such as required skills and where the job is performed. Final, individual compensation will be decided based on demonstrated experience.
Eligibility for specific benefits listed on our careers site may vary based on the job and location. For more on benefits, please visit our BMS Career Site.
Benefit offerings are subject to the terms and conditions of the applicable plans then in effect and may include the following: Medical, pharmacy, dental and vision care. Wellbeing support such as the BMS Living Life Better program and employee assistance programs (EAP). Financial well-being resources and a 401(K). Financial protection benefits such as short- and long-term disability, life insurance, supplemental health insurance, business travel protection and survivor support. Work-life programs include paid national holidays and optional holidays, Global Shutdown days between Christmas and New Year’s holiday, up to 120 hours of paid vacation, up to two (2) paid days to volunteer, sick time off, and summer hours flexibility. Parental, caregiver, bereavement, and military leave. Family care services such as adoption and surrogacy reimbursement, fertility/infertility benefits, support for traveling mothers, and child, elder and pet care resources. Other perks like tuition reimbursement and a recognition program.
Remote
#QLR
If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
Show more
Show less","Genomics, Structural informatics, Molecular informatics, Computational biology, Systems biology, Patient selection, translational biomarker, Knowledge science, Epidemiology, Machine learning, Data integration, Data solutions, Cloud computing, Docker, Fargate, ECS, ECR, Postgres, Elasticsearch, Redshift, Aurora, AWS, Python, Java, SQL, R, Shiny, Vue, React, Agile SDLC, Distributed source code control, Project management, Issue tracking, CI/CD","genomics, structural informatics, molecular informatics, computational biology, systems biology, patient selection, translational biomarker, knowledge science, epidemiology, machine learning, data integration, data solutions, cloud computing, docker, fargate, ecs, ecr, postgres, elasticsearch, redshift, aurora, aws, python, java, sql, r, shiny, vue, react, agile sdlc, distributed source code control, project management, issue tracking, cicd","agile sdlc, aurora, aws, cicd, cloud computing, computational biology, data integration, data solutions, distributed source code control, docker, ecr, ecs, elasticsearch, epidemiology, fargate, genomics, issue tracking, java, knowledge science, machine learning, molecular informatics, patient selection, postgres, project management, python, r, react, redshift, shiny, sql, structural informatics, systems biology, translational biomarker, vue"
Senior Business Analyst - Data Analytics,Precise Solutions,"Lake County, IL",https://www.linkedin.com/jobs/view/senior-business-analyst-data-analytics-at-precise-solutions-3736253619,2023-12-17,Kenosha,United States,Mid senior,Onsite,"At Precise Solutions, we are looking for top talent consultants to bring on as employees of our organization and service our clients in the various Life Sciences Industries. We are much more than a consulting firm! Precise Solutions provides competitive compensation packages with great salaries, benefits, health insurance, paid time off and employer-based 401k contributions.
We currently have an immediate need for the following:
Job Title: Senior Business Systems Analyst- Data Analytics
Location:
Lake County, IL USA
Compensation:
$65.00 per hour, Paid Time off, Company 401k contributions, Health, dental, and vision insurance. Total Annual Compensation including benefits is $150,000.00
Resume Requirements
The following resume requirements must be met for resume to be considered
Candidates first and last name (legal spelling) must be at the top of the resume
Valid email address must be at the top of the resume. LinkedIn links will not be accepted it must be a valid email address so our recruiters can respond to your resume
Please refrain from submitting resumes with candidates photo they will not be considered
Senior Business Systems Analyst (BSA)- Data Analytics: Overview
A Senior Business Systems Analyst is required to join a team that has been developing capabilities and data solutions throughout all levels of the business including business seek a talented business analyst to deliver on our project roadmap and help create and shape solutions for future work. The Senior BSA role will require a combination of independent analytical work in
SQL/Analysis tools
, meeting with customers and stakeholders to understand their business needs, working collaboratively with other functional groups to leverage existing data sources, and combine these into powerful solutions that deliver business value. There will be a heavy focus on requirements management, documentation and testing.
Senior Business Analyst (BA) - Data Analytics: Key Responsibilities
The Senior BA will be able to quickly begin working with key stakeholders to gain relevant knowledge and will consistently strive to understand our business partners’ needs. Ultimately, the Senior BA will be the lead on developing the data, technology architecture & solutions which we will be required to meet these high-level needs.
The Senior BA will work to elicit, create, and
edit Agile and Waterfall user stories, requirements specifications, and design documents.
The Senior BA will analyze existing data sets and accurately convey findings to different audiences
The Senior BA will write, coordinate, and execute testing on initiatives and changes, to ensure business acceptance criteria are met.
The Senior BA will work within blended teams to deliver system changes, leveraging our system life cycle.
The Senior BA will maintain system documentation to ensure a robust, shared internal knowledge base.
The Senior BA will facilitate both in-person and virtual meetings, capturing and assigning action items.
The Senior BA will Lead and facilitate meetings with customer stakeholders related to the above topics
The Senior BA will deliver against committed timelines, and partner in estimation activities
The Senior BA will be responsible for adherence to AbbVie System Lifecycle activities
The Senior BA should have fluency in SQL and business intelligence solutions like PowerBI, Tableau, QlikSense
Senior Business Analyst (BA) - Data Analytics: Requirements
Bachelors Degree Required.
At least five years of experience delivering technical solutions.
Outstanding customer service and communication skills.
Strong understanding of data/analytics and deriving metrics from underlying data sources.
Proficiency in office productivity tools like Outlook, Excel, Word, and PowerPoint.
Strong understanding of good story and test case writing.
Experience analyzing data using Excel, SQL query tools, or BI applications such as Qlik, Tableau, Spotfire.
Deep experience writing clear, traceable requirements and design documentation.
Show more
Show less","SQL, Data Analytics, Agile, Waterfall, PowerBI, Tableau, QlikSense, BI applications, Qlik, Spotfire, Excel, Business Intelligence, Office Productivity Tools, Outlook, Word, PowerPoint","sql, data analytics, agile, waterfall, powerbi, tableau, qliksense, bi applications, qlik, spotfire, excel, business intelligence, office productivity tools, outlook, word, powerpoint","agile, bi applications, business intelligence, dataanalytics, excel, office productivity tools, outlook, powerbi, powerpoint, qlik, qliksense, spotfire, sql, tableau, waterfall, word"
Azure Data Architect,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/azure-data-architect-at-zortech-solutions-3782029083,2023-12-17,Etobicoke, Canada,Mid senior,Onsite,"Role- Azure Data Architect
Location- Toronto, ON
Duration- Full Time
ualifications
What Do We Expect From You
Minimum of 8 years of ICT experience with large-scale data and analytics solutions
Experience in turning business use cases and requirements into technical solutions
Experience in business processing mapping of data and analytics solutions
Experience in technology solutions, practice development, architecture, consulting, and/or cloud/infrastructure technologies
Experience in managing teams, being a technical coach
Experience driving knowledge transfer and/or training programs
Experience with agile delivery methodologies, proof of concepts and prototyping
Experience with Azure ARM templates, PowerShell and CI/CD using Azure DevOps
Experience with preparing data for Data Science and Machine Learning purposes
Experience with Azure data, Machine Learning and Artificial Intelligence solutions
Ability to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows
Experience with different database technologies (e.g., SQL database(s), NoSQL such as Hadoop, MongoDB, Redis, etc.
Experience with exposing data to end-users via Power BI, Azure API Apps.
Knowledge of Azure Data & Analytics PaaS Services: Azure Data Factory, Azure Data Lake, Azure Synapse Analytics, Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics and Azure SQL DB
Knowledge of Lambda and Kappa architecture patterns and Mastery of SQL
Knowledge of Master Data Management (MDM) and Data Quality tools
An excellent level of English (C1-C2 according to the Common European Framework of Reference for Languages (CEFR)) with outstanding written and verbal communication skills is a must
Master’s degree or equivalent in IT, Computer Science, Engineering, or related field
Required
c
ertifications
You have at least 3 of the below certifications:
Microsoft Certified: Azure AI Engineer Associate
Microsoft Certified: Azure Enterprise Data Analyst Associate
Microsoft Certified: Power BI Data Analyst Associate
Microsoft Certified: Azure Data Engineer Associate
Microsoft Certified: Azure Data Scientist Associate
Microsoft Certified: Power Platform Solution Architect Expert
Show more
Show less","Azure, ARM templates, PowerShell, CI/CD, Azure DevOps, Data Science, Machine Learning, Artificial Intelligence, Data profiling, Data cataloguing, Data mapping, SQL, Hadoop, MongoDB, Redis, Power BI, Azure API Apps, Azure Data Factory, Azure Data Lake, Azure Synapse Analytics, Azure Databricks, Azure IoT, Azure HDInsight, Spark, Azure Cosmos DB, Azure Stream Analytics, Azure SQL DB, Lambda architecture, Kappa architecture, Master Data Management (MDM), Data Quality tools","azure, arm templates, powershell, cicd, azure devops, data science, machine learning, artificial intelligence, data profiling, data cataloguing, data mapping, sql, hadoop, mongodb, redis, power bi, azure api apps, azure data factory, azure data lake, azure synapse analytics, azure databricks, azure iot, azure hdinsight, spark, azure cosmos db, azure stream analytics, azure sql db, lambda architecture, kappa architecture, master data management mdm, data quality tools","arm templates, artificial intelligence, azure, azure api apps, azure cosmos db, azure data factory, azure data lake, azure databricks, azure devops, azure hdinsight, azure iot, azure sql db, azure stream analytics, azure synapse analytics, cicd, data cataloguing, data mapping, data profiling, data quality tools, data science, hadoop, kappa architecture, lambda architecture, machine learning, master data management mdm, mongodb, powerbi, powershell, redis, spark, sql"
Data Visualization Analyst,Experfy,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-visualization-analyst-at-experfy-3761302228,2023-12-17,Etobicoke, Canada,Mid senior,Onsite,"Opportunity Description
Our client is looking for a Data Visualization Analyst to join our digital data team for handling market data, forecast, performance tracking according to the company's strategy, as well transforming data into insights that drive business value (data analytics, data visualization and data modelling techniques). She/He will also drive digitalization & process excellence by supporting Customer & Consumer teams in finding growth opportunities as well as conducting Market Research. Manages and maintains the automated processing and reporting of sales data to support the business.
Tools
Expert in PowerBI, and Tableau
Additional visualization skills : seaborn, matplolib, plotly, ggplot, bokeh
Snowflake, SQL, Excel, PowerPoint, Word, and Python
Expert in Data manipulation techniques (esp. querying tables with SQL, Numpy)
Good knowledge of cloud computing and data storages. Knowing AWS is a plus
Project management & support: JIRA projects & service desk, Confluence, Teams
Skills
Excellent people management and communication skills
Established experience with project management, SCRUM, & different variants of AGILE techniques
Good communication, Fluent English spoken and written compulsory
Strong capacity to develop consistent, clear analysis and report potential issues
Data quality audit techniques (Electronic Data Discovery)
Soft Skills
Pragmatic and capable of solving complex issues
Ability to understand business needs
Push innovative solutions
Service-oriented, flexible & team player
Self-motivated, take initiative
Attention to detail & technical intuition
Behavior Competencies
Cooperate transversally
Curiosity and entrepreneurship
Leadership and autonomous
Experience
Close to 5 years of experience developing reports in a deadline driven role
Experience in a healthcare company is a strong plus along with good understanding of IQVIA data
Strong experience building and deploying PowerBI, and Tableau data visualizations and dashboards
Extensive knowledge of SQL, and Excel
Strong understanding of data visualization concepts. Thoughtfully connects these concept business metrics and KPIs to achieve business goals
Ability to understand and query disparate data sources, including relational structures
Analytical thinking to turn information into compelling stories
Accuracy and detail-orientation
Ability to handle pressure and commit to and respect deadlines
Preferred Qualifications
Master's degree in Computer Science, Mathematics, Statistics, Information Management, or Applied quantitative field
Requirements
Responsibilities
Manage multiple sources of data, IQVIA master data, develops standard & project-based reports/dashboards (from single or multiple systems)
Build and develop market knowledge with a focus on understanding available market potential and opportunities, market penetration and customer valuation models
Provide ongoing reports from different data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
Provide insights related to the specific reports/analyses in order to support Consumer Teams in data driven decision making processes
Extract monthly (or weekly) sales data from relative data sources (Ex: IQVIA) and create reports at national and subnational level for respective divisions
Implementing visualization dashboards for CHC market analytics
Ensuring high performance of dashboarding solution based on key requirements (Ex: size of data, # of users, etc.)
Collaborate with Customer & Consumer Teams for ad-hoc solutions
Reviewing integrity of data
Provides performance indicators definition from a functional / business angle
Identify metrics required to calculate performance indicators. Identify source systems for each metrics and all necessary metrics attributes needed to define indicators
Support the analysis of change requests proposed by Business community Digital teams, to enable the development & evolution of KPIs
Review functional specifications created to make KPI available in visualization & analysis solutions
Support users in realization & documentation of user acceptance tests (UAT)
Develop new Power BI dashboards for market analytics based on mock-ups
Assist deployment of PowerBI in client infrastructure (incl. user access rights, security)
Migrate market analytics dashboards in Tableau to PowerBI
Monitor performance of solution and propose
Show more
Show less","PowerBI, Tableau, Seaborn, Matplolib, Plotly, Ggplot, Bokeh, Snowflake, SQL, Python, Numpy, AWS, Jira Projects, Service Desk, Confluence, Teams, Scrum, Electronic Data Discovery, IQVIA, Excel, PowerPoint, Word","powerbi, tableau, seaborn, matplolib, plotly, ggplot, bokeh, snowflake, sql, python, numpy, aws, jira projects, service desk, confluence, teams, scrum, electronic data discovery, iqvia, excel, powerpoint, word","aws, bokeh, confluence, electronic data discovery, excel, ggplot, iqvia, jira projects, matplolib, numpy, plotly, powerbi, powerpoint, python, scrum, seaborn, service desk, snowflake, sql, tableau, teams, word"
Data Analyst,J&M Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-j-m-group-3768032390,2023-12-17,Etobicoke, Canada,Mid senior,Onsite,"Must have an Insurance domain knowledge specifically in the area of investments like in bond / securities, asset classification like fixed asset, Real-estate, mortgages, commercial loans etc
Understanding of balance sheet and income statements and the flow specifically for profit and loss (gain/loss)
General understanding on taxation guidelines like USTAX, GGAAP, NAIC
Understanding of accounting framework like IFRS/ IAS39 will be an added advantage Functional Requirement -
To analyze, business requirement and propose solutions for business areas and to prepare functional specifications.
Collecting, understanding, and transmitting the business requirements for the project, and translating these into functional specifications though a BI tool (PBI)
To provide the link between the customer, development team and any third-party regarding software functionality, throughout the development lifecycle starting from the requirement gathering
Responsible for documenting client's requirements as Software Requirement Specification (SRS), BRD /FRD and for sign off by the client.
Interaction with the internal development team to solutionism required by the client.
Manage client calls, requirement gathering and analysing project requirements.
Interact with client-side BA and business users to understand their business processes Technical Requirement
Sound knowledge in databases like - Oracle, SQL Server, Mysql.
Functional knowledge in ETL Testing, BI Testing \uF0F0
Good understanding of BI tool preferably PBI
Show more
Show less","Insurance, Investments, Bonds, Securities, Assets, Fixed assets, Real estate, Mortgages, Commercial loans, Balance sheet, Income statements, Profit and loss, Taxation, USTAX, GGAAP, NAIC, IFRS, IAS39, Functional analysis, Business requirements, Functional specifications, Business intelligence, PBI, Software functionality, Software Requirement Specification, SRS, BRD, FRD, Client requirements, Development team, Client calls, Project requirements, Business processes, Oracle, SQL Server, MySQL, ETL Testing, BI Testing","insurance, investments, bonds, securities, assets, fixed assets, real estate, mortgages, commercial loans, balance sheet, income statements, profit and loss, taxation, ustax, ggaap, naic, ifrs, ias39, functional analysis, business requirements, functional specifications, business intelligence, pbi, software functionality, software requirement specification, srs, brd, frd, client requirements, development team, client calls, project requirements, business processes, oracle, sql server, mysql, etl testing, bi testing","assets, balance sheet, bi testing, bonds, brd, business intelligence, business processes, business requirements, client calls, client requirements, commercial loans, development team, etl testing, fixed assets, frd, functional analysis, functional specifications, ggaap, ias39, ifrs, income statements, insurance, investments, mortgages, mysql, naic, oracle, pbi, profit and loss, project requirements, real estate, securities, software functionality, software requirement specification, sql server, srs, taxation, ustax"
Senior Data Engineer,Aviva Canada,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-aviva-canada-3770032269,2023-12-17,Etobicoke, Canada,Mid senior,Onsite,"Individually we are people, but together we are Aviva. Individually these are just words, but together they are our Values – Care, Commitment, Community, and Confidence.
We are looking for a collaborative and resourceful
Senior Data Engineer
. You are dedicated, naturally inquisitive and are comfortable in a fast-paced environment.
This role will be part of and a member of our Data Delivery Group, you will be responsible for analysis, design and implementation in high-performing, experienced team. You'll be required to apply your depth of knowledge and expertise to all many areas including requirements, infrastructure, and solutioning. Aviva has embarked on an exciting journey to modernize, craft and build a next generation of data platform to support the growing data needs for data engineering, analytics and Data Science.
We embrace a culture challenging the status quo and constantly look to efficiently simplify processes, technology, and workflow.
This position reports to the AVP – Data Delivery.
What You’ll Do
Design, build and operationalize large scale enterprise data solutions and using AWS data and analytics services.
Demonstrates outstanding understanding of AWS cloud services especially in the data engineering and analytics space.
Analyze, re-architect and re-platform on premise big data platforms to AWS.
Work will also encompass crafting & developing solution designs for data acquisition/ingestion of multifaceted data sets (internal/external), data integrations & data warehouse/marts.
You are collaborative with business partners, product owners, partners, functional specialists, business analysts, IT architecture, and developers to develop solution designs adhering to architecture standards.
Responsible for supervising and ensuring that solutions adhere to enterprise data governance & design standards.
Act as a point of contact to resolve architectural, technical and solution related challenges from delivery teams for best efficiency.
Design and Develop ETL Pipeline to ingest data into Hadoop from different data sources (Files, Mainframe, Relational Sources, NoSQL Etc.) using Informatica BDM
Advocate importance of data catalogs, data governance and data quality practices.
Outstanding problem-solving skills.
Work in an Agile delivery framework to evolve data models and solution designs to deliver value incrementally.
You are a self-starter with experience working in a fast-paced agile development environment.
Strong mentoring and coaching skills and ability to lead by example for junior team members.
Outcome focused with strong decision making and critical thinking skills to challenge the status quo which impacts delivery pace and performance and striving for efficiencies.
What You’ll Bring
University degree in Computer Engineering or Computer Science.
5+ years of experience crafting solutions for data lakes, data integrations, data warehouses/marts.
Solid grasp/experience with data technologies & tools (Hadoop, PostgreSQL, Informatica, etc.,)
Outstanding knowledge and experience in ETL with Informatica product suite.
Knowledge/experience in Cloud Data Lake Design – preferred AWS technologies like S3, EMR, Redshift, Snowflake, could data catalog etc.,
Experience implementing Data Governance principles and efficiencies.
Understanding of reporting/analytics tools (QlikSense, SAP Business Objects, SAS, DataIku, etc.,).
Familiar with the Agile software development.
Excellent verbal and written communication skills.
Insurance knowledge an asset-Ability to foundationally understand complex business process driving technical systems.
What You’ll Get
Competitive rewards package including base compensation, eligibility for annual bonus, retirement savings, share plan, health benefits, personal wellness, and volunteer opportunities.
Exceptional Career Development opportunities.
We’ll support your professional development education.
Additional Information:
Aviva Canada has an accommodation process in place to provide accommodations for employees with disabilities. If upon commencement of employment you require a specific accommodation because of a disability, please contact your Talent Acquisition Partner so that an appropriate accommodation can be arranged. This process applies throughout your career with Aviva Canada.
Show more
Show less","Cloud Computing, AWS, Hadoop, PostgreSQL, Informatica, ETL, Data Lakes, Data Integration, Data Warehouse, Data Governance, Data Analytics, Data Quality, Agile, QlikSense, SAP Business Objects, SAS, Dataiku, S3, EMR, Redshift, Snowflake","cloud computing, aws, hadoop, postgresql, informatica, etl, data lakes, data integration, data warehouse, data governance, data analytics, data quality, agile, qliksense, sap business objects, sas, dataiku, s3, emr, redshift, snowflake","agile, aws, cloud computing, data governance, data integration, data lakes, data quality, dataanalytics, dataiku, datawarehouse, emr, etl, hadoop, informatica, postgresql, qliksense, redshift, s3, sap business objects, sas, snowflake"
Sr. Data Engineer,EY,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-ey-3755162231,2023-12-17,Etobicoke, Canada,Mid senior,Onsite,"At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.
The Opportunity:
We are seeking a highly skilled and experienced Senior Data Engineer with demonstrated success delivering complex data integration requirements, to join our dynamic team. In this role, you will play a pivotal part in architecting, developing, and optimizing data integration solutions that drive our client’s data-driven decisions. You will collaborate with cross-functional teams to design, implement, and maintain data pipelines that support our client’s growing business needs. Strong client-facing and communication skills are essential, as you will engage with clients to understand their requirements and provide technical guidance.
Key Responsibilities:
Data Integration and ETL Development
Design, develop, and maintain complex data integration solutions using third-party technologies including Informatica Intelligent Cloud Services (IICS) and Azure Data Factory (ADF)
Create ETL processes for data extraction, transformation, and loading (ETL) into target storage systems based on batch, micro-batch, or real-time ingestion needs
Architecture and Design
Collaborate with architects and data engineers to design scalable, efficient, and robust data pipelines based on technical best practices
Translate business requirements into technical solutions, ensuring data quality and integrity
Design and deployment of analytical data models
Client Engagement
Interact with clients to understand their data needs and provide technical guidance and solutions
Communicate effectively with non-technical stakeholders to ensure clear project requirements and objectives
Performance Optimization
Identify and resolve performance bottlenecks in data integration processes and recommend best practices for optimization
Data Quality and Governance
Implement data quality checks and data governance principles to maintain high data integrity
Monitor and resolve data issues promptly
Documentation
Create and maintain technical documentation for data integration processes, configurations, and best practices
Continuous Learning and Innovation
Stay updated on industry best practices and emerging technologies related to IICS, ADF, Snowflake and other relevant technologies
Recommend and implement innovative solutions to enhance data processes
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or a related field
Minimum of 10 years of experience in data integration and ETL development
Minimum 5 years of practical, hands-on project experience with Informatica Intelligent Cloud Services (IICS) and Azure Data Factory (ADF); Certifications with Informatica IICS, ADF considered an asset
Demonstrated experience with other ETL platforms considered an asset (e.g., Talend, DBT, DataStage)
Demonstrated experience/knowledge with ingesting data from a broad range of methods including files, APIs, event streaming (e.g., Kafka, Kinesis)
Demonstrated experience with design and implementation of various workloads on the Snowflake Data Cloud platform; SnowPro Core and SnowPro Advanced Architect certifications considered an asset
Proficiency in SQL, Python, or other scripting languages
Knowledge of data modeling, data warehousing, and ETL principles
Demonstrated experience designing and deploying various analytical data modeling methodologies (e.g., Star, Snowflake, Data Vault 2.0)
Strong problem-solving skills and attention to detail
Excellent communication, interpersonal and collaboration skills
Familiarity with cloud services such as Azure, AWS, or GCP
Experience with Agile and traditional SDLC delivery methodologies
Experience working independently, efficiently, and effectively under tight timelines and delivering results by critical deadlines
What We Offer
At EY, our Total Rewards package supports our commitment to creating a leading people culture - built on high-performance teaming - where everyone can achieve their potential and contribute to building a better working world for our people, our clients and our communities. It's one of the many reasons we repeatedly win awards for being a great place to work.
We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package allows you decide which benefits are right for you and which ones help you create a solid foundation for your future. Our Total Rewards package includes a comprehensive medical, prescription drug and dental coverage, a defined contribution pension plan, a great vacation policy plus firm paid days that allow you to enjoy longer long weekends throughout the year, statutory holidays and paid personal days (based on province of residence), and a range of exciting programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Support and coaching from some of the most engaging colleagues in the industry
Learning opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that’s right for you
Diversity and Inclusion at EY
Diversity and inclusiveness are at the heart of who we are and how we work. We’re committed to fostering an environment where differences are valued, policies and practices are equitable, and our people feel a sense of belonging. We embrace diversity and are committed to combating systemic racism, advocating for the 2SLGBT+ community, promoting our Neurodiversity Centre of Excellence and Accessibility initiatives, and are dedicated to amplifying the voices of Indigenous people (First Nations, Inuit, and Métis) nationally as we strive towards reconciliation. Our diverse experiences, abilities, backgrounds, and perspectives make our people unique and help guide us. Because when people feel free to be their authentic selves at work, they bring their best and are empowered to build a better working world.
EY | Building a better working world
EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.
Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.
Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.
Show more
Show less","Data Integration, ETL Development, Informatica Intelligent Cloud Services (IICS), Azure Data Factory (ADF), Snowflake Data Cloud Platform, SQL, Python, Data Modeling, Data Warehousing, Agile, SDLC, Cloud Services, Azure, AWS, GCP, Star Schema, Snowflake Schema, Data Vault 2.0, Kafka, Kinesis","data integration, etl development, informatica intelligent cloud services iics, azure data factory adf, snowflake data cloud platform, sql, python, data modeling, data warehousing, agile, sdlc, cloud services, azure, aws, gcp, star schema, snowflake schema, data vault 20, kafka, kinesis","agile, aws, azure, azure data factory adf, cloud services, data integration, data vault 20, datamodeling, datawarehouse, etl development, gcp, informatica intelligent cloud services iics, kafka, kinesis, python, sdlc, snowflake data cloud platform, snowflake schema, sql, star schema"
Senior Data Engineer,LTI - Larsen & Toubro Infotech,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-lti-larsen-toubro-infotech-3780231002,2023-12-17,Gadsden,United States,Associate,Onsite,"About Us:
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title:
Sr Data Engineer
Work Location
Charlotte, NC
Job Description:
Primary Skill :
Snowflake engineer (certified) with a working knowledge of AWS Platform
The Data Engineer is responsible for contributing towards goals and objectives of the Data & Analytics Engineering team. This team is undertaking a data modernization effort to build a cloud first data lake and warehouse. They’re also transforming the technology & processes around machine learning and data science. This role will be responsible for working on various solution components like AWS, Talend, Snowflake as part of the end-to-end data engineering solutions. This role will bring deep technical expertise in data solutions by building data engineering components and data pipelines. This person will be a consummate team player and participate in code reviews, design reviews, and operate with a mindset of continuous improvement
Responsibilities
• Build and maintain the necessary frameworks and technology architecture for data pipelines (ETL / ELT) and machine learning pipelines
• Contribute actively to processes needed to achieve operational excellence in all areas, including project management and system reliability.
• Design, Build and launch new data pipelines in production in partnership with the business stakeholders.
• Design and support of new machine learning pipelines as well as dashboards and reports in production.
Secondary Skill:
Working Knowledge of ETL Tool such as Talend.
Required Qualifications
• BA/BS in Computer Science, Math, Physics, or other technical fields.
• 3+ years of experience in Data Engineering, BI, or Data Warehousing.
• 3+ years of strong experience in SQL and Data Analysis
• 3+ years in development of data pipelines/ETL for data ingestion, data preparation, data integration, data aggregation, feature engineering, etc.
• Strong analytical skills with high attention to detail and accuracy
• Working knowledge of AWS and Snowflake
Preferred Qualifications
Experience working in Financial Technology companies.
Solid working knowledge of Data Warehousing concepts.
Working knowledge of Tableau and Talend
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Snowflake, AWS, Talend, Tableau, ETL / ELT, Machine learning, SQL, Data analysis, Data pipelines, Data ingestion, Data preparation, Data integration, Data aggregation, Feature engineering, Data Warehousing","snowflake, aws, talend, tableau, etl elt, machine learning, sql, data analysis, data pipelines, data ingestion, data preparation, data integration, data aggregation, feature engineering, data warehousing","aws, data aggregation, data ingestion, data integration, data preparation, dataanalytics, datapipeline, datawarehouse, etl elt, feature engineering, machine learning, snowflake, sql, tableau, talend"
Data Engineer,Holovis,"Orlando, FL",https://www.linkedin.com/jobs/view/data-engineer-at-holovis-3773656260,2023-12-17,Gadsden,United States,Associate,Onsite,"Role Purpose
Design, develop, and implement advanced data analytics products and services to support exciting entertainment projects worldwide. You will provide insight across existing Holovis solutions, and support advanced capabilities of new Holovis solutions that become new, innovative stand-alone advanced Analytics systems in their own right.
Key Objectives
Deliver advanced Analytics products and services across three areas; Existing Holovis solutions, Analytics and AI capabilities in new Holovis Solutions
Standalone Analytics products and services e.g. Descriptive/Predictive/Prescriptive Models.
Contribute to the design of a ‘Analytics as a Service’ (AaaS) model.
Deliver a flexible and scalable development pipeline that allows rapid prototyping, model training, monitoring and continuous deployment across productionised products and services.
Your experience
Experience working in a production-orientated environment, focusing on rapid prototyping and minimum viable product delivery.
Experience in utilising distributed storage and processing systems to design innovative, scalable, low latency Data Products and services (internally and/or externally).
Comfortable working collaboratively in a project-orientated environment with a variety of stakeholders and subject matter experts.
Your Skills
MS Azure, EventHub, PowerBI, Databricks, Data Pipeline (eg. Apache NiFi), Apache Supersets and Postgres.
Development of Reporting and Visualisations
Requirement gathering and documentation
Linux administration & shell scripting.
Strong SQL skills and a good understanding of data management principles.
About You
You have the right to live, work and drive in the USA, and happy to work at our facility in Orlando
You are confident travelling globally, ready to travel to sites across the USA and worldwide
You enjoy working with diverse teams across multiple disciplines, countries and time zones
Show more
Show less","Data Analytics, Product Development, Azure, EventHub, PowerBI, Apache Supersets, PostgreSQL, Data Pipeline, Apache NiFi, Databricks, Reporting, Visualizations, Requirement Gathering, Documentation, Linux Administration, Shell Scripting, SQL, Data Management","data analytics, product development, azure, eventhub, powerbi, apache supersets, postgresql, data pipeline, apache nifi, databricks, reporting, visualizations, requirement gathering, documentation, linux administration, shell scripting, sql, data management","apache nifi, apache supersets, azure, data management, data pipeline, dataanalytics, databricks, documentation, eventhub, linux administration, postgresql, powerbi, product development, reporting, requirement gathering, shell scripting, sql, visualizations"
10025 - Big Data Engineer,Hyundai AutoEver America,"Fountain Valley, CA",https://www.linkedin.com/jobs/view/10025-big-data-engineer-at-hyundai-autoever-america-3709728114,2023-12-17,Gadsden,United States,Associate,Onsite,"10025 – Big Data Engineer
Job Summary
Design, build, and maintain the Information/Proposed Changes platform that enables large-scale data processing and analysis. Responsible for developing and maintaining data pipelines, data lakes, and other data-related platform. Work closely with data scientists, analysts, and other stakeholders to ensure that data is properly collected, stored, and processed for analysis and reporting purposes. Implement and maintain data security and access controls to ensure that data is protected. Responsible for troubleshooting and resolving technical issues related to data infrastructure and ensuring that data systems are scalable and efficient. Play a critical role in enabling organization to derive insights and value from their data assets.
Essential Functions
This job requires experience in building and maintaining scalable data pipelines and robust data models from structured and unstructured sources for AI/ML.
The ideal candidate should have advanced SQL skills and be able to query and transform large structured/unstructured datasets using Spark/PySpark, Spark SQL/Hive and Hive/NoSQL.
They should also have experience in developing Big Data pipelines in orchestration tools such as Airflow and Oozie, designing tooling for access management, monitoring, data controls, and self-service ETL/Analytics pipelines.
Other requirements include hands-on experience with On-Prem Big Data Platform, sound knowledge of Distributed Data Processing frameworks like YARN, and proficiency in writing data pipelines using Spark, Python and Scala.
The ideal candidate should also have experience in developing frameworks/utilities in Python, working in a Dev/Ops environment, and following development best practices such as code reviews and unit testing.
Additionally, the candidate should be able to diagnose software issues and engineering workarounds, have a good understanding of BI tools such as Tableau/Power BI and MicroStrategy for Big Data, and be able to lead, guide and assist team members with project development and problem solving.
The candidate should also be flexible and able to learn and use new technologies, work well in a team environment as well as independently to achieve goals.
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Required Skills, Attributes & Education
Bachelor’s degree OR Master’s degree preferred
5+ years of experience working with and maintaining IT Application support.
5+ years of experience with Big Data Engineer
Big Data Technologies: Knowledge of Big Data technologies such as Hadoop, Spark, Hive, Pig, Kafka, and NoSQL databases such as MongoDB, Cassandra, and HBase.
Distributed Systems: Understanding of distributed systems and distributed computing principles.
Programming Languages: Proficiency in programming languages such as Java, Python, Scala, and SQL.
Data Modeling: Knowledge of data modeling techniques and tools to design efficient data structures for Big Data systems.
Data Processing: Experience with data processing and ETL (Extract, Transform, Load) tools and techniques.
Cloud Computing: Familiarity with cloud computing platforms such as AWS, Azure, and Google Cloud.
Data Security: Knowledge of data security principles and experience implementing security measures for Big Data systems.
Data Warehousing: Understanding of data warehousing concepts and experience designing and maintaining data warehouses.
Analytics and Machine Learning: Familiarity with analytics and machine learning tools and techniques and their implementation in Big Data systems.
Performance Tuning: Experience with performance tuning and optimization techniques for Big Data systems to ensure scalability, reliability, and high availability.
Certifications
Cloudera/Hortonworks Spark/Hadoop certification
Salary Range 91,810- 131,285
Show more
Show less","Spark, PySpark, Spark SQL, Hive, Hive/NoSQL, Airflow, Oozie, YARN, Python, Scala, Tableau, Power BI, MicroStrategy, Big Data, Hadoop, Pig, Kafka, MongoDB, Cassandra, HBase, Java, SQL, ETL, AWS, Azure, Google Cloud, Cloudera, Hortonworks","spark, pyspark, spark sql, hive, hivenosql, airflow, oozie, yarn, python, scala, tableau, power bi, microstrategy, big data, hadoop, pig, kafka, mongodb, cassandra, hbase, java, sql, etl, aws, azure, google cloud, cloudera, hortonworks","airflow, aws, azure, big data, cassandra, cloudera, etl, google cloud, hadoop, hbase, hive, hivenosql, hortonworks, java, kafka, microstrategy, mongodb, oozie, pig, powerbi, python, scala, spark, spark sql, sql, tableau, yarn"
Data Engineer,LTI - Larsen & Toubro Infotech,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-at-lti-larsen-toubro-infotech-3750853447,2023-12-17,Gadsden,United States,Associate,Onsite,"About LTIMindtree
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please
visit
www.ltimindtree.com
About Role Description:
The Specialist Data Engineer is a position responsible for leading a variety of engineering activities including the design, acquisition and deployment of hardware, software and network infrastructure in coordination with the Technology team. The overall objective of this role is to lead efforts to ensure quality standards are being met within existing and planned framework.
Responsibilities:
Serve as a technology subject matter expert for internal and external stakeholders and provide direction for all firm mandated controls and compliance initiatives, all projects within the group and in creating a technology domain roadmap
Ensure that all integration of functions meet business goals
Define necessary system enhancements to deploy new products and process enhancements
Recommend product customization for system integration
Identify problem causality, business impact and root causes
Exhibit knowledge of how own specialty area contributes to the business and apply knowledge of competitors, products and services
Advise or mentor junior team members
Impact the engineering function by influencing decisions through advice, counsel or facilitating services
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency
Qualifications:
8+ Years of experience in Big Data and Data Engineering using Python , Java or Scala.
Designing and developing Python/Scala related data pipelines and REST API.
Ability to write robust code in Python.
Deep knowledge in spark & PySpark API Experience is required.
Good experience in Hadoop, Big Data, Hadoop centric Schedulers
Big Data and reporting System integration through API knowledge is added advantage.
Understanding of data structures, data modelling and software architecture
Outstanding analytical and problem-solving skills
Banking domain knowledge is an Add-on Advantage
Knowledge of Regulatory Reporting domain will be a strong plus.
Excellent communication skills
Job Location:
Tampa, Florida
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Big Data, Data Engineering, Python, Java, Scala, Hadoop, Spark, PySpark, SQL, REST API, Data structures, Data modeling, Software architecture, Analytical skills, Problemsolving skills, Banking domain knowledge, Regulatory Reporting, Communication skills","big data, data engineering, python, java, scala, hadoop, spark, pyspark, sql, rest api, data structures, data modeling, software architecture, analytical skills, problemsolving skills, banking domain knowledge, regulatory reporting, communication skills","analytical skills, banking domain knowledge, big data, communication skills, data engineering, data structures, datamodeling, hadoop, java, problemsolving skills, python, regulatory reporting, rest api, scala, software architecture, spark, sql"
Data Engineer,Top Prospect Group,"Stamford, CT",https://www.linkedin.com/jobs/view/data-engineer-at-top-prospect-group-3774826509,2023-12-17,Gadsden,United States,Associate,Onsite,"We are seeking a highly skilled and motivated Data Engineer/Analyst with 2-3 years of professional experience, specializing in
SQL, Azure, and Azure Synapse
. The ideal candidate will have a strong educational background, holding a degree in a relevant field. As a member of our dynamic team, you will play a crucial role in managing, analyzing, and optimizing our data infrastructure to support business intelligence and decision-making processes.
Data Management:
Design, develop, and maintain scalable and efficient data pipelines.
Implement data integration processes to consolidate data from various sources into Azure Synapse.
Ensure data accuracy, integrity, and security across all datasets.
SQL Development:
Write and optimize complex SQL queries to extract, transform, and load (ETL) data.
Perform data profiling, cleansing, and validation to enhance data quality.
Azure Expertise:
Utilize Azure services, particularly Azure Synapse, to build and manage data solutions.
Work with Azure Data Factory, Azure Databricks, and other relevant tools to orchestrate data workflows.
Data Analysis:
Collaborate with business stakeholders to understand data requirements and provide actionable insights.
Perform exploratory data analysis to uncover trends, patterns, and opportunities.
Performance Optimization:
Identify and implement optimizations to enhance the performance of data processing and analysis.
Documentation:
Document data engineering processes, data flows, and system architecture.
Prepare comprehensive reports and dashboards for internal and external stakeholders.
Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field.
2-3 years of hands-on experience in data engineering and analysis.
Proficiency in SQL and experience with database technologies.
Strong expertise in working with Azure services, especially Azure Synapse.
Familiarity with data integration tools such as Azure Data Factory.
Knowledge of data warehousing concepts and best practices.
Excellent problem-solving and analytical skills.
Benefits:
Competitive salary and benefits package.
Opportunities for professional development and growth.
Collaborative and inclusive work environment.
Show more
Show less","SQL, Azure, Azure Synapse, Data Management, Data Engineering, Data Analysis, Data Pipelines, Data Integration, Data Profiling, Data Cleansing, Data Validation, Data Warehousing, Data Optimization, Data Documentation, Azure Data Factory, Azure Databricks, Business Intelligence, Data Visualization, Dashboards, ProblemSolving, Analytical Skills, Computer Science, Information Technology, Data Integration Tools, Data Quality, Data Security","sql, azure, azure synapse, data management, data engineering, data analysis, data pipelines, data integration, data profiling, data cleansing, data validation, data warehousing, data optimization, data documentation, azure data factory, azure databricks, business intelligence, data visualization, dashboards, problemsolving, analytical skills, computer science, information technology, data integration tools, data quality, data security","analytical skills, azure, azure data factory, azure databricks, azure synapse, business intelligence, computer science, dashboard, data documentation, data engineering, data integration, data integration tools, data management, data optimization, data profiling, data quality, data security, data validation, dataanalytics, datacleaning, datapipeline, datawarehouse, information technology, problemsolving, sql, visualization"
Data Engineer,INSPYR Solutions,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-at-inspyr-solutions-3780037374,2023-12-17,Gadsden,United States,Associate,Onsite,"Role: Data Engineer
Duration: Direct Hire
Location: Houston, TX 77040 (HYBRID)
Work Authorization: US Citizen, Green Card Holders or Authorized to work in the US
Seeking talented Data Analyst / Engineers with expertise in SQL/SSIS who are looking to grow into Data Scientists / Analytics Professionals.
Position Summary
The Data Engineer will have a background in data analysis, data analytics, data visualization, dashboards, KPIs & metrics reporting and will be proficient in SQL, SSIS, & SSRS. The candidate should have knowledge of structured and unstructured data concepts and tools, experience with OLAP tools, and the ability to code against APIs using Python for data extraction, transformation, and loading (ETL). This role will entail hands-on day to day data manipulation, transmission, and ETL work and also play and integral role in guiding the organization forward with larger analytics projects.
Responsibilities:
Be the primary support for our data analytics and business intelligence software.
Analyzes existing business processes in search of productivity gains through automation.
Provides data integration services around our corporate data warehouse.
Customize 3rd party software applications.
Completes database, web, and client programming as needed.
Requirements:
Bachelor's degree in MIS or CS or equivalent work experience.
Master's degree in Data Science or Data Analytics preferred.
2 - 5 years of technical and relevant experience as Data Analyst focused on ETL work.
Proficient in SQL, SSIS, & SSRS
Ability to code against APIs using Python for data extraction, transformation, and loading (ETL).
Prior experience working in data analytics, data visualization, dashboards, KPIs & metrics reporting.
Knowledge of structured and unstructured data concepts and tools.
Experience with OLAP tools, including IBM Planning Analytics, is a preferred.
About INSPYR Solutions:
Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities
Show more
Show less","SQL, SSIS, SSRS, Python, ETL, Data visualization, Dashboards, KPIs, Metrics reporting, APIs, Data extraction, Data transformation, Data loading, Structured data, Unstructured data, OLAP tools, IBM Planning Analytics, Data analysis, Data analytics, Data science, Data engineering","sql, ssis, ssrs, python, etl, data visualization, dashboards, kpis, metrics reporting, apis, data extraction, data transformation, data loading, structured data, unstructured data, olap tools, ibm planning analytics, data analysis, data analytics, data science, data engineering","apis, dashboard, data engineering, data extraction, data loading, data science, data transformation, dataanalytics, etl, ibm planning analytics, kpis, metrics reporting, olap tools, python, sql, ssis, ssrs, structured data, unstructured data, visualization"
Senior Data Engineer,NBCUniversal,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-nbcuniversal-3770036808,2023-12-17,Gadsden,United States,Associate,Onsite,"Company Description
NBCUniversal owns and operates over 20 different businesses across 30 countries including a valuable portfolio of news and entertainment television networks, a premier motion picture company, significant television production operations, a leading television stations group, world-renowned theme parks and a premium ad-supported streaming service.
Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. We strive to foster a diverse and inclusive culture where our employees feel supported, embraced and heard. We believe that our workforce should represent the communities we live in, so that together, we can continue to create and deliver content that reflects the current and ever-changing face of the world. Click here to learn more about Comcast NBCUniversal’s commitment and how we are making an impact.
Job Description
Our Direct-to-Consumer (DTC) portfolio is a powerhouse collection of consumer-first brands, supported by media industry leaders, Comcast, NBCUniversal and Sky. When you join our team, you'll work across our dynamic portfolio including Peacock, NOW, Fandango, SkyShowtime, Showmax, and TV Everywhere, powering streaming across more than 70 countries globally. And the evolution doesn't stop there. With unequalled scale, our teams make the most out of every opportunity to collaborate and learn from one another. We're always looking for ways to innovate faster, accelerate our growth and consistently offer the very best in consumer experience. But most of all, we're backed by a culture of respect. We embrace authenticity and inspire people to thrive.
In the Senior Data Engineer role, the Playback Analytics Engineering team at Peacock is looking for a passionate problem solver who’s looking to build the next generation of data pipelines and applications. You will be owning full segments of a data engineering project, managing expectations, and driving delivery with the appropriate stakeholders while maintaining the consensus for engineering architecture. As part of the Global Video Engineering organization, the PAE team collects client-side and server-side telemetry for Peacock and prepares it for analytics workloads.
Essential Functions (Responsibilities)
Design, build, and scale data pipelines across a variety of source systems and streams, distributed/elastic environments, and downstream applications.
Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Maintain ownership of a given pipeline or domain and raise flags to senior leadership when appropriate regarding architectural concerns.
Provide mentorship to junior members of the engineering team to align to overall practice standards and approach.
Participate in development sprints, demos, and retrospectives, as well as release and deployment.
Build and manage relationships with supporting IT teams in order to effectively deliver work products to production.
Qualifications
Bachelor’s degree in Computer Science or a relevant field or equivalent work experience
Minimum of five (5) years of data science, machine learning, advanced analytics experience in high velocity, high-growth companies
Strong coding skills in general purpose languages like Scala or Python, and familiarity with software engineering principles around testing, code reviews and deployment
Experience with distributed data processing systems like Spark, and proficiency in SQL
Experience owning and managing a full pipeline within a large project, including interfacing with business stakeholders, architects and senior leadership to effectively communicate status and concerns.
Experience collaborating with and understanding the needs of stakeholders from a variety of business functions: Product, Engineering and Technical Operations
Familiarity with ML and MLOps concepts and technologies, such as model training, deployment, and monitoring.
Desired
Analytical – You have experience in delivering data analytics solutions that promote data discovery.
Experience with Databricks, Snowflake, Amazon Web Services, or related cloud platforms a plus
Understanding of big data technology stacks (Hive / Spark etc.) is a plus.
Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film.
Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical
Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust.
Strong understanding of Agile principles and best practices
You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment.
This position is eligible for company sponsored benefits, including medical, dental and vision insurance, 401(k), paid leave, tuition reimbursement, and a variety of other discounts and perks. Learn more about the benefits offered by NBCUniversal by visiting the Benefits page of the Careers website.
Salary range: $115,000 - $145,000
Additional Information
NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.
Show more
Show less","Python, Scala, Spark, SQL, MLOps, Databricks, Snowflake, AWS, Agile, Hadoop, Hive","python, scala, spark, sql, mlops, databricks, snowflake, aws, agile, hadoop, hive","agile, aws, databricks, hadoop, hive, mlops, python, scala, snowflake, spark, sql"
Data Engineer,Hedge Fund,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-hedge-fund-3784599285,2023-12-17,Gadsden,United States,Associate,Onsite,"Position Overview:   Reporting to the Head of Data Analytics & Strategy, the Data Engineer will design, implement, and maintain complex data storage, pipeline, and analytics systems that support and further develop the data infrastructure of the organization’s Quantitative and Fundamental investment groups.  Responsibilities include building large data pipelines and developing APIs to retrieve data, all to innovate the next level of analytics systems that effectively integrate with and enhance current research applications
Responsibilities:
•           Work with researchers and portfolio managers to analyze and improve the architecture of or proprietary and derived data sets
•           Solve interesting data engineering problems in support of quantitative researches leveraging time-series data
•           Implement cloud data platforms capable of handling the computational complexities of machine learning workflows on large data sets
•           Write code that provides reliable automated data pipelines, included automated quality checks
•           Take ownership of the data engineering code base, focusing on high-quality delivery of both code and data
•           Be a technical leader by building pipelines to explore and leverage alternative non-traditional data sources
Qualifications:
•           1-8 years’ relevant experience and a degree in Computer Science (or related field) with a strong GPA
•           Strong foundational skills in SQL, query performance tuning, and bash/shell scripting
•           Expert-level programming skills in Python and proficient with one of the following: Java/Scala, C/C++
•           Expertise in data modeling and choosing an appropriate model given the situation
•           Capital Markets and Asset Management experience and related datasets/vendors is a plus
•           Ability to provide thought leadership and contribute to the firm's overall technology strategy
Show more
Show less","Data Analytics, Data Storage, Data Pipelines, Data Quality, Machine Learning, Python, Java, Scala, C, C++, SQL, Bash, Shell Scripting, Data Modeling","data analytics, data storage, data pipelines, data quality, machine learning, python, java, scala, c, c, sql, bash, shell scripting, data modeling","bash, c, data quality, data storage, dataanalytics, datamodeling, datapipeline, java, machine learning, python, scala, shell scripting, sql"
Senior Data Engineer,Professional Diversity Network,"Montana, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788314244,2023-12-17,Montana,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a3eeb2f-47b9-4ae8-a9f5-b88c4789cf79
Show more
Show less","Data processing pipeline, Data centralization, Data curation, Data engineering, Data modeling, Data visualization, Data warehousing, Agile development, Testdriven development, API development, SQL, Netezza, Datastage, BitBucket, JIRA, Confluence, R, SAS, Python, SPSS, Healthcare, Data integration tools, Predictive modeling, Continuous delivery, Deployment automation, Tableau, Power BI","data processing pipeline, data centralization, data curation, data engineering, data modeling, data visualization, data warehousing, agile development, testdriven development, api development, sql, netezza, datastage, bitbucket, jira, confluence, r, sas, python, spss, healthcare, data integration tools, predictive modeling, continuous delivery, deployment automation, tableau, power bi","agile development, api development, bitbucket, confluence, continuous delivery, data centralization, data curation, data engineering, data integration tools, data processing pipeline, datamodeling, datastage, datawarehouse, deployment automation, healthcare, jira, netezza, powerbi, predictive modeling, python, r, sas, spss, sql, tableau, testdriven development, visualization"
Staff Data Engineer,Recruiting from Scratch,"Bozeman, MT",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744391938,2023-12-17,Montana,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, Hadoop, Hive, Pig, Oozie, Sqoop, Flume, Kafka, Storm, SparkStreaming, SQL, TDD, Pair Programming, Continuous Integration, ETL, Data Warehouses, Data Governance, Data Processing, Data Analysis, Data Science, Machine Learning","python, snowflake, airflow, kubernetes, docker, helm, spark, hadoop, hive, pig, oozie, sqoop, flume, kafka, storm, sparkstreaming, sql, tdd, pair programming, continuous integration, etl, data warehouses, data governance, data processing, data analysis, data science, machine learning","airflow, continuous integration, data governance, data processing, data science, data warehouses, dataanalytics, docker, etl, flume, hadoop, helm, hive, kafka, kubernetes, machine learning, oozie, pair programming, pig, python, snowflake, spark, sparkstreaming, sql, sqoop, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Bozeman, MT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395134,2023-12-17,Montana,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, ETL, Data Warehousing, Data Classification, Data Retention, Agile, TDD, Pair Programming, Continuous Integration, Continuous Delivery, Automated Testing, Deployment, Data Engineering, Business Intelligence, Data Science, Hadoop, D3.js","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, etl, data warehousing, data classification, data retention, agile, tdd, pair programming, continuous integration, continuous delivery, automated testing, deployment, data engineering, business intelligence, data science, hadoop, d3js","agile, airflow, automated testing, business intelligence, continuous delivery, continuous integration, d3js, data classification, data engineering, data retention, data science, datawarehouse, deployment, docker, etl, hadoop, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Bozeman, MT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833030,2023-12-17,Montana,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Automation, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention","data engineering, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","agile engineering, airflow, automated testing, automation, continuous delivery, continuous integration, data classification, data engineering, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Bozeman, MT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708633,2023-12-17,Montana,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Apache Airflow, Apache Kafka, Apache Spark, Apache Storm, Bash, Big data, Cloud computing, Data engineering, Data mining, Data modeling, Data normalization, Data pipelines, Data preprocessing, Data postprocessing, Data visualization, Docker, DynamoDB, ETL, Git, Hadoop, Helm, Java, Kubernetes, Machine learning, Microservices, Natural language processing, NoSQL, Pandas, Python, R, Relational databases, Snowflake, SQL, Stream processing, Text data processing","apache airflow, apache kafka, apache spark, apache storm, bash, big data, cloud computing, data engineering, data mining, data modeling, data normalization, data pipelines, data preprocessing, data postprocessing, data visualization, docker, dynamodb, etl, git, hadoop, helm, java, kubernetes, machine learning, microservices, natural language processing, nosql, pandas, python, r, relational databases, snowflake, sql, stream processing, text data processing","apache airflow, apache kafka, apache spark, apache storm, bash, big data, cloud computing, data engineering, data mining, data normalization, data postprocessing, data preprocessing, datamodeling, datapipeline, docker, dynamodb, etl, git, hadoop, helm, java, kubernetes, machine learning, microservices, natural language processing, nosql, pandas, python, r, relational databases, snowflake, sql, stream processing, text data processing, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Bozeman, MT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086804,2023-12-17,Montana,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Mining, Data Cleaning, Data Normalizing, Data Modeling, Data Platforms, Data Frameworks, Data Governance, Data Compliance, Data Infrastructure, ML Data Engine, ML Data Ops, ML Models, Data Preprocessing, Data Postprocessing, Data Pipelines, Data Extraction, Data Ingestion, Data Normalization, Data Processing, Data Analytics, Data Visualization, Pandas, R, NLP, Large Language Models, Text Data Processing, Stream Processing, Big Data","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data mining, data cleaning, data normalizing, data modeling, data platforms, data frameworks, data governance, data compliance, data infrastructure, ml data engine, ml data ops, ml models, data preprocessing, data postprocessing, data pipelines, data extraction, data ingestion, data normalization, data processing, data analytics, data visualization, pandas, r, nlp, large language models, text data processing, stream processing, big data","airflow, aws, azure, bash, big data, data cleaning, data compliance, data extraction, data frameworks, data governance, data infrastructure, data ingestion, data mining, data normalization, data normalizing, data platforms, data postprocessing, data preprocessing, data processing, dataanalytics, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, large language models, machine learning, ml data engine, ml data ops, ml models, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm, stream processing, text data processing, visualization"
Senior Incentives and Proficiency Data Analyst,Threat Tec,"North Liberty, NC",https://www.linkedin.com/jobs/view/senior-incentives-and-proficiency-data-analyst-at-threat-tec-3574146143,2023-12-17,Winston-Salem,United States,Mid senior,Hybrid,"Threat Tec, LLC is the leader of Operational Environment (OE) replication and Threat Emulation/Wargaming solutions . Threat Tec brings innovati ve thinking and extensive experience to complex challenges for public and private sector customers . W e work alongside our nation's defenders, developing solutions that drive success and protect our future. Join a team that is embodied by an unwavering commitment to professionalism, honesty, and innovation .
Threat Tec , LLC , a Veteran-Owned Small Business, has an immediate opening for a Senior Incentives and Proficiency Data Analyst to join our growing company in Fort Bragg, NC .
DUTIES AND RESPONSIBILITIES BUT NOT LIMITED TO:
Support the database and Foreign Language Proficiency Bonus (FLPB) task, this includes input and query to the Special Operations Foreign Language Office (SOFLO) database in order to assess and analyze language training database statistics; thereby enhancing the effectiveness of the Component Training Program.
Collect and synchronize all data and databases from subordinate commands for the purpose of conducting consolidated analysis and compiling required reports from subordinate Commands.
Manage the FLPB Program for the organization, IAW Service and organizational policy.
Develop new and improve existing policies and procedures.
The contractor shall be responsible for managing all paperwork and procedures to assist the organization change Control Languages for applicable SOF personnel through Service processes.
MINIMUM QUALIFICATIONS AND EDUCATION REQUIREMENTS:
At least 10-years’ experience working on military pay or personnel issues
Able to use all current Microsoft Office applications and SharePoint.
Top Secret Clearance required
TRAVEL:
Travel is required to various CONUS and locations.
Nothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time. This description reflects management’s assignment of essential functions; it does not proscribe or restrict the tasks that may be assigned. This job description is subject to change at any time.
Threat Tec , LLC is an Equal Employment Opportunity/Affirmative Action Employer s (EEO/AA). All employment and hiring decisions are based on qualifications, merit, and business needs without regard to race, religion, color, sexual orientation, nationality, gender, ethnic origin, disability, age, sex, gender identity, veteran status, marital status, or any other characteristic protected by applicable law.
If you are a qualified individual with a disability and/or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to access job openings or apply for a job on this site as a result of your disability. You can request assistance by contacting
HR@threattec.com
or calling 757-
240
-4305.
#TT
Show more
Show less","Microsoft Office, SharePoint, Top Secret Clearance, Special Operations Foreign Language Office (SOFLO) database","microsoft office, sharepoint, top secret clearance, special operations foreign language office soflo database","microsoft office, sharepoint, special operations foreign language office soflo database, top secret clearance"
Data Analyst,Hays,"Portsmouth, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-hays-3779817567,2023-12-17,South Hampshire, United Kingdom,Associate,Hybrid,"Your new company
You will be joining a joint venture that has the pedigree of two huge companies, but the ethos of a small business where people are valued and empowered to succeed. It's a great place to work, and they bring opportunity for everyone in the team. This is an exciting and pivotal time to join the organisation, the planned growth and development is definitely something to be part of.
Your new role
You will be a Data Analyst within the Delivery Capability Team with responsibility for providing the business with Analytics, Management Information and Business Information reporting.
Information types within the remit of the Analytics team cover both MOD asset information (utilising graphical, structured and unstructured types across strategic, operational and project teams) and operational business information used for delivery of the contract and operation of the joint venture business.
What you'll need to succeed
Experience of delivering reporting and analytics solutions using Microsoft Power Platform, especially Power BI.
Knowledge of data modelling, data quality, data governance and data security principles and practices.
The ability to communicate effectively with stakeholders at all levels and translate and present findings to non-technical audiences.
Experience of working in a complex and dynamic environment with multiple clients and partners.
What you'll get in return
A competitive pension scheme with up to 8% employer contribution.
Discretionary bonus.
Employee assistance programme.
Annual events.
A supportive and collaborative work culture.
A chance to work on exciting and challenging projects that make a difference.
An opportunity to develop your skills and career.
Show more
Show less","Microsoft Power Platform, Power BI, Data modeling, Data quality, Data governance, Data security, Data analytics, Analytics, Stakeholder communication, Complex and dynamic environment, Multiple clients and partners, Presentation skills, Reporting, Management information, Business information","microsoft power platform, power bi, data modeling, data quality, data governance, data security, data analytics, analytics, stakeholder communication, complex and dynamic environment, multiple clients and partners, presentation skills, reporting, management information, business information","analytics, business information, complex and dynamic environment, data governance, data quality, data security, dataanalytics, datamodeling, management information, microsoft power platform, multiple clients and partners, powerbi, presentation skills, reporting, stakeholder communication"
Power BI Data Analyst,University of Portsmouth,"Portsmouth, England, United Kingdom",https://uk.linkedin.com/jobs/view/power-bi-data-analyst-at-university-of-portsmouth-3776600859,2023-12-17,South Hampshire, United Kingdom,Mid senior,Onsite,"Power BI Data Analyst
The job requirements are detailed below. Where applicable the skills, qualifications and memberships required for this job have also been included.
Job details
Salary: £39,347 - £42,978 per annum
Contractual hours: 37
Basis: Full-Time
Job category/type: Fixed Term - Full-Time
Job reference: REC00001182
Closing Date: 02/01/2024
Job description
The University of Portsmouth is a global employer of choice where exceptional people create, share and apply knowledge that makes a difference.
Experience the pride of being part of a select group – one of only four universities in the south-east of England to achieve a prestigious Gold rating in the Teaching Excellence Framework. Additionally, we proudly rank among the top 140 universities globally in the Times Higher Young University World Rankings.
Ambition, Responsibility and Openness drive our every endeavour. Join our esteemed institution with a proven track record of success, and where dedication to excellence is key. We want people to make their mark in a professional community that truly values people, innovation, and achievement.
THE ROLE:
With a real drive to provide data to drive action, the Power BI Data Analyst(s) will play a crucial role in helping us to provide a more proactive data insights service. This role will require extraction of data from source systems, loading and transforming of the data and then building dashboards and reports in Power BI for University-wide consumption. The outputs will be used by several departments across the University to improve the staff and student experience, increase student retention and address data quality in all of our key systems of record.
To be successful you will have proven experience of using Microsoft Power BI to build reports, data models and dashboards.
Fixed term contract until 31st July 2024.
For internal candidates it may be possible to apply for this role under the Secondment Policy . If you wish to apply as a secondee, you must have approval of your line manager in advance of submitting your application form. Please confirm in your application that your line manager has approved this.
DISCOVER THE ADVANTAGES OF JOINING OUR UNIVERSITY. WE OFFER A RANGE OF ATTRACTIVE BENEFITS AND OPPORTUNITIES TO ENHANCE YOUR WORK EXPERIENCE:
Competitive salary, including incremental progression within your scale.
Contributory defined benefits pension schemes – Local Government Pension Scheme (LGPS) for support and research staff and Teachers Pensions for our academic staff.
Generous leave entitlement of 32–35 days a year, plus bank holidays, and an additional Christmas closure.
Family-friendly policies supporting flexible working.
Staff discounts and loyalty schemes.
Staff car parking and discounted public travel.
Excellent training and development opportunities.
Staff wellbeing programmes.
Long service awards and recognition awards, which are awarded to staff for exceptional performance.
Recreation facilities, including discounted gym membership, food on campus schemes, use of the Library and staff social activities.
Discounted learning.
Start-up business mentoring from expert entrepreneurs via our Entrepreneurs in Residence programme.
The perks don’t stop there - click this link for further information on My Reward and Benefits
UKVI STATEMENT
Prior to submitting your application, kindly ensure that you can either demonstrate or acquire the necessary right to work in the UK. If you currently do not possess the right to work in the UK, please be aware that our offer of employment is conditional upon you obtaining it.
ED&I STATEMENT:
We are strongly dedicated to embedding equality, diversity and inclusion (EDI) within our community. As an Athena SWAN and Race Equality Charter award holder, a member of Stonewall and a Disability Confident Employer we are passionate about creating a welcoming and inclusive environment, regardless of your background. We welcome applications from all talented people. In addition, we want our workforce to be representative of our diverse student population. Please see our EDI Framework and objectives.
Show more
Show less","Power BI, Data Analyst, Data Extraction, Data Loading, Data Transformation, Dashboard Design, Report Design, Data Quality Management, Microsoft Office Suite, SQL, Python","power bi, data analyst, data extraction, data loading, data transformation, dashboard design, report design, data quality management, microsoft office suite, sql, python","dashboard design, data extraction, data loading, data quality management, data transformation, dataanalytics, microsoft office suite, powerbi, python, report design, sql"
Senior Data Engineer,SSE plc,Greater Portsmouth Area,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-sse-plc-3784560969,2023-12-17,South Hampshire, United Kingdom,Mid senior,Hybrid,"SSE has big ambitions to be a leading energy company in a low carbon world. Following our commitment to invest £20.5 billion in low carbon projects to 2027, we have significant growth plans and are well on our way to achieving our ambition to build a world that's more sustainable and inclusive for you, your family, the community you live in and for generations to come.
Join us on our journey to net zero and help us power change.
About The Role
Base Location:
Reading or Havant
Salary:
£38,700 - £58,100 + performance related bonus + a range of benefits to support your finances, wellbeing and family.
Working Pattern:
Permanent | Full Time | Flexible First options available
What is the Role?
As Senior Data Engineer your role is to build, optimise and maintain ETL, data applications, systems and services that comply with technical design and security guidelines and guardrails.
You'll provide leadership and accountability for ensuring the ongoing maintenance and developed service improvements to our core applications as well as analysing system requirements and designing, testing, modelling and modifying data pipelines and scripts to agreed company standards. Collaborating with product teams to deliver business outcomes in agile, DevOps environment is key to this role.
What do I need?
To be considered for this role, we would love you to have:
Understanding of the full software development lifecycle (SDLC).
Experience with designing, building, and operating analytics solutions using Azure cloud technologies with extensive experience in Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
Working knowledge of Synapse and ETL technologies.
Solid experience of relational databases and data storage technologies such as: SQL Server, Oracle, MongoDB, MySQL, NoSQL, Data Warehousing and Databricks with experience in interpreting data, analysing results using statistical techniques.
Fully conversant with Agile and DevOps development methodology and concepts as applied to data driven analytics projects including CI/CD Coding, security testing best practice and standards.
Proven knowledge of report building with Power BI skill and experience of Python (including Pandas, Numpy, PySpark, Jupyter, etc) IT.
Microsoft Azure certifications are a plus.
About Our Business
SSE IT underpins the technology needs of all the different businesses that make up the SSE group. From emerging technologies to data and analytics to cyber security - we power SSE's growth and enable it to generate value, while keeping it secure. As a trusted business partner that helps SSE lead in a low carbon world, we are proud of our service. Working for SSE IT is all about equipping SSE for now and the future.
What's in it for you?
We offer an excellent package with 34 days annual leave entitlement. Enhanced maternity/paternity leave, discounted healthcare, salary sacrifice car leasing and much more, view our full benefits package on our careers site.
As an equal opportunity employer we encourage diversity and are committed to creating an inclusive environment for all employees. We encourage applicants from all protected characteristics and commit to providing any reasonable adjustments you need during the application, assessment and upon joining SSE. Search for 'Inclusion & Diversity at SSE' to find out more.
Next Steps
All applications should be made online, and I'll be back in touch after the vacancy closing date to let you know the outcome.
If you would like to discuss any working flexibly requirements or adjustments you may require throughout the recruitment and selection process, please contact David on David.Brickell@sse.com / 01738 275846.
Before commencing your role with SSE, you'll need to complete our pre-employment screening process. This will consist of a criminality and credit check.
Show more
Show less","Software Development Lifecycle (SDLC), Azure cloud technologies, Cloud migration methodologies, Azure Data Factory, Event Hub, Synapse, ETL technologies, SQL Server, Oracle, MongoDB, MySQL, NoSQL, Data Warehousing, Databricks, Python, Datadriven analytics projects, CI/CD Coding, Power BI, Pandas, Numpy, PySpark, Jupyter, Microsoft Azure certifications, Agile, DevOps","software development lifecycle sdlc, azure cloud technologies, cloud migration methodologies, azure data factory, event hub, synapse, etl technologies, sql server, oracle, mongodb, mysql, nosql, data warehousing, databricks, python, datadriven analytics projects, cicd coding, power bi, pandas, numpy, pyspark, jupyter, microsoft azure certifications, agile, devops","agile, azure cloud technologies, azure data factory, cicd coding, cloud migration methodologies, databricks, datadriven analytics projects, datawarehouse, devops, etl technologies, event hub, jupyter, microsoft azure certifications, mongodb, mysql, nosql, numpy, oracle, pandas, powerbi, python, software development lifecycle sdlc, spark, sql server, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Southampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728582558,2023-12-17,South Hampshire, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Database Support Engineer,Ordnance Survey,"Southampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/database-support-engineer-at-ordnance-survey-3784391470,2023-12-17,South Hampshire, United Kingdom,Mid senior,Hybrid,"Our growing Technology and Design team plays a key role in ensuring OS is at the cutting edge of geospatial capability and is looking for people to join them. Its mission is to work across the business to provide customer centric design and technology services.
Join us and you’ll have an opportunity to make an impact. To empower projects that deliver real-world benefits across Britain and internationally. To hear our customers say they couldn’t have done it without us. And to be central to OS’s vision: to be recognised as world leaders in geospatial services; creating location insight for positive impact.
About the role
As an MS SQL Server Database Administrator with at least 3 years of experience you will join our highly motivated and supportive DBA Team where the role will involve the following:
Security – Build and maintain making sure that best practice is set and adhered to with documentation to support.
Backup – Plan, implement and execute procedures for backup and recovery.
Monitoring – Monitoring at scale is intended with up to a thousand databases located in both Ordnance Survey data centre and Azure Cloud.
Performance – Tuning is required for both our on-prem databases and Azure cloud.
Cost Optimisation – Always looking to remove waste and reduce cost.
Communication – Being proactive is key to build and maintain relationships with customers and those that you will work with.
Ordnance Survey Technology team is working to develop the next generation of cloud-based systems, that drive our world leading geospatial and business applications. This will provide great learning and career development where you will play a key role in finding opportunities to improve performance and reduce cost.
What we’re looking for
If you are interested in joining a team that lies at the heart of what Ordnance Survey is about, we are looking for someone that can demonstrate essential skills and experience in:
MS SQL Server versions 2012 to 2019.
T-SQL and database administration tools.
Microsoft SQL Server and high availability for both on-prem and Azure. Upgrades, Security, Patching and Performance.
Evidence of proposing or implementing efficiency improvements.
Great investigative and problem-solving skills.
Ability to prioritise tasks and deliver to deadlines.
Desirable Skills
Knowledge of SSIS, SSRS.
Oracle WebLogic server installation and maintenance.
Knowledge of Databases running PostgreSQL and PostGIS.
Skills in Oracle Database, RAC and Oracle OEM 13c.
Familiar with RMAN, RMAN Catalog, ASM, Dataguard.
SQLNet and Connection configuration, understanding of firewalls and RAC scan configuration.
Linux to Administrative level.
Exposure to Oracle Geospatial and Workspace manager.
We’ll give you
Salary – £ 37,511 - £ 44,130
Performance related bonus
A competitive pension scheme
We adopt flexible working and can consider different working hours dependent on the role and your personal circumstances
25 days annual leave - (rising by 1 day each year to 30 days after five years) bank holidays and an extra 3 days over Christmas
Plus, a suite of excellent additional benefits
Location
We embrace a hybrid working model at OS and understand there is no one size fits all in relation to how we work. We have a fantastic HQ in Southampton, Hampshire where you will be required to spend 2 days a week with the remaining time being a personal choice as to whether you work from home, one of our regional offices or spend more time at OSHQ.
The team operates a rota system to ensure that the hours between 08:00 and 17:00, Monday to Friday are supported.
Additionally, the team provide 24/7 support for Production Databases out of hours, and you will be expected to join the on-call team when a vacancy arises.
This will be on a rota-based system, with an allowance of up to £300 per month.
Security
OS conducts DBS background checks for all joiners and some of our roles require additional security clearance, including to SC or DV level in some cases. We will tell candidates at the appropriate time during the recruitment process if additional clearance will be required for this role.
Closing date
: Sunday 31 December 2023
We believe diversity and inclusion is about working together – in an encouraging and respectful environment to reach our full potential. We believe combining different backgrounds, experiences and perspectives will help us reach our vision and be trusted and admired across the globe for setting the standards and leading the way.
We are looking for passionate people from a range of backgrounds and welcome applications from any race, age, gender, background or religion.
We’re individually talented and collectively powerful, and we give you the space to take your career in whichever direction you want.
Show more
Show less","Database Administration, MS SQL Server, TSQL, Oracle WebLogic Server, PostgreSQL, PostGIS, Oracle Database, RAC, Oracle OEM 13c, RMAN, RMAN Catalog, ASM, Dataguard, SQLNet, Linux, Oracle Geospatial, Workspace Manager, SSIS, SSRS","database administration, ms sql server, tsql, oracle weblogic server, postgresql, postgis, oracle database, rac, oracle oem 13c, rman, rman catalog, asm, dataguard, sqlnet, linux, oracle geospatial, workspace manager, ssis, ssrs","asm, database administration, dataguard, linux, ms sql server, oracle database, oracle geospatial, oracle oem 13c, oracle weblogic server, postgis, postgresql, rac, rman, rman catalog, sqlnet, ssis, ssrs, tsql, workspace manager"
Commercial Pricing Specialist - Data Analyst,Hays,"Eastleigh, England, United Kingdom",https://uk.linkedin.com/jobs/view/commercial-pricing-specialist-data-analyst-at-hays-3783495347,2023-12-17,South Hampshire, United Kingdom,Mid senior,Hybrid,"Your new company
A leading provider of insurance and financial services in the UK and internationally, with a vision of being the most loved and trusted brand in the market. With over 30 million customers and 300 years of heritage, this company offers a diverse and inclusive workplace where you can grow and thrive.
Your new role
As a Commercial Pricing Specialist, you will play a key role in ensuring that the company's products and services are priced fairly and competitively across different customer segments and distribution channels. You will work closely with various teams, such as proposition, pricing, conduct and assurance, to review product performance, recommend actions, and provide regular and ad-hoc reporting.
You will also act as a customer advocate in key pricing decision processes, ensuring that the company meets its consumer duty obligations and delivers value to its customers.
What you'll need to succeed
To succeed in this role, you will need:
Experience producing management information which analyses trends in the commercials for products or services, providing recommendations to improve performance.
Confidence to present your own analysis to key stakeholders with a mix of commercial and non-commercial backgrounds, tailoring the delivery to the audience.
Experience of, or enthusiasm to learn data analytic tools, such as SAS and RStudio.
Independent thinking, able to manage, create and prioritise your own workload.
What you\'ll get in return
Competitive annual salary of up to £55,000
Discretionary bonus opportunity of 10% of annual salary
Generous pension scheme - with employer contribution up to 14%, depending on what you put in.
29 days holiday plus bank holidays
Brilliantly supportive policies, including parental and carer's leave.
Flexible benefits to suit you, including sustainability options such as cycle to work.
A whole host of other perks and benefits - the list goes on!
What you need to do now
If you\'re interested in this role, click \'apply now\' to forward an up-to-date copy of your CV, or call us now.
If this job isn\'t quite right for you, but you are looking for a new position, please contact us for a confidential discussion about your career.
Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.uk
Show more
Show less","Data Analysis, Reporting, Communication, SAS, RStudio, Problem Solving, Microsoft Office","data analysis, reporting, communication, sas, rstudio, problem solving, microsoft office","communication, dataanalytics, microsoft office, problem solving, reporting, rstudio, sas"
Staff Data Engineer,Recruiting from Scratch,"Tucson, AZ",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392703,2023-12-17,Tucson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Data Warehouses, ETL, Kafka, Storm, Spark Streaming, TDD, Pair Programming, Continuous Integration, Agile, Automated testing, Deployment, Dimensional data modeling, Schema design","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, data warehouses, etl, kafka, storm, spark streaming, tdd, pair programming, continuous integration, agile, automated testing, deployment, dimensional data modeling, schema design","agile, airflow, automated testing, continuous integration, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748826681,2023-12-17,Tucson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Data management tools, Data classification, Data retention, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Business Data Analyst,Intuit,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-business-data-analyst-at-intuit-3780412686,2023-12-17,Tucson,United States,Mid senior,Onsite,"Overview
The SBSEG Customer Success (CS) Analytics team has an exciting opportunity to improve customer experiences using analytics. We are seeking an exceptional Sr. Business Data Analyst to make a difference and become part of our core team. We are looking for a data-driven individual with excellent data and analytical skills and a passion for delivering data-driven insights.
This is an exciting time to join Customer Success at Intuit as we are building an analytics function to help our customers and business stakeholders. This is your opportunity to join this exciting and transformational journey.
What You'll Bring
Experienced practicioner in data and information space with 5+ years of experience working in Product Analytics, Marketing Analytics, Operations Analytics or other data-driven strategic insights analytic role
Bachelor’s Degree in Statistics, Mathematics, Data Analytics, Finance, Advanced Analytics, or related field; Master’s Degree preferred
Proficient in SQL, Excel, and data visualization software such as Qlik, Tableau, PowerBI, etc.
Experienced in experimentation design and execution
Ability to manage multiple projects simultaneously to meet objectives and key deadlines, balancing short & long term strategic implications to meet and/or exceed business objectives
Outstanding communication skills across a diverse audience of business stakeholders in both technical and non-technical forums
Ability to tell stories with data, educate effectively, and instill confidence, motivating stakeholders to act on recommendations
Experience working in a large matrixed organization, aligning cross functional stakeholders, and serving as the glue that holds it all together
Ability to collaborate across organizational boundaries to solve complex problems and apply data-driven decision making principles
Proficient organizational skills, documentation, time management, portfolio prioritization experience, and accountability required
Technical undergraduate degree required (Engineering, Computer Science, Statistics, Analytics etc.) or equivalent experience
Some Travel required (~10%)
How You Will Lead
Partner with a diverse group of business functions, team members, and other analytics teams to enable decision support and key customer insights for the Small Business Self Employed Group (SBSEG)
Collaborate with stakeholders and other analysts to ensure analytical solutions are scalable, repeatable, effective, and meet expectations of various stakeholders
Provide guidance and thought partnership to business leaders and stakeholders on how best to harness available data in support of critical business needs and goals
Manage the full cycle of iterative big exploration, including hypothesis formulation, data generation and cleansing, testing, insight generation/visualization, and action planning
Leverage analytics acumen to develop data-backed insights from complex data sources and make strategic business decisions for unique business problems
Provide business stakeholders with entrepreneurial guidance essential for appropriately interpreting and building on findings, and fully incorporating the insights revealed through the research
Deliver analytically sound insights and recommendations as a working manager (player-coach role) and promote a growth and development culture for the team
Show more
Show less","Data Analytics, Statistics, Mathematics, SQL, Excel, Qlik, Tableau, PowerBI, Experimentation Design, Project Management, Communication, Data Visualization, DataDriven Decision Making, Organizational Skills, Documentation, Time Management, Portfolio Prioritization, Collaboration, Hypothesis Formulation, Data Generation, Data Cleansing, Testing, Insight Generation, Action Planning, Business Intelligence, Big Data, Data Exploration, Data Interpretation, Data Mining, Data Warehousing, Data Modeling, Machine Learning, Artificial Intelligence","data analytics, statistics, mathematics, sql, excel, qlik, tableau, powerbi, experimentation design, project management, communication, data visualization, datadriven decision making, organizational skills, documentation, time management, portfolio prioritization, collaboration, hypothesis formulation, data generation, data cleansing, testing, insight generation, action planning, business intelligence, big data, data exploration, data interpretation, data mining, data warehousing, data modeling, machine learning, artificial intelligence","action planning, artificial intelligence, big data, business intelligence, collaboration, communication, data exploration, data generation, data interpretation, data mining, dataanalytics, datacleaning, datadriven decision making, datamodeling, datawarehouse, documentation, excel, experimentation design, hypothesis formulation, insight generation, machine learning, mathematics, organizational skills, portfolio prioritization, powerbi, project management, qlik, sql, statistics, tableau, testing, time management, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744394323,2023-12-17,Tucson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehouses, etl, data classification, data retention","airflow, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709530,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Mining, Pandas, R, Airflow, KubeFlow, SQL, Python, Java, Bash, Git, Snowflake, Kubernetes, Docker, Helm, Spark, Spark Streaming, Kafka, Storm, DynamoDB, Machine Learning","data engineering, data mining, pandas, r, airflow, kubeflow, sql, python, java, bash, git, snowflake, kubernetes, docker, helm, spark, spark streaming, kafka, storm, dynamodb, machine learning","airflow, bash, data engineering, data mining, docker, dynamodb, git, helm, java, kafka, kubeflow, kubernetes, machine learning, pandas, python, r, snowflake, spark, spark streaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090515,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming","airflow, aws, azure, bash, data engineering, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Electrical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-electrical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3775561451,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a WSP company, is looking for a
Senior Electrical Engineer
for our
kW Tempe, Arizona office.
As an Electrical Engineer with us, you will design complex power and other building systems including generator plants, medium voltage distribution, uninterruptible power systems, lighting, fire alarm, and grounding.
Your Impact
Design electrical systems for buildings including lighting, receptable, general, essential, and critical electrical infrastructure
Work in a team or independently, planning and executing engineering tasks within projects
Demonstrate significant understanding of the range of services provided by the kW MCE engineering teams & related practices
Independently, support the team during design and construction stages of projects
Lead the electrical design of complex projects
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Lead and mentor entry-level and junior engineers
Collaborate and coordinate with internal project discipline team members, equipment vendors, and manufacturers
Communicate complex electrical engineering concepts and decisions to clients and stakeholders
Integrate complex electrical engineering requirements into facility designs
Interact regularly with clients to maintain current relationships and develop new relationships
Lead meetings with internal and external stakeholders
Research & recommend fundamental components identified in electrical designs
Develop equipment rooms layouts, floor plan, and one-line diagrams
Schedule equipment
Prepare short circuit, coordination and arc flash calculations
Perform construction administration tasks
Survey and evaluate existing conditions
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client and construction team members.
Required Qualifications
Bachelor’s degree in Electrical Engineering or Architectural Engineering with electrical building systems emphasis
5+ years of experience in designing electrical systems for the high performing, commercial, industrial or mission critical/data center buildings.
Excellent interpersonal skills, teamwork, and communication skills, both written and verbal
Proficiency with applicable software packages including AutoCAD, Revit, SKM, eTap or Cyme.
Knowledge of building, electrical and energy codes.
Ability to organize and present design information to project staff
Attention to detail, highly organized, self-starter
Ability to travel to project sites
Preferred Qualifications:
EIT or Registered Professional Engineer (PE), if applicable
Experience with the analysis and modeling of short circuit, coordination, and arc flash analysis
Experience with the design of highly reliable, robust and concurrently maintainable Medium and Low Voltage infrastructure
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 20%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-MO-Kansas City
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, AutoCAD, SKM, eTap, Cyme, Electrical engineering, Power systems, Generator plants, Medium voltage distribution, Uninterruptible power systems, Fire alarm, Grounding, Lighting design, Receptacle design, Infrastructure design, Electrical systems design, Construction administration, Building Information Modeling, Liaising, Short circuit analysis, Coordination analysis, Arc flash analysis, Medium voltage infrastructure, Low voltage infrastructure, Data center design","revit, autocad, skm, etap, cyme, electrical engineering, power systems, generator plants, medium voltage distribution, uninterruptible power systems, fire alarm, grounding, lighting design, receptacle design, infrastructure design, electrical systems design, construction administration, building information modeling, liaising, short circuit analysis, coordination analysis, arc flash analysis, medium voltage infrastructure, low voltage infrastructure, data center design","arc flash analysis, autocad, building information modeling, construction administration, coordination analysis, cyme, data center design, electrical engineering, electrical systems design, etap, fire alarm, generator plants, grounding, infrastructure design, liaising, lighting design, low voltage infrastructure, medium voltage distribution, medium voltage infrastructure, power systems, receptacle design, revit, short circuit analysis, skm, uninterruptible power systems"
Lead Electrical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Tucson, AZ",https://www.linkedin.com/jobs/view/lead-electrical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3691009076,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
What You’ll Do:
kW Mission Critical Engineering
is currently initiating a search for a
Lead Electrical Engineer
that can be located for our
kW Tempe, Arizona office or our St. Louis, MO office.
As a Lead Electrical Engineer with us, you will design complex power and other building systems including generator plants, medium voltage distribution, uninterruptible power systems, lighting, fire alarm, and grounding while leading projects and a team of electrical engineers.
Your Impact
Produce high quality technical and professional deliverables for projects and proposals
Apply deep knowledge of engineering techniques across multiple technical functions
Utilize advanced analytical and design techniques to solve technical problems
Exemplify well-developed advanced experience in electrical discipline
Lead the development of initial electrical system concepts
Present complex technical solutions to clients
Manage and coordinate project teams and projects
Develop work plans to address technical issues within project time and budget
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend and lead client meetings
Manage and mentor junior staff
Collaborate and coordinate with internal project discipline team members, equipment vendors and manufacturers
Perform project management activities including writing proposals, establishing budgets, and managing client interactions
Coordinate activities concerned with technical development, scheduling, and resolving engineering design issues
Coordinate the activities of technical staff from project award through project completion
Design complex and large electrical medium voltage and low voltage distribution systems and electrical building systems (i.e. general power, lighting, grounding, etc.)
Survey and evaluation of existing conditions
Develop project specifications
Perform construction administration
Develop and maintain client relationships
Contribute and interact with team, develop and manage high quality technical and professional deliverables on projects and proposals
Participate in local professional organization (attend meetings/lectures), i.e., poster sessions, participate in conference panel
Exercise responsible and ethical decision-making regarding company funds, resources and conduct and adhere to WSP""s Code of Conduct and related policies and procedures
Proven track record of upholding workplace safety and ability to abide by WSP""s health, safety and drug/alcohol and harassment policies
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client and construction team members. Candidate will have previous experience as a lead project electrical engineer capable of directing the project team.
Required Qualifications
Bachelor’s degree in Electrical Engineering or Architectural Engineering with electrical building systems emphasis
7+ years of experience in designing electrical systems for the high performing, commercial, industrial or mission critical/data center buildings
Registered Professional Engineer (PE)
Experience mentoring and training others in field
Strong verbal and written communication skills
Ability to interact well with others as well as develop and contribute to high quality technical and professional deliverables on projects and proposals
Strong working knowledge of electrical systems and codes
Attention to detail, highly organized, self-starter
Participate in conference programs including panels, lectures, poster sessions, papers and presentations
Preferred Qualifications:
Enhancing credentials (LEED, Uptime ATD, etc.) preferred
Experience with the analysis and modeling of short circuit coordination and arc flash studies
Mission Critical/Data Center experience
Experience with international projects and knowledge of international codes and standards
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 10%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-MO-Creve Coeur, US-MO-St Louis
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, Building Information Modeling, Arc flash studies, Short circuit coordination, Construction management, Electrical Engineering, Architectural Engineering, LEED, Uptime ATD, AutoCAD, Multimeters, SolidWorks, MicroStation, Siemens NX, SAP ERP, Control systems, PLC programming, Instrumentation, Project management, Communication skills, Attention to detail, Teamwork","revit, building information modeling, arc flash studies, short circuit coordination, construction management, electrical engineering, architectural engineering, leed, uptime atd, autocad, multimeters, solidworks, microstation, siemens nx, sap erp, control systems, plc programming, instrumentation, project management, communication skills, attention to detail, teamwork","arc flash studies, architectural engineering, attention to detail, autocad, building information modeling, communication skills, construction management, control systems, electrical engineering, instrumentation, leed, microstation, multimeters, plc programming, project management, revit, sap erp, short circuit coordination, siemens nx, solidworks, teamwork, uptime atd"
Lead Mechanical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Tucson, AZ",https://www.linkedin.com/jobs/view/lead-mechanical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3786441113,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a member of WSP USA
, is currently initiating a search for a
Lead Mechanical Engineer
that can be located out of our
kW Tempe, Arizona office or St. Louis, MO office.
As a Mechanical Engineer with us, you will design complex cooling and HVAC systems including air distribution systems, chiller plants, and alternative energy solutions for mission critical facilities.
Your Impact
Independently support the project team during design and construction stages of projects
Design air distribution, hydronic and automated temperature controls systems
Integrate complex mechanical engineering requirements into facility designs
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Collaborate and coordinate with internal project discipline team members and external vendors and manufacturers
Communicate mechanical engineering concepts and decisions to clients and stakeholders
Interact regularly with clients, which includes maintaining current relationships and developing new relationships
Mentor and train junior engineers
Track and coordinate all mechanical disciplines: HVAC, Energy, Controls, Fire Protection, Fire Alarm, Plumbing, Fuel Oil Storage / Management / Distribution
Provide oversight of all aspects of mechanical design, review systems, drawings prior to issuance
Perform Computational Fluid Dynamic (CFD) evaluations for existing and new facilities, both internal and external
Select and schedule major equipment
Develop project specifications
Survey and evaluate existing conditions
Perform construction administration tasks
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client, and construction team members. Candidate should be willing to travel to client sites.
Required Qualifications
Bachelor’s degree in Mechanical Engineering or Architectural Engineering with mechanical building systems emphasis
7+ years of experience in designing mechanical systems for the high performing, commercial, industrial or mission critical/data center buildings
EIT or Registered Professional Engineer (PE), if eligible
Excellent interpersonal skills, teamwork, and written and verbal communication skills
Proficiency with applicable software including AutoCAD, Revit, Trane Trace and Pipeflow
Knowledge of building, mechanical and energy codes
Preferred Qualifications:
Experience with the analysis and modeling of interior and exterior Computational Fluid Dynamics of airflow
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 10%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-NV-Las Vegas
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Mechanical Engineering, HVAC systems, Revit, AutoCAD, Trane Trace, Pipeflow, Computational Fluid Dynamics (CFD), Building Information Modeling (BIM), Project Management, Design, Construction, Energy Codes, Fire Protection, Plumbing","mechanical engineering, hvac systems, revit, autocad, trane trace, pipeflow, computational fluid dynamics cfd, building information modeling bim, project management, design, construction, energy codes, fire protection, plumbing","autocad, building information modeling bim, computational fluid dynamics cfd, construction, design, energy codes, fire protection, hvac systems, mechanical engineering, pipeflow, plumbing, project management, revit, trane trace"
Senior Lead Mechanical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Tucson, AZ",https://www.linkedin.com/jobs/view/senior-lead-mechanical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3691002983,2023-12-17,Tucson,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a member of WSP USA
, is currently initiating a search for a
Senior Lead Mechanical Engineer
for our
kW Tempe, Arizona office: 40 E. Rio Salado Parkway, Suite 445 Tempe AZ 85281.
As a Senior Lead Mechanical Engineer with us, you will design complex cooling and HVAC systems including air distribution systems, chiller plants, and alternative energy solutions while leading projects and a team of mechanical engineers.
Your Impact
Produce high quality technical and professional deliverables for projects and proposals
Design complex and large air distribution (supply, return, exhaust) and hydronic systems (chilled water, heating water, etc.)
Design Automatic building automation controls systems and specification for large air distribution and heat rejection systems
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and lead discussions
Communicate complex mechanical engineering concepts and decisions to clients and stakeholders
Directing complex mechanical engineering requirements and integrate into facility designs
Collaborate and coordinate with internal project discipline team members and external equipment vendors and manufacturers
Act as the lead mechanical engineer on projects and direct the project team
Lead and mentor a team of engineers
Perform project management activities including writing proposals, establishing budgets, and managing client interactions
Coordinate activities concerned with technical development, scheduling, and resolving engineering design issues
Survey and evaluate existing conditions
Perform construction administration
Who You Are
The ideal candidate is a technically sound mechanical engineer with the ability to lead and develop complex projects and foster the growth of junior engineers. Candidate has strong communication skills, and an interest in developing and maintaining relationships with internal and external design, client, and construction team members. Candidate is willing to travel to client sites.
Required Qualifications:
Proven history of Mechanical or Architectural Engineering with mechanical building systems emphasis
10+ years of experience in designing mechanical systems for the high performing, commercial, industrial or mission critical/data center buildings
Registered Professional Engineer (PE)
Experience mentoring and training others in field
Strong verbal and written communication skills
Ability to interact well with others as well as develop and contribute to high quality technical and professional deliverables on projects and proposals
Strong working knowledge of mechanical systems and codes
Proven track record of upholding workplace safety and ability to abide by WSP’s health, safety and drug/alcohol and harassment policies
Must be flexible to a variety of schedules to meet business needs and able to prioritize responsibilities and quickly adapt to change in a pressure work environment
Occasional travel may be required depending on project-specific requirements
Preferred Qualifications
Experience with the analysis of interior and exterior Computational Fluid Dynamics of airflow
Mission Critical/Data Center experience
Experience with international projects and knowledge of international codes and standards
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 20%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Mechanical engineering, HVAC systems, Air distribution systems, Chiller plants, Alternative energy solutions, Building automation controls systems, Multidiscipline project teams, Technical documentation, Client meetings, Communication, Leadership, Mentoring, Project management, Scheduling, Construction administration, Computational Fluid Dynamics, Mission Critical/Data Center, International projects, Workplace safety, Flexible work schedules, Travel, Medical benefits, Dental benefits, Vision benefits, Disability benefits, Life insurance, Retirement savings","mechanical engineering, hvac systems, air distribution systems, chiller plants, alternative energy solutions, building automation controls systems, multidiscipline project teams, technical documentation, client meetings, communication, leadership, mentoring, project management, scheduling, construction administration, computational fluid dynamics, mission criticaldata center, international projects, workplace safety, flexible work schedules, travel, medical benefits, dental benefits, vision benefits, disability benefits, life insurance, retirement savings","air distribution systems, alternative energy solutions, building automation controls systems, chiller plants, client meetings, communication, computational fluid dynamics, construction administration, dental benefits, disability benefits, flexible work schedules, hvac systems, international projects, leadership, life insurance, mechanical engineering, medical benefits, mentoring, mission criticaldata center, multidiscipline project teams, project management, retirement savings, scheduling, technical documentation, travel, vision benefits, workplace safety"
Data Engineer,BLACKBIRD.AI,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-blackbird-ai-3778950703,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Blackbird.AI helps organizations discover emergent threats and stay one step ahead of real-world harm through our AI-powered Narrative and Risk Intelligence Platform. Our commitment is to prioritize safety and security, providing the tools to identify potential risks and ensure a safer environment proactively. No matter the job or where it’s located, we’re all connected by a shared vision:
To lead and enhance the landscape of risk intelligence.
Reporting to the Head of Data Engineering, the Data Engineer will optimize data pipelines for our cutting-edge real-time streaming cloud-hosted analytics platform. You will be the key driver of our collection, analysis, and visualization processes and will play a pivotal role in shaping the future of our platform.
The Data Engineer will encompass writing ETL processes, designing database systems, and developing tools for query and analytic processing, all focusing on real-time streaming applications. Additionally, you will elevate performance tradeoffs, own the database architecture, and lead build automation, continuous integration, and deployment efforts while ensuring adherence to security requirements.
As the Data Engineer, you’ll have the chance to:
Write ETL processes to support the ingestion and normalization of various data formats from social media, news, and web sources
Design robust database systems and developing tools for query and analytic processing, especially for real-time streaming applications
Conduct performance analysis and empirical studies to evaluate tradeoffs (e.g., cost vs. throughput/latency)
Develop, manage, and own the database architecture for our real-time streaming cloud-hosted analytics platform
Lead build automation, continuous integration, deployment, and performance optimization efforts while ensuring compliance with security requirements
Lead design of test suites and inline instrumentation as needed to ensure data correctness
Requirements
What you’ll bring
:
Bachelor’s degree in Computer Science or related field
Proven success in deploying products in the cloud and SaaS model, with expertise in building optimized processing pipelines for streaming analytics applications and cloud-agnostic solutions (e.g., Kubernetes, Docker)
Expertise in databases and query optimization, including PostgresSQL, ElasticSearch, MongoDB, Redis, and Druid, with additional experience in NoSQL and graph databases being advantageous
Experience in horizontally scaling databases
Proficiency in Kafka and Airflow, with a deep understanding of runtime profiling tools to optimize throughput and latency, and able to establish comparative performance benchmarks
Strong skills in build automation, continuous integration, and deployment (CI/CD) tools like Webpack, Buddy, Jenkins, and Docker
Expert-level Python coding skills
Experience collaborating with distributed teams
Helpful to have
:
Technical background in Artificial Intelligence (AI) and Machine Learning (ML)
Experience designing and implementing interactive query-driven main-machine intelligence systems
Solid skills in Java
We’ve outlined specific skills, experience, and requirements for this position, but don’t stress if you don’t meet every single one. Our Talent Team is dedicated to discovering exceptional individuals, and they might identify a relevant aspect of your background that suits this role or another opportunity within Blackbird.AI.
If you have passion for the role, please still apply.
What’s in it for you
:
Blackbird.AI is embarking on an exciting growth journey with numerous opportunities for career development within the company. You will join a nurturing, inclusive, and experienced team.
Join us as we soar to new heights!
Values
:
At Blackbird.AI, our core values shape how we work and make decisions. Our values inspire us to be authentic and continue improving.
We embrace a strong sense of responsibility to society, recognizing the vital role our services play in empowering governments, communities, and individuals to foster critical thinking and empowerment. We believe in integrating personal and professional lives with societal needs, emphasizing the importance of creating an environment that attracts top talent and provides substantial growth opportunities. We are motivated by the potential of science and technology to impact humanity positively.
Benefits
Why you’ll love working here
:
Competitive compensation package, 401(k), and equity - everyone has a stake in our growth!
Comprehensive health benefits for you and your loved ones, including wellness days and monthly wellness reimbursements - an apple a day doesn’t always keep the doctor away!
Generous vacation policy, encouraging you to take the time you need - we trust you to strike the right work/life balance!
A flexible work environment with opportunities to collaborate with your team in person - you can have it all!
Inclusion and Impact - soar to new heights!
Bi-annual offsites - have fun with your colleagues!
Professional development stipend - never stop learning!
Pay Transparency
:
For individuals assigned and/or hired to work in New York, Blackbird.AI is required by law to include a reasonable estimate of the compensation range for this role. This compensation range is specific to New York. It takes into account the wide range of factors that are considered in making compensation decisions, including, but not limited to, skill sets, experience and training, licensure and certifications, and other business and organizational needs. At Blackbird.AI, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current compensation range for this position is expected to be
$130,000-$170,000
. This range may vary for positions outside of New York and as it has not been adjusted for the applicable geographic differential associated with the location where the position may be filled.
Regardless of location, candidates can expect during the first few conversations with Blackbird.AI’s Talent Team and Hiring Managers to share any approved budget.
Apply Today
Equal Opportunity Employer
Show more
Show less","Data Engineering, ETL processes, Database systems, Query, Analytics, Streaming, Realtime, Cloud computing, Performance analysis, Optimization, Build automation, Continuous integration, Deployment, Security, Kafka, Airflow, Runtime profiling, Python, Distributed teams, Artificial intelligence, Machine learning, Interactive query, Java, PostgresSQL, ElasticSearch, MongoDB, Redis, NoSQL, Graph databases, Horizontally scaling databases","data engineering, etl processes, database systems, query, analytics, streaming, realtime, cloud computing, performance analysis, optimization, build automation, continuous integration, deployment, security, kafka, airflow, runtime profiling, python, distributed teams, artificial intelligence, machine learning, interactive query, java, postgressql, elasticsearch, mongodb, redis, nosql, graph databases, horizontally scaling databases","airflow, analytics, artificial intelligence, build automation, cloud computing, continuous integration, data engineering, database systems, deployment, distributed teams, elasticsearch, etl, graph databases, horizontally scaling databases, interactive query, java, kafka, machine learning, mongodb, nosql, optimization, performance analysis, postgressql, python, query, realtime, redis, runtime profiling, security, streaming"
Data Engineer (GCP),Experfy,United States,https://www.linkedin.com/jobs/view/data-engineer-gcp-at-experfy-3761188188,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Required Qualifications:
Hands on-experience working with cloud technologies - GCP knowledge strongly preferred
ETL development experience with strong SQL background
Hands-on experience of building and operationalizing data processing systems
Experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka
Experience with any traditional RDBMS (e.g., Teradata, Oracle, DB2)
Experience working with data platforms (Data warehouse, Data Lake, ODS)
Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)
Requirements
Preferred Qualifications:
GCP (google cloud platform) experience
Python
Experience working on healthcare / clinical data
Data analysis / Data mapping skills
JSON and XML
Familiarity with HL7 / FHIR
Show more
Show less","GCP (Google Cloud Platform), SQL, Python, R, Scala, Java, Hive, Spark, Kafka, Teradata, Oracle, DB2, Data warehouse, Data Lake, ODS, Jenkins, GIT, ControlM, HL7, FHIR, CI/CD pipelines, ETL development, NoSQL, JSON, XML, Data analysis, Data mapping, Handson experience","gcp google cloud platform, sql, python, r, scala, java, hive, spark, kafka, teradata, oracle, db2, data warehouse, data lake, ods, jenkins, git, controlm, hl7, fhir, cicd pipelines, etl development, nosql, json, xml, data analysis, data mapping, handson experience","cicd pipelines, controlm, data lake, data mapping, dataanalytics, datawarehouse, db2, etl development, fhir, gcp google cloud platform, git, handson experience, hive, hl7, java, jenkins, json, kafka, nosql, ods, oracle, python, r, scala, spark, sql, teradata, xml"
"Data Engineer, Database Engineering",Experfy,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590299677,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, RPC, Data warehouse, Cluster computing, Fault tolerance, Open source software, Software engineering, Data science, Data management, Big data, Distributed systems, Query planning, Query optimization, Data routing, Workload management, Performance tuning, Scalability, Best practices, Code reuse, Team leadership, Technical writing, Communication skills, Industry trends, Data acquisition, Data processing, Data engineering, Web3, Blockchain, Cryptocurrency, Decentralization","scala, apache spark, apache arrow, apache kafka, sql, rpc, data warehouse, cluster computing, fault tolerance, open source software, software engineering, data science, data management, big data, distributed systems, query planning, query optimization, data routing, workload management, performance tuning, scalability, best practices, code reuse, team leadership, technical writing, communication skills, industry trends, data acquisition, data processing, data engineering, web3, blockchain, cryptocurrency, decentralization","apache arrow, apache kafka, apache spark, best practices, big data, blockchain, cluster computing, code reuse, communication skills, cryptocurrency, data acquisition, data engineering, data management, data processing, data routing, data science, datawarehouse, decentralization, distributed systems, fault tolerance, industry trends, open source software, performance tuning, query optimization, query planning, rpc, scala, scalability, software engineering, sql, team leadership, technical writing, web3, workload management"
"Data Engineer, Database Engineering",Experfy,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590300667,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, Data warehouse, Cloud analytics, Distributed SQL processing, Query optimization, Parallel computing, Fault tolerance, Data routing, Cluster management, Open source, Web3, Blockchain, Data acquisition, Data processing, Data engineering, Data management","scala, apache spark, apache arrow, apache kafka, sql, data warehouse, cloud analytics, distributed sql processing, query optimization, parallel computing, fault tolerance, data routing, cluster management, open source, web3, blockchain, data acquisition, data processing, data engineering, data management","apache arrow, apache kafka, apache spark, blockchain, cloud analytics, cluster management, data acquisition, data engineering, data management, data processing, data routing, datawarehouse, distributed sql processing, fault tolerance, open source, parallel computing, query optimization, scala, sql, web3"
Data Engineer,HANDLE® Global,"Prospect, KY",https://www.linkedin.com/jobs/view/data-engineer-at-handle%C2%AE-global-3787929083,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!
Company Overview:
HANDLE Global, a leading healthcare supply chain analytics and fulfillment solutions provider, is dedicated to revolutionizing the healthcare industry by providing cutting-edge software solutions. Our innovative products help healthcare providers optimize operational efficiency, enhance patient care, and drive business growth.
What Sets You Apart:
Adventure Awaits: Dive into the cutting-edge world of HANDLE Global. We stand at the vanguard of healthcare supply chain analytics, and we're looking for a Data Engineer who's eager to shape the industry with us.
Data Maestro: Spearhead the creation and maintenance of our warehouse data models. As the primary architect and developer for our BI dashboards, you'll ensure our visualizations aren't just accurate, but also insightful, optimizing data extraction and interpretation at every turn.
Project Trailblazer:.Embrace the responsibility for ad-hoc analytics and distinct project undertakings. As healthcare evolves, so do our business needs. Your innovative analytics solutions will be instrumental in steering our trajectory.
Stakeholder Connector: Liaise seamlessly with internal and external partners, translating business requirements into actionable technical solutions. Your ability to bridge the gap between complex datasets and real-world needs will be invaluable.
ELT Enthusiast: Dive into the depths of data source ELT development and maintenance. Your collaborations with our data engineering resources will help streamline customer data pipelines and ensure consistency in our analytics endeavors.
Code Curator & Communicator: Own the documentation for your developed code, ensuring it's clear and easily referenced. Beyond that, articulate your findings and methodologies to a diverse audience, making the complex easily comprehensible.
Opportunity Harbinger: As challenges arise, seize them. Whether they're on the radar or unforeseen, your ability to navigate, innovate, and drive us forward will set the pace for our growth.
Qualifications and skills:
Essentials:
Bachelor’s degree in Computer Science, IT, or equivalent combination of education, training, and experience.
5+ years of experience with SQL. and 3+ years with Python / Spark
Intrigued by the term “modern data stack"" and embody a lifelong learning attitude.
Demonstrated prowess in problem-solving.
Proficient with cloud data warehouses such as Databricks, Snowflake, BigQuery, etc.
Knowledge of data quality standards and schema enforcement.
Familiarity with version control platforms like GitLab or Github.
Thrives when balancing both analyst and engineering responsibilities.
Excellence in documentation - both in interpretation and creation/curation.
Enjoys the journey of defining requirements, capturing business logic, and generating value.
Preferred:
Agile or startup environment background
Knowledge in constructing medallion (bronze, silver, gold) style pipelines.
Expertise in query optimization
Experience with dbt, Delta Live Tables, or BI semantics layers for data model building and maintenance
Experience with CI/CD pipelines and platforms like AWS/Azure/GCP.
Track record of creating comprehensive data dictionaries.
Familiarity with data orchestration tools such as Airflow, Prefect, Dagster, etc.
A penchant for DAGS and Mermaid.
HANDLE Global is an equal opportunity employer committed to valuing and promoting diversity. We base all employment decisions on merit, competence, and business needs, without discrimination based on protected statuses. All qualified applicants are encouraged to apply.
Join HANDLE Global, where your expertise will not just shape our company but the future of healthcare analytics. Apply Now and be part of the revolution!
Powered by JazzHR
oKcqriYu96
Show more
Show less","Data Engineering, Data Warehousing, Business Intelligence, Data Analytics, Data Modeling, Data Extraction, Data Loading, Data Transformation, Data Visualization, SQL, Python, Spark, Cloud Data Warehouses, Databricks, Snowflake, BigQuery, Data Quality, Schema Enforcement, GitLab, Github, Agile, Startup Environment, Medallion Pipelines, Query Optimization, Dbt, Delta Live Tables, BI Semantics Layers, CI/CD Pipelines, AWS, Azure, GCP, Data Dictionaries, Data Orchestration, Airflow, Prefect, Dagster, DAGS, Mermaid","data engineering, data warehousing, business intelligence, data analytics, data modeling, data extraction, data loading, data transformation, data visualization, sql, python, spark, cloud data warehouses, databricks, snowflake, bigquery, data quality, schema enforcement, gitlab, github, agile, startup environment, medallion pipelines, query optimization, dbt, delta live tables, bi semantics layers, cicd pipelines, aws, azure, gcp, data dictionaries, data orchestration, airflow, prefect, dagster, dags, mermaid","agile, airflow, aws, azure, bi semantics layers, bigquery, business intelligence, cicd pipelines, cloud data warehouses, dags, dagster, data dictionaries, data engineering, data extraction, data loading, data orchestration, data quality, data transformation, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta live tables, gcp, github, gitlab, medallion pipelines, mermaid, prefect, python, query optimization, schema enforcement, snowflake, spark, sql, startup environment, visualization"
"Data Engineer, Database Engineering",Experfy,"Hollywood, FL",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590303102,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, Data Science, Database Optimization, Query Planner, Execution Engine, RPC, Data Warehouse, Cluster Management, Meta Data Management, HTAP Database, Web3, Blockchain, Data Acquisition, Data Processing, Data Engineering, Data Management, Proof of Work, Proof of Stake, Consensus Mechanism, Webbased Development, Scripting, Test Automation","scala, apache spark, apache arrow, apache kafka, sql, data science, database optimization, query planner, execution engine, rpc, data warehouse, cluster management, meta data management, htap database, web3, blockchain, data acquisition, data processing, data engineering, data management, proof of work, proof of stake, consensus mechanism, webbased development, scripting, test automation","apache arrow, apache kafka, apache spark, blockchain, cluster management, consensus mechanism, data acquisition, data engineering, data management, data processing, data science, database optimization, datawarehouse, execution engine, htap database, meta data management, proof of stake, proof of work, query planner, rpc, scala, scripting, sql, test automation, web3, webbased development"
"Data Engineer, Database Engineering",Experfy,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3669899571,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershi
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, Distributed SQL processing frameworks, Proprietary data science platforms, Core database optimization, Query planner, Execution engines, RPC calls, Data warehouse clusters, Shared secondary cold storage, Novel query optimization techniques, Highly parallel, Efficient, Faulttolerant, Accessible reusable patterns, Templates, Code bases, Meta data capturing, Management, HTAP database, Industry trends, Data acquisition processing, Engineering, Management, Web3, Blockchains, Query planning, Optimizing, Distributed data warehouse systems, Blockchain indexing, Web3 compute paradigms, Proofs, Consensus mechanisms, Rapid development cycles, Webbased environment, Scripting, Test automation","scala, apache spark, apache arrow, apache kafka, sql, distributed sql processing frameworks, proprietary data science platforms, core database optimization, query planner, execution engines, rpc calls, data warehouse clusters, shared secondary cold storage, novel query optimization techniques, highly parallel, efficient, faulttolerant, accessible reusable patterns, templates, code bases, meta data capturing, management, htap database, industry trends, data acquisition processing, engineering, management, web3, blockchains, query planning, optimizing, distributed data warehouse systems, blockchain indexing, web3 compute paradigms, proofs, consensus mechanisms, rapid development cycles, webbased environment, scripting, test automation","accessible reusable patterns, apache arrow, apache kafka, apache spark, blockchain indexing, blockchains, code bases, consensus mechanisms, core database optimization, data acquisition processing, data warehouse clusters, distributed data warehouse systems, distributed sql processing frameworks, efficient, engineering, execution engines, faulttolerant, highly parallel, htap database, industry trends, management, meta data capturing, novel query optimization techniques, optimizing, proofs, proprietary data science platforms, query planner, query planning, rapid development cycles, rpc calls, scala, scripting, shared secondary cold storage, sql, templates, test automation, web3, web3 compute paradigms, webbased environment"
"Data Engineer, Database Engineering",Experfy,"San Diego, CA",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3646111644,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, Distributed Systems, Query Optimization, Database Optimization, Data Warehousing, Data Science, Cloud Computing, Web3, Blockchain, Data Acquisition, Data Engineering, Data Management, Data Analytics, Software Engineering, Team Leadership, Scripting, Test Automation, Open Source Tools, Apache Stack, Proofs, Consensus Mechanisms","scala, apache spark, apache arrow, apache kafka, sql, distributed systems, query optimization, database optimization, data warehousing, data science, cloud computing, web3, blockchain, data acquisition, data engineering, data management, data analytics, software engineering, team leadership, scripting, test automation, open source tools, apache stack, proofs, consensus mechanisms","apache arrow, apache kafka, apache spark, apache stack, blockchain, cloud computing, consensus mechanisms, data acquisition, data engineering, data management, data science, dataanalytics, database optimization, datawarehouse, distributed systems, open source tools, proofs, query optimization, scala, scripting, software engineering, sql, team leadership, test automation, web3"
"Data Engineer, Database Engineering",Experfy,"California, United States",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3719412209,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job
Show more
Show less","Apache Spark, Apache Arrow, Apache Kafka, Scala, Database optimization, Query planning, Clustertocluster communication, Workload management, Meta data capturing, Meta data management, HTAP database, Web3, Blockchains, Data acquisition, Data processing, Data engineering, Data management, Query optimization, Distributed data warehouse systems, Blockchain indexing, Web3 compute paradigms, Proofs, Consensus mechanisms, Scripting, Test automation","apache spark, apache arrow, apache kafka, scala, database optimization, query planning, clustertocluster communication, workload management, meta data capturing, meta data management, htap database, web3, blockchains, data acquisition, data processing, data engineering, data management, query optimization, distributed data warehouse systems, blockchain indexing, web3 compute paradigms, proofs, consensus mechanisms, scripting, test automation","apache arrow, apache kafka, apache spark, blockchain indexing, blockchains, clustertocluster communication, consensus mechanisms, data acquisition, data engineering, data management, data processing, database optimization, distributed data warehouse systems, htap database, meta data capturing, meta data management, proofs, query optimization, query planning, scala, scripting, test automation, web3, web3 compute paradigms, workload management"
Data Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3733209006,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Location: Remote ( Candidates from Atlanta, GA or neighboring states)
Duration: 6 Months
Client is looking for an engineer who specializes in Databricks and event-driven data collection and streaming services. This engineer will play an essential role by helping our clients build-out robust data pipelines to enable their data science and analytics activities. This position comes with the potential of future career development at a fast-growing data science consultancy.
Experience
3~~@~~ years of experience in data engineering, with exposure to Databricks/Delta Lake/Lakehouse architecture and technologies
Experience with event-driven data collection and streaming services
Experience with pipeline orchestration tools such as Airflow or others
Skills
Strong programming skills in Python and Scala
Experience with Apache Spark and SQL
Experience with cloud computing platforms, such as AWS, Azure, or GCP
Knowledge of data engineering best practices
Additional Qualifications
Databricks Certified Data Engineer Associate or similar professional certification
Experience building analytics-oriented data solutions and services
Responsibilities
Design, build, and maintain data pipelines using Databricks
Collect and stream data from event-driven sources
Orchestrate data pipelines using pipeline orchestration tools
Test and monitor data pipelines
Collaborate with data scientists and analysts to ensure that data pipelines meet their needs
Education
Master’s degree in computer science, data science, or a related field preferred
Show more
Show less","Databricks, Lakehouse architecture, Eventdriven data collection, Streaming services, Airflow, Python, Scala, Apache Spark, SQL, AWS, Azure, GCP, Data engineering best practices, Databricks Certified Data Engineer Associate, Analyticsoriented data solutions, Pipeline orchestration tools, Data pipelines, Data science, Data analytics","databricks, lakehouse architecture, eventdriven data collection, streaming services, airflow, python, scala, apache spark, sql, aws, azure, gcp, data engineering best practices, databricks certified data engineer associate, analyticsoriented data solutions, pipeline orchestration tools, data pipelines, data science, data analytics","airflow, analyticsoriented data solutions, apache spark, aws, azure, data engineering best practices, data science, dataanalytics, databricks, databricks certified data engineer associate, datapipeline, eventdriven data collection, gcp, lakehouse architecture, pipeline orchestration tools, python, scala, sql, streaming services"
"Data Engineer, Database Engineering",Experfy,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590301397,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, Distributed SQL, Query planning, Query optimization, Data routing, Clustertocluster communication, Workload management, Meta data capturing, Meta data management, HTAP database, Blockchain indexing, Web3 compute paradigms, Proofs, Consensus mechanisms, Scripting, Test automation","scala, apache spark, apache arrow, apache kafka, distributed sql, query planning, query optimization, data routing, clustertocluster communication, workload management, meta data capturing, meta data management, htap database, blockchain indexing, web3 compute paradigms, proofs, consensus mechanisms, scripting, test automation","apache arrow, apache kafka, apache spark, blockchain indexing, clustertocluster communication, consensus mechanisms, data routing, distributed sql, htap database, meta data capturing, meta data management, proofs, query optimization, query planning, scala, scripting, test automation, web3 compute paradigms, workload management"
Senior Data Engineer,Talener,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-talener-3782284063,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Our client is a performance marketing/ad-tech company based in Fort Lee, NJ, just outside NYC. This group leverages their own proprietary AI platform and billions of ad interactions daily to help brands large and small drive customers.
Job Title:
Senior Data Engineer
Location:
Remote
Required Technology And Skills
Must have 7+ years of professional experience, 4-5+ years of specific data engineering experience
Strong programming skillset with Python and SQL is required
Experience with cloud computing platforms, preferably AWS (open to GCP and Azure)
Experience with Apache Airflow
Experience with DevOps practices and tools, particularly Docker and Kubernetes
Should have experience with either Snowflake or Databricks
Professional experience with databases like PostreSQL, MariaDB, DynamoDB is preferred
Additional Skills
Experience in the ad-tech or performance marketing space is a plus
Experience with business intelligence tools like Looker is a plus
Hourly Rate
/Compensation
:
Competitive base ranging from $145,000.00-155,000.00, depending on experience
Benefits And Perks
Competitive base salary based on experience
Bonus plan – paid quarterly and based on both individual and company performance
Comprehensive medical, dental, and vision plans
401k
Flexible remote schedule – will mainly be only 1x per week in the Fort Lee, NJ office
Additional Information:
For additional information and to apply, please contact Bethany Moulthrop directly at bmoulthrop@talener.com.
Show more
Show less","Python, SQL, Apache Airflow, DevOps, Docker, Kubernetes, Snowflake, Databricks, PostreSQL, MariaDB, DynamoDB, Adtech, Performance marketing, Looker","python, sql, apache airflow, devops, docker, kubernetes, snowflake, databricks, postresql, mariadb, dynamodb, adtech, performance marketing, looker","adtech, apache airflow, databricks, devops, docker, dynamodb, kubernetes, looker, mariadb, performance marketing, postresql, python, snowflake, sql"
"Data Engineer, Database Engineering",Experfy,United States,https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3719412208,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, Distributed SQL processing frameworks, Query optimization, Faulttolerant systems, Data warehouse systems, Web3, Blockchain indexing, Proofs, Consensus mechanisms, Scripting, Test automation","scala, apache spark, apache arrow, apache kafka, sql, distributed sql processing frameworks, query optimization, faulttolerant systems, data warehouse systems, web3, blockchain indexing, proofs, consensus mechanisms, scripting, test automation","apache arrow, apache kafka, apache spark, blockchain indexing, consensus mechanisms, data warehouse systems, distributed sql processing frameworks, faulttolerant systems, proofs, query optimization, scala, scripting, sql, test automation, web3"
"Data Engineer, Database Engineering",Experfy,"Riverside, CA",https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590303110,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing
frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
3+ years experience with Scala and Apache Spark (or Kafka)
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, Apache Kafka, SQL, RPC, HTAP, Meta data, Computer science, Software engineering, Data platforms, Data warehouses, Apache stack, Query planning, Distributed data warehouse systems, Blockchain indexing, Web3 compute paradigms, Proofs and consensus mechanisms, Rapid development cycles, Scripting, Test automation","scala, apache spark, apache arrow, apache kafka, sql, rpc, htap, meta data, computer science, software engineering, data platforms, data warehouses, apache stack, query planning, distributed data warehouse systems, blockchain indexing, web3 compute paradigms, proofs and consensus mechanisms, rapid development cycles, scripting, test automation","apache arrow, apache kafka, apache spark, apache stack, blockchain indexing, computer science, data platforms, data warehouses, distributed data warehouse systems, htap, meta data, proofs and consensus mechanisms, query planning, rapid development cycles, rpc, scala, scripting, software engineering, sql, test automation, web3 compute paradigms"
Data Engineer,ASCENDING Inc.,"Rockville, MD",https://www.linkedin.com/jobs/view/data-engineer-at-ascending-inc-3787772773,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.
Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.
Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!
Powered by JazzHR
U3GJMKlbkr
Show more
Show less","AWS, Big Data, Spark, Hadoop, Hive, SQL, Scala, Python, Java, Kubernetes, Cloud Computing, Data Engineering, SDLC, Agile, Algorithms","aws, big data, spark, hadoop, hive, sql, scala, python, java, kubernetes, cloud computing, data engineering, sdlc, agile, algorithms","agile, algorithms, aws, big data, cloud computing, data engineering, hadoop, hive, java, kubernetes, python, scala, sdlc, spark, sql"
Data Engineer,EPSoft,United States,https://www.linkedin.com/jobs/view/data-engineer-at-epsoft-3689428796,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Requirements
Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.)experience
Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data science
Experience with multi-year, large-scale projects
Expert technical skills with hands-on testing experience using SQL queries.
Extensive experience with both data migration and data transformation testing
Extensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.
Extensive testing Experience with SQL/Unix/Linux.
Extensive experience using Python scripting and Cloud Technologies.
API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumen
Java/Java Script - Implement core Java, Integration, Core Java and API.
Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.
API/Rest API - Rest API and Micro Services using JSON, SoapUI.
Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.
Experience in testing storage tools like S3, HDFS
Experience with one or more industry-standard defect or Test Case management Tools
Great communication skills (regularly interacts with cross functional team members)
Show more
Show less","Data Integration, Informatica, SQL Server, Oracle, Python, Cloud Computing, Data Warehouse, Data Lake, Data Science, SQL, Data Migration, Data Transformation, DBMS, Teradata, DB2, Redshift, Postgres, Sybase, Unix, Linux, Java, JavaScript, API, RESTAssured, Cucumber, Specflow, Kafka, Big Data, Cypress, JSON, SoapUI, Hadoop, Hive, Pig, S3, HDFS, Defect Management Tools, Test Case Management Tools","data integration, informatica, sql server, oracle, python, cloud computing, data warehouse, data lake, data science, sql, data migration, data transformation, dbms, teradata, db2, redshift, postgres, sybase, unix, linux, java, javascript, api, restassured, cucumber, specflow, kafka, big data, cypress, json, soapui, hadoop, hive, pig, s3, hdfs, defect management tools, test case management tools","api, big data, cloud computing, cucumber, cypress, data integration, data lake, data migration, data science, data transformation, datawarehouse, db2, dbms, defect management tools, hadoop, hdfs, hive, informatica, java, javascript, json, kafka, linux, oracle, pig, postgres, python, redshift, restassured, s3, soapui, specflow, sql, sql server, sybase, teradata, test case management tools, unix"
Data Engineer - Remote,KNS IT GROUP,United States,https://www.linkedin.com/jobs/view/data-engineer-remote-at-kns-it-group-3731675404,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job Description
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server, Postgres, Cassandra, etc.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with data integration services solutions from vendors such as Informatica, MuleSoft, Talend, TIBCO, etc.
Experience with cloud-based data services such as AWS (EC2, Glue, EMR, RDS, Redshift, etc.)
Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc.
Experience with object-oriented/object function scripting languages: Python, R, Java, C++, Scala, etc.
Show more
Show less","Hadoop, Spark, Kafka, SQL, NoSQL, Oracle, MS SQL Server, Postgres, Cassandra, Azkaban, Luigi, Airflow, Informatica, MuleSoft, Talend, TIBCO, AWS, EC2, Glue, EMR, RDS, Redshift, Storm, SparkStreaming, Python, R, Java, C++, Scala","hadoop, spark, kafka, sql, nosql, oracle, ms sql server, postgres, cassandra, azkaban, luigi, airflow, informatica, mulesoft, talend, tibco, aws, ec2, glue, emr, rds, redshift, storm, sparkstreaming, python, r, java, c, scala","airflow, aws, azkaban, c, cassandra, ec2, emr, glue, hadoop, informatica, java, kafka, luigi, ms sql server, mulesoft, nosql, oracle, postgres, python, r, rds, redshift, scala, spark, sparkstreaming, sql, storm, talend, tibco"
Data Engineer,JavanTech Inc,United States,https://www.linkedin.com/jobs/view/data-engineer-at-javantech-inc-3665673821,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Locals to United States
Only W2
Python with pyspark data engineer with GIS background preferably
Show more
Show less","Python, PySpark, Data Engineering, GIS","python, pyspark, data engineering, gis","data engineering, gis, python, spark"
Big Data Engineer,Experfy,United States,https://www.linkedin.com/jobs/view/big-data-engineer-at-experfy-3761185972,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Develop fast data infrastructure leveraging data streaming, batch processing, and machine learning to personalize experiences for our customers
Lead work and deliver elegant and scalable solutions
Work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs
#INDEXP
Requirements
Technical Qualifications:
Bachelor-level Degree in engineering, Information Technology or Computer Science
4 years of hands-on experience as a Data Engineer in a Big Data environment (Spark, Hive, HDFS, Sqoop)
Strong SQL knowledge and data analysis skills for data anomaly detection and data quality assurance
Programming experience in Scala, Python, shell scripting and automation
Experience with modern workflow/orchestration tools (e.g. Apache Airflow, Oozie, Azkaban, etc.)
Experience working with PostgreSQL, Teradata, Vertica and/or other DBMS platforms
Preferred Qualifications:
Hadoop Certification or Spark Certification
Experience with BI tools such as Tableau or Qlik to create visualizations and dashboards for various data quality metrics
Show more
Show less","Data Streaming, Batch Processing, Machine Learning, Data Infrastructure, Spark, Hive, HDFS, Sqoop, SQL, Data Analysis, Data Anomaly Detection, Data Quality Assurance, Scala, Python, Shell Scripting, Automation, Workflow/Orchestration Tools, Apache Airflow, Oozie, Azkaban, PostgreSQL, Teradata, Vertica, DBMS Platforms, Hadoop Certification, Spark Certification, Business Intelligence (BI) Tools, Tableau, Qlik, Data Visualization, Dashboards","data streaming, batch processing, machine learning, data infrastructure, spark, hive, hdfs, sqoop, sql, data analysis, data anomaly detection, data quality assurance, scala, python, shell scripting, automation, workfloworchestration tools, apache airflow, oozie, azkaban, postgresql, teradata, vertica, dbms platforms, hadoop certification, spark certification, business intelligence bi tools, tableau, qlik, data visualization, dashboards","apache airflow, automation, azkaban, batch processing, business intelligence bi tools, dashboard, data anomaly detection, data infrastructure, data quality assurance, data streaming, dataanalytics, dbms platforms, hadoop certification, hdfs, hive, machine learning, oozie, postgresql, python, qlik, scala, shell scripting, spark, spark certification, sql, sqoop, tableau, teradata, vertica, visualization, workfloworchestration tools"
Azure Data Engineer,Amsive,"Greenville, SC",https://www.linkedin.com/jobs/view/azure-data-engineer-at-amsive-3787738389,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Amsive is a data-driven performance marketing agency that enhances marketing ROI through innovative customer acquisition and engagement solutions. As a full-service agency, Amsive enables growth by leveraging digital and direct-native expertise, with dedicated teams shaping powerful strategies, creative executions, with direct and digital solutions, including SEO, paid search, media, and performance measurement.
At Amsive, our people are our first priority. With approximately 1,000 hybrid employees across 6 regional offices, we bring together the brightest minds in direct and digital marketing to challenge the status quo as a new kind of agency. With a focus on collaboration and innovation, we deliver measurable results and drive growth for our clients while providing a supportive environment where you can focus on your career growth. We offer competitive wages, excellent benefits, and a positive work environment designed around the commitment to mutual respect and the challenge of contributing to the continued success of our organization. Explore our culture.
This is a REMOTE position. Candidates can be located anywhere in US Time Zones but will work in Eastern/CT*
Job Summary
As an Azure Data Engineer will work with clients, internal business stakeholders, and other data engineers to translate business requirements into modern and innovative data integration routines to move, cleanse, and transform large data sets in an efficient and scalable manner.
Job Responsibilities
Data hygiene, ETL/ELT pipeline development and administration, computations, aggregations, analytics, ad-hoc queries, studies, and maintenance in accordance with enterprise data governance standards
Monitor and maintain data pipelines proactively to ensure high service availability
Troubleshoot incidents, identify root causes, resolve, and document problems, and implement preventive measures including alerting and notifications
Document data flows and mappings as needed
Collaborate with the Architecture team to modify and improve existing data management systems
Work with Data Scientists to understand mathematical models and optimize data solutions accordingly
Work with BI teams to understand reporting requirements and optimize data solutions accordingly
Job Requirements
Deep experience in developing robust, scalable, and resilient data management systems
Bachelor’s degree from an accredited university or college and/or demonstrated technical knowledge and equivalent work experience
3+ years of experience developing queries and ELT/ETL solutions using Microsoft tools (SQL, ADF, ADLS, Azure Synapse), working with structured, unstructured, and semi-structured datasets
2+ years of experience in architecting, designing, developing, implementing, and optimizing cloud solutions leveraging Azure services
1+ years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala, and R
1+ years of experience in Big Data Distributed systems such as Databricks, Apache Spark, etc.
Knowledge of integrating data pipelines with RESTful web services
Strong problem-solving, communication, and interpersonal skills
Must be creative, organized, detail oriented, and able to assimilate information quickly
Ability to work in a team as well as independently
Preferred Competencies
Experience with marketing or campaign management systems is a plus
Azure certifications: Azure Data Engineer Associate, Azure Data Fundamentals
Experience with CI/CD systems (Azure DevOps preferred)
Understanding of Master Data Management concepts
Working experience in Agile Scrum environments
Working knowledge of BI Reporting tools (PowerBI, Tableau)
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
NoSQL experience (particularly GraphQL/neo4j) is a plus
If you need any assistance seeking a job opportunity at Amsive, or if you need reasonable accommodation with the application process, please call (331) 318-7800.
Amsive is proud to be an Equal Opportunity Employer. We are committed to building a supportive and inclusive environment for all employees. It is Amsive’s continuing policy to provide equal employment opportunity and not to discriminate on the basis of race, color, religion, pregnancy or childbirth, marital status, national origin, ancestry or citizenship status, age, disability, sex, sexual orientation, gender identity, veteran status or any other characteristic protected by applicable federal, state or local laws. This policy applies to all aspects of employment, including (but not limited to) application for employment, recruiting, hiring, compensation, benefits, promotions and transfers, training, layoffs, rehires, termination of employment and all other terms and conditions of employment.
As part of the Company's equal employment opportunity policy, Amsive will also take affirmative action as called for by applicable laws and Executive Orders to ensure that minority group individuals, females, disabled veterans, recently separated veterans, other protected veterans, Armed Forces service medal veterans, and qualified disabled persons are introduced into our workforce and considered for promotional opportunities.
#ams123
Powered by JazzHR
LrBX35cWwW
Show more
Show less","Azure, SQL, ADF, ADLS, Azure Synapse, Python, Scala, R, Databricks, Apache Spark, RESTful, PowerBI, Tableau, Docker, Kubernetes, ECR, GraphQL, neo4j, Agile Scrum, ETL, ELT, CI/CD, Azure DevOps, Master Data Management","azure, sql, adf, adls, azure synapse, python, scala, r, databricks, apache spark, restful, powerbi, tableau, docker, kubernetes, ecr, graphql, neo4j, agile scrum, etl, elt, cicd, azure devops, master data management","adf, adls, agile scrum, apache spark, azure, azure devops, azure synapse, cicd, databricks, docker, ecr, elt, etl, graphql, kubernetes, master data management, neo4j, powerbi, python, r, restful, scala, sql, tableau"
Data Engineer,Therapy Brands,"Birmingham, AL",https://www.linkedin.com/jobs/view/data-engineer-at-therapy-brands-3711695981,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Company Description
Therapy Brands
is the leading healthcare technology partner for mental, behavioral, and rehabilitative therapy. Our purpose-built and all-in-one practice management, data, and billing solutions drive exceptional clinical and financial outcomes.
Thousands of therapy practices rely on us as a trusted partner, to make their lives simpler and more efficient, improve revenue, and enable them to focus on patient care.
For more information, explore our solutions at therapybrands.com.
Job Description
We are seeking a talented and motivated Data Engineer with a strong background in handling large volumes of data and expertise in Master Data Management (MDM). As a Data Engineer, you will play a critical role in designing, developing, and maintaining our data infrastructure to support our data-driven decision-making processes. You will work with a team of dedicated professionals and utilize your expertise in Azure Data Factory, Databricks, Python, and MDM to optimize data pipelines, ensure data quality and reliability, and manage data repositories.
Key Responsibilities:
Design, develop, and maintain data pipelines for the ingestion, transformation, and storage of large datasets.
Collaborate with analysts, and other stakeholders to understand data requirements and ensure data availability.
Implement data integration solutions using Azure Data Factory, Databricks, and Python.
Optimize and fine-tune data pipelines for performance, scalability, and cost-efficiency.
Implement data security and compliance best practices.
Develop and maintain documentation for data engineering processes and pipelines.
Design, develop, and maintain Enterprise Analytics
Implement and maintain Master Data Management (MDM) processes, including the management of master data repositories.
Ensure data quality and consistency across the organization by defining and enforcing MDM standards.
Monitor and troubleshoot data pipelines and MDM processes to ensure data accuracy and reliability.
Stay up-to-date with industry best practices and emerging technologies in data engineering and MDM.
Qualifications
Qualifications
:
Proven experience as a Data Engineer, preferably with experience working on large-scale data projects.
Strong proficiency in Azure Data Factory and Databricks for data integration and processing.
Proficiency in Python for data manipulation and scripting.
Familiarity with data warehousing concepts and technologies.
Experience with SQL and NoSQL databases.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.
Experience with Azure Data Lake
Ability to work independently and as part of a team.
Azure certifications (e.g., Azure Data Engineer) a plus.
Experience with Master Data Management (MDM) tools and practices.
Preferred Skills
:
Experience with data orchestration and scheduling tools
Knowledge of data modeling and ETL (Extract, Transform, Load) processes.
Familiarity with big data technologies.
Experience with version control systems (e.g., Git).
Understanding of data governance and data quality principles.
Additional Information
While we've outlined some key qualities we typically seek, it's essential to remember that there might be additional unique strengths and talents you possess that would make you an exceptional match for us, even if they're not explicitly mentioned. Studies have consistently highlighted the significance of this principle, particularly for individuals from disenfranchised backgrounds, including women and other marginalized groups. These individuals often hesitate to apply unless they meet every single requirement, unlike their male counterparts who are more inclined to apply when they meet around 60% of the criteria.
The message we want to convey is that taking a leap of faith and applying can be incredibly rewarding. Your distinct abilities and perspectives could be exactly what we need to create a more diverse and inclusive team. So, don't hesitate—apply today and let's explore the exciting possibilities together!
All your information will be kept confidential according to EEO guidelines.
At
Therapy Brands
, Diversity, Equity, Inclusion, and Belonging aren’t just words. We celebrate what makes us unique, foster an ecosystem of inclusion for all and harness our talents to promote diversity of thought and action in everything we do.
We instill Diversity, Equity, Inclusion, and Belonging into the fabric of our CARING culture and business, as we strive to be recognized not only as the leader in healthcare technology, but also for our intentional efforts to promote a diverse community.
We will champion non-discriminatory practices throughout the employee and customer lifecycle; caring for every person regardless of race, national origin, color, religion, disability, sex, orientation, or familial status.
Therapy Brands
is an equal opportunity employer.
Show more
Show less","Data Engineer, Azure Data Factory, Databricks, Python, Master Data Management (MDM), Data Pipelines, Data Quality, Data Warehousing, SQL, NoSQL, Problemsolving, Communication, Collaboration, Azure Data Lake, Data Orchestration, Data Scheduling, Data Modeling, ETL (Extract Transform Load), Big Data, Version Control, Data Governance","data engineer, azure data factory, databricks, python, master data management mdm, data pipelines, data quality, data warehousing, sql, nosql, problemsolving, communication, collaboration, azure data lake, data orchestration, data scheduling, data modeling, etl extract transform load, big data, version control, data governance","azure data factory, azure data lake, big data, collaboration, communication, data governance, data orchestration, data quality, data scheduling, databricks, dataengineering, datamodeling, datapipeline, datawarehouse, etl extract transform load, master data management mdm, nosql, problemsolving, python, sql, version control"
Data Engineer [Remote],SmartIPlace,United States,https://www.linkedin.com/jobs/view/data-engineer-remote-at-smartiplace-3713002852,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Title:
Data Engineer [Remote]
Experience: 8+ years
Visa: USC Only
Skills
Minimum of 5-10 years of IT/IS experience
Need to have excellent communication skills
5+ years working with Advanced Structured Query Language (SQL) & PL/SQL
5+ years experience with at least Oracle relational database
5+ years experience in software development lifecycle activities
5+ years experience with data loading (ETL, ELT)
5+ years working with data at scale 50+ TB
Experience with data warehouses, operational data stores, data hubs
Experience in end-to-end design of near-real-time and batch data pipelines
Experience working in an Agile environment
Experience developing detailed systems design and written test plans
Experience preparing installation instructions and coordinating installation procedures
Experience documenting data audits, archiving, and restoration processes
Experience with version control systems Git
Experience with tracking and ticket software (Jira, Confluence, etc.)
Familiarity with data architecture, data integration, data governance, and data lineage concepts
EDUCATION
Bachelor's Degree in computer science or a related discipline
Experience
Minimum of 5-10 years of IT/IS experience
Need to have excellent communication skills
5+ years working with Advanced Structured Query Language (SQL) & PL/SQL
5+ years experience with at least Oracle relational database
5+ years experience in software development lifecycle activities
5+ years experience with data loading (ETL, ELT)
5+ years working with data at scale 50+ TB
Experience with data warehouses, operational data stores, data hubs
Experience in end-to-end design of near-real-time and batch data pipelines
Experience working in an Agile environment
Experience developing detailed systems design and written test plans
Experience preparing installation instructions and coordinating installation procedures
Experience documenting data audits, archiving, and restoration processes
Experience with version control systems Git
Experience with tracking and ticket software (Jira, Confluence, etc.)
Familiarity with data architecture, data integration, data governance, and data lineage concepts
NICE TO HAVE
Leading a team
Master's Degree
Health care experience
Professional certifications
Other Programming Language Experience
Experience in Machine learning or Artificial Intelligence
Familiarity with microservices
Familiarity with DevSecOps
2+ years working with one NoSQL database (Hadoop)
Experience with MongoDB, Redshift, Synapse, or others is a plus.
Experience working in one public cloud environment (Azure, AWS, etc.)
Snowflake familiarity
Experience with containerization (Docker, Kubernetes, etc.)
Familiarity with agile and lean methodologies
Familiarity with JSON, XML
PERSONAL
Welcomes new approaches and innovative thinking
Self-organized and responsible with experience in a distributed team
Able to multitask and be responsive/flexible to support customers
Ability to work with others from diverse skill sets and backgrounds
Takes ownership of a situation and sees it through the completion
Able to switch context and complete work processes
Show more
Show less","Advanced Structured Query Language (SQL), PL/SQL, Oracle relational database, Software development lifecycle activities, Data loading (ETL ELT), Data warehouses, Operational data stores, Data hubs, Endtoend design of nearrealtime and batch data pipelines, Agile environment, Version control systems (Git), Tracking and ticket software (Jira Confluence), Data architecture, Data integration, Data governance, Data lineage, Hadoop, MongoDB, Redshift, Synapse, Azure, AWS, Snowflake, Docker, Kubernetes, JSON, XML, Machine learning, Artificial Intelligence, Microservices, DevSecOps","advanced structured query language sql, plsql, oracle relational database, software development lifecycle activities, data loading etl elt, data warehouses, operational data stores, data hubs, endtoend design of nearrealtime and batch data pipelines, agile environment, version control systems git, tracking and ticket software jira confluence, data architecture, data integration, data governance, data lineage, hadoop, mongodb, redshift, synapse, azure, aws, snowflake, docker, kubernetes, json, xml, machine learning, artificial intelligence, microservices, devsecops","advanced structured query language sql, agile environment, artificial intelligence, aws, azure, data architecture, data governance, data hubs, data integration, data lineage, data loading etl elt, data warehouses, devsecops, docker, endtoend design of nearrealtime and batch data pipelines, hadoop, json, kubernetes, machine learning, microservices, mongodb, operational data stores, oracle relational database, plsql, redshift, snowflake, software development lifecycle activities, synapse, tracking and ticket software jira confluence, version control systems git, xml"
Databricks Developer (Data Engineer—Databricks/ Azure),Steneral Consulting,United States,https://www.linkedin.com/jobs/view/databricks-developer-data-engineer%E2%80%94databricks-azure-at-steneral-consulting-3731636871,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job
: Databricks Developer (Data Engineer—Databricks/ Azure)
Location:
Remote 100%
Term
: 8+ months contract with extensions
Start date is targeted for 11/13. Please confirm this with the candidate.
Summary
McLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks. The client has been hoping to deploy this initiative and struggling so McLaren is coming in to assist. They will be moving the primary source data from SAP and Snowflake into Databricks.
They have 120 data product they are building out. The goal is to do 2-week sprints for 2-4 data product deployments at a time.
Responsibilities
Develop and implement data pipelines and transformations on Databricks.
Integrate data from various sources into the Databricks environment.
Ensure code quality, optimization, and adherence to best practices.
Collaborate with architects and business analysts to validate solutions.
Requirements
5+ years of experience in Databricks development and data engineering (AZURE)
Apache Spark
Databricks Notebooks
Scala/Python Programming
Data Transformation and Processing
Git/GitHub
Data Pipeline Development
Show more
Show less","Databricks, AZURE, Apache Spark, Databricks Notebooks, Scala, Python, Data Transformation, Data Processing, Git, GitHub, Data Pipeline Development, Data Mesh, Data Product Deployment, SAP, Snowflake","databricks, azure, apache spark, databricks notebooks, scala, python, data transformation, data processing, git, github, data pipeline development, data mesh, data product deployment, sap, snowflake","apache spark, azure, data mesh, data pipeline development, data processing, data product deployment, data transformation, databricks, databricks notebooks, git, github, python, sap, scala, snowflake"
Sr. Data Engineer,Insite AI,United States,https://www.linkedin.com/jobs/view/sr-data-engineer-at-insite-ai-3775695636,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As the Sr Data Engineer at Insite AI, you will be responsible for leading the architectural design, development, and deployment of scalable data management systems in a high-performance, customer-driven environment. Insite AI's AI-based platform enables autonomous decision-making that creates a connected assortment strategy, category and pricing execution and trade promotion environment. Insite AI's team comes from leading CPGs and brings their experiences to build a fully customizable AI-based platform that gives CPGs and Consumer Brands the most “explain-ability”, foresight, and granularity that results in an unprecedented quality of decision-making. If you are passionate about leveraging the power of AI to help clients reach their full potential, this is the opportunity for you!
Responsibilities
Build and maintain data integration, ETL pipelines, and data warehouse architectures on cloud platforms like AWS, Azure, and GCP
Work with cross-functional teams comprising Data Scientists, Product Managers, and Software Engineers to design, build, and deploy robust and scalable data pipelines that power real-time decision-making
Provide technical mentorship and leadership to a team of data engineers, and drive innovation and best practices in the engineering organization
Work closely with the customer success team to troubleshoot and resolve issues related to data quality, ingestion, and integration
Requirements
8+ years of experience in designing, building, and deploying large-scale data management systems in the cloud
Strong proficiency in programming languages such as Python & Java
Experience with cloud computing platforms such as AWS, Azure, and/or GCP
Experience with data warehousing and ETL technologies such as Databricks, Snowflake, and Airflow
Strong technical leadership skills and ability to mentor and guide team members
Excellent communication skills and ability to work collaboratively in a cross-functional team environment
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Remote - Work From Home
Family Leave (Maternity, Paternity)
Short Term & Long Term Disability
Training & Development
Show more
Show less","Data Engineering, Data Management, Cloud Computing, Data Integration, ETL, Data Warehousing, Python, Java, AWS, Azure, GCP, Databricks, Snowflake, Airflow, Technical Leadership, Communication, Teamwork, Remote Work","data engineering, data management, cloud computing, data integration, etl, data warehousing, python, java, aws, azure, gcp, databricks, snowflake, airflow, technical leadership, communication, teamwork, remote work","airflow, aws, azure, cloud computing, communication, data engineering, data integration, data management, databricks, datawarehouse, etl, gcp, java, python, remote work, snowflake, teamwork, technical leadership"
Data Engineer,Stripe,United States,https://www.linkedin.com/jobs/view/data-engineer-at-stripe-3769105801,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
The Data Science team builds data and intelligence into our product, sales, and operations. This spans across building data foundations and applying statistical techniques and machine learning to measure and optimize our product, build data-driven products, and conduct in-depth analysis to inform strategic decisions.
What you’ll do
We’re looking for people with a strong background in data engineering and analytics to help us scale while maintaining correct and complete data. You’ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs. Your work will provide teams with visibility into how Stripe’s products are being used and how we can better serve our customers.
Responsibilities
You’ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs
Your work will provide teams with visibility into how Stripe’s products are being used and how we can better serve our customers
Identify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across Stripe
Design, develop, and own data pipelines and models that power internal analytics for product and business teams
Help the Data Science team apply and generalize statistical and econometric models on large datasets
Drive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolves
Develop strong subject matter expertise and manage the SLAs for those data pipelines
Who you are
If you are data curious, excited about designing data pipelines, and motivated by having an impact on the business, we want to hear from you.
Minimum Requirements
Have a strong engineering background and are interested in data
5+ years of experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…)
Have an inquisitive nature in diving into data inconsistencies to pinpoint issues
Strong coding skills in Scala, Python, Java or another language for building performance data pipelines.
Strong understanding and practical experience with systems such as Hadoop, Spark, Presto, Iceberg, and Airflow
The ability to communicate cross-functionally with solid stakeholder management to derive requirements and architect scalable solutions.
Show more
Show less","Data Engineering, Data Analytics, Data Pipeline, Spark, Python, Scala, Java, Hadoop, Presto, Iceberg, Airflow","data engineering, data analytics, data pipeline, spark, python, scala, java, hadoop, presto, iceberg, airflow","airflow, data engineering, data pipeline, dataanalytics, hadoop, iceberg, java, presto, python, scala, spark"
Senior Data Engineer - Remote,Enexus Global Inc.,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-enexus-global-inc-3716039716,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Senior Data Engineer
Location - Remote
Contract Type - W2/C2C/1099
Minimum Experience - 12+ Years
Responsibilities
Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Provide technical support and usage guidance to the users of our platform’s services.
Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
Proficiency working in Linux environment
8+ years of advanced working knowledge of SQL, Python, and PySpark
5+ years of experience with using a broad range of AWS technologies
Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline
Experience with platform monitoring and alerts tools
Thanks & Regards
Sahil
510-925-0283 EXT 131
Show more
Show less","Data Processing, Data Orchestration, Data Monitoring, AWS, GitLab Automation, SQL, Python, PySpark, Linux, Git, Bitbucket, Jenkins, CodeBuild, CodePipeline, Platform Monitoring Tools, Alerting Mechanisms","data processing, data orchestration, data monitoring, aws, gitlab automation, sql, python, pyspark, linux, git, bitbucket, jenkins, codebuild, codepipeline, platform monitoring tools, alerting mechanisms","alerting mechanisms, aws, bitbucket, codebuild, codepipeline, data monitoring, data orchestration, data processing, git, gitlab automation, jenkins, linux, platform monitoring tools, python, spark, sql"
Senior Data Engineer - Remote,Enexus Global Inc.,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-enexus-global-inc-3675227051,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Location - Remote
Contract Type - W2/C2C/1099
Minimum Experience - 11+ Years
Responsibilities
Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Provide technical support and usage guidance to the users of our platform's services.
Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
Proficiency working in Linux environment
8+ years of advanced working knowledge of SQL, Python, and PySpark
5+ years of experience with using a broad range of AWS technologies
Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline
Experience with platform monitoring and alerts tools
Show more
Show less","Data Processing, Data Orchestration, Data Monitoring, OpenSource Software, AWS, GitLab Automation, SQL, Python, PySpark, AWS Technologies, Linux, Git/Bitbucket, Jenkins/CodeBuild, CodePipeline, Platform Monitoring Tools, Alerting Tools, Data Pipelines, Distributed Systems, CrossFunctional Teams","data processing, data orchestration, data monitoring, opensource software, aws, gitlab automation, sql, python, pyspark, aws technologies, linux, gitbitbucket, jenkinscodebuild, codepipeline, platform monitoring tools, alerting tools, data pipelines, distributed systems, crossfunctional teams","alerting tools, aws, aws technologies, codepipeline, crossfunctional teams, data monitoring, data orchestration, data processing, datapipeline, distributed systems, gitbitbucket, gitlab automation, jenkinscodebuild, linux, opensource software, platform monitoring tools, python, spark, sql"
Data Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3729200430,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Data Engineer
Remote
1 to 5 years experience
East Coast Preferred
Must have skills:
Design and develop an end-to-end Camunda application.
Have experience with business rules and decision tables.
Have experience in Java technologies.
Additional Skills
Ability to understand Business Problem and provide BPM centric solution through digitized process, automations, User Interventions, and well-defined User Experience.
Conversant with latest versions and features of Camunda (ver 8) and be able to maintain application with reusable components for rapid application building in future
BPM Solution Developer with Camunda Experience Camunda Developer with 3+ years of experience in BPM space and with minimum 1+ years of working experience on Camunda BPM. Roles and Responsibilities Integrate and maintain Camunda BPM solutions through the systems development lifecycle. Hands-on development, coding, debugging of Camunda BPM applications. Skills: Camunda BPM Development
, Java/J2EE.  Required Skills: Design and develop an end-to-end Camunda 8 application. Have experience with business rules and decision tables. Have experience in Java technologies. Ability to understand Business Problem and provide BPM centric solution through digitized process, automations, User Interventions, and well-defined User Experience. Conversant with latest versions and features of Camunda (ver 8) and be able to maintain application with reusable components for rapid application building in future business cases.  Preferred Skills Hands on experience with BPMN, J2EE technologies, REST, open-source products, database, and ability to review variety of code. Have experience integrating with an external content management system. Have experience building dashboards for Camunda Operate. Behavioral Skills: Team player and a good communicator. Result oriented with ability to work with virtual/distributed teams.
Show more
Show less","Camunda, Business rules, Decision tables, Java, BPM, REST, Databases, BPMN, J2EE, Camunda Operate","camunda, business rules, decision tables, java, bpm, rest, databases, bpmn, j2ee, camunda operate","bpm, bpmn, business rules, camunda, camunda operate, databases, decision tables, j2ee, java, rest"
Data Engineer (Snowflake),Workforce Connections,United States,https://www.linkedin.com/jobs/view/data-engineer-snowflake-at-workforce-connections-3766667239,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Data Engineer II (Snowflake)
Remote - Must reside in US
Position Purpose:
Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Developing and maintaining data processing solutions.
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)
Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics
Designs, develops, and maintains real-time processing applications and real-time data pipelines
Ensure quality of technical solutions as data moves across Company’s environments
Provides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutions
Develops, constructs, tests, and maintains architectures using programming language and tools
Identifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automated
Performs other duties as assigned
Complies with all policies and standards
Education/Experience:
A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).
Requires 2 – 4 years of related experience.
Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.
Technical Skills
One or more of the following skills are desired
Experience developing stored procedures in Snowflake (Programming Language)
Experience with SQL (Programming Language)
Experience with AWS
Experience with Healthcare (specifically HEDIS)
Experience with Big Data; Data Processing
Experience with diagnosing system issues, engaging in data validation, and providing quality assurance testing
Experience with Data Manipulation; Data Mining
Experience working in a production cloud infrastructure
Knowledge of Microsoft SQL Servers
Soft Skills
Strong Communication and Organizational skills
Quick learner willing to work with others to understand applications and processes
Show more
Show less","Snowflake, SQL, AWS, Healthcare (HEDITS), Big Data, Data Processing, Data Manipulation, Data Mining, Data Quality, Data Validation, Data Automation, Data Provisioning, Microsoft SQL Server, Stored Procedures, Production Cloud Infrastructure, Data Pipelines, Data Ingestion, Continuous Integration, Continuous Deployment","snowflake, sql, aws, healthcare hedits, big data, data processing, data manipulation, data mining, data quality, data validation, data automation, data provisioning, microsoft sql server, stored procedures, production cloud infrastructure, data pipelines, data ingestion, continuous integration, continuous deployment","aws, big data, continuous deployment, continuous integration, data automation, data ingestion, data manipulation, data mining, data processing, data provisioning, data quality, data validation, datapipeline, healthcare hedits, microsoft sql server, production cloud infrastructure, snowflake, sql, stored procedures"
Sr. Data Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/sr-data-engineer-at-steneral-consulting-3729201419,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Title -Sr. Data Engineer
Location- Remote
Linkedin must.
Please share 2 best candidate.
JD-
A minimum of 12 years experience in Data Engineering.
A minimum of 5 years in Azure.
Must have excellent communication skills.
We are looking for a talented and experienced
Azure Data Engineer
with expertise in
Azure Databricks, Azure Synapse Analytics, Azure Data Factory, RDBMS, and NoSQL databases.
The ideal candidate will play a crucial role in designing, implementing, and optimizing data solutions on the Azure platform to support our data-driven initiatives.
Key Responsibilities
Data Pipeline Development:
Design, develop, and maintain data pipelines
using Azure Data Factory and Azure Databricks/
Spark
to efficiently extract, transform, and load (ETL) data from various sources into data lakes and data warehouses.
Data Modeling:
Create and manage data models and schemas within Azure Synapse Analytics, NoSQL
(Cosmos, MongoDB) to ensure data accuracy, performance, and scalability.
Data Integration: Collaborate with cross-functional teams to integrate data from diverse sources, including RDBMS (e.g., Oracle, SQL Server) and NoSQL databases (e.g., MongoDB, Cosmos DB).
Data Migration : Lead data migration projects, including data extraction, transformation, and loading (ETL), from on-premises systems and other cloud platforms to Azure. Ensure data quality, accuracy, and consistency throughout the migration process.
Data Quality and Profiling: Implement data quality checks, Data Profiling and Cleansing to maintain data integrity, security, and compliance with industry standards and regulations.
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field. Master's degree preferred.
Proven experience as a data engineer with a strong focus on Azure data services.
Proficiency in Azure Databricks/Spark, Azure Synapse Analytics, Azure Data Factory, and other Azure data-related tools.
Strong SQL skills and experience with data modeling in both RDBMS and NoSQL databases.
Familiarity with data warehousing concepts and best practices.
Knowledge of data governance, security, and compliance standards.
Programming skills in languages such as Python and Microservices (Python) is desired.
Excellent problem-solving and communication skills.
Strong Databricks experience in AWS/GCP experience can also be considered.
Ability to work independently and collaboratively within a team environment.
Show more
Show less","Azure, Databricks, Synapse Analytics, Data Factory, RDBMS, NoSQL (Cosmos MongoDB), Data Pipeline Development, Data Modeling, Data Integration, Data Migration, Data Quality and Profiling, Computer Science, Information Technology, Python, Microservices, SQL, Data Warehousing, Data Governance, Security, Compliance","azure, databricks, synapse analytics, data factory, rdbms, nosql cosmos mongodb, data pipeline development, data modeling, data integration, data migration, data quality and profiling, computer science, information technology, python, microservices, sql, data warehousing, data governance, security, compliance","azure, compliance, computer science, data factory, data governance, data integration, data migration, data pipeline development, data quality and profiling, databricks, datamodeling, datawarehouse, information technology, microservices, nosql cosmos mongodb, python, rdbms, security, sql, synapse analytics"
Data Engineer II,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-ii-at-steneral-consulting-3757573233,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Remote role
Need Valid LinkedIn
Need photo ID [ no info must be hidden/blacked out in the IDs]
Need 1-2 strong candidates only on this.
Must be able to obtain a clearance
7+ years of development experience building data pipelines using Cloud technologies.
5+ years of experience in architecture of modern data warehousing
5+ platforms using technologies such as Snowflake or Redshift
Cloud experience – S3, step functions, Glue, Step functions and Airflow .
Good Python development for data transfers and extractions (ELT and ETL).
Show more
Show less","Cloud technologies, Data warehousing, Snowflake, Redshift, S3, Step functions, Glue, Airflow, Python, ELT, ETL","cloud technologies, data warehousing, snowflake, redshift, s3, step functions, glue, airflow, python, elt, etl","airflow, cloud technologies, datawarehouse, elt, etl, glue, python, redshift, s3, snowflake, step functions"
Senior Data Engineer - Azure Data Factory,"Data Ideology, LLC","Pittsburgh, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-azure-data-factory-at-data-ideology-llc-3775481241,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Data Ideology
At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious.For more information about Data Ideology, visitwww.dataideology.com
Senior Data Engineer - Full-time
We are looking for a Sr. Data Engineerto join our growing team. Sr. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.
Key Responsibilities
To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.
Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.
Ability to define and implement best practices across database design and ETL.
Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.
Supervisory Responsibilities: None
Qualifications
Education and Experience:
Proven understanding of Data Warehousing, Data Architecture, and BI.
Experience with data pipelines and architecture/engineering.
Knowledge of modern apps and data platforms.
Cloud-based project implementation.
Azure Data Factory experience is required
Snowflake experience is a plus
Python or Java experience is a plus
Knowledge, Skills, and Abilities:
BI/Data Warehousing (5+ years)
Cloud platforms (2+ years)
ETL (3+ years)
SQL (3+ years)
Data Modeling
Data Vault Modeling
Healthcare experience a plus
Consulting experience a plus
Work Environment:
Remote work from home.
Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands:
Must be able to remain in a stationary position 50% of the time.
The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits:
Unlimited Discretionary Time Off Policy
Insurance (medical, dental, vision) for employees
100% company paid - short and long-term disability insurance for employees
100% company paid - life insurance and AD&D insurance for employees
100% company paid – employee assistance program
Retirement plans with company match
Training and Certification Reimbursement annually
Performance-based incentive program
Commission incentive program
Profit Sharing Plan
Referral Bonuses
Data Ideology is an EEO Employer
Show more
Show less","Data Warehousing, Data Architecture, BI, Cloud platforms, ETL, SQL, Data Modeling, Data Vault Modeling, Healthcare, Consulting, Azure Data Factory, Snowflake, Python, Java","data warehousing, data architecture, bi, cloud platforms, etl, sql, data modeling, data vault modeling, healthcare, consulting, azure data factory, snowflake, python, java","azure data factory, bi, cloud platforms, consulting, data architecture, data vault modeling, datamodeling, datawarehouse, etl, healthcare, java, python, snowflake, sql"
Remote - Job Opportunity - Data Engineer,"Donato Technologies, Inc.",United States,https://www.linkedin.com/jobs/view/remote-job-opportunity-data-engineer-at-donato-technologies-inc-3690943882,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job Title:
Data Engineer
Location:
Remote
Duration:
Long Term
Required Experience
Data Engineer with Enterprise Data warehouse/Datamart and ETL background.
Experience in Property & Casualty (P&C) Insurance domain is required.
Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.
Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database
Experience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.
Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.
Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).
Understanding of Data modeling for Enterprise Data Warehouse ecosystems
Knowledge in Spark, PySpark preferable.
Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues.
Working knowledge on different file formats such as JSON, Parquet, CSV, etc.
Familiarity with data encryption, data masking
Database experience in SQL Server
Qualifications
Bachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science
Show more
Show less","Data Engineering, Data Warehousing, Datamart, ETL, Property & Casualty (P&C) Insurance, Python, DataFrame, APIs, Batch processing, Data pipelines, Azure platform, Azure Data Lake, Data Bricks, Delta Lake, Data Factory, Azure SQL Database, SQL, Excel, Data modeling, Spark, PySpark, Data lake, Data streaming, JSON, Parquet, CSV, Data encryption, Data masking, SQL Server","data engineering, data warehousing, datamart, etl, property casualty pc insurance, python, dataframe, apis, batch processing, data pipelines, azure platform, azure data lake, data bricks, delta lake, data factory, azure sql database, sql, excel, data modeling, spark, pyspark, data lake, data streaming, json, parquet, csv, data encryption, data masking, sql server","apis, azure data lake, azure platform, azure sql database, batch processing, csv, data bricks, data encryption, data engineering, data factory, data lake, data masking, data streaming, dataframe, datamart, datamodeling, datapipeline, datawarehouse, delta lake, etl, excel, json, parquet, property casualty pc insurance, python, spark, sql, sql server"
Senior Data Engineer,TrustEngine,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-trustengine-3722210170,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job Summary:
TrustEngine is on a mission to revolutionize the mortgage tech industry and empower borrowers to achieve their financial goals. With our Borrower Intelligence Platform and the ethical use of data and machine learning, we equip lenders with the tools for meaningful conversations that support borrowers in pursuing their life goals. We're transforming the industry, fostering trust, making a positive impact on lives, and creating a future of #NoBorrowerLeftBehind. Join us on this thrilling journey. Unlock the full potential of our organization and accelerate our growth!
Bring your deep expertise in building out data pipelines and implementing data lake architecture. As a senior data engineer, you will elevate your team, foster collaboration, and drive continuous improvement to deliver industry-transforming products with technical excellence. You will contribute to the success of the company by designing and implementing a scalable data platform as the foundation of our tech stack. Collaborating with cross-functional teams, you will serve as a subject matter expert to drive data-driven decision-making and create innovative solutions for our customers.
Join us and shape the future of mortgage tech!
Responsibilities:
Design and develop robust data pipelines and ETL processes for ingesting, transforming, and integrating data from various sources into the data lake
Optimize data infrastructure for performance, reliability, and scalability, considering factors such as data volume, velocity, and variety
Ensure data quality, integrity, and security across the data lake, implementing data governance practices and monitoring mechanisms
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and develop solutions that meet business needs
Champion engineering best practices and drive improvements in data engineering methodologies, processes, and technologies
Stay updated with emerging trends and advancements in data engineering, sharing knowledge and insights with the team and the broader organization
Effectively communicate technical concepts, solutions, and recommendations to both technical and non-technical stakeholders
Requirements
Required Qualifications:
4+ years of professional experience in data engineering, building scalable data pipelines and ETL processes
Strong proficiency in programming languages commonly used in data engineering, such as Java, Scala, or Python
In-depth knowledge of distributed systems, data processing frameworks (such as Hadoop, Spark, or Flink), data lake, and cloud computing platforms (e.g., AWS, Google Cloud, Azure)
Proficient in SQL and relational database technologies (e.g., MySQL, PostgreSQL)
Strong knowledge of data modeling, and data warehousing concepts
Familiarity with data governance, security, access controls, and compliance principles and solutions
Demonstrated ability to optimize data pipelines for performance, scalability, and reliability
Excellent problem-solving and analytical skills, with a passion for tackling complex data engineering challenges
Excellent communication skills, with the ability to contribute to a shared vision
Preferred Qualifications:
Experience with real-time data processing and streaming frameworks (e.g., Kafka, AWS Kinesis)
Knowledge of modern data lakehouse technologies such as Apache Hudi, Delta Lake, or Apache Iceberg
Experience with NoSQL databases (e.g., Redis, Cassandra)
Experience with data visualization tools (e.g., Tableau, Metabase) and data exploration techniques
Contributions to open-source data engineering projects or active participation in the data engineering community
Education and Experience:
Bachelor's degree in Computer Science or a related technical field. Advanced degrees are preferred but not required. Equivalent practical experience will also be considered
Benefits
Our benefits include but are not limited to the following: 100% company paid medical; company matching 401(k), paid maternity and paternity leave, unlimited FTO package, ongoing professional development and certification opportunities, competitive salary, special employee discounts and health and wellness perks.
Show more
Show less","Data engineering, Data lake architecture, Data pipelines, ETL, Hadoop, Spark, Flink, SQL, MySQL, PostgreSQL, Data modeling, Data warehousing, Data governance, Security, AWS, Google Cloud, Azure, Kafka, Apache Hudi, Delta Lake, Apache Iceberg, NoSQL, Redis, Cassandra, Tableau, Metabase","data engineering, data lake architecture, data pipelines, etl, hadoop, spark, flink, sql, mysql, postgresql, data modeling, data warehousing, data governance, security, aws, google cloud, azure, kafka, apache hudi, delta lake, apache iceberg, nosql, redis, cassandra, tableau, metabase","apache hudi, apache iceberg, aws, azure, cassandra, data engineering, data governance, data lake architecture, datamodeling, datapipeline, datawarehouse, delta lake, etl, flink, google cloud, hadoop, kafka, metabase, mysql, nosql, postgresql, redis, security, spark, sql, tableau"
Developer/Data Engineer || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/developer-data-engineer-remote-at-steneral-consulting-3665291336,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"remote role
Key Experience(s)
Prior experience as a SQL or PL/SQL developer (5+ years)
Previously developed in Power BI (5+ years)
Previous experience working with Databricks
Previous ETL data validation and reconciliation experience
Previously participated in Data Conversion
Understands how to iteratively develop
Understanding of Agile (SCRUM) practices
Previously run or contributed to discovery interviews
Previous work on FISERV products, specifically DNA
Previous financial services/banking experience is a plus
Key Skill(s)
Good communicator, both written and verbal
Well organized
Attention to detail
Team Player
Key Technology /Certification(s)
PowerBI, SQL, Databricks
Show more
Show less","SQL, PL/SQL, Power BI, ETL, Data Conversion, Agile, SCRUM, FISERV, DNA","sql, plsql, power bi, etl, data conversion, agile, scrum, fiserv, dna","agile, data conversion, dna, etl, fiserv, plsql, powerbi, scrum, sql"
"Data Engineer Seattle, WA (remote)","Conch Technologies, Inc",United States,https://www.linkedin.com/jobs/view/data-engineer-seattle-wa-remote-at-conch-technologies-inc-3765052211,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Hi,
Greetings from Conch Technologies Inc
Data Engineering Lead – With Databricks and Unity Catalog
Location – Seattle, WA (remote)
Duration – 12 months
Need:
Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.
As a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.
Key Responsibilities
Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.
Collaborate with vendor and internal teams to address technical issues.
Drive planning and prioritization in conjunction with the SCRUM master.
Coordinate with onsite and offshore teams for timely project delivery.
Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.
Collect and manage user-reported issues, ensuring prompt resolution and communication.
Define comprehensive test plans, oversee their execution, and document results.
--
With Regards,
Nagesh G
Mobile:
408-381-5645
Desk:
901-313-3066
Email: nagesh@conchtech.com
Web:
www.conchtech.com
Show more
Show less","Databricks, Unity Catalog, Privacera, Collibra, Scrum, Data architecture, Data engineering, Data governance, Data quality, Data integration, Data analytics, Big Data, Data visualization, Data warehousing, Data mining, Machine learning, Artificial intelligence","databricks, unity catalog, privacera, collibra, scrum, data architecture, data engineering, data governance, data quality, data integration, data analytics, big data, data visualization, data warehousing, data mining, machine learning, artificial intelligence","artificial intelligence, big data, collibra, data architecture, data engineering, data governance, data integration, data mining, data quality, dataanalytics, databricks, datawarehouse, machine learning, privacera, scrum, unity catalog, visualization"
Senior Data Engineer,Act Digital Consulting,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-act-digital-consulting-3643167692,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Our client is looking for a Senior Data Engineer who will be responsible for designing and building scalable and robust data systems. The ideal candidate will be an expert in data engineering technologies, have a strong understanding of data architecture, and be able to work with large and complex data sets. The Senior Data Engineer will also work closely with the data science and analytics teams to ensure data integrity and develop data pipelines.
Duties & Responsibilities
Design and build large scale data pipelines to process and analyze large volumes of data.
Build and maintain efficient data infrastructure, ensuring data quality and consistency.
Work with the data science and analytics teams to ensure data accuracy and completeness.
Collaborate with other engineers to build scalable and maintainable data systems.
Develop and implement data governance and security policies.
Continuously evaluate and improve data processes to optimize system performance.
Keep up to date with the latest data engineering technologies and trends.
Technical Skills
Expertise in SQL and NoSQL databases, data warehousing, and data modeling.
Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.
Experience using data pipeline frameworks such as Apache Beam or Apache Spark at scale.
Experience using data orchestration / automation frameworks such as Airflow, Databricks and MLFlow.
Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Experience with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.
Experience with data visualization tools such as Tableau or Power BI.
Minimum Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or related field.
Excellent problem-solving skills and ability to work independently.
Strong communication and collaboration skills.
Preferred Qualifications
Master's degree in Computer Science, Engineering, or a related field.
Experience in machine learning or data science.
All positions with our client require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Client's employment policies. You will be notified during the hiring process which checks are required for the position.
Our client is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice Client will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
Show more
Show less","SQL, NoSQL, Data warehousing, Data modeling, Python, Java, Scala, Go, R, Apache Beam, Apache Spark, Airflow, Databricks, MLFlow, GCP, AWS, Azure, Cloud Formation, Terraform, Tableau, Power BI","sql, nosql, data warehousing, data modeling, python, java, scala, go, r, apache beam, apache spark, airflow, databricks, mlflow, gcp, aws, azure, cloud formation, terraform, tableau, power bi","airflow, apache beam, apache spark, aws, azure, cloud formation, databricks, datamodeling, datawarehouse, gcp, go, java, mlflow, nosql, powerbi, python, r, scala, sql, tableau, terraform"
4242 - Data Engineer,Mission Box Solutions,"Dayton, OH",https://www.linkedin.com/jobs/view/4242-data-engineer-at-mission-box-solutions-3787913375,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.
Must-Haves
:
Minimum of 6 years of experience as a data engineer
Strong SQL skills in multiple data base platforms
Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python
Cloud experience: Azure, AWS, or GCP
Develop and maintain ETL pipelines
Database design and principles
Data modeling, schema development, and data-centric documentation
Experience integrating data from a variety of data source types
Recommend and advise on optimal data models for data ingestion, integration, and visualization
Experience improving code performance and query optimization
Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment
Outstanding problem-solving skills
Excellent verbal and written communication skills and the ability to interact professionally with a diverse group, including executives, managers, and subject matter experts
Salary
: $100,000-$140,000
Location
: Varies by client:
Columbus, OH, US
Cincinnati, OH, US
Dayton, OH, US
Remote Considered
Benefits:
Generous PTO
Medical Insurance
Dental & Vision Insurance
401K Match
Short/Long term Disability Insurance
Mission Box Solutions is an Equal Opportunity Employer. We value the benefits of diversity in our workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity and expression, national origin, disability, protected Veteran status, or any other attribute or protected characteristic by law. Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship is required. Our strategic partner is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract of employment and are subject to change at the discretion of our strategic partner.
Powered by JazzHR
x62P458ugO
Show more
Show less","Data Engineering, Data Management, Analytics, SQL, Snowflake, Databricks, Spark SQL, PySpark, Python, Azure, AWS, GCP, ETL Pipelines, Database Design, Data Modeling, Schema Development, DataCentric Documentation, Data Integration, Data Visualization, Code Performance Improvement, Query Optimization, Continuous Integration, Continuous Delivery, ProblemSolving","data engineering, data management, analytics, sql, snowflake, databricks, spark sql, pyspark, python, azure, aws, gcp, etl pipelines, database design, data modeling, schema development, datacentric documentation, data integration, data visualization, code performance improvement, query optimization, continuous integration, continuous delivery, problemsolving","analytics, aws, azure, code performance improvement, continuous delivery, continuous integration, data engineering, data integration, data management, database design, databricks, datacentric documentation, datamodeling, etl pipelines, gcp, problemsolving, python, query optimization, schema development, snowflake, spark, spark sql, sql, visualization"
Remote Job Opportunity - Data Engineer – (Need: Core Data Engineer),"Donato Technologies, Inc.",United States,https://www.linkedin.com/jobs/view/remote-job-opportunity-data-engineer-%E2%80%93-need-core-data-engineer-at-donato-technologies-inc-3731654219,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job Title:
Data Engineer – (Need: Core Data Engineer)
Location:
Remote
Duration:
Long Term
Required Experience
Data Engineer with Enterprise Data warehouse/Datamart and ETL background.
Experience in Property & Casualty (P&C) Insurance domain is required.
Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.
Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL Database
Experience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.
Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.
Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).
Understanding of Data modeling for Enterprise Data Warehouse ecosystems
Knowledge in Spark, PySpark preferable.
Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues.
Working knowledge on different file formats such as JSON, Parquet, CSV, etc.
Familiarity with data encryption, data masking
Database experience in SQL Server
Qualifications
Bachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science
Show more
Show less","Core Data Engineer, Data Engineer, Data Warehouse, Datamart, ETL, Property & Casualty (P&C) Insurance, Python, DataFrame, APIs, Batch processing, Data pipelines, Azure, Azure Data Lake, DataBricks, Delta Lake, Azure SQL Database, SQL, Excel, Pivot Tables, Data Modeling, Spark, PySpark, Data lake environment, Data streaming, Data encryption, Data masking, SQL Server, Data science, Computer science, Mathematics, Statistics","core data engineer, data engineer, data warehouse, datamart, etl, property casualty pc insurance, python, dataframe, apis, batch processing, data pipelines, azure, azure data lake, databricks, delta lake, azure sql database, sql, excel, pivot tables, data modeling, spark, pyspark, data lake environment, data streaming, data encryption, data masking, sql server, data science, computer science, mathematics, statistics","apis, azure, azure data lake, azure sql database, batch processing, computer science, core data engineer, data encryption, data lake environment, data masking, data science, data streaming, databricks, dataengineering, dataframe, datamart, datamodeling, datapipeline, datawarehouse, delta lake, etl, excel, mathematics, pivot tables, property casualty pc insurance, python, spark, sql, sql server, statistics"
Senior Data Engineer,Wise Skulls,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-wise-skulls-3753079691,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Title: Senior Data Engineer
Location: Remote
Duration: 6 Months
Implementation Partner: Infosys
End Client: To be disclosed
Jd
Experience establishing data flow patterns that cover ADLS 2 -> Bronze (preferably via Autoloader w/ event integration from ADLS + automatic schema mapping) -> Silver -> Gold.
Well versed in differentiating between datasets destined for simple “data lake” consumption vs datasets that should be incorporated into a dimensional or aggregated model under the gold layer of the medallion architecture.
Experience laying the foundation for generic data flows to support our goals around re-use and developer experience.
Experience integrating and controlling Databricks, Azure Infrastructure, and other BI Pipeline artifacts with source control under a CI/CD paradigm.
Experience with best practices for ad-hoc querying under Databricks (SQL Serverless) and data consumption under Power BI (certified datasets, snapshots, direct query)
Experience with the Databricks “Unity Catalog” (we understand the hive metastore that comes out of the box is not compatible with the metastore used for unity catalog, but are interested in using features under Unity Catalog to drive Data Discovery and Governance).
Experience migrating existing, on-premise data warehouse to cloud solutions using ADF to support hybrid migration strategies.
Show more
Show less","Data Lake, Data Flow, ADLS 2, Bronze, Autoloader, Schema Mapping, Dimensional Model, Medallion Architecture, Generic Data Flows, Databricks, Azure Infrastructure, CI/CD Paradigm, Databricks Unity Catalog, Hive Metastore, Data Discovery, Governance, ADF, Hybrid Migration Strategies","data lake, data flow, adls 2, bronze, autoloader, schema mapping, dimensional model, medallion architecture, generic data flows, databricks, azure infrastructure, cicd paradigm, databricks unity catalog, hive metastore, data discovery, governance, adf, hybrid migration strategies","adf, adls 2, autoloader, azure infrastructure, bronze, cicd paradigm, data discovery, data flow, data lake, databricks, databricks unity catalog, dimensional model, generic data flows, governance, hive metastore, hybrid migration strategies, medallion architecture, schema mapping"
Data Engineer,IVY TECH SOLUTIONS INC,United States,https://www.linkedin.com/jobs/view/data-engineer-at-ivy-tech-solutions-inc-3787767772,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Remote Position
Data Engineer
Only w2
Please send the resume to
sowmya.g@ivytechsol.us
Job Description
:
DataStage Data Engineer US/GC/Certain EADs – 6 month remote contract.
Together we are building a culture that values diversity and creates a space of belonging for all our team members. We believe that investing in your success is an investment in our customers and our business. Our people are what sets us apart and make us great. As a Data Engineer, you’ll provide your talents in contributing to the success of the client’s team by delivering the following:
Serve in the goalie rotation to support the Production environment.
Responsible for maintaining enterprise-grade platforms that enable data-driven solutions.
Search for ways to automate and maintain scalable infrastructure.
Ensure delivery of highly available and scalable systems.
Monitor all systems and applications and ensure optimal performance.
Analyzes and designs technical solutions to address production problems.
Participate in troubleshooting applications and systems issues.
Identifies, investigates, and proposes solutions to technical problems.
While providing technical support for issues, develop, test, and modify software to improve efficiency of data platforms and applications.
Monitors system performance to maintain consistent up time.
Prepares and maintains necessary documentation.
Participate in daily standups, team backlog grooming, and iteration retrospectives.
Coordinate with data operations teams to deploy changes into production.
Highest level may function as a lead.
Other duties as assigned.
Qualifications:
Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems. Prefer experience with IBM DataStage.
Various programming languages like Java and Python, orchestration tools and processes or other directly related experience.
A combination of education and experience may meet qualifications.
Excellent analytical, organizational, and problem-solving skills.
Ability and desire to learn new technologies quickly.
Ability to work independently and collaborate with others at all levels of technical understanding.
Able to meet deadlines.
Good judgment and project management skills.
Ability to communicate both verbally and in writing with both technical and non-technical staff.
Ability to work in a team environment and have good interpersonal skills.
Ability to adapt to changing technology and priorities.
Must be able to work independently, handle multiple concurrent tasks, with an ability to prioritize and manage tasks effectively
Powered by JazzHR
gDX0uvPo7F
Show more
Show less","DataStage, SQL, UNIX/Linux, Big data, Java, Python, Orchestration, ETL, Production support, Troubleshooting, Problemsolving, Analytical, Communication, Teamwork, Project management, Time management, Adaptability","datastage, sql, unixlinux, big data, java, python, orchestration, etl, production support, troubleshooting, problemsolving, analytical, communication, teamwork, project management, time management, adaptability","adaptability, analytical, big data, communication, datastage, etl, java, orchestration, problemsolving, production support, project management, python, sql, teamwork, time management, troubleshooting, unixlinux"
Data Engineer,Anika Systems,"Leesburg, VA",https://www.linkedin.com/jobs/view/data-engineer-at-anika-systems-3787766893,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Anika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.
Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance
Responsibilities
Designs, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.
Defines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.
Advises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.
Designs and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.
Uses data mapping, data mining, and data transformational analysis tools to design and develop databases.
Determines data storage and optimum storage requirements.
Prepares system requirements, source analysis, and process analyses and designs throughout the database implementation.
Required Skills And Experience
BA/BS and 1 year of relevant experience
Experience with DataBricks, SQL, Python
Applications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.
Excellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success
Desired Skills And Experience
Demonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.
Powered by JazzHR
TlKWIorucJ
Show more
Show less","Data Engineering, DataBricks, SQL, Python, Database Management Systems, Data Mining, Data Transformation, Data Storage, System Requirements, Source Analysis, Process Analysis, Data Architecture, Communication Skills, Organizational Skills, Database Modeling, Relational Database Architecture, Metadata, Repository Creation, Configuration Management","data engineering, databricks, sql, python, database management systems, data mining, data transformation, data storage, system requirements, source analysis, process analysis, data architecture, communication skills, organizational skills, database modeling, relational database architecture, metadata, repository creation, configuration management","communication skills, configuration management, data architecture, data engineering, data mining, data storage, data transformation, database management systems, database modeling, databricks, metadata, organizational skills, process analysis, python, relational database architecture, repository creation, source analysis, sql, system requirements"
Data Engineer,IVY TECH SOLUTIONS INC,United States,https://www.linkedin.com/jobs/view/data-engineer-at-ivy-tech-solutions-inc-3787778176,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Job Title: Data Engineer
Remote
C2C Position
Please send the resume to supriya.g@ivytechsol.us
Job Description:
Data Engineer
Position can be 100% remote
Skills
hive; KAFKA; Python; SPARK; UNIX; Hadoop platform
Required (Individual Role)
Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka
Significant experience in Python programming
Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms.
Hands-on experience creating automated data integration applications using data models, data mappings and business rules specifications to load data warehouses, operational data stores, data marts, and data lakes while programmatically handling exceptions including late arriving, missing, or erroneous data.
Experience in UNIX shell scripting
Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls)
Demonstrated experience enabling access to data by way of databases or dashboards.
Demonstrated experience cleaning, filtering, transforming data, and/or enriching data
Thanks & Regards,
Supriya/ Recruiter
224-348-8788
Ivytech Solutions Inc
Powered by JazzHR
g3o7gDvWps
Show more
Show less","Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, Kafka, Python, UNIX, Data ingestion, Data integration, Data mapping, Data warehouse, Data mart, Data lake, Data quality, Data movement, Data cleaning, Data filtering, Data transformation, Data enrichment","apache hdfs, sqoop, spark, hive, impala, hbase, kafka, python, unix, data ingestion, data integration, data mapping, data warehouse, data mart, data lake, data quality, data movement, data cleaning, data filtering, data transformation, data enrichment","apache hdfs, data cleaning, data enrichment, data filtering, data ingestion, data integration, data lake, data mapping, data mart, data movement, data quality, data transformation, datawarehouse, hbase, hive, impala, kafka, python, spark, sqoop, unix"
Sr. Data Engineer || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/sr-data-engineer-remote-at-steneral-consulting-3702365686,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Must have 5+ years US based work experience
Top Skills
5+ years of data engineering experience
Experience providing Cloud solutions is a must
Must have Hadoop experience
End to end use case implementation, Ingestion to consumption
Hands on with wide variety of use cases like ETL, Data Hubs, Data warehousing, Data lakes
Programming experience using Python
Working knowledge in one of the ETL tools (Teradata/Informatica/Infoworks/Ab initio)
Experience with Azure Cloud or AWS environments
Show more
Show less","Data Engineering, Cloud Solutions, Hadoop, ETL, Data Hubs, Data Warehousing, Data Lakes, Python, Teradata, Informatica, Infoworks, Ab Initio, Azure Cloud, AWS","data engineering, cloud solutions, hadoop, etl, data hubs, data warehousing, data lakes, python, teradata, informatica, infoworks, ab initio, azure cloud, aws","ab initio, aws, azure cloud, cloud solutions, data engineering, data hubs, data lakes, datawarehouse, etl, hadoop, informatica, infoworks, python, teradata"
Data Engineer  - Need candidates from Texas ( REMOTE ),Pulivarthi Group (PG),United States,https://www.linkedin.com/jobs/view/data-engineer-need-candidates-from-texas-remote-at-pulivarthi-group-pg-3770685212,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/
Pulivarthi Group LLC
is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.
We’ve served some of the largest healthcare, financial services, and government entities in the U.S.
Data Engineer - FT only - Azure SQL with data lakes experience
Data Engineer -
the resource requirement for this Data Engineer role needed more clarification.
Our customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.
Once the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.
The Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration.
Show more
Show less","Azure, SQL, Data Lakes, CI/CD, Continuous Integration, Continuous Deployment, DevOps, Data Sources","azure, sql, data lakes, cicd, continuous integration, continuous deployment, devops, data sources","azure, cicd, continuous deployment, continuous integration, data lakes, data sources, devops, sql"
Senior Data Engineer,Tiger Analytics,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tiger-analytics-3780467078,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
We are seeking an experienced Data Engineer to join our data team. As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Dataiku. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and support data-driven decision making across the organization.
Requirements
8+ years of overall industry experience specifically in data engineering
Strong knowledge of data engineering principles, data integration, and data warehousing concepts
Strong understanding of the pharmaceutical domain, including knowledge of clinical data, drug development processes, regulatory requirements, and healthcare data
Proficiency in data engineering technologies and tools, such as SQL, Python, ETL frameworks, data integration platforms, and data warehousing solutions
Experience with data modeling, database design, and data architecture principles
Familiarity with big data technologies (e.g., Hadoop, Spark) and cloud platforms - AWS, Azure
Strong analytical and problem-solving skills, with the ability to work with large and complex datasets
Strong communication and collaboration abilities
Attention to detail and a focus on delivering high-quality work
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
Show more
Show less","Data Engineering, Machine Learning, Artificial Intelligence, Dataiku, SQL, Python, ETL frameworks, Data integration platforms, Data warehousing, Data modeling, Database design, Data architecture, Hadoop, Spark, AWS, Azure, Big data, Cloud platforms, Analytical skills, Problemsolving skills, Communication skills, Collaboration skills, Attention to detail","data engineering, machine learning, artificial intelligence, dataiku, sql, python, etl frameworks, data integration platforms, data warehousing, data modeling, database design, data architecture, hadoop, spark, aws, azure, big data, cloud platforms, analytical skills, problemsolving skills, communication skills, collaboration skills, attention to detail","analytical skills, artificial intelligence, attention to detail, aws, azure, big data, cloud platforms, collaboration skills, communication skills, data architecture, data engineering, data integration platforms, database design, dataiku, datamodeling, datawarehouse, etl frameworks, hadoop, machine learning, problemsolving skills, python, spark, sql"
Senior Data Engineer,CY9,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-cy9-3787512861,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Job description:
Responsibilities:
• Export data from the Hadoop ecosystem to ORC or Parquet file
• Build scripts to move data from on-prem to GCP
• Build Python/PySpark pipelines
• Transform the data as per the outlined data model
• Proactively improve pipeline performance and efficiency
‘
Must Have’ Experience:
• 4+ years of Data Engineering work experience(ETL, SSIS, SSRS)
• 2+ years of building Python/PySpark pipelines
• 2+ years working with Hadoop/Hive
• 4+ years of experience with SQL
• Any cloud experience – AWS, Azure, GCP (GCP Desired)
• Experience with Data Warehousing & Data Lake
• Understanding of Data Modeling
• Understanding of data files format like ORC, Parquet, Avro
‘
Nice to Have’ Experience:
• Google experience – Cloud Storage, Cloud Composer, Dataproc & BigQuery
• Experience using Cloud Warehouses like BigQuery (preferred), Amazon Redshift, Snowflake
• etc.
• Working knowledge of Distributed file systems like GCS, S3, HDFS etc.
• Understanding of Airflow / Cloud Composer
• CI/CD and DevOps experience
• ETL tools e.g., Informatica (IICS) Ab Initio, Infoworks, SSIS
Show more
Show less","Apache Hadoop, Apache Hive, Google Cloud Platform (GCP), Python, PySpark, SQL, Data Warehousing, Data Lake, Data Modeling, ORC, Parquet, Avro, Cloud Storage, Cloud Composer, Dataproc, BigQuery, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Storage, Google Cloud Composer, Google Dataproc, Google BigQuery, Google Cloud Warehouses, Amazon Redshift, Snowflake, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Hadoop Distributed File System (HDFS), Apache Airflow, CI/CD, DevOps, Informatica (IICS), Ab Initio, Infoworks, SQL Server Integration Services (SSIS)","apache hadoop, apache hive, google cloud platform gcp, python, pyspark, sql, data warehousing, data lake, data modeling, orc, parquet, avro, cloud storage, cloud composer, dataproc, bigquery, amazon web services aws, microsoft azure, google cloud storage, google cloud composer, google dataproc, google bigquery, google cloud warehouses, amazon redshift, snowflake, google cloud storage gcs, amazon simple storage service s3, hadoop distributed file system hdfs, apache airflow, cicd, devops, informatica iics, ab initio, infoworks, sql server integration services ssis","ab initio, amazon redshift, amazon simple storage service s3, amazon web services aws, apache airflow, apache hadoop, apache hive, avro, bigquery, cicd, cloud composer, cloud storage, data lake, datamodeling, dataproc, datawarehouse, devops, google bigquery, google cloud composer, google cloud platform gcp, google cloud storage, google cloud storage gcs, google cloud warehouses, google dataproc, hadoop distributed file system hdfs, informatica iics, infoworks, microsoft azure, orc, parquet, python, snowflake, spark, sql, sql server integration services ssis"
Senior Data Engineer - XC,Bosch USA,"Plymouth, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-xc-at-bosch-usa-3581292494,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Company Description
As one of the largest North American automotive suppliers, Bosch develops Driver Assistance functions like Adaptive Cruise Control (ACC), Predictive Emergency Brake Systems (PEBS), Lane Departure Warning/Keeping Systems (LDW, LKS), Predictive Pedestrian Protection, Road Sign recognition, head light control, Advanced Parking Assistance, and many more.
We develop state of the art systems as well as advanced features leading to partly/highly automated driving; we have all of the necessary sensors in our portfolio (e.g. radar, camera, ultrasonic sensors, etc.) Join us to become part of the exciting and growing field of Driver Assistance.
Job Description
We are on the mission to turn latest technology into outstanding Bosch products and services. In our team we are developing the perception of the next generation of automatic parking systems. We are driving the development of the computer vision, ultrasonic sensor perception and creating the necessary SW to turn the sensor raw information into a vector space representation.
This vector space representation is the foundation for industry leading automatic parking functions. With our latest sensor generations, we have successfully introduced machine learning in our products. This was the first step on an exciting journey. There is a lot of opportunity ahead.
When you are the kind of person who combines a deep software engineering background with a we “can make it happen” attitude, let’s have a more detailed chat.
As a Senior Data Engineer you will develop and operate data pipelines delivering data from our engineering and the customer fleet to power our machine learning pipelines and provide data for decision making.
You will shape the future of automatic parking systems and establish best practices for embedded AI projects.
You will enable function developers to make use of your pipeline artifacts
As part of an agile team your ideas will be heard and impact the decision-making process. With our goal to invent for life, you will work on solutions that are both, innovative and ethical.
You will collaborate with scientists, electrical engineers, and machine learning engineers, ML Ops engineers to have real world impact.
Lifelong learning is crucial for long term success and we encourage you to stay current with latest research by visiting conferences and sharing your knowledge throughout the enterprise.
Qualifications
Basic Qualifications:
Education: Bachelor's or Master's degree in Computer Science, Electric Engineering, other Engineering discipline or foreign equivalent
3+ years of experience with building data pipelines within a Cloud or Cloud-hybrid setup; in-depth understanding of relational database systems (e.g. Oracle, MS SQLServer).
3+ years of experience with distributed computing frameworks (e.g. k8s, Spark)
3+ years of experience in object-oriented software development, (e.g. Python, Java, or Go).
2+ years of experience with Linux.
Preferred
Education: successfully completed Master's degree in Computer Science or other engineering discipline
Experience with recent non-relational storage technologies (NoSQL and distributed)
Experience with workflow automation tools (e.g. Jenkins, Ansible)
Experience with ECU SW re-simulation
Experience with various messaging systems (e.g. Kafka)
Experience in designing data models and choice of respective data formats
Experience with in-vehicle data collection skills, structured and analytical connectivity
Additional Information
All your information will be kept confidential according to EEO guidelines.
By choice, we are committed to a diverse workforce - EOE/Protected Veteran/Disabled.
BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives
FIRST Robotics (For Inspiration and Recognition of Science and Technology)
AWIM (A World In Motion)
Show more
Show less","Computer Science, Electric Engineering, Data pipelines, Cloud computing, Relational database systems, Distributed computing frameworks, Objectoriented software development, Linux, Nonrelational storage technologies, Workflow automation tools, ECU SW resimulation, Messaging systems, Data models, Data formats, Invehicle data collection, Structured connectivity, Python, Java, Go, Kubernetes, Spark, Oracle, MS SQLServer, NoSQL, Jenkins, Ansible, Kafka","computer science, electric engineering, data pipelines, cloud computing, relational database systems, distributed computing frameworks, objectoriented software development, linux, nonrelational storage technologies, workflow automation tools, ecu sw resimulation, messaging systems, data models, data formats, invehicle data collection, structured connectivity, python, java, go, kubernetes, spark, oracle, ms sqlserver, nosql, jenkins, ansible, kafka","ansible, cloud computing, computer science, data formats, data models, datapipeline, distributed computing frameworks, ecu sw resimulation, electric engineering, go, invehicle data collection, java, jenkins, kafka, kubernetes, linux, messaging systems, ms sqlserver, nonrelational storage technologies, nosql, objectoriented software development, oracle, python, relational database systems, spark, structured connectivity, workflow automation tools"
Mostly Remote: Data Engineer,Stellar Professionals,United States,https://www.linkedin.com/jobs/view/mostly-remote-data-engineer-at-stellar-professionals-3718987119,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Skills Required
Strong knowledge of SQL to aid in data visualization
Working knowledge of Hadoop tools, such as Spark, Impala, Hue, and Kafka, to create, query, and manage ETL data flows
working knowledge of data connectors for embedding data visualizations from Tableau Server or PowerBI into SharePoint
Knowledge of Tableau Server and/or Microsoft PowerBI
Python experience is a bonus
Show more
Show less","SQL, Spark, Impala, Hue, Kafka, Tableau Server, PowerBI, SharePoint, Python","sql, spark, impala, hue, kafka, tableau server, powerbi, sharepoint, python","hue, impala, kafka, powerbi, python, sharepoint, spark, sql, tableau server"
Data Engineer,IVY TECH SOLUTIONS INC,United States,https://www.linkedin.com/jobs/view/data-engineer-at-ivy-tech-solutions-inc-3787773282,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Job Title: Data Engineer
Remote
C2C Position
Please send the resume to sowmya.g@ivytechsol.us OR CALL: 2243488595
Job Description:
Data Engineer
Position can be 100% remote
Skills
hive; KAFKA; Python; SPARK; UNIX; Hadoop platform
Required (Individual Role)
Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka
Significant experience in Python programming
Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms.
Hands-on experience creating automated data integration applications using data models, data mappings and business rules specifications to load data warehouses, operational data stores, data marts, and data lakes while programmatically handling exceptions including late arriving, missing, or erroneous data.
Experience in UNIX shell scripting
Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls)
Demonstrated experience enabling access to data by way of databases or dashboards.
Demonstrated experience cleaning, filtering, transforming data, and/or enriching data
Thanks & Regards,
Sowmya/ Recruiter
2243488595
IVYTECH solutions inc
Powered by JazzHR
oeUP0YiVgP
Show more
Show less","Hive, Kafka, Python, Spark, Unix, Hadoop, Sqoop, Impala, HBase, Data models, Data mappings, Business rules, Data warehouses, Data marts, Data lakes, UNIX shell scripting, Data Quality checks, Data Movement controls, Databases, Dashboards, Data cleaning, Data filtering, Data transformation, Data enrichment","hive, kafka, python, spark, unix, hadoop, sqoop, impala, hbase, data models, data mappings, business rules, data warehouses, data marts, data lakes, unix shell scripting, data quality checks, data movement controls, databases, dashboards, data cleaning, data filtering, data transformation, data enrichment","business rules, dashboard, data cleaning, data enrichment, data filtering, data lakes, data mappings, data marts, data models, data movement controls, data quality checks, data transformation, data warehouses, databases, hadoop, hbase, hive, impala, kafka, python, spark, sqoop, unix, unix shell scripting"
Senior Data Engineer,Pierce,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-pierce-3748512527,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Strategy Creation: Collaborate with cross-functional teams to define the data engineering strategy aligned to business objectives, including data modeling that unifies data assets across a range of source systems used to manage the operations of our partnering hospitals.
Pipeline Development: Define and execute processes needed to develop, test, deploy, and maintain high quality data pipelines. Oversee the end-to-end development of data pipelines from source data extraction through to production-grade analytical dataset delivery, ensuring data quality and security throughout the pipeline.
Performance Optimization: Continuously monitor and optimize data processing performance and efficiency. Identify and address bottlenecks, optimize query performance, and improve overall system stability.
Data Governance: Establish and enforce data quality management policies, data access controls, and data privacy standards.
Technical Leadership: Stay abreast of the latest developments in engineering tools and best practices. Provide guidance to the team about technical challenges.
Documentation: Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to ensure knowledge sharing and team continuity.
Third-party Management: Evaluate and manage relationships with third-party vendors and tools, making informed decisions about when to leverage external solutions.
Requirements
6+ years in data engineering roles with progressively increasing responsibilities.
Deep understanding of data modeling, data architecture, and data integration best practices.
Strong hands-on experience with Apache Spark and cloud computing technologies.
Advanced proficiency in Python and SQL.
Familiarity with data governance, security, and privacy principles.
Comfort using collaboration tools such as GitHub or equivalent to manage development life cycle.
Excellent data modeling and engineering skills, and a talent for translating business objectives into technical solutions.
High energy, humble team player with “get it done” attitude, seeking collaboration with colleagues.
Ability to manage multiple projects simultaneously
Experience engineering in Databricks strongly preferred.
3+ years of software engineering with python in a production environment.
Experience with the Azure cloud ecosystem.
Experience developing production-ready, real-time machine learning model serving pipelines.
Comfort developing in the Apache Spark Structured Streaming paradigm.
Experience working in the veterinary services industry, or a private equity-backed services company.
Working knowledge of Microsoft Excel and Office 365.
Show more
Show less","Data engineering strategy, Data modeling, Data pipelines, Data quality, Data security, Performance optimization, Data governance, Technical leadership, Documentation, Thirdparty management, Apache Spark, Cloud computing, Python, SQL, Data governance, Data security, Data privacy, GitHub, Collaboration tools, Databricks, Software engineering, Azure cloud ecosystem, Machine learning, Apache Spark Structured Streaming, Veterinary services, Microsoft Excel, Office 365","data engineering strategy, data modeling, data pipelines, data quality, data security, performance optimization, data governance, technical leadership, documentation, thirdparty management, apache spark, cloud computing, python, sql, data governance, data security, data privacy, github, collaboration tools, databricks, software engineering, azure cloud ecosystem, machine learning, apache spark structured streaming, veterinary services, microsoft excel, office 365","apache spark, apache spark structured streaming, azure cloud ecosystem, cloud computing, collaboration tools, data engineering strategy, data governance, data privacy, data quality, data security, databricks, datamodeling, datapipeline, documentation, github, machine learning, microsoft excel, office 365, performance optimization, python, software engineering, sql, technical leadership, thirdparty management, veterinary services"
Cloud Data Engineer,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/cloud-data-engineer-at-steneral-consulting-3742030826,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Remote role
W2 candidates only
Need valid LinkedIn
Description
The Senior Cloud Data Engineer will work directly with our product lead on multiple algorithmic data science products. design, code, test, and analyze software programs and applications. This includes researching, designing, documenting, and modifying software specifications throughout the production lifecycle. This role will also create business critical reports, analyze, and amend software errors in a timely and accurate fashion and provide status reports where required. The position responsibilities outlined below are not all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.
Responsibilities
Work with Product team to determine requirements and propose approaches to address users' needs
Analyze requirements to determine approach/proposed solution
Design and build solutions using relevant programming languages
Thoroughly test solutions using relevant approaches and tools
Conduct research into software-related issues and products
Bring out-of-box thinking and solutions to address challenging issues
Effectively prioritize and execute tasks in a fast-paced environment
Work both independently and in a team-oriented, collaborative environment
Flexible and adaptable to learning and understanding new technologies
Highly self-motivated and directed
Demonstrate a commitment to Hyatt core values
Experience And Skills
Experience in designing, developing, and maintaining data pipelines on AWS cloud platform
Experience in developing solutions using Snowflake database
Hands-on software troubleshooting experience
Proven analytical and problem-solving abilities
Experience with every phase of the software development life cycle including requirements gathering, requirements analysis, design, development, and testing
Work closely with the product leads to identify and prioritize data needs for the algorithmic data science products
Collaborate with cross-functional teams to ensure data quality and accuracy
Continuously monitor and optimize data pipeline performance
Stay up-to-date with the latest technologies and industry trends in data engineering
Must Have Skills
Hands on experience with AWS cloud architecture and development data pipelines using S3, Redshift, Athena, DynamoDB, Lambda, Glue, EMR, Kinesis, API Gateway, and other AWS technologies
Hands on experience is building and monitoring CloudWatch alarms
Hands on experience with AWS CICD suite (Code commit, Code pipeline, Cloud formation)
Hands on experience is using Python (intermediate/expert level)
Hands on experience using Spark (strongly preferred)
Hands on experience building solutions using Snowflake database
Hands on experience in trouble shooting complex SQL problems
Experience working with cross-functional teams in a fast-paced, dynamic environment
Strong problem-solving skills and attention to detail
Good communication and collaboration skills
Good To Have Skills
Hands on experience building visualizations/reports
Good understanding of data modeling
Show more
Show less","AWS, CICD, Cloud Computing, Cloud Data Engineer, CloudWatch, Code Pipeline, Code Commit, Data Analysis, Data Engineering, Data Modeling, Data Pipeline, Data Science, Data Visualization, DynamoDB, EMR, Glue, Kinesis, Lambda, Python, Redshift, S3, Snowflake, Spark, SQL","aws, cicd, cloud computing, cloud data engineer, cloudwatch, code pipeline, code commit, data analysis, data engineering, data modeling, data pipeline, data science, data visualization, dynamodb, emr, glue, kinesis, lambda, python, redshift, s3, snowflake, spark, sql","aws, cicd, cloud computing, cloud data engineer, cloudwatch, code commit, code pipeline, data engineering, data pipeline, data science, dataanalytics, datamodeling, dynamodb, emr, glue, kinesis, lambda, python, redshift, s3, snowflake, spark, sql, visualization"
Senior Data Engineer,Wise Skulls,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-wise-skulls-3655551739,2023-12-17,Saint Boniface, Canada,Mid senior,Remote,"Title: Senior Data Engineer
Location: Remote (Within USA)
Duration: 6+ months
Implementation Partner: Infosys
End Client: To be disclosed
Jd
Minimum Years of Experience: 8+ Years
Work with the Enterprise Data Governance team and with the various analytics teams to build the Enterprise business Glossary for various data domains using Informatica CDGC tool.
Create Business Glossary resources, associate business terms to the assets in collaboration with Business Data Stewards & SME's.
Build Approval Workflows, create roles and permissions within CDGC platform.
Collaborate with various business and technical teams to gather requirements around data quality rules and design and develop these rules with IICS/IDQ.
Develop mappings, rules, transformations, mapplets, workflow, schedulers using Informatica IDQ 10.x.
Create & Run data quality specific ETL jobs (address standardization and validation, email cleanups, name cleanup, parsing, etc.) utilizing IDQ and IICS.
Develop and configure data lineage, custom resources, custom data lineage, relationships, data domains, data domain groups and composite data domains.
Performing statistical tests, profiling on large datasets to determine data quality and integrity.
Show more
Show less","Informatica CDGC, Business Glossary, Data Governance, Informatica IICS/IDQ, Informatica IDQ 10.x, ETL (Extract Transform Load), Data Quality, Data Lineage, Data Profiling","informatica cdgc, business glossary, data governance, informatica iicsidq, informatica idq 10x, etl extract transform load, data quality, data lineage, data profiling","business glossary, data governance, data lineage, data profiling, data quality, etl extract transform load, informatica cdgc, informatica idq 10x, informatica iicsidq"
"Senior Software Engineer, Data Infrastructure","Roberts Recruiting, LLC","Cambridge, MA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-infrastructure-at-roberts-recruiting-llc-3576884366,2023-12-17,Lowell,United States,Mid senior,Onsite,"The Data Infrastructure teams are responsible for building and maintaining data storage technologies across the product. Teams work with a variety of open source technologies like
MySQL, Vitess, Hadoop HBase, Kafka, Spark, and Elastic Search all running on AWS.
One of the primary objectives is to automate the deployments, configuration, and recoverability of data stores to efficiently scale storage systems. Teams also take on projects to improve the client-side interaction with the data stores with analysis tools and wrapper client interfaces. A number of our teams are active contributors to the Open Source projects that they work on.
We’re operationally responsible for a huge volume of traffic to and from these data stores. Our HBase clusters serve over 3 million requests/second across 220+ tables, while our ElasticSearch clusters serve over 20k searches/second and 50k indexes/second to 90+ billion documents.
Streaming that data to and from applications amounts to more than 3 GB/sec of data through our Kafka clusters, with hundreds of producers and consumers.
What We’re Looking For
We’re looking for talented software engineers to help us build the vision of making our database access simple, intuitive, highly performant and highly reliable to our customers and ohapplication developers.
The goal of the team is to implement a comprehensive set of intelligent features that will drive the productivity of using any of our data stores at scale and with the highest possible reliability.
Bonus Points For Being
Someone with experience developing automation or is a power user of one or more of the above-mentioned data storage technologies.
Willingness and ability to work with the user base, in this case, other developers, to define and build tools that make the lives of application developers easier and their code less error-prone through automation.
Interested in working with and supporting infrastructure at scale.
Someone who has shown that they can solve complicated technical problems and analyze tradeoffs with empathy for the developers, yet building creative solutions.
Able to demonstrate pragmatic decision making and problem-solving abilities.
About Us
We help millions of organizations grow better, and we’d love to grow better with you. Our business builds the software and systems that power the world’s small to medium-sized businesses. Our company culture builds connections, careers, and employee growth. How? By creating a workplace that values flexibility, autonomy, and transparency. If that sounds like something you’d like to be part of, we’d love to hear from you.
Show more
Show less","MySQL, Vitess, Hadoop HBase, Kafka, Spark, Elastic Search, AWS, Automation, Open Source, Data Storage, ClientSide Interaction, Data Stores, Analysis Tools, Wrapper Client Interfaces, Streaming, Data Infrastructure, Scalability, Reliability, Performance, User Base, Empathy, Pragmatism, Decision Making, Problem Solving","mysql, vitess, hadoop hbase, kafka, spark, elastic search, aws, automation, open source, data storage, clientside interaction, data stores, analysis tools, wrapper client interfaces, streaming, data infrastructure, scalability, reliability, performance, user base, empathy, pragmatism, decision making, problem solving","analysis tools, automation, aws, clientside interaction, data infrastructure, data storage, data stores, decision making, elastic search, empathy, hadoop hbase, kafka, mysql, open source, performance, pragmatism, problem solving, reliability, scalability, spark, streaming, user base, vitess, wrapper client interfaces"
Lead Data Engineer,Cirkul,"Watertown, MA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-cirkul-3769527818,2023-12-17,Lowell,United States,Mid senior,Onsite,"What is Cirkul?
Cirkul is a rapidly growing beverage technology company on a mission to make a healthier world by helping people enjoy drinking more water.
The team at Cirkul developed an innovative beverage delivery system that makes drinking more water delicious, fun, and personalized. The technology reduces the shipping weight of bottled beverages by 96% and uses 84% less plastic. Cirkul offers its customers 100+ unique flavors, all with no sugar, zero calories, no artificial colors, and a range of functional enhancements online, at Walmart, and other retailers across the United States. Hundreds of thousands of consumers are using Cirkul to transition from single-use plastics and sugar-filled beverages to healthier, better-for-you alternatives.
What is this role?
We are looking for a Lead Data Engineer to join our growing data team. Our new team member will manage Cirkul’s data pipeline architecture and optimize data flow and collection. This new hire should be advanced in Python and SQL, and be able to work with modern technologies in a fast paced, startup environment.
The Lead Data Engineer will support our data initiatives and will ensure an optimal data delivery architecture that is consistent throughout ongoing projects. They must be a self-starter, and comfortable supporting the data needs of multiple teams, systems and products. The right candidate should be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
What does an average day look like?
Driving Results:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and/or Node.js, SQL (using dbt for transformation), commercial SaaS and OSS offerings, and AWS ‘big data’ technologies.
Create and maintain optimal data pipeline architecture. Keep team members on-task and heading-off scope creep.
Taking Ownership:
Manage requirement-gathering and compiling business problem/solution definitions.
Manage infrastructure and configuration of data platform in the Cloud, ensuring health, reliability and uptime.
Making Decisions:
Work with management to identify projects that can deliver value.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Cultivating Relationships:
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Instilling Trust:
Ensure successful delivery of business value by communicating directly with stakeholders.
Customer Focus:
Onboard retail partners, standing up automated data ingestion pipelines and ensuring data integrity.
What background should you have?
8+ years of experience in a Data Engineer role at a fast-paced organization
8+ years of experience in any one of the Cloud providers such as AWS, Azure or GCP
Familiarity with AWS Services (Redshift, RDS, EKS, S3, EMR, Glue, Lambda) is preferred
Advanced working knowledge of Python
Exceptional fluency with SQL; you conquered the join venn diagram long ago and have moved on to explaining cost based optimization to your peers on the engineering team
Experience with modern columnar data warehouses such as Snowflake, Redshift, BigQuery
Experience with orchestration tools such as Airflow, Dagster, or Prefect, with big data tools such as Kafka & Spark, and with dbt
Experience ingesting, processing, and visualizing data sources of varying types - structured/relational and unstructured
Experience developing, managing, and manipulating large, complex datasets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environmen
What do we offer?
Competitive salary based on experience and market
Generous benefits, 401k match, and PTO (even insurance for your fur babies!)
Employee discounts on Cirkul products
New corporate office space with great amenities
Opportunity to work with a best in class team, in a hyper growth company, taking over the hydration industry
A culture that rewards results
Please review our privacy policy here .
Cirkul, Inc. is an Equal Opportunity Employer. We believe in hiring a diverse workforce and are committed to sustaining an equitable and inclusive, people-first environment. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. If you'd like more information about your EEO rights as an applicant under the law, please download the available
EEO is the Law
&
EEO is the Law Supplement documents.
Show more
Show less","Python, SQL, AWS, Redshift, RDS, EKS, S3, EMR, Glue, Lambda, Snowflake, Airflow, Dagster, Prefect, Kafka, Spark, dbt, Data pipelines, Data architecture, Data engineering, Data collection, Big data, Data visualization, Data analysis, Root cause analysis, Project management, Crossfunctional teams","python, sql, aws, redshift, rds, eks, s3, emr, glue, lambda, snowflake, airflow, dagster, prefect, kafka, spark, dbt, data pipelines, data architecture, data engineering, data collection, big data, data visualization, data analysis, root cause analysis, project management, crossfunctional teams","airflow, aws, big data, crossfunctional teams, dagster, data architecture, data collection, data engineering, dataanalytics, datapipeline, dbt, eks, emr, glue, kafka, lambda, prefect, project management, python, rds, redshift, root cause analysis, s3, snowflake, spark, sql, visualization"
Senior Data Engineer (AWS),Dice,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-aws-at-dice-3787363672,2023-12-17,Lowell,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, ARK Infotech Spectrum, is seeking the following. Apply via Dice today!
POSITION OVERVIEW:
We are currently seeking a Senior Data Engineer (AWS) to join our team
Job Duties and Responsibilities:
Understand client requirements and understand case studies or current implementation for Predictive models, able to understand business domain needs and implement data pipelines & predictive models,
Experience in Agile projects, work with multiple stakeholders like business, project team and deployment teams, ability to work independently and switch technical skills based on the project needs. Detail oriented self-starter capable of working independently.
Experience in ETL and ETL cloud services like AWS Data Pipeline Product Details, AWS Glue Experience with private or public cloud technology. Excellent written and verbal communication skills with ability to document and design proposals.
Expert in writing software packaging and deploying into a fully automated environment. Experience in Service Now.
Basic Qualifications:
5+ years of Release or Automation or Software Engineering experience, or equivalent.
5+ years Linux experience.
5+ years programming experience with Java or Python and scripting 3+ years of experience in DevSecOps toolchains/automation to achieve CICD, Blue-Green deployments, feature toggles (Git, Jenkins, uDeploy).
3+ years with Agile Scrum (Daily Standup, Sprint Planning and Sprint Retrospective meetings) and Kanban. CADM-Cloud Apps-AWS (Amazon)- 3-5 years Data and Intelligence-ETL-Architecture-ETL Tools - 3-5 years
Show more
Show less","AWS Data Pipeline, AWS Glue, Java, Python, Git, Jenkins, uDeploy, DevSecOps, Agile Scrum, Kanban, CADM, ETL Architecture, ETL Tools","aws data pipeline, aws glue, java, python, git, jenkins, udeploy, devsecops, agile scrum, kanban, cadm, etl architecture, etl tools","agile scrum, aws data pipeline, aws glue, cadm, devsecops, etl architecture, etl tools, git, java, jenkins, kanban, python, udeploy"
"Senior Software Engineer, Data Science",Motional,"Boston, MA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-science-at-motional-3763113993,2023-12-17,Lowell,United States,Mid senior,Onsite,"Mission Summary:
The Metrics Engine team oversees the extraction of data to evaluate the behavior of our av-stack, all the way from devising the metric formulation based on traffic laws, ethics and safety, to extracting the information necessary to compute these metrics from the logs. Beyond evaluating system-level behavior, we also derive metrics to evaluate subsystems that collectively participate to AV system behavior.
As we are outputting a growing amount of metrics at various stages of the development process, our team is looking for a Senior Engineer to analyze the progression of our av-stack performance with respect to those metrics, and surface key trends. This person will get to make reports and recommendations that are of direct interest to many different teams across the organization, including leadership.
The Motional global headquarters are located at 100 Northern Avenue in Boston, MA. Nestled in the bustling Seaport district with sweeping views of Boston Harbor and downtown Boston, the offices are located close to major transit lines and a quick walk to various restaurants and popular attractions.
What you'll be doing:
Analyze the output of our metric evaluator to provide high-level insights on the progress of our stack
Communicate with the Testing, Operations, and Autonomy teams to understand their day-to-day analysis needs
Provide recommendations on how to design experiments to get a stronger signal from the metrics
Question the results of certain metrics, dig into their formulations and contribute to improving them regularly
What we're looking for:
Strong Python coding experience
MSc in Statistics, Math, Physics, Computer Science or equivalent + 3 years of experience in data science in a tech-heavy industry
Python/Julia experience in statistical analysis
Commitment to rigorous analysis - if assumptions need to be made to infer results, they should be spelled out
Ability to communicate clearly to non-domain experts about potentially complex behaviors
An interest in contributing to the metric formulation and implementation
Bonus points:
Experience in evaluating human or autonomous driving behavior
Experience in robotics
The salary range for this role is an estimate based on a wide range of compensation factors including but not limited to specific skills, experience and expertise, role location, certifications, licenses, and business needs. The estimated compensation range listed in this job posting reflects base salary only. This role may include additional forms of compensation such as a bonus or company equity. The recruiter assigned to this role can share more information about the specific compensation and benefit details associated with this role during the hiring process.
Candidates for certain positions are eligible to participate in Motional's benefits program. Motional's benefits include but are not limited to medical, dental, vision, 401k with a company match, health saving accounts, life insurance, pet insurance, and more.
Salary Range
$159,000—$207,000 USD
Motional is a driverless technology company making autonomous vehicles a safe, reliable, and accessible reality. We're driven by something more.
Our journey is always people first.
We aren't just developing driverless cars; we're creating safer roadways, more equitable transportation options, and making our communities better places to live, work, and connect. Our team is made up of engineers, researchers, innovators, dreamers and doers, who are creating a technology with the potential to transform the way we move.
Higher purpose, greater impact.
We're creating first-of-its-kind technology that will transform transportation. To do so successfully, we must design for everyone in our cities and on our roads. We believe in building a great place to work through a progressive, global culture that is diverse, inclusive, and ensures people feel valued at every level of the organization. Diversity helps us to see the world differently; it's not only good for our business, it's the right thing to do.
Scale up, not starting up.
Our team is behind some of the industry's largest leaps forward, including the first fully-autonomous cross-country drive in the U.S, the launch of the world's first robotaxi pilot, and operation of the world's longest-standing public robotaxi fleet. We're driven to scale; we're moving towards commercialization of our technology, and we need team members who are ready to embrace change and challenges.
Formed as a joint venture between Hyundai Motor Group and Aptiv, Motional is fundamentally changing how people move through their lives. Headquartered in Boston, Motional has operations in the U.S and Asia. For more information, visit www.Motional.com and follow us on Twitter, LinkedIn, Facebook, Instagram and YouTube.
Motional AD Inc. is an EOE. We celebrate diversity and are committed to creating an inclusive environment for all employees. To comply with Federal Law, we participate in E-Verify. All newly-hired employees are queried through this electronic system established by the DHS and the SSA to verify their identity and employment eligibility.
Show more
Show less","Python, Statistics, Math, Physics, Computer Science, Data Science, Metric Evaluation, Experimental Design, Statistical Analysis, Communication, Autonomous Driving, Robotics","python, statistics, math, physics, computer science, data science, metric evaluation, experimental design, statistical analysis, communication, autonomous driving, robotics","autonomous driving, communication, computer science, data science, experimental design, math, metric evaluation, physics, python, robotics, statistical analysis, statistics"
Data Analyst,Infosys,"Boston, MA",https://www.linkedin.com/jobs/view/data-analyst-at-infosys-3783533784,2023-12-17,Lowell,United States,Mid senior,Onsite,"Job Description
Infosys is seeking a
Data Analyst
- In this role you will work along with product owners in defining the requirements, breaking down epics into multiple user stories, Create use case documents, functional flow diagrams. Candidate will be responsible to interface with end-users, IT teams and key stakeholders to gather system requirements. Will also apply technical proficiency across different stages of the software development life cycle including requirements elicitation; this opportunity is to be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued. You will also support knowledge transfer with the objective of providing value-adding consulting solutions that enable our clients to meet the changing needs of the global landscape.
Candidate must be located within commuting distance of
Boston, MA or Quincy, MA
be willing to relocate to the area. This position may require travel to project locations occasionally.
Required Qualifications:
• Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.
• At least 5 years of Information Technology experience.
• At least 3 years of experience in Data Engineering encompassing Tableau, Alteryx, Databases/ SQL/ PLSQL.
Preferred Qualifications:
• At least 4 years of experience in translating functional/non-functional requirements to system requirements.
• Proficient in Word, Excel, PowerPoint, MS Project.
• Experience in servicing as single point of contact with clients for projects.
• Experience with requirements analysis and documentation.
• experience in Capital Markets/ Custodian domain.
• Ability to work in team environment and client interfacing skills.
• Ability to work in a diverse/ multiple stakeholder environment.
• Strong Analytical skills.
• Knowledge in waterfall and agile methodologies.
• Detail oriented with excellent communication and organizational skills.
• Strong people management and negotiation skills.
Show more
Show less","Tableau, Alteryx, Data Engineering, Databases, SQL, PLSQL, Word, Excel, PowerPoint, MS Project, Requirements Analysis, Documentation, Waterfall, Agile, Communication, Organizational Skills, People Management, Negotiation","tableau, alteryx, data engineering, databases, sql, plsql, word, excel, powerpoint, ms project, requirements analysis, documentation, waterfall, agile, communication, organizational skills, people management, negotiation","agile, alteryx, communication, data engineering, databases, documentation, excel, ms project, negotiation, organizational skills, people management, plsql, powerpoint, requirements analysis, sql, tableau, waterfall, word"
Senior Data Engineer (Remote),MMS,"Raleigh, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782250645,2023-12-17,Vineland,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data Engineering, Data Science, Data Modeling, Data Lineage, Data Architecture, Data Curation, Data Warehousing, Data Analytics, Data Visualization, TSQL, Stored Procedures, Window Functions, Common Table Expressions, Derived Tables, Dynamic TSQL, Code Refactoring, Design Patterns, Abstract Thinking, Encapsulation, Logical Thinking, Problem Solving, Communication, Business Needs Analysis, Requirements Gathering, Azure Data Factory, Microsoft SQL, Microsoft Azure, Star Schemas, Audit Trails, CDISC, FHIR, OMOP, 21 CFR Part 11, FDA, GCP, Clinical Research, Clinical Data, Pharmaceutical Development, ISO 9001, ISO 27001, Data Privacy, Data Anonymization, MS Office","data engineering, data science, data modeling, data lineage, data architecture, data curation, data warehousing, data analytics, data visualization, tsql, stored procedures, window functions, common table expressions, derived tables, dynamic tsql, code refactoring, design patterns, abstract thinking, encapsulation, logical thinking, problem solving, communication, business needs analysis, requirements gathering, azure data factory, microsoft sql, microsoft azure, star schemas, audit trails, cdisc, fhir, omop, 21 cfr part 11, fda, gcp, clinical research, clinical data, pharmaceutical development, iso 9001, iso 27001, data privacy, data anonymization, ms office","21 cfr part 11, abstract thinking, audit trails, azure data factory, business needs analysis, cdisc, clinical data, clinical research, code refactoring, common table expressions, communication, data anonymization, data architecture, data curation, data engineering, data lineage, data privacy, data science, dataanalytics, datamodeling, datawarehouse, derived tables, design patterns, dynamic tsql, encapsulation, fda, fhir, gcp, iso 27001, iso 9001, logical thinking, microsoft azure, microsoft sql, ms office, omop, pharmaceutical development, problem solving, requirements gathering, star schemas, stored procedures, tsql, visualization, window functions"
Sr. Data Analytics Engineer,JoCo,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-analytics-engineer-at-joco-3776668230,2023-12-17,Vineland,United States,Mid senior,Onsite,"What is the position?
The Sr. Data Analytics Engineer will be responsible for working with data architects, software developers, and analysts to optimize data architecture and flow.
What will you do?
As a Sr. Data Analytics Engineer, you will:
Conceptualize and analyze data pipelines and data architectures, ensuring business requirements are met and identifying acquisition opportunities.
Increase efficiency and productivity through the automation of data preparation and integration tasks.
Support integrations utilizing various programming languages and tools.
Prepare data for predictive and prescriptive modeling.
Suggest methods for continuous data improvement.
Establish procedures for data mining, modeling, and production.
Advocate the organization's analytics and data capabilities to leaders to attain their business objectives.
Uphold data compliance initiatives to guarantee responsible use of provisioned data by users.
What are the requirements?
Bachelor's degree in information systems, computer science, or related field
5+ years of experience in data management (data modeling, integration, optimization, etc.)
3+ years of experience working with stakeholders/teams to support data analytics and management
A background working with Oil & Gas equipment data is preferred
Experience with analytics tools using Python, Java, C++, Scala
Experience with programming languages such as SQL, PL/SQL, etc.
Knowledge of NoSQL/Hadoop-oriented databases such as Cassandra, MongoDB, etc.
Experience using SQL on Hadooop tools
Experience using open-source, commercial message queing technologies such as Kafka, Azure, JMS, Amazon Simple Queuing Service, Service Bus, etc.
Experience using stream data technologies such as Amazon Kinesis, Apache Nifi, Apache Kafka Streams, Apache Beam, etc.
Knowledge of technologies such as Impala, HIVE, Presto, etc. (open source perspective) and Dremio, Informatica, HDF, Talend, etc. (commercial vendor perspective)
Knowledge of agile methodologies
Familiar with data analytics & business intelligence software tools such as Tableau, PowerBi, Qlik, etc.
Familiar with troubleshooting, generating solutions, and providing solutions to those with minimal technical knowledge
Ability to work across many deployment environments and operating systems
Ability to optimize data pipelines/ integrated datasets
Ability to integrate IT output into business processes
Verbal and written communication skills, attention to detail, and interpersonal skills
Willingness to travel (~5%)
You would be really happy working here if:
You can strategize, understanding the goals of the company and creating effective plans to achieve those goals.
You are left-brained and understand that the devil’s in the details, proactively seeking them out and understanding how they contribute to the whole of a project.
Show more
Show less","Data Analytics, Data Architecture, Data Pipelines, Automation, Data Integration, Python, Java, C++, Scala, SQL, PL/SQL, NoSQL, Hadoop, Cassandra, MongoDB, Kafka, Azure, JMS, Amazon Simple Queuing Service, Service Bus, Amazon Kinesis, Apache Nifi, Apache Kafka Streams, Apache Beam, Impala, HIVE, Presto, Dremio, Informatica, HDF, Talend, Agile Methodologies, Tableau, PowerBi, Qlik, Troubleshooting, Data Pipelines Optimization, Data Integration, IT Integration, Verbal Communication, Written Communication, Attention to Detail, Interpersonal Skills, Travel (5%), Strategizing, Planning","data analytics, data architecture, data pipelines, automation, data integration, python, java, c, scala, sql, plsql, nosql, hadoop, cassandra, mongodb, kafka, azure, jms, amazon simple queuing service, service bus, amazon kinesis, apache nifi, apache kafka streams, apache beam, impala, hive, presto, dremio, informatica, hdf, talend, agile methodologies, tableau, powerbi, qlik, troubleshooting, data pipelines optimization, data integration, it integration, verbal communication, written communication, attention to detail, interpersonal skills, travel 5, strategizing, planning","agile methodologies, amazon kinesis, amazon simple queuing service, apache beam, apache kafka streams, apache nifi, attention to detail, automation, azure, c, cassandra, data architecture, data integration, data pipelines optimization, dataanalytics, datapipeline, dremio, hadoop, hdf, hive, impala, informatica, interpersonal skills, it integration, java, jms, kafka, mongodb, nosql, planning, plsql, powerbi, presto, python, qlik, scala, service bus, sql, strategizing, tableau, talend, travel 5, troubleshooting, verbal communication, written communication"
Sr. Data Engineer,Experfy,"San Antonio, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3590332063,2023-12-17,Vineland,United States,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibilities
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Show more
Show less","Data warehouse architecture, ETL processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, SQL, Python, PySpark, Java, Spring Framework, Spring Boot, Spring Cloud, Spring Data, AWS Expertise, EMR, S3, Athena, NoSQL, Hadoop, Spark, NEO4J","data warehouse architecture, etl processing, confluent kafka, kinesis, glue, lambda, snowflake, sql server, sql, python, pyspark, java, spring framework, spring boot, spring cloud, spring data, aws expertise, emr, s3, athena, nosql, hadoop, spark, neo4j","athena, aws expertise, confluent kafka, data warehouse architecture, emr, etl processing, glue, hadoop, java, kinesis, lambda, neo4j, nosql, python, s3, snowflake, spark, spring boot, spring cloud, spring data, spring framework, sql, sql server"
Product Engineering Intern - Systems Engineer (Data Analytics),KLA,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/product-engineering-intern-systems-engineer-data-analytics-at-kla-3782767516,2023-12-17,Waterloo,United States,Associate,Onsite,"Base Pay Range: $24.00 - $40.00 per hour based on pursuit of a Bachelor’s, Master’s, or Ph.D.
Primary Location: USA-MI-Ann Arbor-KLA
KLA’s total rewards package for employees may also include participation in performance incentive programs and eligibility for additional benefits identified below. Interns are eligible for some of the benefits identified below. Our pay ranges are determined by role, level, and location. The range displayed above reflects the minimum and maximum pay for this position in the primary location identified in this posting. Actual pay depends on several factors, including location, job-related skills, experience, and relevant education level or training. If applicable, your recruiter can share more about the specific pay range for your preferred location during the hiring process.
Company Overview
KLA is a global leader in diversified electronics for the semiconductor manufacturing ecosystem. Virtually every electronic device in the world is produced using our technologies. No laptop, smartphone, wearable device, voice-controlled gadget, flexible screen, VR device or smart car would have made it into your hands without us. KLA invents systems and solutions for the manufacturing of wafers and reticles, integrated circuits, packaging, printed circuit boards and flat panel displays. The innovative ideas and devices that are advancing humanity all begin with inspiration, research and development. KLA focuses more than average on innovation and we invest 15% of sales back into R&D. Our expert teams of physicists, engineers, data scientists and problem-solvers work together with the world’s leading technology providers to accelerate the delivery of tomorrow’s electronic devices. Life here is exciting and our teams thrive on tackling really hard problems. There is never a dull moment with us.
Group/Division
With over 40 years of semiconductor process control experience, chipmakers around the globe rely on KLA to ensure that their fabs ramp next-generation devices to volume production quickly and cost-effectively. Enabling the movement towards advanced chip design, KLA's Global Products Group (GPG), which is responsible for creating all of KLA’s metrology and inspection products, is looking for the best and the brightest research scientist, software engineers, application development engineers, and senior product technology process engineers. Central Engineering is KLA's largest engineering organization comprised of 9 Centers-of-Excellence (CoE) in various disciplines applied across all product groups in the company. These CoE include Handling & Automation, Precision Motion Control, Sensors & Image Acquisition, Platform Design, and Packaging Engineering, among others. Talent includes over 500 engineers across global centers in Israel, China, India, and the US. Each CoE contributes not just talent and deliverables per discipline toward product programs, but also subject matter expertise, best practices, roadmaps, specialized facilities, apparatus, models, and analytics. These differentiate KLA not only in WHAT we do, but also in HOW we do it.
Job Description/Preferred Qualifications
As a Product Engineering Intern focused on systems engineering and data analytics, you will play a crucial role in creating insights based on operational and parametric data to enhance product development and service
You will collaborate with cross-functional teams to identify patterns, trends, and correlations that drive product innovation and efficiency.
Key Responsibilities:
Collect, process, and clean operational and parametric data from tools
Apply statistical methods and data modeling techniques to identify trends, patterns, and correlations in large datasets
Collaborate with engineering and service teams to understand requirements, and build use cases
Participate in the design and implementation of algorithms for predictive analytics and machine learning models
Propose, justify, and develop dashboards for actionable insights on the tool
Qualification:
Highly creative and eager to learn (systems engineering mindset)
Excellent communication skills and experience working with cross-functional teams
Currently pursuing a degree in engineering or a related field
Strong analytical skills with an understanding of statistical techniques
Proficiency in Python programming language
Familiarity with data visualization tools (Power BI)
Basic knowledge of machine learning algorithms and principles
Minimum Qualifications
Working toward BS, MS or PhD.
The company offers a total rewards package that is competitive and comprehensive including but not limited to the following: medical, dental, vision, life, and other voluntary benefits, 401(K) including company matching, employee stock purchase program (ESPP), student debt assistance, tuition reimbursement program, development and career growth opportunities and programs, financial planning benefits, wellness benefits including an employee assistance program (EAP), paid time off and paid company holidays, and family care and bonding leave.
KLA is proud to be an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, national origin, sex, gender identity, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other status protected by applicable law. We will ensure that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us at talent.acquisition@kla.com to request accommodation.
Show more
Show less","Systems Engineering, Data Analytics, Statistical Methods, Data Modeling, Python, Power BI, Machine Learning Algorithms, CrossFunctional Collaboration, Product Development, Service, Predictive Analytics","systems engineering, data analytics, statistical methods, data modeling, python, power bi, machine learning algorithms, crossfunctional collaboration, product development, service, predictive analytics","crossfunctional collaboration, dataanalytics, datamodeling, machine learning algorithms, powerbi, predictive analytics, product development, python, service, statistical methods, systems engineering"
Staff Data Engineer,Recruiting from Scratch,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744391560,2023-12-17,Waterloo,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830435,2023-12-17,Waterloo,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data governance, TDD, Continuous delivery, Data modeling, ETL, SQL, Python, Kafka, Storm, SparkStreaming, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Snowflake","data engineering, data governance, tdd, continuous delivery, data modeling, etl, sql, python, kafka, storm, sparkstreaming, airflow, kubernetes, docker, helm, spark, pyspark, snowflake","airflow, continuous delivery, data engineering, data governance, datamodeling, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744393224,2023-12-17,Waterloo,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Spark, pySpark, SQL, Kafka, Data Governance, ETL, Realtime Streaming Technologies, Data Warehouses, Data Modeling, Data Classification","data engineering, tdd, continuous delivery, python, snowflake, airflow, kubernetes, docker, spark, pyspark, sql, kafka, data governance, etl, realtime streaming technologies, data warehouses, data modeling, data classification","airflow, continuous delivery, data classification, data engineering, data governance, data warehouses, datamodeling, docker, etl, kafka, kubernetes, python, realtime streaming technologies, snowflake, spark, sql, tdd"
Sr. Data Storage Analyst - NetApp/Pure,KLA,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/sr-data-storage-analyst-netapp-pure-at-kla-3778761087,2023-12-17,Waterloo,United States,Mid senior,Onsite,"Base Pay Range: $100,000.00 - $170,000.00 Annually
Primary Location: USA-MI-Ann Arbor-KLA
KLA’s total rewards package for employees may also include participation in performance incentive programs and eligibility for additional benefits identified below. Interns are eligible for some of the benefits identified below. Our pay ranges are determined by role, level, and location. The range displayed above reflects the minimum and maximum pay for this position in the primary location identified in this posting. Actual pay depends on several factors, including location, job-related skills, experience, and relevant education level or training. If applicable, your recruiter can share more about the specific pay range for your preferred location during the hiring process.
Company Overview
KLA is a global leader in diversified electronics for the semiconductor manufacturing ecosystem. Virtually every electronic device in the world is produced using our technologies. No laptop, smartphone, wearable device, voice-controlled gadget, flexible screen, VR device or smart car would have made it into your hands without us. KLA invents systems and solutions for the manufacturing of wafers and reticles, integrated circuits, packaging, printed circuit boards and flat panel displays. The innovative ideas and devices that are advancing humanity all begin with inspiration, research and development. KLA focuses more than average on innovation and we invest 15% of sales back into R&D. Our expert teams of physicists, engineers, data scientists and problem-solvers work together with the world’s leading technology providers to accelerate the delivery of tomorrow’s electronic devices. Life here is exciting and our teams thrive on tackling really hard problems. There is never a dull moment with us.
Group/Division
The Information Technology (IT) group at KLA is involved in every aspect of the global business. IT’s mission is to enable business growth and productivity by connecting people, process, and technology. It focuses not only on enhancing the technology that enables our business to thrive but also on how employees use and are empowered by technology. This integrated approach to customer service, creativity and technological excellence enables employee productivity, business analytics, and process excellence.
Job Description/Preferred Qualifications
We are looking for a Sr. Data Storage Analyst with expertise in NetApp and Pure. This candidate must have hands on experience, planning, architecting, implementing and leading the storage systems infrastructure including security and optimizing key functions.
Requirements
Perform periodic upgrades, performance tuning, and system audit tasks
Mentor and provide technical guidance to the other team members
Knowledge and troubleshooting experience on NetApp storage including replication and automation
Knowledge on enterprise storage systems (SAN and NAS)
Hands-on experience in handling SAN switch Cisco
Strong testing and problem-solving skills
Fast learner, independent and detail oriented
Familiar with organizational change and approval workflow
Experience in working under SOX compliance
Good social skills
Minimum Qualifications
Must have a Bachelor's Degree, and a minimum five (5) years of hands-on experience supporting Pure / NetApp storage
The company offers a total rewards package that is competitive and comprehensive including but not limited to the following: medical, dental, vision, life, and other voluntary benefits, 401(K) including company matching, employee stock purchase program (ESPP), student debt assistance, tuition reimbursement program, development and career growth opportunities and programs, financial planning benefits, wellness benefits including an employee assistance program (EAP), paid time off and paid company holidays, and family care and bonding leave.
KLA is proud to be an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, national origin, sex, gender identity, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other status protected by applicable law. We will ensure that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us at talent.acquisition@kla.com to request accommodation.
Show more
Show less","NetApp, Pure, Network Storage, Network Security, SAN, NAS, Storage Architecture, Cisco, Storage Auditing, IT Compliance, Data Backup, Automation, Enterprise Storage, Troubleshooting","netapp, pure, network storage, network security, san, nas, storage architecture, cisco, storage auditing, it compliance, data backup, automation, enterprise storage, troubleshooting","automation, cisco, data backup, enterprise storage, it compliance, nas, netapp, network security, network storage, pure, san, storage architecture, storage auditing, troubleshooting"
Data Science Analyst Senior/Intermediate,Michigan Medicine,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/data-science-analyst-senior-intermediate-at-michigan-medicine-3773592314,2023-12-17,Waterloo,United States,Mid senior,Onsite,"How to Apply
A cover letter is required for consideration for this position and should be attached as the first page of your resume. The cover letter should address your specific interest in the position and outline skills and experience that directly relate to this position.
Summary
Michigan Anesthesiology Informatics and Systems Improvement Exchange (MAISE) is part of the University of Michigan Department of Anesthesiology. We are a group made up collectively of IT, business, and health care professionals. Our goal is to leverage IT to its fullest to improve patient safety and creates better health care outcomes.
As MAISE Data Scientist, you will direct efforts to integrate clinical, genetic, financial and other related datasets to develop models that enable innovative delivery of care that improves patient outcomes and population health. The scientist will employ data science, AI/deep learning, advance machine learning and traditional statistical techniques to address clinical quality initiatives and organizational research needs. The Data Scientist will develop, validate and help translate risk prediction, and clinical care/hospital workflow models to make data meaningful, accessible and actionable. This position will interface with MAISE?s work stream leadership members, in addition to multidisciplinary clinical care providers and researchers. Fostering collaborations with software developers and information technology (IT) to form valuable technology solutions that promote capability and capacity in health data science and are scalable across the clinical, operation, and research enterprise.
Responsibilities*
Basic Function And Responsibility
Designs and constructs relational databases for data warehousing.
Develops data modeling and is responsible for data acquisition and access analysis and design; and archive, recovery, and load strategy design and implementation.
Coordinates new data development ensuring consistency and integration with existing warehouse structure.
Reviews business requests for data and data usage, researches data sources for new and better data feeds.
Assists in continuous improvement efforts in enhancing performance and providing increased functionality.
Application Of Data Science And AI
Research, design, develop and implement AI and advanced machine learning applications to support Department's predictive analytics initiatives
Develop processes and tools to monitor and analyze performance. Application of statistical rigor and machine learning to develop, test and monitor performance of models
Propose novel solutions to issues that arise, evaluate the feasibility of the solution, adapt and refine the model as applicable
Collaborate closely with Health Implementations work stream leads, clinicians, researchers and IT in a multidisciplinary environment to facilitate deployment of implementation and innovative solutions
Development
Lead efforts for data acquisition, access analysis, archive, recovery and load strategy design and implementation
Run complex queries (i.e. SQL) and existing automation to correlate disparate data
Maintain fluency in existing and emerging data science technologies
Supervision Received: Direct supervision is received from Clinical Application Services Director
Required Qualifications*
Intermediate
Bachelors Degree in Computer Science or related field
1 - 3 years of experience in programming languages such as C, C++, C# or Java. Experience in R, JScript, Python and other programming languages/platforms like MCV, Spring, REACT, ASP is desirable
Relational database experience in SQL or ORACLE is desirable
Health system experience is desirable
Strong organizational, presentation and written communication skills
Strong and effective communication skill
Ability to interact effectively with a wide variety of people, internally and externally
Ability to act effectively both independently and as a member of a team
Flexibility to meet changing priorities
Senior
Bachelors Degree in Computer Science or related fields with at least 3 - 5 years of experience in programming languages such as C, C++, C# or Java ? OR - Masters Degree in Computer Science or related fields with at least 2 years of experience in the previously mentioned languages
Experience working with relational databases like SQL, Oracle, PostgreSQL, MySQL
Experience working with non-relational databases like MongoDB is desirable
Underfill Statement
This position may be underfilled at a lower classification depending on the qualifications of the selected candidate.
Background Screening
Michigan Medicine conducts background screening and pre-employment drug testing on job candidates upon acceptance of a contingent job offer and may use a third party administrator to conduct background screenings. Background screenings are performed in compliance with the Fair Credit Report Act. Pre-employment drug testing applies to all selected candidates, including new or additional faculty and staff appointments, as well as transfers from other U-M campuses.
Application Deadline
Job openings are posted for a minimum of seven calendar days. The review and selection process may begin as early as the eighth day after posting. This opening may be removed from posting boards and filled anytime after the minimum posting period has ended.
U-M EEO/AA Statement
The University of Michigan is an equal opportunity/affirmative action employer.
Show more
Show less","Data Acquisition, Data Warehousing, Data Modeling, Data Integration, Data Analysis, SQL, Data Mining, AI/Deep Learning, Machine Learning, Predictive Analytics, Modeling, Statistical Analysis, Collaboration, Deployment of Solutions, Relational Databases, Programming Languages (C C++ C# Java R JScript Python), Frameworks (MCV Spring REACT ASP), NonRelational Databases (MongoDB), IT, Health System Experience, Presentation Skills, Communication Skills, Teamwork, Flexibility","data acquisition, data warehousing, data modeling, data integration, data analysis, sql, data mining, aideep learning, machine learning, predictive analytics, modeling, statistical analysis, collaboration, deployment of solutions, relational databases, programming languages c c c java r jscript python, frameworks mcv spring react asp, nonrelational databases mongodb, it, health system experience, presentation skills, communication skills, teamwork, flexibility","aideep learning, collaboration, communication skills, data acquisition, data integration, data mining, dataanalytics, datamodeling, datawarehouse, deployment of solutions, flexibility, frameworks mcv spring react asp, health system experience, it, machine learning, modeling, nonrelational databases mongodb, predictive analytics, presentation skills, programming languages c c c java r jscript python, relational databases, sql, statistical analysis, teamwork"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711058,2023-12-17,Waterloo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Preprocessing, Data Postprocessing, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Pipeline Tools, Natural Language Processing, Large Language Models, Git, Python, Java, Bash, SQL, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, ETL, Kafka, Storm, SparkStreaming, Microservices","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, data preprocessing, data postprocessing, statistical analysis, data visualization, pandas, r, airflow, kubeflow, pipeline tools, natural language processing, large language models, git, python, java, bash, sql, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, etl, kafka, storm, sparkstreaming, microservices","airflow, aws, azure, bash, data cleaning, data engineering, data mining, data normalization, data postprocessing, data preprocessing, datamodeling, docker, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, microservices, natural language processing, nosql databases, pandas, pipeline tools, python, r, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Ann Arbor, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086800,2023-12-17,Waterloo,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Legal compliance, Data management, Data classification, Data retention","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management, data retention, docker, dynamodb, etl, gcp, git, java, kafka, kubernetes, legal compliance, machine learning, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Junior AWS Data Engineer (Hybrid),Latitude Inc,"Arlington, VA",https://www.linkedin.com/jobs/view/junior-aws-data-engineer-hybrid-at-latitude-inc-3787722949,2023-12-17,Bethesda-Chevy Chase,United States,Mid senior,Onsite,"We are seeking a skilled AWS Data Engineer to join our team and contribute to the design, development, and implementation of robust data solutions on the AWS platform. As a Data Engineer, you will work closely with cross-functional teams to understand data requirements, architect data pipelines, and ensure efficient data processing, storage, and retrieval. Your expertise in AWS services and data engineering best practices will be instrumental in optimizing our data infrastructure and enabling data-driven decision making.
Responsibilities:
Collaborate with stakeholders to understand data requirements and translate them into scalable and efficient data solutions on the AWS platform.
Design, develop, and maintain data pipelines and ETL workflows to extract, transform, and load data from various sources into our data lake or data warehouse.
Implement data quality controls, data governance frameworks, and monitoring mechanisms to ensure data accuracy, integrity, and compliance.
Optimize and fine-tune AWS services, including AWS Glue, Amazon Redshift, Amazon S3, and AWS Lambda, to maximize data processing speed, scalability, and cost efficiency.
Collaborate with data scientists and analysts to provide them with clean, reliable, and accessible data for advanced analytics and reporting purposes.
Stay updated with the latest AWS services, tools, and trends in data engineering and proactively identify opportunities to enhance our data infrastructure and processes.
Powered by JazzHR
ZscNwcaCzw
Show more
Show less","AWS, Cloud Computing, Data Engineering, Data Pipelines, Data Quality Control, Data Governance, AWS Glue, Amazon Redshift, Amazon S3, AWS Lambda, Data Lake, Data Warehouse, Data Analytics, Reporting, ETL, Data Accuracy, Data Integrity, Data Compliance, Data Processing, Data Storage, Data Retrieval, Datadriven decision making","aws, cloud computing, data engineering, data pipelines, data quality control, data governance, aws glue, amazon redshift, amazon s3, aws lambda, data lake, data warehouse, data analytics, reporting, etl, data accuracy, data integrity, data compliance, data processing, data storage, data retrieval, datadriven decision making","amazon redshift, amazon s3, aws, aws glue, aws lambda, cloud computing, data accuracy, data compliance, data engineering, data governance, data integrity, data lake, data processing, data quality control, data retrieval, data storage, dataanalytics, datadriven decision making, datapipeline, datawarehouse, etl, reporting"
Database Track Sr.Engineer,Capleo Global,"McLean, VA",https://www.linkedin.com/jobs/view/database-track-sr-engineer-at-capleo-global-3785507824,2023-12-17,Bethesda-Chevy Chase,United States,Mid senior,Hybrid,"Position :: PostgreSQL & DB2 - Database Track Sr.Engineer
Location :: McLean, VA (Day 1 Onsite 3 days Onsite 2 days Remote)
Skill Needed:- PostgreSQL, DB2, AWS, Migration
Must have 7+ years DBA working experience with PostgreSQL open source and good understanding of other RDBMS such as DB2 , Sybase, Oracle and MS SQL server.
Experience in migrating Sybase / Oracle databases to PostgreSQL
Experience in database migration using industry standard tools like AWS DMS and more
Experienced in setting up PostgreSQL clusters and supporting HA/DR solutions
Strong skills in UNIX / Linux Shell Scripting
Install, monitor and maintain PostgreSQL software, implement monitoring and alerting.
Experience in building high-scalable clusters, failover and hot standby solutions based on Maximum available architecture
Experience in architecting, configuring, setting up Enterprise Fail over manager or similar technology for High availability solution.
Experience in leading efforts related to system and SQL performance tuning and assisting business process integration with various data sources.
Ability to work well with development and infrastructure teams in doing database design, changes and enhancement to new and existing applications
Proven ability to provide High Availability and Disaster Recovery solutions
Show more
Show less","PostgreSQL, DB2, AWS, Migration, Sybase, Oracle, SQL Server, AWS DMS, UNIX, Linux Shell Scripting, HA/DR, Maximum Available Architecture, Enterprise Failover Manager, System and SQL Performance Tuning, Data Source Integration, Database Design, High Availability, Disaster Recovery","postgresql, db2, aws, migration, sybase, oracle, sql server, aws dms, unix, linux shell scripting, hadr, maximum available architecture, enterprise failover manager, system and sql performance tuning, data source integration, database design, high availability, disaster recovery","aws, aws dms, data source integration, database design, db2, disaster recovery, enterprise failover manager, hadr, high availability, linux shell scripting, maximum available architecture, migration, oracle, postgresql, sql server, sybase, system and sql performance tuning, unix"
"Data Center Engineer, Senior","Aktion Associates, Inc.","Maumee, OH",https://www.linkedin.com/jobs/view/data-center-engineer-senior-at-aktion-associates-inc-3741488387,2023-12-17,Toledo,United States,Mid senior,Onsite,"About Aktion
Aktion Associates, Inc. is a national ERP software Reseller in the top 20 of the VAR 100 reseller market. We have a high growth technical environment with a workforce of over 200 professionals. We have a (3) year growth initiative to build our total workforce to 300. The workforce is highly skilled and consists of application consultants, software engineers, and networking engineers located throughout the U.S.
Role And Responsibilities
VMware Expertise: Design, configure, and manage VMware virtualization solutions, including vSphere and vCloudDirector.
Networking Proficiency: Configure and maintain IOS-based network equipment and firewalls.
Windows Server and Active Directory:
Manage Windows Server environments, including installation, configuration, and patch management.
Administer Active Directory, including user account management, group policies, SSO, and domain controller maintenance.
Monitoring and Automation Proficiency:
Utilize N-central to monitor and manage network infrastructure and endpoints.
Create and customize monitoring scripts and alerts to proactively address issues.
Security and Compliance: Implement and maintain security measures to protect data center assets.
Infrastructure Design and Implementation: Collaborate with cross-functional teams to design and deploy data center solutions that meet business needs.
Education, Experience And Skills
Bachelor's degree in computer science, information technology, or related field (preferred).
Extensive experience in data center engineering and administration.
Proficiency in enterprise VMware products, Cisco and Arista networking, Windows Server, Active Directory, and N-central.
Strong problem-solving skills and the ability to work under pressure.
Excellent communication and teamwork skills.
Relevant certifications (e.g., VMware Certified Professional, Cisco CCNA/CCNP, Microsoft Certified: Azure Administrator) are a plus.
This position offers a competitive salary, quarterly bonus, with the potential for performance-based year-end bonus. Aktion Associates offers a comprehensive benefits plan including an employer matching 401k plan.
If, because of a medical condition or disability, you need a reasonable accommodation for any part of the application process, or in order to perform the essential functions of a position, please contact HR at hr@aktion.com and let us know the nature of your request and your contact information.
Aktion Associates is an Equal Opportunity Employer. Please visit www.aktion.com for more information about Aktion Associates
Show more
Show less","VMware, vSphere, vCloudDirector, IOS, Windows Server, Active Directory, Ncentral, Cisco, Arista, Microsoft Azure, VMware Certified Professional, Cisco CCNA, Cisco CCNP, Microsoft Certified: Azure Administrator","vmware, vsphere, vclouddirector, ios, windows server, active directory, ncentral, cisco, arista, microsoft azure, vmware certified professional, cisco ccna, cisco ccnp, microsoft certified azure administrator","active directory, arista, cisco, cisco ccna, cisco ccnp, ios, microsoft azure, microsoft certified azure administrator, ncentral, vclouddirector, vmware, vmware certified professional, vsphere, windows server"
Data Engineer Azure Cloud,eNGINE,Greater Pittsburgh Region,https://www.linkedin.com/jobs/view/data-engineer-azure-cloud-at-engine-3788843273,2023-12-17,Aliquippa,United States,Mid senior,Onsite,"eNGINE
builds Technical Teams. We are a Solutions and Placement firm shaped by decades of interaction with Technical professionals. Our inspiration is continuous learning and engagement with the markets we serve, the talent we represent, and the teams we build. Our Consulting Workforce is encouraged to enjoy career fulfillment in the form of challenging projects, schedule flexibility, and paid training/certifications. Successful outcomes start and finish with
eNGINE
.
eNGINE
is seeking an Azure Data Engineer to support our client in the transportation space. This individual will be responsible for activities related to the development, implementation, and structure of data. This individual will also coordinate with various internal stakeholders to determine what is in scope, and what data should be used for buildout of new computer vision applications.
This is a hybrid role requiring
1-2 days/week on-site in Pittsburgh, PA
What you will do:
Play a pivotal role in designing and implementing data architectures for AI-based data and computer vision applications
Design, develop, test, monitor, manage, and validate data warehouse activity, including data extraction, transformation, movement, loading, cleansing, and updating processes.
Exhibit a strong understanding of data structures, and a passion for innovative data solutions.
What we need from you:
4+ years of experience in Azure environments
Strong database schema and data quality experience
Proficiency with Azure Data Explorer
Working knowledge of data warehouse approaches, architecture, and development.
No C2C
Apply today and see how
eNGINE
can make a difference in your career!
Show more
Show less","Azure, Data Engineer, Data Warehouse, Data Extraction, Data Transformation, Data Loading, Data Cleansing, Data Updating, Data Structures, Data Solutions, Data Visualization, Computer Vision, Data Architecture","azure, data engineer, data warehouse, data extraction, data transformation, data loading, data cleansing, data updating, data structures, data solutions, data visualization, computer vision, data architecture","azure, computer vision, data architecture, data extraction, data loading, data solutions, data structures, data transformation, data updating, datacleaning, dataengineering, datawarehouse, visualization"
Senior Data Engineer,"YinzCam, Inc.","Pittsburgh, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-yinzcam-inc-3771780843,2023-12-17,Aliquippa,United States,Mid senior,Onsite,"NOTE: This in an in-office role in Pittsburgh, PA, USA.
ABOUT YINZCAM
Our digital products are used by 200+ professional sports teams and leagues around the world, including clubs in the NFL, NBA, MLS, NHL, LigaMX, AFL (Australia), and more.
THE ROLE
As a
Senior Data Engineer
, you will be responsible for collecting, organizing and analyzing terabytes of mobile-app data on a daily basis.
Your responsibilities will include:
Implementing ETL workflows for custom queries requested by our clients.
Monitoring the status of these ETL workflows on a daily basis.
Providing daily/weekly/monthly analytics reports to our clients
Providing analytics-centric Quality Assurance of our products.
Creating, optimizing, and managing complex SQL queries.
Creating data-export workflows to send data into externally-hosted data warehouses.
THE REQUIREMENTS
This is a programming-intensive role, requiring multiple years of hands-on programming experience with SQL, Java, Python, DynamoDB, Redshift, Hadoop/Hive clusters, and relational databases.
5+ years of database design and operation, and writing complex SQL queries
5+ years of programming in Java
5+ years of programming in Python
3+ years of experience with AWS technologies, such as DynamoDB, Redshift, Kinesis, serverless Lambda functions.
Experience with ETL pipelines, and ETL frameworks such as Airflow
Appetite and ability to thrive in a high energy fast-paced digital environment.
Willingness and ability to work the non-traditional hours of the sports industry.
THE BENEFITS
Paid time off every year
Paid maternity and paternity leave
Full medical, dental and vision health insurance
Paid gym membership in LA Fitness
Paid Coursera plan to take 8000+ courses around the world
Beverages and snacks
Paid parking near the office
Relocation assistance to Pittsburgh, if needed
Building products for well-known sports teams
Show more
Show less","Data Engineering, SQL, Java, Python, DynamoDB, Redshift, Hadoop/Hive, Relational Databases, AWS, Kinesis, Lambda, ETL, Airflow","data engineering, sql, java, python, dynamodb, redshift, hadoophive, relational databases, aws, kinesis, lambda, etl, airflow","airflow, aws, data engineering, dynamodb, etl, hadoophive, java, kinesis, lambda, python, redshift, relational databases, sql"
Data Scientist | Scientifique des données (AP-01 / AP FIN-01 / AP VFM-01 and AP-02 / AP FIN-02 / AP VFM-02),Office of the Auditor General of Canada / Bureau du vérificateur général du Canada,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-scientist-scientifique-des-donn%C3%A9es-ap-01-ap-fin-01-ap-vfm-01-and-ap-02-ap-fin-02-ap-vfm-02-at-office-of-the-auditor-general-of-canada-bureau-du-v%C3%A9rificateur-g%C3%A9n%C3%A9ral-du-canada-3786555739,2023-12-17,Hull, Canada,Associate,Onsite,"At the OAG, we serve Parliamentarians and Canadian society with the intention of supporting their trust in our public institutions and continued excellence of our public service, one audit at a time.
Do you want challenging and meaningful work in an inclusive environment where professional development meets work-life balance? Welcome home! The OAG is the place you’ve been looking for. Here’s why:
- a workplace where we strive to remove barriers related to the hiring process and accessibility, where we celebrate the diverse backgrounds and talents of our employees through open and inclusive dialogue
- recognition of your personal needs, both during and after work hours—for example, by offering
Tuesday afternoons without meetings for all employees.
flexibility in regular working hours (for example, compressed daily working hours to allow for an extra day off every 2 weeks, a self-funded leave between Boxing Day and New Year’s Day).
a development fund to support you if you wish to pursue academic studies and professional certifications (for example, MBA, M.Sc., PMP, Agile, CompTIA A+, and ITIL)
paid overtime, time off for medical appointments, paid family leave, and paid personal leave (in addition to paid sick leave and vacation leave).
a special, one-time, one week vacation leave following your second anniversary of service within the Federal Public Service.
- your peace of mind provided by our comprehensive benefits package, including
public service pension, health, and dental plans
disability benefits
life insurance
a confidential employee assistance program
Type of advertisement
1-3 indeterminate, term, deployment, acting, assignment, and Interchange Canada positions
Location
Hybrid: Telework and a minimum of 37.5 dispersible hours per month on site. Telework must be performed in Canada.
About the position
The Audit Services Group provides technical support, guidance, and advice to auditors within our financial statement and special examinations audit practice and our performance audit practice. The Data Analytics and Research Methods (DARM) team is one of several specialist teams within the Audit Services Group and is looking to grow.
The team is responsible for:
Data analytics services such as:
Exploring and assessing the available data sources to deepen our understanding of the business and its data, to support audit risk assessments and recommend audit approaches.
Data wrangling and preparation for analysis
Modeling data to provide audit evidence
Evaluating and concluding on results
Designing and developing effective visualizations.
Research methods advice and review services for subjects such as:
research methodologies and results measurement,
survey design,
sampling and extrapolation, and
reporting of results.
Support to our Professional Development team as subject matter experts responsible to ensure ongoing relevance and completeness of training materials.
Support to the OAG’s innovation activities as they relate to data analytics and research methods.
Your Role as Data Scientist
As a Data Scientist, you will assist team’s overall activities. You will use your expertise and knowledge of data science to:
Participate in data analytics and research methodology components of audit and other engagements ensuring that good statistical practices, audit methodology, research methodology, and OAG’s policies are properly followed;
Support the implementation of innovative solutions leveraging data analytics and research methods;
Develop, administer, and support professional development activities relating to data analytics and research methodology;
Provide on-the-job coaching to team members (Applies to AP-02); and
Contribute to the advancement of the application of data analytics and research methodologies in audit engagements.
Closing date
January 3, 2024 at 11:59 pm EDT
Essential qualifications
Education
Bachelor's degree from a recognized university and a Canadian Chartered Professional Accountant (CPA) designation,
or
Master's or other graduate degree from a recognized university. Degree must include multiple courses pertaining to statistical and research methodology.
Language requirement
Various linguistic profiles: English Essential, or French Essential, or Bilingual Imperative (BBB).
Conditions of employment
Security clearance: Secret
This is an abridged version of our posting. View it’s full version on our website.
______________________________________________________________________
Au BVG, nous servons les parlementaires et la société canadienne avec l’intention de soutenir la confiance dans nos institutions publiques et l’excellence de notre fonction publique, un audit à la fois.
Vous désirez un travail stimulant et significatif dans un environnement inclusif permettant de concilier perfectionnement professionnel et équilibre travail-vie personnelle? Le BVG est l’endroit pour vous. Voici pourquoi :
- Un milieu de travail où nous nous efforçons d’éliminer les obstacles liés au processus d’embauche et à l’accessibilité et où nous célébrons les origines et les talents variés de notre personnel au moyen d’événements et d’un dialogue ouvert et inclusif;
- La reconnaissance de vos besoins personnels, pendant et après les heures de travail. Par exemple, nous offrons :
les mardis après-midi sans réunion dans l’ensemble du BVG;
des heures de travail flexibles (p. ex. horaire comprimé permettant un jour de congé supplémentaire toutes les deux semaines, un congé autofinancé entre le lendemain de Noël et le jour de l’An);
un fonds de perfectionnement pour vous appuyer si vous souhaitez poursuivre des études ou obtenir des certifications professionnelles (p. ex. horaire comprimé permettant un jour de congé supplémentaire toutes les deux semaines);
le temps supplémentaire rémunéré, des congés payés pour des rendez-vous médicaux et pour obligations familiales et des congés personnels rémunérés (en plus de congés de maladie et de congés annuels payés).
Un crédit unique de congé annuel d'une (1) semaine lors de votre deuxième anniversaire de service au sein de la fonction publique fédérale.
- Votre tranquillité d’esprit grâce à notre ensemble complet d’avantages sociaux comprenant :
les régimes de retraite, de soins de santé et de soins dentaires de la fonction publique;
un programme de prestations d’invalidité;
une assurance-vie;
un Programme d’aide aux employés confidentiel.
Lieu
Hybride: télétravail et un minimum de 37,5 heures flexibles par mois au bureau. Le télétravail doit être effectué au Canada.
Type d'affichage
1-3 poste de durée indéterminée ou déterminée, mutation, nomination intérimaire, affectation et Échange Canada
À propos du poste
Le Groupe des services à la vérification fournit un soutien technique, une orientation et des conseils au personnel d’audit de nos pratiques d’audit d’états financiers et d’audit de performance. L’équipe de l’Analyse des données et méthodes de recherche est l’une des nombreuses équipes de spécialistes au sein du Groupe des services à la vérification, et elle cherche à s’agrandir.
L’équipe est chargée de ce qui suit :
les services d’analyse des données, tels que les suivants :
explorer et évaluer les sources de données disponibles pour approfondir notre compréhension des activités et des données de l’entité, afin d’appuyer les évaluations du risque d’audit et de recommander des stratégies d’audit;
effectuer le traitement des données et la préparation à l’analyse;
modéliser les données pour fournir des éléments probants;
évaluer les résultats et en tirer des conclusions;
concevoir et élaborer des visualisations efficaces des données;
la formulation de conseils et la prestation de services d’examen portant sur les méthodes de recherche pour des sujets tels que les suivants :
les méthodes de recherche et la mesure des résultats;
la conception et l’administration de sondages;
l’échantillonnage et l’extrapolation;
la communication des résultats;
la prestation de soutien auprès de notre équipe du Perfectionnement professionnel par l’entremise d’experts en la matière responsables de veiller à ce que le matériel de formation soit toujours pertinent et exhaustif;
le soutien aux activités d’innovation du BVG qui concernent l’analyse des données et les méthodes de recherche.
Votre rôle en tant que scientifique des données
En tant que scientifique des données (PA-02), vous participerez aux activités générales de l’équipe. Vous utiliserez votre expertise et vos connaissances dans le domaine de la science des données pour :
participer aux composantes d’analyse des données et de méthodes de recherche des missions d’audit et d’autres missions, en veillant à ce que les bonnes pratiques statistiques, les méthodes d’audit, les méthodes de recherche et les politiques du BVG soient respectées;
soutenir la mise en œuvre de solutions innovantes qui tirent parti de l’analyse des données et les méthodes de recherche;
élaborer des activités de perfectionnement professionnel liées à l’analyse des données et aux méthodes de recherche, les diriger et offrir un soutien;
fournir un encadrement en cours d’emploi aux membres de l’équipe (PA-0D/PA-0S à PA-01);
contribuer à une application plus vaste de l’analyse des données et des méthodes de recherche dans le cadre des missions d’audit.
Date limite
3 janvier 2024 à 23 h 59 (HE)
Qualifications essentielles
Études
Baccalauréat d’une université reconnue et titre de comptable professionnel agréé (CPA) du Canada;
ou
Maîtrise ou autre diplôme d’études supérieures d’une université reconnue. Le programme ayant mené à l’obtention du diplôme doit avoir compris un volet sur la méthodologie statistique et de recherche composé de plusieurs cours.
Exigences linguistiques
Divers profils linguistiques : anglais essentiel, français essentiel ou bilingue, nomination impérative (BBB).
Conditions d’emploi
Cote de sécurité : Secret
Cet affichage est abrégé. Veuillez en lire la version complète sur notre site web.
Show more
Show less","Data Analytics, Research Methods, Statistical Analysis, Data Wrangling, Data Modeling, Data Visualization, Survey Design, Sampling and Extrapolation, Reporting, Professional Development, OntheJob Coaching, Advancement of Data Analytics, Research Methodologies, Audit Engagements, Bachelor's Degree, Master's Degree, Graduate Degree, Canadian Chartered Professional Accountant (CPA), Statistical Methodology, Research Methodology, English, French, Secret Clearance","data analytics, research methods, statistical analysis, data wrangling, data modeling, data visualization, survey design, sampling and extrapolation, reporting, professional development, onthejob coaching, advancement of data analytics, research methodologies, audit engagements, bachelors degree, masters degree, graduate degree, canadian chartered professional accountant cpa, statistical methodology, research methodology, english, french, secret clearance","advancement of data analytics, audit engagements, bachelors degree, canadian chartered professional accountant cpa, data wrangling, dataanalytics, datamodeling, english, french, graduate degree, masters degree, onthejob coaching, professional development, reporting, research methodologies, research methodology, research methods, sampling and extrapolation, secret clearance, statistical analysis, statistical methodology, survey design, visualization"
Data Scientist SME,CACI International Inc,"Liberty, NC",https://www.linkedin.com/jobs/view/data-scientist-sme-at-caci-international-inc-3750173475,2023-12-17,Asheboro,United States,Mid senior,Onsite,"Job Category: Science
Time Type: Full time
Minimum Clearance Required to Start: TS/SCI
Employee Type: Regular
Percentage of Travel Required: Up to 10%
Type of Travel: Outside Continental US
Are you a passionate Data Scientist who thrives in a creative environment applying non-traditional approaches to large-scale analysis of structured and unstructured data? Do you enjoy developing cross-cutting solutions to effectively integrate and optimize the various and disparate sources, methods, technologies to effectively translate and integrate academic research and commercial best practices into sustainable analytic workflows? Join our team as a Data Scientist! You will get to use your computational and technical skills in data science, in conjunction with your expertise applying efficient and effective search and discovery; meaningful exploitation and analysis; and rapidly integrate and leverage new sources, methods and technology that account for non- technical end users in support of informed resource allocation and optimization, actionable insight, and meaningful anticipation and influence. Your expertise with advanced tools and computational analytics will be critical to interpret, connect, predict, and make discoveries in complex data in support of actionable intelligence executed by America’s most elite forces.
More About The Role
You will work as part of a team to solve the most critical operational problems involving big data analytics in support of operational forces. Your experience and understanding of data analytics, machine learning, algorithm development, and data clustering supports predictive and prescriptive intelligence insights crucial to building actionable intelligence that will be utilized to protect and defend U.S. interests. Your experience and understanding of data science methodologies will be crucial to development of strategies in anticipation of current and future operational missions. As part of a team, you will deploy data science capabilities using multiple technological and computational methodologies, to provide insight and understanding of the operational environment or point of interest.
You’ll Bring These Qualifications
Top Secret Clearance AND meet the requirements of DCID 6/4
Experience – Ph.D. in at least ONE of the following domains
Social Sciences
Computer Science
Mathematics
Minimum of 20 years of experience in scientific research with acumen towards law enforcement, military science and advanced targeting using proven market analytics.
Expertise implementing computational modeling, visualization of human behavior, statistics, applied mathematics, computer science, human behavior, social science research, and/or data analytics.
Experience implementing machine learning, data mining, and statistical algorithms for pattern recognition and the development of predictive models.
Documented or published work that includes peer review journals.
Shall possess strong oral and written communication skills
Capable of effectively directing subordinates.
What We Can Offer You
We’ve been named a Best Place to Work by the Washington Post.
Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives.
We offer competitive benefits and learning and development opportunities.
We are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities.
For over 60 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success
Company Overview: At CACI, you will have the opportunity to make an immediate impact by providing information solutions and services in support of national security missions and government transformation for Intelligence, Defense, and Federal Civilian customers. CACI is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other protected characteristic.
Show more
Show less","Data Science, Machine Learning, Algorithm Development, Data Clustering, Computational Modeling, Visualization of Human Behavior, Applied Mathematics, Computer Science, Social Science Research, Data Analytics, Pattern Recognition, Predictive Models, Statistical Algorithms, Structured Data, Unstructured Data, Computational Analytics, Predictive and Prescriptive Intelligence, Actionable Intelligence, Natural Language Processing, Big Data Analytics, Predictive Analytics, CrossCutting Solutions, Data Integration, Data Optimization, Analytic Workflows, Advanced Tools, Data Mining","data science, machine learning, algorithm development, data clustering, computational modeling, visualization of human behavior, applied mathematics, computer science, social science research, data analytics, pattern recognition, predictive models, statistical algorithms, structured data, unstructured data, computational analytics, predictive and prescriptive intelligence, actionable intelligence, natural language processing, big data analytics, predictive analytics, crosscutting solutions, data integration, data optimization, analytic workflows, advanced tools, data mining","actionable intelligence, advanced tools, algorithm development, analytic workflows, applied mathematics, big data analytics, computational analytics, computational modeling, computer science, crosscutting solutions, data clustering, data integration, data mining, data optimization, data science, dataanalytics, machine learning, natural language processing, pattern recognition, predictive analytics, predictive and prescriptive intelligence, predictive models, social science research, statistical algorithms, structured data, unstructured data, visualization of human behavior"
Senior Data Engineer,Parker B Associates,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-parker-b-associates-3786679041,2023-12-17,Vermillion,United States,Mid senior,Hybrid,"Senior Data Engineer
Permanent
New York / Hybrid
€200,000 - €225,000
Our client is a leading insurance organisation looking for a Senior Data Engineer to join the team permanently.
You will lead and innovate data products to support the organization's data strategy, while also working on and enhancing data-centric projects. You will take on a leadership role in the design and delivery of projects, reporting to senior-level management.
· Do you want to work for one of the World’s leading Insurance organisations?
· Would you like to work for an organisation investing in advanced technologies?
· Are you interested to work for a billion-dollar company with a start-up culture?
Responsibilities:
Act as technical lead of a team of 5 engineers, data scientists, and analysts.
Partner with senior members to create and evangelize best-in-class engineering competency and tooling.
Enforce strong development standards across the team.
Establish strong relationships with internal clients as an engineering representative for the data strategy.
Contribute to the overall Data Strategy vision and execution.
Develop, implement, and deploy custom data pipelines powering machine learning algorithms.
Innovate new ways to leverage large and small datasets.
Architect engineering solutions using the latest cloud technologies.
The full job spec will be provided upon request.
Skills/ Experience:
Lead expertise
Start-up
Python
SQL
Spark
Azure - bonus
If interested or for further information, please apply ASAP.
Keywords - Data engineering, data engineer lead, lead data engineer, principal data engineer, staff data engineer,
Show more
Show less","Data Engineering, Data Engineer Lead, Python, SQL, Spark, Azure, Leadership, Cloud Technologies","data engineering, data engineer lead, python, sql, spark, azure, leadership, cloud technologies","azure, cloud technologies, data engineer lead, data engineering, leadership, python, spark, sql"
"Computational Research Scientist III, Physiological Data Modeling",The Henry M. Jackson Foundation for the Advancement of Military Medicine,"Frederick, MD",https://www.linkedin.com/jobs/view/computational-research-scientist-iii-physiological-data-modeling-at-the-henry-m-jackson-foundation-for-the-advancement-of-military-medicine-3672240597,2023-12-17,Emmitsburg,United States,Mid senior,Onsite,"Join the HJF Team!
The Henry M. Jackson Foundation for the Advancement of Military Medicine (HJF) is a nonprofit organization dedicated to advancing military medicine. We serve military, medical, academic and government clients by administering, managing and supporting preeminent scientific programs that benefit members of the armed forces and civilians alike. Since its founding in 1983, HJF has served as a vital link between the military medical community and its federal and private partners. HJF's support and administrative capabilities allow military medical researchers and clinicians to maintain their scientific focus and accomplish their research goals.
HJF is seeking a
Physiological Data Modeling
Computational Research Scientist III.
This position will be in support of Biotechnology High-Performance Computing Software Applications Institute ( BHSAI ). The mission of the BHSAI is to perform interdisciplinary research, combining physical, computational, and life-science research, to improve the efficiency and efficacy in the development of militarily relevant medical products for Force Health Protection.
Research in this area focuses on the development of individual-specific mathematical models and artificial intelligence (AI) algorithms for preventing non-battle injuries, optimizing and enhancing Soldier performance, and optimizing casualty care.
Research efforts in this area support military operational medicine and combat casualty care missions. For operational support, we are developing phenomenological, first-principle, and data-driven adaptive mathematical models to provide Soldiers with tools to assess the risk of head or cold injuries, acute mountain sickness, or alertness impairment under operation conditions, as well as evidence-based countermeasures to mitigate such risks. For casualty care, we are developing hardware and software platforms, as well as artificial intelligence-based decision-support algorithms, to improve the management of combat casualties in pre-hospital and in-hospital settings.
To learn more: http://bhsai.org/research/physiological/
Responsibilities
Formulates research projects, develops innovative approaches, designs and implements experimental protocols related to computational models.
Continuously surveys the scientific literature to generate novel ideas and advance projects and programs. Develops necessary new technologies and protocols.
Trains, and in some instances supervises, junior research scientists.
Collects, processes, and interprets experimental data and model outputs. Presents research methods and scientific concepts at appropriate scientific conferences.
Prepares research proposals, progress reports, and manuscripts for submission to scientific journals and present research methods and findings at appropriate forums.
Assists in the identification, development, and maintenance of collaborations with both internal and external research groups for development of new approaches to existing research problems.
May perform other duties and responsibilities as assigned or directed by the supervisor. This may include attendance of and participation in required training for role.
Required Knowledge, Skills And Abilities
Proven computational experience with solid knowledge of informatics tools and visualization/data mining package
Proficiency in common database/programming/scripting languages (e.g., SQL, R, Python, Perl, Java, MATLAB, C++) and their application in biomedical research
Excellent communication skills
Ability to troubleshoot technical procedures
Good analytical skills
Ability to work independently in a deadline-driven environment.
Supervisory Responsibilities
Assigned Lead: May recommend the following: employee hiring, disciplinary action, and starting salaries; provide input on employee performance evaluations.
Qualifications
Work Environment
This position will take place primarily in an office setting.
Education And Experience
Doctoral Degree in Engineering, Computational Physics, Mathematics or related field.
Minimum of 6-8 years of experience (post doctorate degree) required.
Some positions or sites may require that the incumbent be fully vaccinated against COVID-19. Proof of vaccination may be required.
Employment with HJF is contingent upon successful completion of a background check, which may include, but is not limited to, contacting your professional references, verification of previous employment, education and credentials, a criminal background check, and a department of motor vehicle (DMV) check if applicable. Any qualifications to be considered as equivalents, in lieu of stated minimums, require the prior approval of the Chief Human Resources Officer.
Equal Opportunity Employer/Protected Veterans/Individuals With Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Any qualifications to be considered as equivalents, in lieu of stated minimums, require the prior approval of the Chief Human Resources Officer.
Show more
Show less","Computational Research, Bioinformatics, Data Mining, Java, MATLAB, C++, SQL, R, Python, Perl, Artificial Intelligence, Machine Learning, Algorithm Development, Mathematical Modeling, Scientific Computing, Data Analysis, Data Visualization, HighPerformance Computing, Scientific Writing, Research Proposal Writing, Research Collaboration, Team Leadership, Research Supervision, Problem Solving, Communication, Troubleshooting, Analytical Thinking, Independent Work, Engineering, Computational Physics, Mathematics","computational research, bioinformatics, data mining, java, matlab, c, sql, r, python, perl, artificial intelligence, machine learning, algorithm development, mathematical modeling, scientific computing, data analysis, data visualization, highperformance computing, scientific writing, research proposal writing, research collaboration, team leadership, research supervision, problem solving, communication, troubleshooting, analytical thinking, independent work, engineering, computational physics, mathematics","algorithm development, analytical thinking, artificial intelligence, bioinformatics, c, communication, computational physics, computational research, data mining, dataanalytics, engineering, highperformance computing, independent work, java, machine learning, mathematical modeling, mathematics, matlab, perl, problem solving, python, r, research collaboration, research proposal writing, research supervision, scientific computing, scientific writing, sql, team leadership, troubleshooting, visualization"
Senior Data Engineer,Professional Diversity Network,"Alaska, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788315130,2023-12-17,Alaska,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a3eeb2f-2c8a-420d-9f85-7c385fb9d5cd
Show more
Show less","Data Engineering, Data Pipeline Development, SQL, SDLC, Source Control, Project Management, Test Driven Development, API Development, Data Visualization, Dashboard Development, Netezza, Datastage, BitBucket, JIRA, Confluence, R, SAS, Python, SPSS, Agile, Hadoop, Spark, Hive","data engineering, data pipeline development, sql, sdlc, source control, project management, test driven development, api development, data visualization, dashboard development, netezza, datastage, bitbucket, jira, confluence, r, sas, python, spss, agile, hadoop, spark, hive","agile, api development, bitbucket, confluence, dashboard development, data engineering, data pipeline development, datastage, hadoop, hive, jira, netezza, project management, python, r, sas, sdlc, source control, spark, spss, sql, test driven development, visualization"
Master Data Senior Analyst/Analyst,Meraki Talent Ltd,"North Lanarkshire, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/master-data-senior-analyst-analyst-at-meraki-talent-ltd-3778847329,2023-12-17,Greater Glasgow, United Kingdom,Associate,Hybrid,"Master Data Senior Analyst/Analyst
Permanent
Lanarkshire (Hybrid, 3 days per week in the office)
Posted Mon 11 Dec 23
CVs ASAP
Start date Jan – Apr 24
Meraki Talent are actively looking for a Master Data Senior Analyst/Analyst to join their newly created, and well funded team. A greenfield site, the Master Data Senior Analyst/Analyst will have the opportunity to operate at an oversight level, as well as being hands on. The Master Data Senior Analyst/Analyst will support the implementation of the Master Data Program first locally, then across the globe. This would suit an experienced Master Data professional who is looking for more breadth and scope in role, an excellent work life balance, or perhaps looking to move into a more oversight rather than hands on role.
Responsibilities of the Master Data Senior Analyst/Analyst:
Implement the Global Master Data governance framework, enforcing policies within domain, compliance with data definitions and standards
Develop a comprehensive catalogue of master data elements
Document processes, responsibilities, and ownership of data over various domains
Define and update data definitions
Collaborate with global data owners to assist in the enrichment of master data
Identify quality issues and contribute to resolution
Support the creation of process maps, work instructions, and control points related to master data processes
Background of the Master Data Senior Analyst/Analyst:
Experience working within a Master Data capacity is absolutely essential for this role
An understanding of supply chain is preferable but not essential
Experience working within a project environment is helpful
Excellent attention to detail and ability to work within an oversight capacity
Data minded professional with an inquisitive mindset
Happy to work in the office 3 days per week
Is this job for you? At Meraki, we love recruitment and love words. Is this you?
Gordon wants: Master Data, Supply Chain, Project, Documentation, Governance
Please see our website page headed (eg) ‘Privacy Notice’ for an explanation about how we use information we collect about you’
Show more
Show less","Master Data Governance, Data Definitions, Process Documentation, Data Mapping, Data Quality Management, Supply Chain, Project Management, Data Enrichment, Data Analytics, Data Ownership, Data Standards, Data Policies","master data governance, data definitions, process documentation, data mapping, data quality management, supply chain, project management, data enrichment, data analytics, data ownership, data standards, data policies","data definitions, data enrichment, data mapping, data ownership, data policies, data quality management, data standards, dataanalytics, master data governance, process documentation, project management, supply chain"
Data Cabling Engineer,Digital Waffle,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3732194818,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"Our client is a leading provider of internet services, dedicated to delivering high-speed connectivity to businesses and individuals alike. We are seeking a highly skilled Data Cabling Engineer to join our team. In this role, you will be responsible for designing, installing, and maintaining data cabling systems that support our internet infrastructure. You will work closely with other members of our team to ensure that our network is running smoothly and that our customers are receiving the best possible experience. The ideal candidate for this position is a detail-oriented problem solver who is comfortable working in a fast-paced environment.
Responsibilities
Install and configure data cabling systems for business customers
Conduct site surveys to determine customers’ cabling requirements
Troubleshoot problems with existing cabling systems and recommend solutions
Work with other team members to ensure that network performance is optimized
Maintain accurate records of all cabling installations and repairs
Follow all safety protocols when working with cabling systems
Stay up-to-date on the latest technology and techniques in the field
Requirements
At least 3 years of experience in data cabling installation and maintenance
Excellent knowledge of network protocols and technologies
Familiarity with copper and fiber optic cabling, and associated hardware
Strong analytical and problem-solving skills
Able to work independently or as a team
Good communication skills, both oral and written
Attention to detail and care for quality workmanship
Show more
Show less","Data cabling, Network protocols, Copper cabling, Fiber optic cabling, Network performance","data cabling, network protocols, copper cabling, fiber optic cabling, network performance","copper cabling, data cabling, fiber optic cabling, network performance, network protocols"
Data Cabling Engineer- Contract,Digital Waffle,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-contract-at-digital-waffle-3760453341,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"Job Description
Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Glasgow
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labelling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labelling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high-quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Copper cabling, Metal trunking, Containment, Cat6, Cat6a, TIA/EIA, ISO/IEC, Cable testers, Certification tools, Signal quality, Continuity, Performance, Troubleshooting, Cable stripping, Cable cutting, RJ45 crimping, Punchdown tool, Cable certification, Tone generator, Probe, Cable labeling, Markers, Label printer, Measuring, Alignment, Cable ties, Velcro straps, Cable clips, Mounts, Power drill, Bits, Screwdrivers, Wall anchors, Safety glasses, Work gloves, Tool bag, Notepad, Pen, Mobile device, Steeltoed boots, Hard hat, Cable fish tape, Cable rods, Cable lubricant","copper cabling, metal trunking, containment, cat6, cat6a, tiaeia, isoiec, cable testers, certification tools, signal quality, continuity, performance, troubleshooting, cable stripping, cable cutting, rj45 crimping, punchdown tool, cable certification, tone generator, probe, cable labeling, markers, label printer, measuring, alignment, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, notepad, pen, mobile device, steeltoed boots, hard hat, cable fish tape, cable rods, cable lubricant","alignment, bits, cable certification, cable clips, cable cutting, cable fish tape, cable labeling, cable lubricant, cable rods, cable stripping, cable testers, cable ties, cat6, cat6a, certification tools, containment, continuity, copper cabling, hard hat, isoiec, label printer, markers, measuring, metal trunking, mobile device, mounts, notepad, pen, performance, power drill, probe, punchdown tool, rj45 crimping, safety glasses, screwdrivers, signal quality, steeltoed boots, tiaeia, tone generator, tool bag, troubleshooting, velcro straps, wall anchors, work gloves"
Senior Data Analyst,Cabinet Office,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-cabinet-office-3786578078,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"Location
Glasgow, York
About The Job
Job summary
Building a strong commercial function is at the heart of a far-reaching programme of Civil Service reform. The award-winning Commercial Capability Team exists within Cabinet Office to support the growing Government Commercial Organisation (GCO) and wider profession in recruiting and building commercial expertise across government.
The GCO’s inaugural reward strategy was published in 2023 and outlines our mission over the next three years to design and implement reward policies and practices that attract, motivate and retain colleagues, ultimately improving commercial outcomes for Government.
This role offers a fantastic challenge for a passionate and experienced data analyst. You will join a supportive and committed HR team comprising c. 20 people, within a larger People Services team. We offer a friendly working environment, flexible and hybrid working arrangements and a commitment to ongoing professional development. The role can be undertaken on a full time or part time (minimum 30 hours per week) basis.
Job Description
In this role, you will be pivotal to the successful delivery of the reward strategy, working with key employee data such as pay, performance and diversity information to model scenarios, analyse trends, and visualise outputs. Your work will contribute to tangible results which have a positive and long-term impact on the lives of our employees.
Reporting to the Senior Reward Lead, the exact allocation of work will be shaped with you and the team.
Responsibilities are likely to include:
Understanding and interpreting business problems to identify and implement appropriate data solutions;
Building accurate, complex pay scenario models to inform annual pay award recommendations;
Benchmarking internal and external market pay and benefits data;
Collating, analysing and visualising salary, performance and talent information;
Extracting, cleaning, aggregating and manipulating data from a variety of systems and sources;
Choosing and employing appropriate statistical techniques and visualisation tools to deliver impactful analysis and engage stakeholders;
Ensuring analytical outputs are robust and effective, and that methods can withstand audit scrutiny;
Contributing data and insights to papers and reports, and providing briefings as needed;
Automating data management activities where possible;
Ensuring that lessons learned are captured and fed into wider process improvement activities;
Supporting day-to-day business operations and working with colleagues across the wider team to deliver against priorities.
Person specification
Essential criteria:
A genuine passion for data and analytics, with the curiosity to investigate, understand and explain patterns and trends;
Demonstrable experience translating business issues into data problems for resolution;
Significant experience working with large, qualitative and quantitative datasets;
Strong analytical skills, including experience employing a range of techniques, including basic statistical techniques, and modelling a variety of scenarios;
Advanced MS Excel skills;
Experience developing a range of clear, engaging data visualisations;
Excellent accuracy and attention to detail;
Experience balancing competing priorities/demands and working to tight deadlines;
Strong verbal communication skills with the ability to challenge assumptions and explain technical issues to stakeholders without the use of jargon;
Good mathematics and IT skills gained through qualification and/or experience.
Desirable criteria:
A good working knowledge of a programming language such as SQL, Python and/or R;
Experience using Tableau and/or Power BI;
Experience of pay modelling.
Behaviours
We'll assess you against these behaviours during the selection process:
Changing and Improving
Communicating and Influencing
Delivering at Pace
Seeing the Big Picture
We only ask for evidence of these behaviours on your application form:
Changing and Improving
Benefits
Alongside your salary of £40,850, Cabinet Office contributes £11,029 towards you being a member of the Civil Service Defined Benefit Pension scheme. Find out what benefits a Civil Service Pension provides.
Learning and development tailored to your role.
An environment with flexible working options.
A culture encouraging inclusion and diversity.
A Civil Service Pension which provides an attractive pension, benefits for dependants and average employer contributions of 27%.
A minimum of 25 days of paid annual leave, increasing by one day per year up to a maximum of 30.
Things you need to know
Selection process details
This vacancy is using Success Profiles (opens in a new window) , and will assess your Behaviours and Experience.
Application process
Please provide a CV & 750 word max personal statement outlining your suitability for this job role. Please pay particular attention to the essential criteria when writing your statement.
This section will be used for the sift to assess your suitability to be invited to an interview. You do not have to refer to the behaviours in your statement as you will be asked to provide a separate 250-word statement on the listed behaviour.
Should a large number of applications be received, an initial sift may be undertaken using the lead behaviour, Changing and Improving. Candidates who pass the initial sift may be progressed to a full sift, or progressed straight to assessment/interview.
Selection process
During the interview you will be assessed in your experience and the listed behaviours.
Expected timeline (subject to change)
Expected sift date – w/c 18 December 2023
Expected interview dates - January 2023
Interview location - virtual on Google Meet.
Reasonable adjustments
If a person with disabilities is put at a substantial disadvantage compared to a non-disabled person, we have a duty to make reasonable changes to our processes.
If you need a change to be made so that you can make your application, you should:
Contact Government Recruitment Service via cabinetofficerecruitment.grs@cabinetoffice.gov.uk as soon as possible before the closing date to discuss your needs.
Complete the ‘Assistance required’ section in the ‘Additional requirements’ page of your application form to tell us what changes or help you might need further on in the recruitment process. For instance, you may need wheelchair access at interview, or if you’re deaf, a Language Service Professional.
Further information
If you are experiencing accessibility problems with any attachments on this advert, please contact the email address in the 'contact point for applicants' section.
Please note that this role requires CTC clearance, which would normally need 3 years' UK residency in the past 3 years. This is not an absolute requirement, but supplementary checks may be needed where individuals have not lived in the UK for that period. This may mean your security clearance (and therefore your appointment) will take longer or, in some cases, not be possible.
Please be aware the levels of national security clearance are changing which may impact on the level needed for this role by the time of appointment. All efforts will be made to keep candidates informed of any changes and what that will mean in terms of vetting criteria. For more information please See our vetting charter
Please note terms and conditions are attached. Please take time to read the document to determine how these may affect you.
Any move to Cabinet Office from another employer will mean you can no longer access childcare vouchers. This includes moves between government departments. You may however be eligible for other government schemes, including Tax Free Childcare. Determine your eligibility at https://www.childcarechoices.gov.uk
A reserve list will be held for a period of 12 months, from which further appointments can be made.
If successful and transferring from another Government Department a criminal record check may be carried out.
In order to process applications without delay, we will be sending a Criminal Record Check to Disclosure and Barring Service /Disclosure Scotland on your behalf.
However, we recognise in exceptional circumstances some candidates will want to send their completed forms direct. If you will be doing this, please advise Government Recruitment Service of your intention by emailing Pre-EmploymentChecks.grs@cabinetoffice.gov.uk stating the job reference number in the subject heading.
For further information on the Disclosure Scotland confidential checking service telephone: the Disclosure Scotland Helpline on 0870 609 6006 and ask to speak to the operations manager in confidence, or email Info@disclosurescotland.co.uk
New entrants are expected to join on the minimum of the pay band.
Applicants who are successful at interview will be, as part of pre-employment screening, subject to a check on the Internal Fraud Database (IFD). This check will provide information about employees who have been dismissed for fraud or dishonesty offences. This check also applies to employees who resign or otherwise leave before being dismissed for fraud or dishonesty had their employment continued. Any applicant’s details held on the IFD will be refused employment.
A candidate is not eligible to apply for a role within the Civil Service if the application is made within a 5 year period following a dismissal for carrying out internal fraud against government.
Feedback will only be provided if you attend an interview or assessment.
Security
Successful candidates must undergo a criminal record check.
Successful candidates must meet the security requirements before they can be appointed. The level of security needed is counter-terrorist check (opens in a new window) . See our vetting charter (opens in a new window) .
People working with government assets must complete baseline personnel security standard (opens in new window) checks.
Nationality Requirements
This job is broadly open to the following groups:
UK nationals
nationals of the Republic of Ireland
nationals of Commonwealth countries who have the right to work in the UK
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities with settled or pre-settled status under the European Union Settlement Scheme (EUSS) (opens in a new window)
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities who have made a valid application for settled or pre-settled status under the European Union Settlement Scheme (EUSS)
individuals with limited leave to remain or indefinite leave to remain who were eligible to apply for EUSS on or before 31 December 2020
Turkish nationals, and certain family members of Turkish nationals, who have accrued the right to work in the Civil Service
Further information on nationality requirements (opens in a new window)
Working for the Civil Service
The Civil Service Code (opens in a new window) sets out the standards of behaviour expected of civil servants.
We recruit by merit on the basis of fair and open competition, as outlined in the Civil Service Commission's recruitment principles (opens in a new window) .
The Civil Service embraces diversity and promotes equal opportunities. As such, we run a Disability Confident Scheme (DCS) for candidates with disabilities who meet the minimum selection criteria.
The Civil Service also offers a Redeployment Interview Scheme to civil servants who are at risk of redundancy, and who meet the minimum requirements for the advertised vacancy.
Apply and further information
This vacancy is part of the Great Place to Work for Veterans (opens in a new window) initiative.
The Civil Service welcomes applications from people who have recently left prison or have an unspent conviction. Read more about prison leaver recruitment (opens in new window) .
Once this job has closed, the job advert will no longer be available. You may want to save a copy for your records.
Contact point for applicants
Job contact :
Name : Nicola Peatfield
Email : nicola.peatfield@cabinetoffice.gov.uk
Recruitment team
Email : cabinetofficerecruitment.grs@cabinetoffice.gov.uk
Further information
Appointment to the Civil Service is governed by the Civil Service Commission’s Recruitment Principles. If you feel that your application has not been treated in accordance with the Recruitment Principles, and wish to make a complaint, then in the first instance you should contact Government Recruitment Service by email at : cabinetofficerecruitment.grs@cabinetoffice.gov.uk
If you are not satisfied with the response you receive, then you can contact the Civil Service Commission at info@csc.gov.uk. For further information on the Recruitment Principles. and bringing a complaint to the Civil Service Commission, please visit their website at: https://civilservicecommission.independent.gov.uk.
Show more
Show less","Data Analysis, Pay Modelling, Data Visualisation, Excel, Statistics, SQL, Python, R, Tableau, Power BI, MS Office, Maths, Communication, Teamwork, ProblemSolving","data analysis, pay modelling, data visualisation, excel, statistics, sql, python, r, tableau, power bi, ms office, maths, communication, teamwork, problemsolving","communication, data visualisation, dataanalytics, excel, maths, ms office, pay modelling, powerbi, problemsolving, python, r, sql, statistics, tableau, teamwork"
Senior Database Reliability Engineer,Arnold Clark,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-reliability-engineer-at-arnold-clark-3785834578,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"We are seeking a Database Reliability Engineer with a keen interest in automation and site reliability practices, to join our talented Digital team.
What We Offer
33 days’ annual leave with room to grow
Highly competitive salary
Flexible working
Training and conference funds to keep your skills current
Generous employee and retail discounts
Private healthcare and sick pay cover
Life assurance
Workplace pension
Maternity and paternity packages
Simplyhealth – all employees have access to a GP 24/7 and can claim money back on medical and dental treatments
Hours
Full time hybrid working: Monday – Friday, 9am - 5pm (with on call rota)
About The Role
Arnold Clark’s Database Reliability Engineering team are responsible for ensuring our database systems are reliable, available, and performant.
As part of our DBRE team, you will manage all aspects of our on-premises SQL Server estate. This will include planning and implementing high-availability, performance monitoring and tuning, capacity planning and ensuring we have robust backup, restore and failover process. You will also play a significant role in shaping our expanding Cloud database systems.
You will work collaboratively with other cross-functional teams across Arnold Clark Digital, including our Data, DevOps, and Product Development teams. You will assist in database design, planned migrations, and pre-emptively identifying database related issues. Where there are unforeseen incidents, you will be part of the incident response.
You will have a site reliability engineering mindset, pro-actively working towards identifying problems, and engineering solutions in an automated and repeatable manner.
Finally, you will be joining a small, well-respected team as we take on some exciting new projects.
Day-to-day duties
Ensure the reliability, availability, and performance of our database systems.
Design and implement robust backup, restore and failover strategies, ensuring data integrity while minimising service interruption.
Implement and extend our alerting and monitoring solutions, to identify potential issues before they occur.
Plan for changes in capacity requirements and growth in workloads.
Provide SQL expertise to our product engineering teams, assisting and coaching for efficient and performant use of the database.
Participate in incident response and problem diagnosis.
Implement automated processes to streamline database operations, configuration, and deployments.
Essential Skills And Experience
In-depth Microsoft SQL Server knowledge, with a proven record of managing a SQL Server estate
Solid understanding of always-on availability groups, security and encryption techniques such as Always Encrypted
Hands-on experience with cloud-based relational databases, e.g., Azure SQL
For a Lead Engineer position
Line management experience, including running one-to-one sessions
Task management, prioritisation, and progress reporting
Being the point of contact for cross-functional team activity and technical direction.
Desirable But Not Essential
Experience with Change Data Capture (CDC) tools
Experience with infrastructure as code tools, such as Terraform or PowerShell Desired State Configuration
Arnold Clark is committed to creating a diverse and inclusive workplace. We strive to create an environment where collaboration, unique perspectives and multiple approaches are celebrated. We care about our employees and our communities, we nurture talent and encourage ambition, and we are passionate about people who take pride in their work. Our employees are at the heart of everything we do – diverse in our make-up, united in our goals.
Show more
Show less","Digital team, Database Reliability Engineer, Microsoft SQL Server, Onpremises SQL Server, Highavailability, Performance monitoring, Tuning, Capacity planning, Backup, Restore, Failover, Cloud database systems, Data, DevOps, Product Development, SQL expertise, Incident response, Problem diagnosis, Automated processes, Configuration, Deployments, Azure SQL, Line management, Task management, Prioritisation, Progress reporting, Crossfunctional team activity, Technical direction, Change Data Capture (CDC) tools, Infrastructure as code tools, Terraform, PowerShell Desired State Configuration","digital team, database reliability engineer, microsoft sql server, onpremises sql server, highavailability, performance monitoring, tuning, capacity planning, backup, restore, failover, cloud database systems, data, devops, product development, sql expertise, incident response, problem diagnosis, automated processes, configuration, deployments, azure sql, line management, task management, prioritisation, progress reporting, crossfunctional team activity, technical direction, change data capture cdc tools, infrastructure as code tools, terraform, powershell desired state configuration","automated processes, azure sql, backup, capacity planning, change data capture cdc tools, cloud database systems, configuration, crossfunctional team activity, data, database reliability engineer, deployments, devops, digital team, failover, highavailability, incident response, infrastructure as code tools, line management, microsoft sql server, onpremises sql server, performance monitoring, powershell desired state configuration, prioritisation, problem diagnosis, product development, progress reporting, restore, sql expertise, task management, technical direction, terraform, tuning"
"Senior Data Engineer, Financial Crime Technology",Jobs via eFinancialCareers,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-financial-crime-technology-at-jobs-via-efinancialcareers-3784984567,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"Join us as a Senior Data Engineer, Financial Crime Technology
This is an opportunity for a technically minded individual to join us as a Senior Data Engineer within Financial Crime Technology
You'll be working with new and innovative technology to deliver high impact solutions
Hone your existing data engineering skills and advance your career in this critical role
What You'll Do
You'll apply widely agreed data software engineering principles and methodologies to design, develop, test and maintain applications and services to achieve the stated business and technology goals within required budgets and timelines. We'll also look to you to oversee the quality of work, making sure that it meets the technical standards for all services output.
You'll also:
Design and develop new reusable services and APIs for use across the bank
Design and develop data components that are amenable for a greater automation of build, release testing and deployment process on all environments
Drive the reuse and sharing of platform components and technologies within the software engineering teams
Deliver data components to enable the delivery of platforms, applications and services
Write unit and integration tests, in automated test environments to ensure code quality
The Skills You'll Need
You'll need a strong background in data engineering, software design or database design and architecture, as well as extensive experience of developing in a micro-services architecture. You should also have development experience in a programming language, experience of using industry recognised frameworks and development tooling, and a background of implementing programming best practice, especially around scalability, availability and performance.
You'll also need:
Proven track record of solving business and technical problems using data in micro-service architecture
Extensive experience of test-driven development alongside the use of automated test frameworks, mocking and stubbing and unit testing tools
Knowledge of the key phases of software delivery lifecycle and established software development methodologies
Experience of working in an environment where products must be delivered to specific timescales
An understanding of data engineering tools including Hadoop, streamSets, MongoDB, postGresDB, REST and python
The ability to understand and support, modify and maintain systems and code developed by other engineering teams
Show more
Show less","Data engineering, Software design, Database design, Database architecture, Microservices architecture, Programming, Industryrecognized frameworks, Development tooling, Programming best practices, Scalability, Availability, Performance, Testdriven development, Automated test frameworks, Mocking, Stubbing, Unit testing, Software delivery lifecycle, Software development methodologies, Hadoop, StreamSets, MongoDB, PostgreSQL, REST, Python","data engineering, software design, database design, database architecture, microservices architecture, programming, industryrecognized frameworks, development tooling, programming best practices, scalability, availability, performance, testdriven development, automated test frameworks, mocking, stubbing, unit testing, software delivery lifecycle, software development methodologies, hadoop, streamsets, mongodb, postgresql, rest, python","automated test frameworks, availability, data engineering, database architecture, database design, development tooling, hadoop, industryrecognized frameworks, microservices architecture, mocking, mongodb, performance, postgresql, programming, programming best practices, python, rest, scalability, software delivery lifecycle, software design, software development methodologies, streamsets, stubbing, testdriven development, unit testing"
Master Data Analyst x 2,Meraki Talent Ltd,"South Lanarkshire, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/master-data-analyst-x-2-at-meraki-talent-ltd-3784868971,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Onsite,"Master Data Senior Analyst/Analyst
Permanent
Lanarkshire (Hybrid, 3 days per week in the office)
Posted Thur 14 Dec 23
CVs ASAP
Start date Jan – Apr 24
Meraki Talent are actively looking for a Master Data Senior Analyst/Analyst to join their newly created, and well funded team. A greenfield site, the Master Data Senior Analyst/Analyst will have the opportunity to operate at an oversight level, as well as being hands on. The Master Data Senior Analyst/Analyst will support the implementation of the Master Data Program first locally, then across the globe. This would suit an experienced Master Data professional who is looking for more breadth and scope in role, an excellent work life balance, or perhaps looking to move into a more oversight rather than hands on role.
Responsibilities of the Master Data Senior Analyst/Analyst:
Implement the Global Master Data governance framework, enforcing policies within domain, compliance with data definitions and standards
Develop a comprehensive catalogue of master data elements
Document processes, responsibilities, and ownership of data over various domains
Define and update data definitions
Collaborate with global data owners to assist in the enrichment of master data
Identify quality issues and contribute to resolution
Support the creation of process maps, work instructions, and control points related to master data processes
Background of the Master Data Senior Analyst/Analyst:
Experience working within a Master Data capacity is absolutely essential for this role
An understanding of supply chain is preferable but not essential
Experience working within a project environment is helpful
Excellent attention to detail and ability to work within an oversight capacity
Data minded professional with an inquisitive mindset
Happy to work in the office 3 days per week
Is this job for you? At Meraki, we love recruitment and love words. Is this you?
Gordon wants: Master Data, Supply Chain, Project, Documentation, Governance
Please see our website page headed (eg) 'Privacy Notice' for an explanation about how we use information we collect about you'
Show more
Show less","Master Data, Governance, Data Definitions, Data Standards, Data Catalog, Data Processes, Data Ownership, Data Quality, Process Maps, Work Instructions, Control Points, Supply Chain, Project Management, Documentation, Attention to Detail, Oversight Capacity, DataMinded, Inquisitive Mindset","master data, governance, data definitions, data standards, data catalog, data processes, data ownership, data quality, process maps, work instructions, control points, supply chain, project management, documentation, attention to detail, oversight capacity, dataminded, inquisitive mindset","attention to detail, control points, data catalog, data definitions, data ownership, data processes, data quality, data standards, dataminded, documentation, governance, inquisitive mindset, master data, oversight capacity, process maps, project management, supply chain, work instructions"
Data Engineer,Peaple Talent,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-peaple-talent-3767993795,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"Peaple Talent have partnered with one of the UK's largest ferry/shipping services who are looking for a Data Engineer to join our clients expanding Data/Tech team. Forming a key part of the Architecture, Digital and Data team, this is a rare and exciting opportunity to join their internal data engineering team
We are searching for a highly motivated Data Engineer/BI Developer who can work at a senior level, you will develop and support a range of system integrations, ETL processes, data-at-rest solutions, internal API's and reporting tools, using cutting-edge Azure Data Platform technology as we continue our transformation.
The Role:
Based in Scotland, working in a hybrid-remote model you will be reporting to the Data Engineering lead and will be tasked with be responsibility for the build of data-streaming systems with a focus on Implementing flows to connect operational systems to analytics and business intelligence (BI) systems employing Data Lake architecture.
Skills, and Experience
Re-engineering manual data flows to enable scaling and repeatable use
Producing ETL designs and code with a view to optimal ETL performance
Building accessible data for analysis
Be responsible for the build of a connected enterprise estate, applying expert knowledge of the following systems integration approaches:
Messaging and queuing
Recognise and share opportunities to re-use existing data flows between systems
Inspire best practice for data products and services across the Information Technology team and the wider business
Build data engineering capability by providing technical leadership and career development for the team
The Candidate:
The ideal candidate for this role will have strong experience working with:
SQL
Python
ETL pipeline development
Azure cloud services (ADF, Data Lake, Data Bricks, Synapse Analytics, Azure DevOps)
Experience with designing flows for data visualization using Power BI
Strong in data Governance, including Master Data Management (MDM) and Data Quality tools and processes
Does that sound like you? If so, we’d love to see your CV.
The Package:
Basic salary: £40,000-£45,000
per annum DOE +
37 days holiday
Staff travel pass for yourself, family & dependents
Impressive pension scheme
Flexible working arrangements
Show more
Show less","SQL, Python, ETL pipeline development, Azure cloud services, ADF, Data Lake, Data Bricks, Synapse Analytics, Azure DevOps, Power BI, Data Governance, Master Data Management (MDM), Data Quality tools and processes","sql, python, etl pipeline development, azure cloud services, adf, data lake, data bricks, synapse analytics, azure devops, power bi, data governance, master data management mdm, data quality tools and processes","adf, azure cloud services, azure devops, data bricks, data governance, data lake, data quality tools and processes, etl pipeline development, master data management mdm, powerbi, python, sql, synapse analytics"
Data Engineer,BJSS,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3039186282,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Python, Java, C++, HTML, SQL, R, Git, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, CI/CD, OOP, Data storage, Data processing, Parallel computing, Workflow management, Relational databases, Nonrelational databases","python, java, c, html, sql, r, git, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, cicd, oop, data storage, data processing, parallel computing, workflow management, relational databases, nonrelational databases","athena, aws, azure, bigquery, c, cicd, cloud data fusion, data factory, data processing, data storage, databricks, gcp, git, glue, html, java, kafka, nonrelational databases, oop, parallel computing, python, r, redshift, relational databases, s3, sql, synapse, workflow management"
Senior Data Engineer,Bright Purple,"Glasgow City, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-bright-purple-3779287362,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"An exciting new Data Engineering role has arisen with a leading satellite imagery tech company. They are looking for an expert SQL Developer with the impulse to develop cutting edge pipelines and data solutions on the cloud to enhance their services.
As you can imagine, they have an extremely high-calibre data infrastructure, given the demands placed upon it by their work. So, you will play a crucial role in designing, implementing, and optimizing their cloud based (AWS) data infrastructure whilst your expertise in SQL Server query optimization and performance tuning will be instrumental in ensuring the efficiency and reliability of their data systems.
Key Experience:
- Strong SQL Server
- T-SQL
- Experience of Amazon Web Services (AWS) or GCP
- Programming knowledge i.e. C# or Python
- Agile DevOps
You’d be working closely with their Machine Learning Engineers, Data Scientists and Product Engineers to ensure that their data architecture is fit for requirements, and also explore how new and emerging database or streaming technologies can boost their operations.
They meet up once a month in the Glasgow office, and the role offers £50-70k DoE. They have a strong benefits package on top of this.
Show more
Show less","SQL, TSQL, AWS, GCP, C#, Python, Agile, DevOps, Machine Learning, Data Science, Data Architecture, Database, Streaming","sql, tsql, aws, gcp, c, python, agile, devops, machine learning, data science, data architecture, database, streaming","agile, aws, c, data architecture, data science, database, devops, gcp, machine learning, python, sql, streaming, tsql"
Senior Data Engineer,Jefferson Frank,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-jefferson-frank-3716446435,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"We are an online retail platform based in office in Dundee and Edinburgh, on the lookout for a Senior Data Engineer to join our team to work on a greenfield project building out a new, modern data platform.
You Will Have Experience With
AWS
SQL
Python
And Any Of The Following
Redshift
Databricks
Snowflake
Location:
Edinburgh - 1 day a month for a meet-up
Salaries
Up to £85,000
Show more
Show less","Data Engineering, AWS, SQL, Python, Redshift, Databricks, Snowflake","data engineering, aws, sql, python, redshift, databricks, snowflake","aws, data engineering, databricks, python, redshift, snowflake, sql"
Finance Data Analyst,Sanderson,"Gourock, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/finance-data-analyst-at-sanderson-3784545323,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"Company Overview:
Our clients business has developed over a number of years, introducing more modern technical
solutions for both customers and stakeholders including the new online ticketing solution
which was installed in May 2023. Following this modernisation, we require a Finance Data
Analyst to join the team, supporting the Finance function including FP&A the successful analyst
will help transform processes, draw insight from advanced analytics, and support automation.
The company are undergoing an exciting period of change and this role is an opportunity for someone to join the business and make their mark.
Key Responsibilities include:
Prepare reports to support the key finance activities including, group consolidation,
end reporting across key business areas, P&L, balance sheet, cashflow.
Support the FP&A role throughout the business planning process, group consolidation
plans, and performing analytical reviews
Support the FPA&A role to produce financial forecasting models, to support longer term financial forecasting. Analysing past financial performance, to inform future financial forecast performance.
Help prepare financial reports as required by Finance Director/ Senior Management.
Support the implementation of new systems/system updates into key Finance functions, with a focus on financial data, coding, P2P, Revenue and finance processes.
Identify Finance process improvements, as a result of more efficient use of financial
to streamline and remove manual effort.
Support ad-hoc analysis requests as required, providing timely and accurate financial insights
Experience and Knowledge:
Part Qualified Accountant, (preference with a Maths degree)
Experienced in preparing analysis and reports for senior management, numerically articulate with strong attention to detail
Comfortable with working with multiple data sources and complex financial data
Strong excel skills up to Visual Basic Application (VBA) level
Highly organised and able to manage large amounts of complex data in an efficient manner
Financial modelling (preference)
Knowledge of group reporting and group accounting processes would be beneficial
Show more
Show less","Visual Basic Application (VBA), Excel, Data Analysis, Reporting, Financial modeling, Financial Planning & Analysis (FP&A), Finance management, Data visualization, Accounting principles, Microsoft Office Suite, Group reporting, Group accounting, SQL, ERP, Automation","visual basic application vba, excel, data analysis, reporting, financial modeling, financial planning analysis fpa, finance management, data visualization, accounting principles, microsoft office suite, group reporting, group accounting, sql, erp, automation","accounting principles, automation, dataanalytics, erp, excel, finance management, financial modeling, financial planning analysis fpa, group accounting, group reporting, microsoft office suite, reporting, sql, visual basic application vba, visualization"
Senior Data Engineer,hackajob,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-hackajob-3780135310,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with Social Security Scotland helping them to hire the best talent and build the future.
In order to be matched you have to fill out the form by clicking
""Apply""
DDaT Pay Supplement
This post attracts a £5000 Digital, Data and Technology (DDaT) pay supplement after a 3-month DDaT competency qualifying period. The payment will be backdated to your start date in the role. Pay supplements are temporary payments designed to address recruitment and retention issues caused by market pressures and are subject to regular review. This post is part of the Scottish Government DDaT profession. As a member of the profession, you will join the professional development system, currently BCS RoleModelplus.
Joining the team at Social Security Scotland as a Senior Data Engineer offers the opportunity to play a pivotal role in driving the organization's data initiatives. The successful candidate, as a data enthusiast, will have the chance to leverage cutting-edge AWS services to harness the power of data, ensuring its accuracy, reliability, and security. Contributing significantly to the design, development, and optimization of the data infrastructure, the Senior Data Engineer will enable the delivery of essential services to the people of Scotland.
As part of the Cloud and Engineering branch, this role involves collaboration with colleagues from various technical teams and the wider organization. The Senior Data Engineer will be responsible for designing and leading the implementation of data flows that connect operational systems and data for analytics and BI systems.
Within the engineering team, the Senior Data Engineer will take on a leadership role, managing and mentoring junior staff members. Close collaboration with Developers, Business Analysts, and Quality Assurance Analysts is essential for success in this role. Working closely with other Senior Data Engineers, there will be opportunities to learn from and support each other, contributing to the achievement of business objectives.
Social Security Scotland, as an executive agency of the Scottish Government, is undergoing the largest and most complex IT and digital change program since devolution. With a lifetime budget exceeding £300m, the organization is dedicated to delivering a social security system that will support the people of Scotland for decades to come.
The organization is committed to fostering a positive and inclusive culture within Social Security Scotland, where individuals are supported to flourish. This involves creating a working environment where everyone is treated with dignity and respect, and where contributions are recognized and valued.
Show more
Show less","AWS, Data Infrastructure, Data Flows, Operational Systems, Analytics, BI Systems, Leadership, Mentoring, Developers, Business Analysts, Quality Assurance Analysts, Data Engineering, Cloud Engineering, IT, Digital Change, Social Security","aws, data infrastructure, data flows, operational systems, analytics, bi systems, leadership, mentoring, developers, business analysts, quality assurance analysts, data engineering, cloud engineering, it, digital change, social security","analytics, aws, bi systems, business analysts, cloud engineering, data engineering, data flows, data infrastructure, developers, digital change, it, leadership, mentoring, operational systems, quality assurance analysts, social security"
"Senior Data Engineer, Financial Crime Technology",NatWest Group,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-financial-crime-technology-at-natwest-group-3783817376,2023-12-17,Greater Glasgow, United Kingdom,Mid senior,Hybrid,"Our people work differently depending on their jobs and needs. From hybrid working to flexible hours , we have plenty of options that help our people to thrive.
This role is based in the United Kingdom and as such all normal working days must be carried out in the United Kingdom.
Join us as a Senior Data Engineer, Financial Crime Technology
This is an opportunity for a technically minded individual to join us as a Senior Data Engineer within Financial Crime Technology
You’ll be working with new and innovative technology to deliver high impact solutions
Hone your existing data engineering skills and advance your career in this critical role
What you'll do
You’ll apply widely agreed data software engineering principles and methodologies to design, develop, test and maintain applications and services to achieve the stated business and technology goals within required budgets and timelines. We’ll also look to you to oversee the quality of work, making sure that it meets the technical standards for all services output.
You’ll also:
Design and develop new reusable services and APIs for use across the bank
Design and develop data components that are amenable for a greater automation of build, release testing and deployment process on all environments
Drive the reuse and sharing of platform components and technologies within the software engineering teams
Deliver data components to enable the delivery of platforms, applications and services
Write unit and integration tests, in automated test environments to ensure code quality
The skills you'll need
You’ll need a strong background in data engineering, software design or database design and architecture, as well as extensive experience of developing in a micro-services architecture. You should also have development experience in a programming language, experience of using industry recognised frameworks and development tooling, and a background of implementing programming best practice, especially around scalability, availability and performance.
You’ll also need:
Proven track record of solving business and technical problems using data in micro-service architecture
Extensive experience of test-driven development alongside the use of automated test frameworks, mocking and stubbing and unit testing tools
Knowledge of the key phases of software delivery lifecycle and established software development methodologies
Experience of working in an environment where products must be delivered to specific timescales
An understanding of data engineering tools including Hadoop, streamSets, MongoDB, postGresDB, REST and python
The ability to understand and support, modify and maintain systems and code developed by other engineering teams
If you need any adjustments to support your application, such as information in alternative formats or special requirements to access our buildings, or if you’re eligible under the Disability Confident Scheme please contact us and we’ll do everything we can to help.
Apply for this job
Show more
Show less","Data Engineering, Software Design, Database Design, Database Architecture, Microservices Architecture, Programming Languages, Industry Recognized Frameworks, Development Tooling, Programming Best Practices, Scalability, Availability, Performance, TestDriven Development, Automated Test Frameworks, Mocking, Stubbing, Unit Testing Tools, Software Delivery Lifecycle, Software Development Methodologies, Hadoop, StreamSets, MongoDB, PostgreSQL, REST, Python","data engineering, software design, database design, database architecture, microservices architecture, programming languages, industry recognized frameworks, development tooling, programming best practices, scalability, availability, performance, testdriven development, automated test frameworks, mocking, stubbing, unit testing tools, software delivery lifecycle, software development methodologies, hadoop, streamsets, mongodb, postgresql, rest, python","automated test frameworks, availability, data engineering, database architecture, database design, development tooling, hadoop, industry recognized frameworks, microservices architecture, mocking, mongodb, performance, postgresql, programming best practices, programming languages, python, rest, scalability, software delivery lifecycle, software design, software development methodologies, streamsets, stubbing, testdriven development, unit testing tools"
Sr. Data Analyst,Dew Software,"Hudson, OH",https://www.linkedin.com/jobs/view/sr-data-analyst-at-dew-software-3781950149,2023-12-17,Norton,United States,Mid senior,Remote,"Dew Software, a prominent player in the Digital Transformation space, is looking for a skilled Sr. Data Analyst to join our dynamic team. With our commitment to quality and excellence, Dew Software operates from 14 development centers across 9 countries, leveraging the expertise and talent of diverse professionals. Collaborating with Fortune 500 companies, we utilize cutting-edge technologies and innovative approaches to drive their digital innovation and transformation initiatives. As a Sr. Data Analyst, you will play a pivotal role in providing strategic insights and actionable recommendations to our esteemed clients, empowering them to stay ahead in today's competitive digital landscape.
Responsibilities
Lead and conduct complex data analyses to identify trends, patterns, and insights that drive business decision-making
Collaborate with cross-functional teams, including business stakeholders, to define data analysis objectives and requirements
Design, develop, and implement advanced analytics models and techniques to solve complex business problems
Evaluate and optimize existing data analytics processes and solutions to improve efficiency and effectiveness
Create and maintain data visualizations, dashboards, and reports to communicate analysis findings to stakeholders
Act as a subject matter expert on data analytics methodologies, tools, and techniques
Keep abreast of industry trends and best practices in data analysis to continuously enhance skills and knowledge
Requirements
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Systems)
Proven experience as a Data Analyst, preferably in the consumer goods industry
Strong knowledge and experience with Salesforce CRM and related tools (e.g., Sales Cloud, Service Cloud, Marketing Cloud)
Proficient in data analysis and visualization tools (e.g., Excel, Tableau, Power BI)
Solid understanding of SQL for data extraction, manipulation, and analysis
Excellent problem-solving skills and attention to detail
Strong communication and presentation skills, with the ability to convey complex data insights to non-technical stakeholders
Ability to work independently and collaboratively in a fast-paced environment
Show more
Show less","Data Analytics, Business Intelligence, Data Mining, Data Visualization, Data Modeling, Salesforce CRM, Sales Cloud, Service Cloud, Marketing Cloud, Excel, Tableau, Power BI, SQL, Python, R, Machine Learning, Artificial Intelligence","data analytics, business intelligence, data mining, data visualization, data modeling, salesforce crm, sales cloud, service cloud, marketing cloud, excel, tableau, power bi, sql, python, r, machine learning, artificial intelligence","artificial intelligence, business intelligence, data mining, dataanalytics, datamodeling, excel, machine learning, marketing cloud, powerbi, python, r, sales cloud, salesforce crm, service cloud, sql, tableau, visualization"
Business Data Analyst,Westfield,"Westfield Center, OH",https://www.linkedin.com/jobs/view/business-data-analyst-at-westfield-3782258164,2023-12-17,Norton,United States,Mid senior,Hybrid,"Job Description
Job Title: Business Data Analyst
Employee Status: Regular
Schedule: Full-time
Location: Hybrid - defined as three days in office if residence is within 50 miles of Westfield Center, OH or Remote - if residence is over 50 miles from Westfield Center, OH
About Us
You’re ready to make your mark where people care about each other. Where your work is meaningful. And where your unique perspectives are welcome. Think about joining Westfield, a company focused on what’s most important – the people behind our policies.
Westfield was founded in 1848 by a small group of hard-working farmers who believed in the promise of the future and the power of the individual. Today, as one of the nation’s leading property and casualty (P&C) companies, we remain true to their vision and are dedicated to making a positive difference in our customers’ lives.
The Business Intelligence Analyst will have opportunities to deliver self-service analytical capabilities with highest priority across Westfield. In this role, they will work directly with stakeholders across the organization to understand business needs and translate those needs into data, metrics, dashboards and reports. They will collaborate with BI Agile teams to deliver these self-service capabilities and ensure effective utilization of these assets through training and change management with end-users.
Show more
Show less","Business Intelligence, Data Analysis, Stakeholder Engagement, Data Visualization, Metrics, Dashboards, Reporting, Agile Development, Training, Change Management","business intelligence, data analysis, stakeholder engagement, data visualization, metrics, dashboards, reporting, agile development, training, change management","agile development, business intelligence, change management, dashboard, dataanalytics, metrics, reporting, stakeholder engagement, training, visualization"
Sr. Business Data Analyst,Westfield,"Westfield Center, OH",https://www.linkedin.com/jobs/view/sr-business-data-analyst-at-westfield-3782252785,2023-12-17,Norton,United States,Mid senior,Hybrid,"Job Description
Job Title: Sr. Business Data Analyst
Employee Status: Regular
Schedule: Full-time
Location:Hybrid - defined as three days in office located in Westfield Center, OH
About Us
You’re ready to make your mark where people care about each other. Where your work is meaningful. And where your unique perspectives are welcome. Think about joining Westfield, a company focused on what’s most important – the people behind our policies.
Westfield was founded in 1848 by a small group of hard-working farmers who believed in the promise of the future and the power of the individual. Today, as one of the nation’s leading property and casualty (P&C) companies, we remain true to their vision and are dedicated to making a positive difference in our customers’ lives.
Show more
Show less","Data Analysis, Business Intelligence","data analysis, business intelligence","business intelligence, dataanalytics"
Senior Data Analyst - Marketing,Remitly,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-data-analyst-marketing-at-remitly-3738858051,2023-12-17,Bremerton,United States,Mid senior,Onsite,"Job Description
Remitly is on a mission to transform the lives of immigrants and their families by providing the most trusted financial products and services on the planet. Since 2011, we have been tirelessly delivering on our promises to immigrants sending their hard earned money home. Today, we are reimagining international payments at scale and building new products to create deeper relationships with our customers and their loved ones across the globe. Join over 2,700 employees across 10 offices who are growing their careers while having a positive impact on people globally.
Remitly is registered as a Money Services Business in the U.S., Canada, EU, United Kingdom, Singapore and Australia. Each of these jurisdictions require, among other items, that Remitly maintain a comprehensive Risk and Compliance Program.
About The Role
As a Senior Marketing Analyst you will report to the Senior Manager for Marketing Analytics. You will drive analytical decisions for Remitly's owned marketing channels, specifically SEO and ASO.This includes campaign design, measurement and optimization for the channel, contributing directly to growth in acquisition of customers. You would also provide expertise on designing A/B tests and evaluating across marketing channels and be a thought-partner and mentor to analysts and marketers.You will lead the development and management of data and reporting tools, defining important KPIs, tracking and monitoring metrics movements and identifying insights.
You Will
Provide ongoing campaign analysis and optimizations using a defined set of KPIs for owned marketing channels, specifically SEO and ASO.
Build measurement plans based on strategic imperatives, communication objectives and media strategies
Proactively identify and interpret important drivers behind marketing and media performance, and deliver insights to channel owners to improve the efficiency of spend and performance of media as aligned with our goals
Design and implement reporting dashboards that track important metrics and performance trends, and provide actionable insights to marketing leadership.
Provide leadership in streamlining our data stack and improving and / or maintaining the quality and reliability of our owned marketing channels data, including data ingested from third-party partners.
Partner closely with the Marketing team to design tests and execute analysis to generate actionable insights.
You Have
7+ years of experience in A/B testing, digital marketing analytics, and comfort with basic statistical techniques (including causal inference and regression analysis).
Expertise with SEO and ASO and knowledge of tools / APIs such as Amplitude, Google Analytics and Lighthouse
Ability to represent complex information in easily understood visualizations (including Tableau, Excel (can perform complex functions)).
7+ years experience with SQL, R or Python, and data visualization tools (e.g. Tableau, Periscope).
Experience building regular reports/dashboards to measure channel performance, support decisions on P&L management, find opportunities and performance trends while also helping analyze metric trends
Experience managing stakeholders, communication with executives, channel owners, partners, and contributing to projects and meetings
Quantitative, and analytics skills must be very comfortable with data analysis and Influencing decision-making
Biased for action: willing to build the first prototype, and continuously improve based on learnings.
Our Benefits
Flexible paid time off
Health, dental, and vision benefits + 401k plan with company matching
Company contributions to your HSA plan, if you choose one
Employee Stock Purchase Plan (ESPP) available for eligible employees
Continuing education and corridor travel benefits
Compensation Details.
The starting base salary range for this position is typically $120,000-$150,000. In the U.S., Remitly employees are shareholders in our Company and equity is part of our total compensation plan. Your recruiter can share more information about medical benefits offered, as well as other financial benefits and total compensation components offered with this role.
Your recruiter can share more information about medical benefits offered, as well as other financial benefits and total compensation components offered with this role.
Remitly is an Equal Opportunity Employer
. Equal employment opportunity has been, and will continue to be, a fundamental principle at Remitly. We are committed to nondiscrimination across our global organization and in all of our business operations. Employment is determined based upon personal capabilities and qualifications without discrimination on the basis of race, creed, color, religion, sex, gender identification and expression, marital status, military status or status as an honorably discharge/veteran, pregnancy (including a woman's potential to get pregnant, pregnancy-related conditions, and childbearing), sexual orientation, age (40 and over), national origin, ancestry, citizenship or immigration status, physical, mental, or sensory disability (including the use of a trained dog guide or service animal), HIV/AIDS or hepatitis C status, genetic information, status as an actual or perceived victim of domestic violence, sexual assault, or stalking, or any other protected class as established by law.
Remitly is an E-Verify Employer
Remitly is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Show more
Show less","SEO, ASO, Digital marketing analytics, Basic statistical techniques, Causal inference, Regression analysis, Amplitude, Google Analytics, Lighthouse, Tableau, Excel, SQL, R, Python, Periscope, Data visualization, Metric trends analysis, Stakeholder management, Communication, Decisionmaking, Equity, Employee Stock Purchase Plan (ESPP), Equal Opportunity Employer","seo, aso, digital marketing analytics, basic statistical techniques, causal inference, regression analysis, amplitude, google analytics, lighthouse, tableau, excel, sql, r, python, periscope, data visualization, metric trends analysis, stakeholder management, communication, decisionmaking, equity, employee stock purchase plan espp, equal opportunity employer","amplitude, aso, basic statistical techniques, causal inference, communication, decisionmaking, digital marketing analytics, employee stock purchase plan espp, equal opportunity employer, equity, excel, google analytics, lighthouse, metric trends analysis, periscope, python, r, regression analysis, seo, sql, stakeholder management, tableau, visualization"
Senior Software Engineer - Data Platform,Snowflake,"Bellevue, WA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-platform-at-snowflake-3766404086,2023-12-17,Bremerton,United States,Mid senior,Onsite,"Build the future of data. Join the Snowflake team.
We're hiring talented Senior Software Engineers to join us! The position will work directly with our engineering team in Bellevue, WA to evolve our elastic, large scale, high-performance computing environment. We need strong engineers who can pick up and understand complex technical areas quickly and who are enthusiastic about building new technologies! This role has the potential to work in a variety of product areas, including making contributions to open source big data projects.
AS A SENIOR SOFTWARE ENGINEER AT SNOWFLAKE YOU WILL:
Design and develop features, understand customer requirements, and meet business goals
Build highly reliable software to meet the needs of the largest customers
Enhance the programmability surface of Snowflake and improve the experience of the data developer
Contribute to big data open source projects to add connectivity and support for Snowflake
Analyze fault-tolerance and high availability issues, performance and scale challenges, and solve them
Ensure operational readiness of the services and meet the commitments to our customers regarding reliability, availability, and performance
OUR IDEAL SENIOR SOFTWARE ENGINEER WILL HAVE:
7+ years industry experience designing and building distributed systems, database systems, compilers, or other enterprise cloud data platforms
Experience building enterprise grade, reliable and trustworthy software or services
Extremely strong fundamental computer science skills
Fluency in Java and/or C++
Ability to work in a team environment, collaborate well, and mentor junior engineers
Ability to work on-site in our downtown Bellevue office
Advanced degree in Computer Science or related field
BONUS POINTS FOR EXPERIENCE WITH THE FOLLOWING:
SQL and/or other database technologies including internal implementations
Data warehouse architecture, design, and experience working with large-scale data processing solutions
ASF open source projects, ideally from the big-data/Hadoop ecosystem
Implementation testing, debugging, documentation, query compilation, compiler design and implementation
Cloud infrastructure, AWS in particular
Delivering compelling experiences in SaaS or web-based solutions
Implementing multi-tenant systems, with focus on isolation and security
Designing and implementing systems for public or private clouds
WHY JOIN THE ENGINEERING TEAM AT SNOWFLAKE?
As a member of our team, you will:
Build an industry-leading data management system that customers love
Solve challenging technical problems related to security, parallel and distributed systems, programming, resource management, large-scale system maintenance, and more!
Learn about and contribute to the most robust and secure enterprise SaaS platform that services hundreds of customers and millions of complex queries daily
Learn about and contribute to a highly-scalable and reliable data processing platform that runs on hundreds and thousands of machines
Join a world-class team of both industry veterans and rising stars
The following represents the expected range of compensation for this role:
The estimated base salary range for this role is $214,000 - $327,750.
Additionally, this role is eligible to participate in Snowflake’s bonus and equity plan.
The successful candidate’s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location. This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits.
Snowflake is growing fast, and we’re scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake.
How do you want to make your impact?
Show more
Show less","Java, C++, AWS, Cloud infrastructure, Software development, Software design, Hadoop, Distributed systems, Data warehouse, SQL, SaaS, Big data, Data processing, Data management, Compilers","java, c, aws, cloud infrastructure, software development, software design, hadoop, distributed systems, data warehouse, sql, saas, big data, data processing, data management, compilers","aws, big data, c, cloud infrastructure, compilers, data management, data processing, datawarehouse, distributed systems, hadoop, java, saas, software design, software development, sql"
Staff Software Engineer - Distributed Data Systems,Databricks,"Bellevue, WA",https://www.linkedin.com/jobs/view/staff-software-engineer-distributed-data-systems-at-databricks-3783870933,2023-12-17,Bremerton,United States,Mid senior,Onsite,"P-988
At Databricks, we are passionate about enabling data teams to solve the world's toughest problems — from making the next mode of transportation a reality to accelerating the development of medical breakthroughs. We do this by building and running the world's best data and AI infrastructure platform so our customers can use deep data insights to improve their business. Founded by engineers — and customer obsessed — we leap at every opportunity to solve technical challenges, from designing next-gen UI/UX for interfacing with data to scaling our services and infrastructure across millions of virtual machines. And we're only getting started.
Modern data analysis employs sophisticated methods such as machine learning that go well beyond the roll-up and drill-down capabilities of traditional SQL query engines. As a software engineer on the Runtime team at Databricks, you will be building the next generation distributed data storage and processing systems that can outperform specialized SQL query engines in relational query performance, yet provide the expressiveness and programming abstractions to support diverse workloads ranging from ETL to data science.
Below Are Some Example Projects
Apache Spark
: Develop the de facto open source standard framework for big data.
Data Plane Storage
: Provide reliable and high performance services and client libraries for storing and accessing humongous amount of data on cloud storage backends, e.g., AWS S3, Azure Blob Store.
Delta Lake
: A storage management system that combines the scale and cost-efficiency of data lakes, the performance and reliability of a data warehouse, and the low latency of streaming. Its higher level abstractions and guarantees, including ACID transactions and time travel, drastically simplify the complexity of real-world data engineering architecture.
Delta Pipelines
: It's difficult to manage even a single data engineering pipeline. The goal of the Delta Pipelines project is to make it simple and possible to orchestrate and operate tens of thousands of data pipelines. It provides a higher level abstraction for expressing data pipelines and enables customers to deploy, test & upgrade pipelines and eliminate operational burdens for managing and building high quality data pipelines.
Performance Engineering
: Build the next generation query optimizer and execution engine that's fast, tuning free, scalable, and robust.
What We Look For
BS (or higher) in Computer Science, related technical field or equivalent practical experience.
Comfortable working towards a multi-year vision with incremental deliverables.
Motivated by delivering customer value and impact.
8+ years of production level experience in either Java, Scala or C++.
Strong foundation in algorithms and data structures and their real-world use cases.
Experience with distributed systems, databases, and big data systems (Spark, Hadoop).
Benefits
Comprehensive health coverage including medical, dental, and vision
401(k) Plan
Equity awards
Flexible time off
Paid parental leave
Family Planning
Gym reimbursement
Annual personal development fund
Work headphones reimbursement
Employee Assistance Program (EAP)
Business travel accident insurance
Pay Range Transparency
Databricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here.
Local Pay Range
$182,400—$247,000 USD
About Databricks
Databricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook.
Our Commitment to Diversity and Inclusion
At Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics.
Compliance
If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.
Show more
Show less","Apache Spark, Java, Scala, C++, Algorithms, Data structures, Distributed systems, Databases, Big data systems, Query optimizer, Execution engine, Hadoop, Data Plane Storage, Delta Lake, Delta Pipelines, Performance Engineering","apache spark, java, scala, c, algorithms, data structures, distributed systems, databases, big data systems, query optimizer, execution engine, hadoop, data plane storage, delta lake, delta pipelines, performance engineering","algorithms, apache spark, big data systems, c, data plane storage, data structures, databases, delta lake, delta pipelines, distributed systems, execution engine, hadoop, java, performance engineering, query optimizer, scala"
Senior Software Engineer - Big Data Pipeline,Truveta,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-software-engineer-big-data-pipeline-at-truveta-3351947560,2023-12-17,Bremerton,United States,Mid senior,Onsite,"Truveta is the world’s first health provider led data platform with a vision of Saving Lives with Data. Our mission is to enable researchers to find cures faster, empower every clinician to be an expert, and help families make the most informed decisions about their care. Achieving Truveta’ s ambitious vision requires an incredible team of talented and inspired people with a special combination of health, software and big data experience who share our company values.
This position is based out of our headquarters in the Greater Seattle area.
Who We Need
Truveta is rapidly building a talented and diverse team to tackle complex health and technical challenges. Beyond core capabilities, we are seeking problem solvers, passionate and collaborative teammates, and those willing to roll up their sleeves while making a difference. If you are interested in the opportunity to pursue purposeful work, join a mission-driven team, and build a rewarding career while having fun, Truveta may be the perfect fit for you.
This Opportunity
If you are seeking a sense of purpose in your software job and are motivated by solving computing problems that can have a direct impact on people’s lives, then we have opportunities for you in an exciting startup environment.
Responsibilities
Leverage software engineering skills to build and deliver reliable, secure and scalable software solutions using modern cloud-based paradigms across different components in Truveta.
Collaborate closely with partner teams inside & outside of Truveta.
Show creativity and curiosity to build integration proof of concepts across the Truveta and partner ecosystems.
Bring your curiosity, entrepreneurial spirit, and passion to deliver on the promise of technology in the difficult, competitive, and exciting industry of healthcare.
We Are Seeking Software Engineers With
Proven experience building reliable, secure and scalable software solutions using modern cloud-based paradigms.
Strong distributed systems design and development experiences using spark engine.
Strong provisioning/network cloud base knowledge (Azure expertise preferred).
Experience building scalable systems and integrating with microservices.
Familiarity with SaaS ecosystem.
Knowledge about language and interpreter design and implementation.
Experience with Lexers, Parsers and (AST) Abstract Syntax Trees.
Ability to learn quickly, knowing how to iterate and evolve a distributed software solution while carefully balancing the needs of privacy, security and reliability against a changing product landscape.
Knowledge and experience in using and designing distributed systems, data pipelines, APIs and consumption experience.
Ability to guide workstreams, mentor and inspire junior developers and cultivate a positive work environment.
Strict adherence to Truveta principles in support of ethical innovation.
Willingness to commit to ongoing training on key topics like privacy, ethics, and security.
Key Qualifications
5+ years of experience building production quality software in a team setting.
2+ years of experience building distributed system
2+ years of experience working on modern cloud-native platforms and languages (AWS, GCP, Azure)
Some familiarity with crafting modular APIs and building engaging and intuitive web experiences.
Familiarity with Big Data platforms such as Databricks
B.S. or M.S. in Computer Science or related field.
Prior work with Azure ADF, Function App, Kubernetes is a plus.
DevOps experience is a plus
Why Truveta?
Be a part of building something special. Now is the perfect time to join Truveta. We have strong, established leadership with decades of success. We are well-funded. We are building a culture that prioritizes people and their passions across personal, professional, and everything in between. Join us as we build an amazing company together.
We Offer
Interesting and meaningful work for every career stage
Great benefits package
Comprehensive benefits with strong medical, dental and vision insurance plans
401K plan
Professional development & training opportunities for continuous learning
Work/life autonomy via flexible work hours and flexible paid time off
Generous parental leave
Regular team activities (virtual and in-person as soon as we are able)
The base pay for this position is $152,000 to $205,000. The pay range reflects the minimum and maximum target. Pay is based on several factors includinments to the technical team) The role is within the Data Warehouse therefore strong ETL/DW skills are required.
Key responsibilities to include:
Creating Business Requirement Documents, Functional Requirement Documents and Mapping documents for the development of ETL applications.
Writing complex SQL queries across multiple tables, data analysis and profiling skills for the creation of the mapping documents and further discussion with data modelers for the necessary table designs.
Translating requirements with expert level verbal and written communication skills to work with various stakeholders, gather and formulate business requirements and translate the same to the technical team.
Source-to-Target mapping and general data warehousing
Support testing, production readiness verification and ensuring accuracy after the deployments.
Required Skills
Strong Data Profiling and Analysis skills
Advanced SQL knowledge
Strong Data Mapping and Functional requirements documentation skills
Ability write business and functional requirements
Strong verbal, writing and active listening skills
Data warehousing knowledge
Healthcare experience is desired
*The role requires direct employment*
*Must be local to DE, PA or NJ - applicants outside of this area will not be considered*
Show more
Show less","SQL, ETL, Data Mapping, Data Analysis, Data profiling, Data modeling, Data warehousing, Business Analysis, Business Requirements Gathering, Functional Requirements, Communication, Healthcare","sql, etl, data mapping, data analysis, data profiling, data modeling, data warehousing, business analysis, business requirements gathering, functional requirements, communication, healthcare","business analysis, business requirements gathering, communication, data mapping, data profiling, dataanalytics, datamodeling, datawarehouse, etl, functional requirements, healthcare, sql"
Sr Data Engineer/ Spark/ AWS,Motion Recruitment,"Philadelphia, PA",https://www.linkedin.com/jobs/view/sr-data-engineer-spark-aws-at-motion-recruitment-3730471161,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Are you interested in joining a rapidly expanding Media/Fintech organization located in the City of Brotherly Love? Our client is dedicated to empowering Creators by enabling them to monetize their catalogs by licensing their existing videos (and/or future video uploads) and receiving instant cash. Creators can then use these funds to support their growth by recruiting resources, investing, or any other means they desire while maintaining their independence.
This project entails collaborating to develop data pipelines using Apache Spark and designing the stream processing system. The ideal candidate should have prior experience working with large amounts of data and contribute to the development of best data practices while providing data insights to colleagues within the company.
COMPANY: Mid-Sized (approximately 250 employees)
Required Skills & Experience
5 yrs. of Software Engineering exp.
3 yrs. of Data Engineering with Spark OR Flink exp.
3 yrs. running Software and Services in cloud
Python OR Scala
SQL
AWS: S3, Redshift, Data-Lake
Desired Skills & Experience
HIVE Metastore
Apache Iceberg
AWS Glue
Tech Breakdown
100% Data Engineering
The Offer
Bonus Eligible
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k)
Bonus (discretionary, depends on performance)
Equity
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Caroline Stranieri
Show more
Show less","Apache Spark, Data pipelines, Data engineering, Data insights, Data practices, Cloud computing, Python, Scala, SQL, AWS, S3, Redshift, Data Lake, Hive Metastore, Apache Iceberg, AWS Glue","apache spark, data pipelines, data engineering, data insights, data practices, cloud computing, python, scala, sql, aws, s3, redshift, data lake, hive metastore, apache iceberg, aws glue","apache iceberg, apache spark, aws, aws glue, cloud computing, data engineering, data insights, data lake, data practices, datapipeline, hive metastore, python, redshift, s3, scala, sql"
Data Analyst,"Liberty Personnel Services, Inc.","Cherry Hill, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-liberty-personnel-services-inc-3666036034,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Details:
Data Analyst
I'm seeking a Data Analyst for a full-time, hybrid position with a stable and growing company in the New Jersey area. This is a great opportunity for a junior data scientist looking to take the next step in his or their career. Interested candidates should have the following.
Bacherlor's degree in Statistics, Applied Mathematics, Computer Science or related discipline.
2-5 years of on the job experience in data anlalytics, business analytics or similar role
Extracting data to provide valuable insights to end users
SQL Reporting
Data visualization, Power BI, Tableau, Python, R
Predictive modeling
Data storytelling via decks and dashboards and ability to communicate to non-technical audience.
If you would like to learn more about this exciting hybrid position, please send your resume to bf@libertyjobs.com. You can also connect with me on LinkedIn as I have multiple positions available in all areas of IT and I would be glad to help you or someone you know learn about better job opportunities.
Brian Patrick Feeley
bf@libertyjobs.com
484-690-9609
www.linkedin.com/pub/brian-patrick-feeley/5/442/498/
www.libertyjobs.com
#IT
#midsenior
Show more
Show less","Data Analytics, Business Analytics, Predictive Modeling, Data Visualization, SQL Reporting, Power BI, Tableau, Python, R, Data Storytelling, Statistics, Applied Mathematics, Computer Science","data analytics, business analytics, predictive modeling, data visualization, sql reporting, power bi, tableau, python, r, data storytelling, statistics, applied mathematics, computer science","applied mathematics, business analytics, computer science, data storytelling, dataanalytics, powerbi, predictive modeling, python, r, sql reporting, statistics, tableau, visualization"
Senior Cloud Data Engineer,BDO USA,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765472147,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Python, Java, C#, Scala, Linux, Microsoft Fabric, Power BI, Azure Analysis Services, Git, Linux, Data Lake Medallion Architecture, AI Algorithms/Machine Learning, UiPath, Alteryx, Computer Vision based AI technologies, SQL, Data Warehousing, Data Modeling, Star Schema Construction, Azure, AWS, SSIS, SSAS, SSRS, PySpark, Delta, Pandas, Spark SQL, dbt, Terraform, Bicep, Data Ops, Purview, Glue","data analytics, business intelligence, artificial intelligence, python, java, c, scala, linux, microsoft fabric, power bi, azure analysis services, git, linux, data lake medallion architecture, ai algorithmsmachine learning, uipath, alteryx, computer vision based ai technologies, sql, data warehousing, data modeling, star schema construction, azure, aws, ssis, ssas, ssrs, pyspark, delta, pandas, spark sql, dbt, terraform, bicep, data ops, purview, glue","ai algorithmsmachine learning, alteryx, artificial intelligence, aws, azure, azure analysis services, bicep, business intelligence, c, computer vision based ai technologies, data lake medallion architecture, data ops, dataanalytics, datamodeling, datawarehouse, dbt, delta, git, glue, java, linux, microsoft fabric, pandas, powerbi, purview, python, scala, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, terraform, uipath"
"Manager, Data Engineer",Arch Capital Services LLC,Greater Philadelphia,https://www.linkedin.com/jobs/view/manager-data-engineer-at-arch-capital-services-llc-3737170102,2023-12-17,Winslow,United States,Mid senior,Hybrid,"With a company culture rooted in collaboration, expertise and innovation, we aim to promote progress and inspire our clients, employees, investors and communities to achieve their greatest potential. Our work is the catalyst that helps others achieve their goals. In short, We Enable Possibility℠.
Job Summary
Strategic Analytics is a growing team at Arch that has established itself as a driving force in how the business is run. This is achieved through the implementation of real-time predictive analytic solutions that move the company forward. Our track record is 100% adoption of our tools and services. Data plays a critical role in our mission. We create best-in-class data solutions from internal and external sources by leveraging a diverse set of cloud technologies like Snowflake, the Azure tech stack, Databricks and Python.
As Manager of Strategic Analytics Services, you will implement complex data pipelines that put analytics at the heart of decision-making. You will work closely with leaders across the company on high-profile analytics projects that drive business strategies. As a key member of the data engineering team, you will extend our capacity to deliver and push the team forward.
Responsibilities
Work with strategic partners to solve business problems by directing junior employees to develop best-in-class data solutions
Manage data engineering throughout the project lifecycle
Build strong partnerships with peers across the organization to support data-related goals
Proactively design intuitive data structures by anticipating analytic needs
Explore new technologies and data sources with curiosity and creativity
Leverage technology to automate data ingestion and link external and internal data
Provide the appropriate documentation of sources and technical solutions
Desired Skills
Demonstrated experience building data structures to support analytics/research/actuarial functions in an insurance company setting
Experience with cloud technologies like Snowflake and Databricks
Familiarity with the MLOps framework
Ability to operate independently – managing tasks and engaging people across the team
Exceptional collaboration and relationship building skills
Comfortable handling ambiguous concepts and breaking down complex problems into manageable pieces
Strong data manipulation skills for analytics
Resilient problem solving and critical thinking skills
Thorough understanding of data warehousing concepts and design
Ability to communicate effectively to different target audiences
Flexibility – able to meet changing requirements and priorities
Required Skills
4+ years of data engineering experience
2+ years of project management experience
Education
College degree in Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, Data Analytics, or equivalent
For individuals assigned or hired to work in California, Colorado, Jersey City, NJ; New York State; and/or Washington State, the base salary range is listed below. This range is as of the time of posting. Position is incentive eligible.
$148,614 - $201,066
Total individual compensation (base salary, short & long-term incentives) offered will take into account a number of factors including but not limited to geographic location, scope & responsibilities of the role, qualifications, talent availability & specialization as well as business needs. The above range may be modified in the future
Click here to learn more on available benefits
Do you like solving complex business problems, working with talented colleagues and have an innovative mindset? Arch may be a great fit for you. If this job isn’t the right fit but you’re interested in working for Arch, create a job alert! Simply create an account and opt in to receive emails when we have job openings that meet your criteria. Join our talent community to share your preferences directly with Arch’s Talent Acquisition team.
Show more
Show less","Cloud technologies, Snowflake, Databricks, Python, Data engineering, Data pipelines, Data structures, MLOps, Data manipulation, Data warehousing, Data analytics, Project management, Collaboration, Problem solving, Critical thinking","cloud technologies, snowflake, databricks, python, data engineering, data pipelines, data structures, mlops, data manipulation, data warehousing, data analytics, project management, collaboration, problem solving, critical thinking","cloud technologies, collaboration, critical thinking, data engineering, data manipulation, data structures, dataanalytics, databricks, datapipeline, datawarehouse, mlops, problem solving, project management, python, snowflake"
Senior Data Analyst,Chubb,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-chubb-3756339189,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Description
By joining Chubb as a Senior Data Analyst, you willanalyze and certify data, troubleshoot data issues, promote data quality, and assist in the delivery of key projects in support of our North America enterprise data warehousing initiative. Our data warehouse platform is a strategic application within the business, feeding into multiple systems and applications both up and downstream with data that directly supports business decisions being made each and every day. With us, you’ll leverage your knowledge of SQL and database tables to analyze data, identify gaps or discrepancies, and partner with applications teams, IT teams across the business, our stakeholders, and our Data Architect to drive solutions that directly impacts how data is used throughout the business. You’ll have opportunities to build upon your technical skills and insurance knowledge as we continue the evolution of our data warehouse and related infrastructure. Through it all, we’ll also look to you to share your ideas and manage data related projects end to end that influence how we incorporate, validate, and distribute data enterprise-wide. This role will directly support the General Ledger Posting process.
In This Role, You Will
Manage assigned work efforts through all phases of the development, testing, and implementation life cycle, reviewing all requirements, creating test scenarios, developing test plans, analyzing results and testing/validating data
Reconcile multiple data sources and identify the root cause of discrepancies in expected output; distinguish between multiple root causes and/or multiple trends in a given data set and articulate results to various stakeholders, management, and technical resources
Analyze and test relational databases and investigate any data load failures or data retrieval issues
Monitor data warehousing systems based on assigned tasks to ensure reliability and accuracy of information loaded into the databases
Clearly define and document requirements, communicating changes in scope or requires, to all stakeholders; initiate change management processes as required
Generate reports, dashboards, and ad hoc extracts for business and/or leadership
Ensure data integrity by implementing quality assurance practices, gathering, and entering missing data, and resolving any anomalies
Qualifications
5+ years of experience in a Data Analysis, Business Analysis, Data Quality Assurance (or similar) role as part of a data warehouse team and supporting data-driven projects
Experience working with DBMS platforms, including a demonstrated understanding of table structures, hierarchies, joins, etc.
Advanced knowledge of SQL, with the ability to write and troubleshoot medium to advanced SQL queries
Knowledge of data warehousing methodologies and tools including their connection to systems both up and down stream
Bachelor’s degree in Mathematics, Engineering, Computer Science, or a related discipline preferred
Prior experience within the insurance industry is preferred
Previous experience with Azure Synapse, Informatica Intelligent Cloud Services (IICS), and/or ETL technologies is preferred
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","SQL, Data Analysis, Data Warehousing, Business Analysis, Data Quality Assurance, DBMS Platforms, ETL Technologies, Azure Synapse, Informatica Intelligent Cloud Services (IICS), Data Integrity, Data Reconciliation, Data Load Failures, Data Retrieval Issues, Dashboard, Adhoc Extracts, Data Modeling, Change Management, Project Management","sql, data analysis, data warehousing, business analysis, data quality assurance, dbms platforms, etl technologies, azure synapse, informatica intelligent cloud services iics, data integrity, data reconciliation, data load failures, data retrieval issues, dashboard, adhoc extracts, data modeling, change management, project management","adhoc extracts, azure synapse, business analysis, change management, dashboard, data integrity, data load failures, data quality assurance, data reconciliation, data retrieval issues, dataanalytics, datamodeling, datawarehouse, dbms platforms, etl technologies, informatica intelligent cloud services iics, project management, sql"
"Sr Data Analyst , Mount Laurel, New Jersey- Fulltime Opportunity",Futran Solutions,"Mt. Laurel, NJ",https://www.linkedin.com/jobs/view/sr-data-analyst-mount-laurel-new-jersey-fulltime-opportunity-at-futran-solutions-3596158212,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Job Title: Business Data Analyst
Location:
Mount Laurel, New Jersey Day 1 Onsite (hybrid)
Experience:
5+ years
Job Description
Reporting to the Senior Manager, BI & Reporting, and will be responsible for the development, design, and maintenance of complex and comprehensive BI reporting packages and dashboards and perform data/business analysis using multiple data sources to support the understanding of key stakeholders and project drivers. The Senior Analyst will also be responsible for multiple deliverables simultaneously, ensuring their timely delivery, the quality of the work and output, and the alignment of the deliverables to business/project objectives.
The Senior Analyst will be responsible for the following key accountabilities:
Communicate analysis and ideas clearly, concisely, and effectively with senior leadership and key stakeholders.
Liaise with key stakeholders to conduct effective discussions on project requirements, data specifications, prioritization, and timeline to ensure timely and accurate delivery.
Create and maintain detailed documentation on all projects and requests.
Produce specialized and highly interactive reporting packages, dashboards & scorecards to understand objectives and deliver real-time insights and commentary that can inform and shape project progress, objectives, and development and support critical business decision-making.
Lead the implementation of initiatives in support of overall business/project strategy.
Conduct analysis to provide deep insights and enhanced BI to inform senior leadership teams and key stakeholders.
Validate the quality and integrity of the data included in the reporting packages and dashboards.
Work with stakeholders to understand business objectives and develop value-added reporting and provide ad-hoc data extracts, reporting and dashboards.
Act as the primary interface with key stakeholders to bridge and translate project requirements to analytics and reporting needs.
Job Requirements
Undergraduate degree in a quantitative discipline is required (Computer Science, Math, etc.).
5+ years of advanced experience in BI Reporting and dashboard development, data analysis, and working with large volumes of data generated by multiple data sources.
Advanced and demonstrated experience working with Tableau and Power BI effectively leveraging it to support diverse reporting and BI functions is required.
Analytical thinker with the ability to manage the reporting requirements of multiple workstreams and projects and meet extremely tight timelines.
Ability to manage significant ambiguity and constantly changing circumstances and priorities in a very fast-paced environment with minimal supervision.
Collaborate with data owners to identify data elements to address reporting requirements.
Strong working knowledge of project management processes, procedures, tools and best practices.
Advanced skills with Office 365 (SharePoint, Excel, PowerPoint, etc.) and extensive experience with Confluence/JIRA and MS SQL, SSIS/SSRS and other related systems is a must.
highly developed organization and time management skills.
Experience with project governance, compliance documentation and reporting.
Demonstrated business acumen, turning concepts into realistic and meaningful recommendations and deliverables.
Ability to learn quickly, promote positive team environment, plan, and adapt to shifting priorities and uncertain outcomes while adhering to deadlines; ability to remain resilient under high pressure projects.
Advanced knowledge of Business Intelligence industry techniques.
Knowledge of broader enterprise reporting platforms and data sources and the ability to integrate multiple data sources is considered an asset.
May be expected to present to senior management and executives.
Identify automation opportunities and act on them.
Master of data extraction and manipulation techniques. Perform data driven analytical work with strong system and business knowledge.
Streamline reports and dashboards to achieve maximum optimization and automation.
Monitor trends to identify opportunities for process improvements, require action/attention, further investigation.
Strong analytically anchored problem-solving skills, with a solid background working with multiple data sources, conducting analysis, interpreting results, and preparing reports and presentations.
Ability to work independently or as part of a team.
Ability to function in a remote work environment
Show more
Show less","Business Intelligence, Tableau, Power BI, Project Management, Office 365, SharePoint, Excel, PowerPoint, Confluence, JIRA, MS SQL, SSIS, SSRS, Data Extraction, Data Manipulation, Data Analysis, Reporting, Dashboards, Scorecards, Data Integrity, Data Quality, Data Visualization, DataDriven Analysis, Project Governance, Compliance Documentation, Business Acumen, Automation, Optimization, ProblemSolving, Data Interpretation, Presentation Skills, Remote Work","business intelligence, tableau, power bi, project management, office 365, sharepoint, excel, powerpoint, confluence, jira, ms sql, ssis, ssrs, data extraction, data manipulation, data analysis, reporting, dashboards, scorecards, data integrity, data quality, data visualization, datadriven analysis, project governance, compliance documentation, business acumen, automation, optimization, problemsolving, data interpretation, presentation skills, remote work","automation, business acumen, business intelligence, compliance documentation, confluence, dashboard, data extraction, data integrity, data interpretation, data manipulation, data quality, dataanalytics, datadriven analysis, excel, jira, ms sql, office 365, optimization, powerbi, powerpoint, presentation skills, problemsolving, project governance, project management, remote work, reporting, scorecards, sharepoint, ssis, ssrs, tableau, visualization"
Sr. Data Analyst,Community Behavioral Health,"Philadelphia, PA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-community-behavioral-health-3720308729,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Position Overview
The Sr. Data Analyst plays a lead role on the Data Analytics’ Data Warehouse team managing and maintaining the agency’s data warehouse systems, ensuring the maximum availability and integrity of its data, and planning for and executing its continuing enhancement.
Core Expectations
Community Behavioral Health (CBH) will hold each employee accountable for the following expectations which align with our mission, vision, and corporate code of conduct.
Perform key responsibilities as detailed in this job description in a dependable, responsible, and positive manner, consistent with all state and federal guidelines.
Serve as a role model by exemplifying professional behavior, language, skills, and attire in order to promptly and accurately serve the needs of stakeholders, members and their families.
Assure adherence to CBH policies and procedures so that all work is of the highest quality and delivered in the most culturally competent and cost-effective manner.
Promote and manage diversity and acceptance within CBH and with all members and stakeholders by honoring and respecting their individuality, dignity and rights.
Offer suggestions and develop solutions to help promote effective and efficient work processes and innovative programs.
Actively participate in required meetings and complete all mandatory trainings.
Maintain high levels of advocacy and member confidentiality to ensure the success of CBH and our mission.
Assist in assuring both internal and external program integrity by being alert to and reporting suspected instances of provider and employee fraud, waste or abuse.
Evaluate/learn tools to help inform which tools best suit solutions.
Essential Functions
Hands-on involvement in resolving any data warehouse ETL data or system related issues.
Work with a team in a lead role capacity developing new data warehouse solutions and updating current data warehouse functions that are brought about by new and changing functionalities of the different data source applications.
Collaborate with stakeholders to understand, define, and document business questions and needs (metrics, dimensions, charts, and dashboards).
Create and collect feedback on data visualizations (reports/dashboards) to bring insight and answers to business problems/KPIs.
Conceptualize and design data models for new business requirements.
Develop and maintain data exchange solutions with CBH’s business partners.
Master existing data warehouse databases and associated ETLs, queries, functions, and procedures.
Champion a standard development and coding process for the data warehouse team
Position Requirements
Education: bachelor’s degree in business, computer science, statistics, mathematics or related field.
License/Certification: N/A
Relevant Work Experience: Minimum of 5 years’ experience as a data warehouse developer or similar role with a strong working understanding of the Microsoft SQL Server technology.
5+ years’ experience with ETL, Data Modeling, Data Warehousing and SSAS preferred.
5+ years’ experience with contemporary data visualization technology (i.e.: Power BI.) preferred.
5+ years’ experience with Excel preferred.
Healthcare experience is preferred.
Skills
Capability and commitment to fully resolve data, business method and technology issues affecting all ETL and data exchange processes.
Ability to tell stories using data & visualizations.
Plan, design and build ETL/ELT solutions for the Data Warehouse in a SQL Server environment.
Knowledge of modern trends, tools and approaches to collection, reporting and visualization.
Additional Skills Required
Excellent written and oral communication skills
Strong interpersonal skills with a customer service orientation
Ability to communicate effectively between subject matter experts (SME) and developers to ensure accurate translation of business needs to implementation efforts.
Solid presentation skills for both technical and non-technical audiences
Excellent training and mentoring skills
Ability to work independently and in a team environment.
Ability to work under tight project deadlines.
Ability to plan, schedule and organize multiple priorities and high volume of work.
Core Competencies
Teamwork and Collaboration: Build and sustain relationships with co-workers and stakeholders and support efforts and deliverables. Encourage unity and help remove barriers to productivity and success.
Respect: Treat each other and our members and stakeholders with respect and sensitivity, recognizing the importance of diversity.
Member Centric: Focus on the needs of our members by providing value-added services, promoting strong relationships and going beyond basic expectations to achieve the best possible outcomes.
Honesty and Integrity: Be open and honest in all we do. Maintain the highest level of integrity at all times.
Commitment to Service Excellence: Challenge ourselves to be forward-thinking and committed to providing total member and stakeholder satisfaction, first-in-class service and high quality, innovative programs.
Compassion and Empathy: Demonstrate a deep appreciation for another’s situation and point of view. Pay attention to emotional cues, listen effectively and show an exceptional level of caring about each person’s perspectives and circumstances.
Problem Solving: Build and implement logical solutions to resolving challenges/issues by using individual knowledge and experience while taking resources, constraints and CBH values into consideration.
Initiative and Self-Directed: Proactively manage time and resources in a way that ensures that all work is done in the most efficient manner while identifying and implementing initiatives without interventions from co-workers, supervisors or stakeholders.
Flexibility and Adaptability: Adjust approaches and behaviors in order to meet the constantly changing environment head-on and accomplish CBH and individual goals. Strive to adapt to, accept and embrace change within areas of responsibility.
Communication: Effectively and appropriately share thoughts, ideas and information - both written and oral. Respectfully listen to co-workers and stakeholders to gain a full understanding of issues/situations.
Covid-19 Vaccine Requirement
The safety and well-being of our candidates, our people, and their families continues to be a top priority. Subject to applicable law, please be aware that CBH requires all employees to be fully vaccinated as a condition of employment.
Equal Employment Opportunity
We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CBH is an equal opportunity employer. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on all qualified individuals. This is without regard to race, ethnicity, creed, color, religion, national origin, age, sex/gender, marital status, gender identity, sexual orientation, gender identity or expression, disability, protected veteran status, genetic information or any other characteristic protected individual genetic information, or non-disqualifying physical or mental handicap or disability in each aspect of the human resources function by applicable federal, state, or local law.
Requesting An Accommodation
CBH is committed to providing equal employment opportunities for individuals with disabilities or religious observance, including reasonable accommodation when needed. If you are hired by CBH and require an accommodation to perform the essential functions of your role, you will be asked to participate in our accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodation once hired.
If you would like to be considered for employment opportunities with CBH and have accommodation needs for a disability or religious observance, please send us an email at CBH.Recruitment@Phila.gov
Show more
Show less","Data Analytics, Data Warehouse, Microsoft SQL Server, ETL, Data Modeling, Data Warehousing, Data Visualization, SSAS, Power BI, Excel, Healthcare, Communication, Collaboration, Problem Solving, Teamwork, Flexibility, Adaptability, Communication","data analytics, data warehouse, microsoft sql server, etl, data modeling, data warehousing, data visualization, ssas, power bi, excel, healthcare, communication, collaboration, problem solving, teamwork, flexibility, adaptability, communication","adaptability, collaboration, communication, dataanalytics, datamodeling, datawarehouse, etl, excel, flexibility, healthcare, microsoft sql server, powerbi, problem solving, ssas, teamwork, visualization"
"Senior Analyst, Real World Data (RWD) Insights",HealthVerity,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-analyst-real-world-data-rwd-insights-at-healthverity-3779253588,2023-12-17,Winslow,United States,Mid senior,Hybrid,"How you will help
As a Senior Analyst, Real World Data (RWD) Insights you will provide exceptional healthcare data analytics expertise for clients and colleagues in support of sales opportunities to license data. You will conduct discovery and translate clients’ research and business questions into actionable analyses to demonstrate data feasibility by leveraging the largest healthcare data ecosystem in the US. In collaboration with colleagues, you will identify and educate clients on the data suppliers in the HealthVerity Marketplace (HVM) that capture the necessary data elements to conduct RWD-based studies and produce regulatory-grade real world evidence (RWE).
What you will do
Efficiently query multiple data types (medical and pharmacy claims, EMR, lab, chargemaster) using SQL to identify populations of interest in HVM data and assess using univariate analysis and data investigations
Empower clients to generate RWE utilizing best-in-class observational research by conducting pre-sale feasibility analyses of varying breadth and depth
Provide clients with RWD training, analytic guidance, and use case support in the post-sale phase
Develop and communicate technical, clinical, operational, and business specifications to internal and external teams
Work cross-functionally to support operational processes to deliver data licensing projects on time and with accuracy
Contribute to the development and maintenance of internal documentation, code templates, analytics automation, and other process improvement initiatives to support internal team efficiency, effectiveness, and growth
How success will be defined
Creatively and strategically position HealthVerity to win against the competition by building trust and credibility as a subject matter expert (SME)
Take ownership of pre- and post-sale projects and drive to completion
Dedicate 5-10% of working hours to team and individual improvement
Ensure minimal errors in presales feasibility and data requirements for delivery
Required Skills And Experience
Graduate degree in Epidemiology, Biostatistics, Clinical Informatics, or related quantitative field
At least 3 years experience in a consultative, client-facing role
At least 3 years experience using SQL, programming against large relational databases leveraging interoperably-linked, patient-level data at scale
Healthcare data expert across various data types (e.g. open/closed claims, inpatient/ambulatory EMR, commercial labs, social determinants, etc.) and codified healthcare data standards (e.g. ICD, CPT, HCPCS, CVX, LOINC, NUCC, NPPES, etc.)
Experience evaluating fit-for-purpose data and implementing research protocols
Experienced applying RWD to specific healthcare and life sciences-related research questions and use cases, such as RWE/epidemiology, HEOR, R&D, commercial, public health
Desired Skills And Experience
Exposure to a wide range of healthcare industry business segments (pharma, biotech, government, consulting, healthcare payer/provider)
Evangelist and champion of RWD/RWE
Skilled at educating, presenting, and demonstrating analytical concepts to internal and external audiences
Strong communicator (both verbally and in writing) with the ability to translate client needs into actionable data insights
Experience supporting business development activities
Experience owning and managing projects with the ability to identify and mitigate risk to intended outcomes
Team player who takes initiative and works collaboratively with colleagues across all levels of the organization
Highly organized, consultative, and proactive critical thinker/problem solver
Committed to personal and professional continuous development
Exceptional attention to detail
Experience coding in Python
Thrives in a rapidly-changing, fast paced environment
Base salary for the role is commensurate with experience and can range between $80,000 - 130,000 + annual bonus opportunity.
Hiring Locations
While HealthVerity does support remote work with quarterly travel to our Philadelphia headquarters, our strong preference is to hire team members in the areas below as well as approved states in the Eastern Time Zone. Expansion beyond these markets will occur only when necessary.
Boston, Massachusetts
New York City, New York
Philadelphia, Pennsylvania
Baltimore, Maryland
Washington D.C
Charlotte, North Carolina
Raleigh-Durham, North Carolina
Atlanta, Georgia
Approved States in the Eastern Time Zone include: CT, DE, FL, GA, IN, MA, MD, MI NC, NJ, NY, OH, PA, RI, TN, and VA.
About HealthVerity
HealthVerity synchronizes transformational technologies with the nation’s largest healthcare and consumer data ecosystem to power previously unattainable outcomes and fundamentally advance the science. We offer a comprehensive, yet flexible approach, based on the foundational elements of Identity, Privacy, Governance and Exchange (IPGE), that synchronizes unparalleled Identity management with built-in Privacy compliance and Governance, providing the ability to discover and Exchange a near limitless combination of data at a record pace. Together with our partners in life sciences, government and insurance, we are Synchronizing the Science. To learn more about HealthVerity, visit healthverity.com .
Why you'll love working here
We are making a difference
– Our technology is at the forefront of some of the biggest healthcare challenges in the world.
We are one team
– Our people define our culture and always will. We take time out to celebrate each other at the end of every week through company-wide shout outs, and acknowledge the value that each of us adds towards our greater mission. Come share all you have to offer.
We are learners
– Every team member is continually learning, no matter if we've been in a role for one year or much longer. We are committed to learning and implementing what is best for our clients, partners, and each other.
Benefits & Perks
Compensation: competitive base salary & annual bonus opportunity (for non-commissioned roles)
Benefits: comprehensive benefits with coverage on Day 1, medical, dental, vision, 401k, stock options
Flexible location: our HQ is in Philadelphia. We offer both hybrid roles and those with quarterly travel.
Generous PTO: Take time off as needed, targeted at 4 weeks per year, including vacation, personal and sick time, plus paid maternity and paternity leave.
Comprehensive and individualized onboarding: mentorship program, departmental talks, and a library of resources are available beginning day 1 for each new team member to minimize the stress of starting a new job
Professional development: biweekly 1:1s, hands-on leadership that is goal-and growth-oriented for each team member, and an annual budget to support professional development pursuits
HealthVerity is an equal opportunity employer devoted to inclusion in the workplace. We believe incorporating different ideas, perspectives and backgrounds make us stronger and encourages an environment where ageism, racism, sexism, ableism, homophobia, transphobia or any other form of discrimination are not tolerated. All qualified job applicants will be given consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability. At HealthVerity, we’re working towards an innovative and connected future for healthcare data and believe the future is better together. We can only do that if everyone has a seat at the table. Read our Equity Inclusion and Diversity Statement .
If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to careers@healthverity.com
Remote opportunities are not available in all areas and require team members to work from a fixed location due to tax and labor law implications - specific questions about remote positions can be discussed during the interview process with your recruiter.
Show more
Show less","SQL, Epidemiology, Biostatistics, Clinical Informatics, Healthcare data, RWD, RWE, HEOR, Pharma, Biotech, Government, Consulting, Healthcare payer, Healthcare provider, Python, Statistics, Data analysis, Patientlevel data, Healthcare industry, Data standards, Research protocols, Life sciences, Public health, Business development, Project management, Critical thinking, Problem solving, Communication, Presentation, Teamwork, Collaboration, Initiative, Organization, Proactivity, Attention to detail","sql, epidemiology, biostatistics, clinical informatics, healthcare data, rwd, rwe, heor, pharma, biotech, government, consulting, healthcare payer, healthcare provider, python, statistics, data analysis, patientlevel data, healthcare industry, data standards, research protocols, life sciences, public health, business development, project management, critical thinking, problem solving, communication, presentation, teamwork, collaboration, initiative, organization, proactivity, attention to detail","attention to detail, biostatistics, biotech, business development, clinical informatics, collaboration, communication, consulting, critical thinking, data standards, dataanalytics, epidemiology, government, healthcare data, healthcare industry, healthcare payer, healthcare provider, heor, initiative, life sciences, organization, patientlevel data, pharma, presentation, proactivity, problem solving, project management, public health, python, research protocols, rwd, rwe, sql, statistics, teamwork"
"Research Scientist, Real World Data (RWD) Insights",HealthVerity,"Philadelphia, PA",https://www.linkedin.com/jobs/view/research-scientist-real-world-data-rwd-insights-at-healthverity-3784854904,2023-12-17,Winslow,United States,Mid senior,Hybrid,"How you will help
As a Research Scientist, Real World Data (RWD) Insights you will provide exceptional healthcare data analytics expertise for clients and colleagues in support of sales opportunities to license data. You will conduct discovery and translate clients’ research and business questions into actionable analyses to demonstrate data feasibility by leveraging the largest healthcare data ecosystem in the US. In collaboration with colleagues, you will identify and educate clients on the data suppliers in the HealthVerity Marketplace (HVM) that capture the necessary data elements to conduct RWD-based studies and produce regulatory-grade real world evidence (RWE).
What you will do
Efficiently query multiple data types (medical and pharmacy claims, EMR, lab, chargemaster) using SQL to identify populations of interest in HVM data and assess using univariate analysis and data investigations
Empower clients to generate RWE utilizing best-in-class observational research by conducting pre-sale feasibility analyses of varying breadth and depth
Provide clients with RWD training, analytic guidance, and use case support in the post-sale phase
Develop and communicate technical, clinical, operational, and business specifications to internal and external teams
Work cross-functionally to support operational processes to deliver data licensing projects on time and with accuracy
Contribute to the development and maintenance of internal documentation, code templates, analytics automation, and other process improvement initiatives to support internal team efficiency, effectiveness, and growth
How success will be defined
Creatively and strategically position HealthVerity to win against the competition by building trust and credibility as a subject matter expert (SME)
Take ownership of pre- and post-sale projects and drive to completion
Dedicate 5-10% of working hours to team and individual improvement
Ensure minimal errors in presales feasibility and data requirements for delivery
Required Skills And Experience
Graduate degree in Epidemiology, Biostatistics, Clinical Informatics, or related quantitative field
At least 3 years experience in a consultative, client-facing role
At least 3 years experience using SQL, programming against large relational databases leveraging interoperably-linked, patient-level data at scale
Healthcare data expert across various data types (e.g. open/closed claims, inpatient/ambulatory EMR, commercial labs, social determinants, etc.) and codified healthcare data standards (e.g. ICD, CPT, HCPCS, CVX, LOINC, NUCC, NPPES, etc.)
Experience evaluating fit-for-purpose data and implementing research protocols
Experienced applying RWD to specific healthcare and life sciences-related research questions and use cases, such as RWE/epidemiology, HEOR, R&D, commercial, public health
Desired Skills And Experience
Exposure to a wide range of healthcare industry business segments (pharma, biotech, government, consulting, healthcare payer/provider)
Evangelist and champion of RWD/RWE
Skilled at educating, presenting, and demonstrating analytical concepts to internal and external audiences
Strong communicator (both verbally and in writing) with the ability to translate client needs into actionable data insights
Experience supporting business development activities
Experience owning and managing projects with the ability to identify and mitigate risk to intended outcomes
Team player who takes initiative and works collaboratively with colleagues across all levels of the organization
Highly organized, consultative, and proactive critical thinker/problem solver
Committed to personal and professional continuous development
Exceptional attention to detail
Experience coding in Python
Thrives in a rapidly-changing, fast paced environment
Base salary for the role is commensurate with experience and can range between $80,000 - 130,000 + annual bonus opportunity.
Hiring Locations
While HealthVerity does support remote work with quarterly travel to our Philadelphia headquarters, our strong preference is to hire team members in the areas below as well as approved states in the Eastern Time Zone. Expansion beyond these markets will occur only when necessary.
Boston, Massachusetts
New York City, New York
Philadelphia, Pennsylvania
Baltimore, Maryland
Washington D.C
Charlotte, North Carolina
Raleigh-Durham, North Carolina
Atlanta, Georgia
Approved States in the Eastern Time Zone include: CT, DE, FL, GA, IN, MA, MD, MI NC, NJ, NY, OH, PA, RI, TN, and VA.
About HealthVerity
HealthVerity synchronizes transformational technologies with the nation’s largest healthcare and consumer data ecosystem to power previously unattainable outcomes and fundamentally advance the science. We offer a comprehensive, yet flexible approach, based on the foundational elements of Identity, Privacy, Governance and Exchange (IPGE), that synchronizes unparalleled Identity management with built-in Privacy compliance and Governance, providing the ability to discover and Exchange a near limitless combination of data at a record pace. Together with our partners in life sciences, government and insurance, we are Synchronizing the Science. To learn more about HealthVerity, visit healthverity.com .
Why you'll love working here
We are making a difference
– Our technology is at the forefront of some of the biggest healthcare challenges in the world.
We are one team
– Our people define our culture and always will. We take time out to celebrate each other at the end of every week through company-wide shout outs, and acknowledge the value that each of us adds towards our greater mission. Come share all you have to offer.
We are learners
– Every team member is continually learning, no matter if we've been in a role for one year or much longer. We are committed to learning and implementing what is best for our clients, partners, and each other.
Benefits & Perks
Compensation: competitive base salary & annual bonus opportunity (for non-commissioned roles)
Benefits: comprehensive benefits with coverage on Day 1, medical, dental, vision, 401k, stock options
Flexible location: our HQ is in Philadelphia. We offer both hybrid roles and those with quarterly travel.
Generous PTO: Take time off as needed, targeted at 4 weeks per year, including vacation, personal and sick time, plus paid maternity and paternity leave.
Comprehensive and individualized onboarding: mentorship program, departmental talks, and a library of resources are available beginning day 1 for each new team member to minimize the stress of starting a new job
Professional development: biweekly 1:1s, hands-on leadership that is goal-and growth-oriented for each team member, and an annual budget to support professional development pursuits
HealthVerity is an equal opportunity employer devoted to inclusion in the workplace. We believe incorporating different ideas, perspectives and backgrounds make us stronger and encourages an environment where ageism, racism, sexism, ableism, homophobia, transphobia or any other form of discrimination are not tolerated. All qualified job applicants will be given consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability. At HealthVerity, we’re working towards an innovative and connected future for healthcare data and believe the future is better together. We can only do that if everyone has a seat at the table. Read our Equity Inclusion and Diversity Statement .
If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to careers@healthverity.com
Remote opportunities are not available in all areas and require team members to work from a fixed location due to tax and labor law implications - specific questions about remote positions can be discussed during the interview process with your recruiter.
Show more
Show less","SQL, Data Analytics, Programming, Epidemiology, Biostatistics, Clinical Informatics, Public Health, Healthcare Data, RWD, RWE, R&D, Machine Learning, Artificial Intelligence, Research Protocols, Data mining, Statistics, Python, SAS, Tableau, Power BI, Clinical Trials, Observational Studies, RealWorld Evidence, Health Economics and Outcomes Research, Market Access, Sales","sql, data analytics, programming, epidemiology, biostatistics, clinical informatics, public health, healthcare data, rwd, rwe, rd, machine learning, artificial intelligence, research protocols, data mining, statistics, python, sas, tableau, power bi, clinical trials, observational studies, realworld evidence, health economics and outcomes research, market access, sales","artificial intelligence, biostatistics, clinical informatics, clinical trials, data mining, dataanalytics, epidemiology, health economics and outcomes research, healthcare data, machine learning, market access, observational studies, powerbi, programming, public health, python, rd, realworld evidence, research protocols, rwd, rwe, sales, sas, sql, statistics, tableau"
Data and Analytics Consultant,SEI,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-and-analytics-consultant-at-sei-3321024350,2023-12-17,Winslow,United States,Mid senior,Hybrid,"Who We Look For
An SEI-er is a master communicator and active listener who understands how to navigate an audience. Self-aware, almost to a fault, SEI-ers keenly understand how to adjust their support and problem solving based on the situation. Following a logical, fact-based approach, SEI-ers possess the superior ability to see correlations others may not, ask the right questions and drive solutions.
As super-connectors, they connect not only people, but data, trends and experiences. Mature, humble, and genuine, SEI-ers frequently go above and beyond for both their clients and their colleagues. SEI-ers are ethical and trustworthy individuals who consistently and repeatedly follow through, and hold true to their values in difficult situations. SEI-ers have an insatiable curiosity and love to learn. These individuals are commonly tech savvy and early adopters. Their passion for learning is infectious and excites others. As every project is different, an SEI-er must be adaptable and comfortable with unexpected situations. SEI-ers define ambition differently. They are authentic, low-maintenance individuals who truly enjoy one another- they like to hang out with colleagues outside of work, collaborate and hold one another accountable. SEI-ers enjoy working with genuine, thoughtful folks who want to steer clear of the traditional grind and share the joy of day-to-day life and activities with colleagues, friends, and family.
What We Do
Our Data and Analytics consultants work with clients at all levels of the organization, from the C-suite to the shop floor, helping them to deliver on their most strategic initiatives. We’re known for making realistic, data-driven decisions that deliver value in tangible ways to our clients. Our clients ask for us on projects that require a superior combination of technical and business capabilities, people and management skills, and a collaborative mindset. We excel in understanding complex programs and strategic initiatives and breaking them into actionable pieces.
We are actively looking for professionals in the following areas:
Data Strategy
Data Governance
Data Modernization
Advanced Analytics
Data Visualization
The ideal candidate’s experience may include but is not limited to the following:
Have experience understanding and solving real business problems
Have experience with presenting business case and strategy to the C-Suite
Bring industry domain knowledge for custom data and analytics strategies
Experience in designing and implementing data driven products and services
Experience in designing and building high performance collaborative business and technology teams that deliver high value data solutions, data products and data analytics
Ideal candidates may call themselves Data Architects, Data Engineers, Data Scientists, Analysts and Data Governance professionals. Experience may include but not limited to the following:
Data Strategy and Governance
Intimately familiar with data technologies, data architecture frameworks, cloud platforms and the emerging trends in technological advancements.
Proven experience in transforming our clients from on-prem data platforms to the cloud.
Lead data modernization initiatives that include platform, technology and tools implementation and adoption.
Must have experience in architecting and implementing data architecture, data engineering, reporting and analytical solutions across multiple business functions and domains.
Conduct data management maturity assessments and identify pain points for including data quality, governance, architecture, analytics, metadata management, master data management.
Demonstrate a rigorous and analytical problem-solving approach, leading information-gathering sessions and preparing and delivering client presentations and work products/prototypes.
Identify data governance gaps and assess data governance maturity in a complex data environment. Should also be able to Identify the implications for data governance that arise from the technology, current processes, and skill levels in a complex data environment.
Data Architecture and Engineering
Extensive experience or knowledge of cloud-based data platform technologies such as AWS, Microsoft Azure or Google Cloud Computing.
Experience designing and building data platforms that support data lakes, data stores, data ingestion and transformations and API services for consuming data
Ability to design and build complex data pipelines that either stream data in
Experience and knowledge of programming and scripting languages, such as but not limited to Python, Node.JS and SQL
Experience and knowledge of data integration tools and platforms
Experience and knowledge of relational and dimensional database structures, theories, principles, and practice used in data warehousing and analytics solutions
Ability to design and build data models using the appropriate technics to meet business objectives
In depth knowledge of modern data technologies including Snowflake, RedShift, Azure SQL/Synaspe, Databricks or similar technologies.
Advanced Analytics and Data Visualizations
Extensive experience or knowledge of data visualization technologies such as Tableau, Power BI, Qlik and Spotfire
Experience with statistical and mathematical modeling, artificial intelligence and machine learning software and methods
Experience and knowledge of programming and scripting languages such as, but not limited to, SQL, Python and R
Experience with data modeling, data prep and machine learning tools like Alteryx, RapidMinder, RStudio and Tableau Prep
Experience and knowledge of relational and dimensional database structures, theories, principles, and practice used in data warehousing and analytics solutions
Experience deploying enterprise-wide reporting solutions while leveraging data visualization best practices
A career at SEI extends well beyond providing great service and thought leadership to our clients. Everyone takes an active role in building and managing our business, in an environment that runs counter to traditional consulting firms. Our consultants have a “seat at the table” and contribute to growing our business in ways that align to their interests such as growing business development opportunities, conducting interviews to support our hiring process, managing internal initiatives that build our brand or organizing trainings to share what you know with your colleagues. There is no telling what an SEI Consultant will be asked to do on a day-to-day basis – we do what it takes to get the job done.
Qualifications
Required-
Alignment to our core values: Excellence, Participation, Integrity and Collaboration
Hungry, Humble, Smart
Demonstrated business and technology acumen
Strong written and verbal communication skills
Understanding and experience solving real business problems
Proven track record of delivering results
Experience working with and/or leading a team
Ability to work across industries, roles, functions & technologies
Authorization for permanent employment in the United States (this position is not eligible for immigration sponsorship)
Preferred-
Bachelor’s degree
8+ years professional experience
Experience across our service offerings
Systems Evolution, Inc. (SEI) is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law
Show more
Show less","Data Strategy, Data Governance, Data Modernization, Advanced Analytics, Data Visualization, Data Architect, Data Engineer, Data Scientist, Analyst, Data Governance professional, Data architecture frameworks, Cloud platforms, AWS, Microsoft Azure, Google Cloud Computing, Python, Node.JS, SQL, Data integration tools, Relational database, Dimensional database, Snowflake, RedShift, Azure SQL/Synaspe, Databricks, Tableau, Power BI, Qlik, Spotfire, Statistical modeling, Mathematical modeling, Artificial intelligence, Machine learning, R, Alteryx, RapidMinder, RStudio, Tableau Prep, Data visualization best practices","data strategy, data governance, data modernization, advanced analytics, data visualization, data architect, data engineer, data scientist, analyst, data governance professional, data architecture frameworks, cloud platforms, aws, microsoft azure, google cloud computing, python, nodejs, sql, data integration tools, relational database, dimensional database, snowflake, redshift, azure sqlsynaspe, databricks, tableau, power bi, qlik, spotfire, statistical modeling, mathematical modeling, artificial intelligence, machine learning, r, alteryx, rapidminder, rstudio, tableau prep, data visualization best practices","advanced analytics, alteryx, analyst, artificial intelligence, aws, azure sqlsynaspe, cloud platforms, data architect, data architecture frameworks, data governance, data governance professional, data integration tools, data modernization, data scientist, data strategy, data visualization best practices, databricks, dataengineering, dimensional database, google cloud computing, machine learning, mathematical modeling, microsoft azure, nodejs, powerbi, python, qlik, r, rapidminder, redshift, relational database, rstudio, snowflake, spotfire, sql, statistical modeling, tableau, tableau prep, visualization"
Remote - Senior Application Engineer (Data Simulation) - PLM - Direct Client - Quick Interview -,TechFetch.com - On Demand Tech Workforce hiring platform,"Canonsburg, PA",https://www.linkedin.com/jobs/view/remote-senior-application-engineer-data-simulation-plm-direct-client-quick-interview-at-techfetch-com-on-demand-tech-workforce-hiring-platform-3786878616,2023-12-17,Steubenville,United States,Mid senior,Remote,"""ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly."" About Outcome Logix:Outcome Logix is tech-enabled technology services company, We are Tech 50 Award Finalist company by Pittsburhg Tech Council.About our Client : Our client is Headquartered in Pittsburgh, Pennsylvania, Our client operates globally with offices and support centers in various countries. Its reach allows it to serve a diverse clientele with varying engineering needs.Role : Senior Application Engineer (Data Simulation) - PLMKey Responsibilities:Utilize expertise in Data Simulation within the context of PLM to address customer needs.Engage in customer-facing roles, providing application engineering and consulting services.Demonstrate a comprehensive understanding of enterprise-class product development systems, including but not limited to SLM, SPDM, ERP, ALM, TDM, MIM/IMM, PDM, and PLM.Proficient in utilizing specific PLM tools such as Aras Innovator, Siemens Teamcenter, Dassault ENOVIA or 3DEXPERIENCE, MSc SimManager, and MSc MaterialCenter.Collaborate with cross-functional teams to deliver effective solutions aligned with customer requirements.Support Ansys field and digital marketing, author conference presentations Contribute to consulting services,Skills and Qualifications:Bachelor's degree in Engineering or a related field.Minimum of 5 years of experience in application engineering or consulting services.Proven expertise in Data Simulation, with a focus on PLM.Strong understanding of enterprise-class product development systems.Familiarity with PLM tools such as Aras Innovator, Siemens Teamcenter, Dassault ENOVIA or 3DEXPERIENCE, MSc SimManager, and MSc MaterialCenter.Excellent communication and interpersonal skills for effective customer interactions.Ability to work collaboratively in a team and individually.Good understanding of enterprise-class product development systems like SLM, SPDM, ERP, ALM, TDM, MIM/IMM, PDM, PLM (e.g. Aras Innovator, Siemens Teamcenter, Dassault ENOVIA or 3DEXPERIENCE, MSc SimManager, MSc MaterialCenter)Demonstrated use of relevant Ansys software or knowledge of other commercial CAE, CAD, EDA, and PLM software packagesCollaborate with the Ansys product development teams to translate customer requirements into exciting new product features; test new releases of Ansys products on industrial problems, develop application best practices.Ability to travel domestically up to 25% of the time
Show more
Show less","Data Simulation, PLM, Aras Innovator, Siemens Teamcenter, Dassault ENOVIA or 3DEXPERIENCE, MSc SimManager, MSc MaterialCenter, Engineering, Ansys software, CAD, EDA","data simulation, plm, aras innovator, siemens teamcenter, dassault enovia or 3dexperience, msc simmanager, msc materialcenter, engineering, ansys software, cad, eda","ansys software, aras innovator, cad, dassault enovia or 3dexperience, data simulation, eda, engineering, msc materialcenter, msc simmanager, plm, siemens teamcenter"
"Data Analyst in Pittsburgh, PA location",Avani Tech Solutions Private Limited,"Myrtle Beach, SC",https://www.linkedin.com/jobs/view/data-analyst-in-pittsburgh-pa-location-at-avani-tech-solutions-private-limited-3752006657,2023-12-17,South Carolina,United States,Associate,Onsite,"Indotronix is seeking a Data Analyst
in Pittsburgh, PA location
Position:
Data Analyst
Location:
Pittsburgh, PA
Duration:
Contract to hire
OT:
potential for 5 hours of overtime per week
Candidates Background
IT risk
IT audit
IT security
Candidate Technical And Skills Profile
Collecting data from various source systems (Oracle Business Intelligence, Tableau, etc.)
Utilizing standard desktop applications to structure the collected data for analysis
Based on established criteria, find items within the analysis that need to be remediated
Meeting with Business and/or Technical support teams to discuss items found in analysis
Must Have
Must have technical skills, tools, or experience:
Advanced knowledge of Microsoft Office and other desktop applications as needed
Familiarity with Tableau (not developer, just user or advanced user)
Familiarity with Oracle Business Intelligence (OBI) reporting (not developer, just user or advanced user)
Familiarity with ServiceNow (not developer, just user or advanced user)
Familiarity with JIRA (not developer, just user or advanced user)
Nice To Have
The ability to work independently
Understanding of LDAP (Oracle Unified Directory & Active Directory)
Understanding of server operating systems (Windows, Linux)
Understanding of Database technologies (Oracle, MS-SQL server, etc.)
Years Of Overall Experience
5 - 7 years' overall experience
Interview Process
1 step video interview
Indotronix Commitment
: A Safe and Inclusive Workplace"" – Promoting a Culture of Inclusion, Respect, Equality, and Diversity: Ensuring Safety and Non-Discrimination.
We actively strive to attract, retain, and empower a diverse range of talented individuals, recognizing that diverse perspectives and experiences enhance our collective performance.
Breaking Barriers
: Your Potential Knows No Limits. Embrace Your Potential, Apply Today!""
Celebrating & Honouring Veteran Contributions:
Approximately 13% of our workforce are veterans (nearly twice the national average). This achievement underscores our deep commitment in fostering Opportunities for success to Civilian Careers
Recognition
- Indotronix has been recognized as one of the largest staffing companies in 2023 by Staffing Industry Analysts – a testament to our continued growth, commitment to excellence, and the trust our clients and candidates place in us.
Compliance
Indotronix upholds good corpo*** citizenship by complying with all applicable laws, including taxation, equal employment opportunity, statutory benefits, and data reporting. In 2022, we hired over 2,000 U.S.-based employees as consultants, contributing to workforce expansion and client service excellence.
Show more
Show less","Data Analysis, Tableau, Oracle Business Intelligence, ServiceNow, JIRA, Microsoft Office, LDAP, Server operating systems, Database technologies","data analysis, tableau, oracle business intelligence, servicenow, jira, microsoft office, ldap, server operating systems, database technologies","dataanalytics, database technologies, jira, ldap, microsoft office, oracle business intelligence, server operating systems, servicenow, tableau"
Supply Chain Data Analyst,MUSC College of Health Professions,"Charleston, SC",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-at-musc-college-of-health-professions-3782602031,2023-12-17,South Carolina,United States,Mid senior,Hybrid,"Job Description Summary
The Data Analyst I reports to the Manager of Analytics in support of MUSC’s academic, research and healthcare missions. Under direct supervision, the Data Analyst I provides a variety of operational, consultative and collaborative functions within Information Solutions. This position assists with the implementation and maintenance of the analytics program supporting decision making, strategy, performance improvement, and other key operational goals through valid, relevant, and quality decision support reports, dashboards, and other analytic tools. Provides proactive and reactive support while maintaining a professional attitude and exhibiting good customer service skills. Maintains professional standards and exhibits excellent customer service skills while performing assigned tasks.
Entity
Medical University Hospital Authority (MUHA)
Worker Type
Employee
Worker Sub-Type
Regular
Cost Center
CC002366 SYS - Strategic Sourcing
Pay Rate Type
Salary
Pay Grade
Health-26
Scheduled Weekly Hours
40
Work Shift
Job Description
The Data Analyst I reports to the Manager of Analytics in support of MUSC’s academic, research and healthcare missions. Under direct supervision, the Data Analyst I provides a variety of operational, consultative and collaborative functions within Information Solutions. This position assists with the implementation and maintenance of the analytics program supporting decision making, strategy, performance improvement, and other key operational goals through valid, relevant, and quality decision support reports, dashboards, and other analytic tools. Provides proactive and reactive support while maintaining a professional attitude and exhibiting good customer service skills. Maintains professional standards and exhibits excellent customer service skills while performing assigned tasks.
Additional Job Description
Education: Bachelors Degree or equivalent Work Experience: 5 years for support or 0-2 years for professional
If you like working with energetic enthusiastic individuals, you will enjoy your career with us!
The Medical University of South Carolina is an Equal Opportunity Employer. MUSC does not discriminate on the basis of race, color, religion or belief, age, sex, national origin, gender identity, sexual orientation, disability, protected veteran status, family or parental status, or any other status protected by state laws and/or federal regulations. All qualified applicants are encouraged to apply and will receive consideration for employment based upon applicable qualifications, merit and business need.
Medical University of South Carolina participates in the federal E-Verify program to confirm the identity and employment authorization of all newly hired employees. For further information about the E-Verify program, please click here: http://www.uscis.gov/e-verify/employees
Show more
Show less","Data Analysis, Decision Making, Data Reporting, Data Visualization, Analytical Tools, Customer Service Skills, Strategic Decision Support, Performance Improvement","data analysis, decision making, data reporting, data visualization, analytical tools, customer service skills, strategic decision support, performance improvement","analytical tools, customer service skills, data reporting, dataanalytics, decision making, performance improvement, strategic decision support, visualization"
Supply Chain Data Analyst,MUSC Health,"Charleston, South Carolina Metropolitan Area",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-at-musc-health-3780794712,2023-12-17,South Carolina,United States,Mid senior,Hybrid,"Job Description Summary
The Data Analyst I reports to the Manager of Analytics in support of MUSC’s academic, research and healthcare missions. Under direct supervision, the Data Analyst I provides a variety of operational, consultative and collaborative functions within Information Solutions. This position assists with the implementation and maintenance of the analytics program supporting decision making, strategy, performance improvement, and other key operational goals through valid, relevant, and quality decision support reports, dashboards, and other analytic tools. Provides proactive and reactive support while maintaining a professional attitude and exhibiting good customer service skills. Maintains professional standards and exhibits excellent customer service skills while performing assigned tasks.
Entity
Medical University Hospital Authority (MUHA)
Worker Type
Employee
Worker Sub-Type
Regular
Cost Center
CC002366 SYS - Strategic Sourcing
Pay Rate Type
Salary
Pay Grade
Health-26
Scheduled Weekly Hours
40
Work Shift
Job Description
The Data Analyst I reports to the Manager of Analytics in support of MUSC’s academic, research and healthcare missions. Under direct supervision, the Data Analyst I provides a variety of operational, consultative and collaborative functions within Information Solutions. This position assists with the implementation and maintenance of the analytics program supporting decision making, strategy, performance improvement, and other key operational goals through valid, relevant, and quality decision support reports, dashboards, and other analytic tools. Provides proactive and reactive support while maintaining a professional attitude and exhibiting good customer service skills. Maintains professional standards and exhibits excellent customer service skills while performing assigned tasks.
Additional Job Description
Education: Bachelors Degree or equivalent Work Experience: 5 years for support or 0-2 years for professional
If you like working with energetic enthusiastic individuals, you will enjoy your career with us!
The Medical University of South Carolina is an Equal Opportunity Employer. MUSC does not discriminate on the basis of race, color, religion or belief, age, sex, national origin, gender identity, sexual orientation, disability, protected veteran status, family or parental status, or any other status protected by state laws and/or federal regulations. All qualified applicants are encouraged to apply and will receive consideration for employment based upon applicable qualifications, merit and business need.
Medical University of South Carolina participates in the federal E-Verify program to confirm the identity and employment authorization of all newly hired employees. For further information about the E-Verify program, please click here: http://www.uscis.gov/e-verify/employees
Show more
Show less","Data Analysis, Decision Making, Data Reporting, Dashboarding, Customer Service, Data Management, Analytics Program Maintenance","data analysis, decision making, data reporting, dashboarding, customer service, data management, analytics program maintenance","analytics program maintenance, customer service, dashboard, data management, data reporting, dataanalytics, decision making"
Data Engineering Lead,Epiq,Utica-Rome Area,https://www.linkedin.com/jobs/view/data-engineering-lead-at-epiq-3783851731,2023-12-17,Herkimer,United States,Mid senior,Remote,"It's fun to work at a company where people truly believe in what they are doing!
Job Description:
About Fireman & Company
Fireman & Company is the leading knowledge management (KM) consultancy in the legal industry. We partner with our clients to make substantial, distinct, and lasting improvements in performance through the effective use of KM resources and disciplines.
Fireman & Company is structured around several practice areas. These practice areas can work independently or together, as required by our clients. Our advisory services deliver trusted advice that helps our clients establish long-term strategies, and execute against near-term priorities. Our design and implementation services allow our clients to deploy practical solutions that lawyers and staff embrace and use. We deliver these services across the full range of KM core competencies including data management, document management, intranets, enterprise search, experience management, artificial intelligence, and additional KM advisory services.
Key Responsibilities
The Data Engineering Lead is expected to work on their own initiative, under the direction of senior leadership and practice leads. The Engineer is measured on their ability to demonstrate experience in the following areas.
Data Engineering
Pull data from SQL, NoSQL and API based data sources using MongoDB or similar.
Clean and enrich data with SQL and Python scripting.
Carry out data hygiene activities to shed light on client data (including data cleansing and enrichment) using SQL and Python scripting.
Create clear recommendations for remediation of data structure and cleanliness, which could impact the overall Enterprise Search experience.
Work with our data practice in creating a law firm data model that could be used across other practices.
Software Engineering
Experience building a modern web UI.
Experience with SOLID Object-Oriented Programming.
Experience with Functional Programming.
Building relationships
Provide direction, motivation, training, and decision support to other Engineers and consultants (internal and client where appropriate).
Recognize and manage staff diversity, performance, and or personality conflicts to work as part of an effective team(s).
Develop a position of trust and expertise by delivering and sharing knowledge.
Continuous improvement
Leverage critical thinking and new tools to enhance project and development quality.
Propose or refine development standards, processes, and methodologies, focusing on streamlining administrative overhead and adding value to our clients and Fireman & Company.
Attributes for success
The Engineer will be able to demonstrate the following attributes:
A strong team player, comfortable working collaboratively, virtually, independently, and remotely.
Excellent communication and networking skills, with the ability to effectively manage and influence a broad range of client and internal stakeholders.
Ability to work under pressure and to tight deadlines, managing multiple projects/priorities effectively and efficiently.
Ability to solve complex technical problems that may require significant analysis to identify the root cause.
Experience
The Engineer will be able to demonstrate real-world experience in the following areas:
Essential
Over 5 years of programming experience.
Delivering multiple engagements through the full life cycle of a project (discovery, design, build, QA, launch, and support) in a technical capacity.
Working on multiple projects concurrently within client-facing professional services roles.
Working as part of a matrixed, remote team.
Strong SQL Skills.
Experience with Java or another object-oriented language.
Experience with Python scripting.
Experience with Web Programming (HTML/CSS/Javascript).
Experience with Git-based source control and Github.
Desirable
Experience with search technologies such as SOLR or Elasticsearch.
Experience with REACT.
Previous experience using SharePoint and PowerBI.
Experience working within the legal sector.
Experience of developing and/or supporting Enterprise Search within a law firm is highly desirable.
Experience of implementing legal systems (eg Handshake, Recommind, iManage, NetDocuments, Intapp, BA Insight).
Candidates based in California, Colorado, Hawaii, New York or Washington:
The Compensation range for this role is 89,245.80 - 178,491.60 USD annually and may be eligible for an annual bonus. Actual compensation within that range will be dependent upon the individual's location, skills, experience and qualifications.
Click Here To Learn About Epiq's Benefits.
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!
It is Epiq’s policy to comply with all applicable equal employment opportunity laws by making all employment decisions without unlawful regard or consideration of any individual’s race, religion, ethnicity, color, sex, sexual orientation, gender identity or expressions, transgender status, sexual and other reproductive health decisions, marital status, age, national origin, genetic information, ancestry, citizenship, physical or mental disability, veteran or family status or any other basis protected by applicable national, federal, state, provincial or local law. Epiq’s policy prohibits unlawful discrimination based on any of these impermissible bases, as well as any bases or grounds protected by applicable law in each jurisdiction. In addition Epiq will take affirmative action for minorities, women, covered veterans and individuals with disabilities. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. Epiq is pleased to provide such assistance and no applicant will be penalized as a result of such a request. Pursuant to relevant law, where applicable, Epiq will consider for employment qualified applicants with arrest and conviction records.
Show more
Show less","Data Engineering, SQL, NoSQL, API, MongoDB, Python, Software Engineering, Web UI, ObjectOriented Programming, Functional Programming, HTML, CSS, Javascript, Git, Github, SOLR, Elasticsearch, REACT, SharePoint, PowerBI, Handshake, Recommind, iManage, NetDocuments, Intapp, BA Insight","data engineering, sql, nosql, api, mongodb, python, software engineering, web ui, objectoriented programming, functional programming, html, css, javascript, git, github, solr, elasticsearch, react, sharepoint, powerbi, handshake, recommind, imanage, netdocuments, intapp, ba insight","api, ba insight, css, data engineering, elasticsearch, functional programming, git, github, handshake, html, imanage, intapp, javascript, mongodb, netdocuments, nosql, objectoriented programming, powerbi, python, react, recommind, sharepoint, software engineering, solr, sql, web ui"
1061736|Manager - Big Data Consultant|Analytics|Analytics|EXL Datasource|EXL Datasource|Analytics||Manager|Analytics,EXL,"Hartford, NY",https://www.linkedin.com/jobs/view/1061736-manager-big-data-consultant-analytics-analytics-exl-datasource-exl-datasource-analytics-manager-analytics-at-exl-3763336249,2023-12-17,Glens Falls,United States,Mid senior,Onsite,"Hartford, CT, USA
New York, NY, USA Req #176
Friday, February 19, 2021
Company Overview And Culture
EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com .
For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses.
EXL Health is seeking a Data Engineer - Healthcare Analytics, this role will initially be remote but once Stay at Home orders are lifted, this role will be located in New York.
The Data Engineer will be responsible for expanding and optimizing healthcare payer data, data pipeline architecture and data flow to enable analysis across several dimensions.
Responsibilities
Experience with Snowflake cloud data platform including hands-on experience with Snowflake utilities like SnowSQL , SnowPipe , and experience in administering Snowflake.
Accountable for clinical data review and analysis for complex, global projects collated from various data sources formatted in various industry standards (HL7, FHIR, C-CDA, JSON etc.)
Understand healthcare data, including clinical data in both proprietary and industry standard formats (FHIR, C-CDA etc.) and develop mappings / transformation solution between various formats.
Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications)
Expert level skills in SQL, data integration, data modeling and data architecture.
Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
Strong Database experience in Hadoop/Hive , DBMS , SQL server
Experience with programming scripting and data science languages such as Python, UNIX, SQL, Pyspark.
Build large-scale batch and real-time data pipelines using Snowflake cloud data technologies.
Working knowledge of Big Data concepts and Hadoop environment.
Some cloud experience as a developer/engineer like Google cloud platform
Qualifications
Bachelor’s degree in Engineering/Computer Science or related quantitative field
3-5 years of relevant experience on Snowflake, experience with data profiling / mapping tools working in a healthcare data management system environment and experience with clinical data code sets / standards like HL7, CCDAs or FHIR.
Good understanding of various clinical industrial data standards such as HL7, FHIR, C-CDA, JSON
Preferred experience in designing and developing clinical reports or dashboards using clinical data
Good understanding and implementation experience in data architecture, data governance, data quality, data modelling, data ingestion, data integration, data pipeline and data orchestration tools.
Advanced SQL knowledge and experience in working with relational databases, as well as a variety of databases
Strong object-oriented programming experience in Python, Pyspark
Knowledge of Hadoop environment, Hive.
Have experience with google cloud platform
What We Offer
EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants.
You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth
Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.
We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.
Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.
EEO/Minorities/Females/Vets/Disabilities
To view our total rewards offered click here —>
https://www.exlservice.com/us-careers-and-benefits
Base Salary Range Disclaimer:
The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience.
The base salary range listed is just one component of EXL's total compensation package for employees.
Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.
Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy.
Application & Interview Impersonation Warning
– Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s).
EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate’s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL.
Other Details
Pay Type Salary
Apply Now
Show more
Show less","Snowflake, SnowSQL, SnowPipe, HL7, FHIR, CCDA, JSON, SQL, Hadoop/Hive, DBMS, SQL Server, Python, UNIX, Pyspark, Big Data, Google Cloud Platform, Engineering, Computer Science, Data Profiling, Data Mapping, Healthcare Data Management, Clinical Data Code Sets, Data Architecture, Data Governance, Data Quality, Data Modeling, Data Ingestion, Data Integration, Data Pipeline, Data Orchestration, ObjectOriented Programming","snowflake, snowsql, snowpipe, hl7, fhir, ccda, json, sql, hadoophive, dbms, sql server, python, unix, pyspark, big data, google cloud platform, engineering, computer science, data profiling, data mapping, healthcare data management, clinical data code sets, data architecture, data governance, data quality, data modeling, data ingestion, data integration, data pipeline, data orchestration, objectoriented programming","big data, ccda, clinical data code sets, computer science, data architecture, data governance, data ingestion, data integration, data mapping, data orchestration, data pipeline, data profiling, data quality, datamodeling, dbms, engineering, fhir, google cloud platform, hadoophive, healthcare data management, hl7, json, objectoriented programming, python, snowflake, snowpipe, snowsql, spark, sql, sql server, unix"
Principle Data Engineer / Architect,Motion Recruitment Partners LLC,"Illinois, United States",https://www.linkedin.com/jobs/view/principle-data-engineer-architect-at-motion-recruitment-partners-llc-3780690347,2023-12-17,Decatur,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!
Principle Data Engineer / Architect
We are seeking a Principal Data Engineer who can also serve as a Data Architect. In this role, you will lead the transition of our monolithic database system to a cloud-native solution. Collaborating with various teams, you'll design data structures, implement quality checks, and enable reporting and metric monitoring. Additionally, you will identify data sources, establish data pipelines, and mentor your team in best practices for scalable data solutions.
We are one of the largest and most trusted retail healthcare business support organizations in the U.S. that has supported over 16,000 healthcare professionals and team members at more than 1,200 health and wellness offices across 46 states in four distinct categories: Dental care, urgent care, medical aesthetics, and animal health. Working in partnership with independent practice. Our team is passionate about revolutionizing the way people interact with technology, and we're looking for talented individuals to help us achieve our vision. We value creativity, collaboration, and a commitment to pushing the boundaries of what's possible in the tech world.
Your journey at our company will be nothing short of extraordinary. If you're passionate about healthcare, data, and pushing the boundaries of what's possible, this is your opportunity to shine. Help us redefine healthcare and join us in building a brighter, data-driven future for all.
Required Skills & Experience
Bachelors Degree or equivalent combination of education, training, and experience; Master's Degree preferred
5+ years of experience in IT, Analytics, or Data Science
2+ years of experience with SQL
3 years of experience in data modeling
2 years of experience in building data pipelines and implementing quality checks
1 year of experience in setting up and maintaining metrics, alarms, and service monitoring
Desired Skills & Experience
Strong Health Care Data Expertise
Cloud Platform Familiarity (AWS, Google Cloud Platform, Azure)
Data Engineering Skills (SQL, SSIS, SSRS)
Analytics and Machine Learning
What You Will Be Doing
Tech Breakdown
40% Cloud Platforms (AWS, Azure, Google Cloud Platform)
25% SQL and NoSQL Databases
20% Python
15% Data Integration and Orchestration Tools
Daily Responsibilities
60% Hands On
20% Management Duties
20% Team Collaboration
The Offer
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k) {including match- if applicable}
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Show more
Show less","SQL, SSIS, SSRS, Data Modeling, Cloud Platforms (AWS GCP Azure), Data Engineering, Data Pipelines, Quality Checks, Metrics and Monitoring, Data Integration, Orchestration Tools, Python, NoSQL Databases, Machine Learning, Analytics, Health Care Data","sql, ssis, ssrs, data modeling, cloud platforms aws gcp azure, data engineering, data pipelines, quality checks, metrics and monitoring, data integration, orchestration tools, python, nosql databases, machine learning, analytics, health care data","analytics, cloud platforms aws gcp azure, data engineering, data integration, datamodeling, datapipeline, health care data, machine learning, metrics and monitoring, nosql databases, orchestration tools, python, quality checks, sql, ssis, ssrs"
Data Analyst//Pay rate: $33.56/hr,Stellar Professionals,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-pay-rate-%2433-56-hr-at-stellar-professionals-3784567023,2023-12-17,Nome,United States,Associate,Onsite,"Applicant must have 2 years of relevant experience with the following:
Bachelor's Degree in Public Health or similar field with a focus on data analysis or epidemiology.
Working knowledge of epidemiological methods, study design & analysis w/understanding of qualitative and quantitative data collection methods/analysis
Proficient knowledge of computer software (SAS, Excel, Tableau, ArcGIS etc.) to manipulate and draw insights from large data sets.
Working exp. w/data collection & analysis and must be able to assimilate and interpret data, including reporting to multiple stakeholders.
Experience w/extensive data sets such as registries, hospitalization, vital records, and Behavioral Risk Factor Surveillance System (BRFSS) preferred
Familiarity with Indiana DOH organizational structure, administrative processes, and quality improvement initiatives
Prior experience w/cancer data analysis and surveillance.
Show more
Show less","Public Health, Data Analysis, Epidemiology, SAS, Excel, Tableau, ArcGIS, Data Collection, Data Manipulation, Data Interpretation, Reporting, Large Data Sets, Registries, Hospitalization, Vital Records, Behavioral Risk Factor Surveillance System (BRFSS), Indiana DOH, Administrative Processes, Quality Improvement, Cancer Data Analysis, Surveillance","public health, data analysis, epidemiology, sas, excel, tableau, arcgis, data collection, data manipulation, data interpretation, reporting, large data sets, registries, hospitalization, vital records, behavioral risk factor surveillance system brfss, indiana doh, administrative processes, quality improvement, cancer data analysis, surveillance","administrative processes, arcgis, behavioral risk factor surveillance system brfss, cancer data analysis, data collection, data interpretation, data manipulation, dataanalytics, epidemiology, excel, hospitalization, indiana doh, large data sets, public health, quality improvement, registries, reporting, sas, surveillance, tableau, vital records"
Senior Data Engineer,Kforce Inc,"New Canaan, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3777075210,2023-12-17,Nome,United States,Associate,Onsite,"Responsibilities
Kforce has a client in New Canaan, CT that is seeking a Senior Data Engineer. Summary: The primary focus of the position will be to leverage Microsoft Azure services, including Azure Data Lake and Synapse Analytics, to optimize data storage, retrieval, and reporting processes. The ideal candidate will have a strong foundation in data analytics, excellent SQL skills, and expertise in building and optimizing data solutions on the Azure platform. The Senior Data Engineer will also apply data visualization skills to create reports and dashboards.
Requirements
4+ years of experience working in the field of Data Analytics or Data Warehousing
Experience with Microsoft Azure Data Lake, Azure Synapse Analytics, and SQL Data Warehouse
Strong experience with data visualization best practices
Experience with programming languages such as C#, Python or R
Deep understanding of data warehousing concepts
Demonstrated ability to be a strong individual contributor and team player, including the ability to constructively interact with all departments and employee levels
Excellent customer service skills
Excellent written and verbal communication skills
Proven ability to effectively prioritize and work on multiple projects
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $106,250 - $143,750 per year
Show more
Show less","Data Analytics, Data Warehousing, Microsoft Azure, Azure Data Lake, Azure Synapse Analytics, SQL Data Warehouse, Data Visualization, C#, Python, R, Data Warehousing Concepts, Constructive Interaction, Customer Service, Written and Verbal Communication, Project Prioritization","data analytics, data warehousing, microsoft azure, azure data lake, azure synapse analytics, sql data warehouse, data visualization, c, python, r, data warehousing concepts, constructive interaction, customer service, written and verbal communication, project prioritization","azure data lake, azure synapse analytics, c, constructive interaction, customer service, data warehousing concepts, dataanalytics, datawarehouse, microsoft azure, project prioritization, python, r, sql data warehouse, visualization, written and verbal communication"
BI Data Analyst,"Amick Brown - SAP, Cloud Technologies and Business Intelligence Staffing & Consulting","California, United States",https://www.linkedin.com/jobs/view/bi-data-analyst-at-amick-brown-sap-cloud-technologies-and-business-intelligence-staffing-consulting-3759355112,2023-12-17,Nome,United States,Associate,Onsite,"Amick Brown is seeking an experienced
BI Data Analyst
for our direct client.
Location:
Remote
Duration:
12 months
Estimated pay range:
$39 - $49 an hour per client contract and candidate skills, experience, and work location.
Job Description
The BI Data Analyst will be a member of the Quality data analytics team and use technical expertise and a solid understanding of business processes and needs to turn data into useful information.
In this role, the analyst will work closely with various business teams to understand, collect, clarify, and translate business data requirements into reports and analytics as required.
The analyst will be heavily involved with the use, distribution, and successful delivery of reports on a periodic basis.
The ideal candidate would have medical device post market experience and have the ability to explore and integrate different data sources using tools such as SQL and Tableau.
Roles And Responsibilities
Partner with various Quality Management System (QMS) business teams to support data needs
Translate non-technical data requirements into metrics, reports, and visualization dashboards
Partner with business teams to define and document business requirements for new metrics and reports
Ability to execute deep dives into available data sets to provide insight on product quality performance
Develop user documentation and a variety of visual representation of data analysis and ensures consistency and accuracy of data and analysis provided
Support business system integrations for data needs and report outputs
Perform ongoing monitoring and refinement of reports and analytical solutions to ensure a good user experience in support reports and dashboards
Partner with architecture and solution delivery teams to enable reports that have long-term value
Perform functional, regression, usability test, etc. as applicable for both new and existing datasets, reports and dashboards.
Document and summarize test results and findings as required
Required Skills
Education: Bachelor’s or higher degree in Statistics, Mathematics, Computer and Information Science or equivalent blend of work and certification experience required
Minimum 5 years of data analytics experience, with 3+ years hands on using Tableau or another BI Tool required
Minimum 3+ years of experience with Microsoft SQL or equivalent required
Programming skills and experience, especially for data exploration and presentation to management and other users of data required.
Familiarity with Python preferred.
Highly organized, self-starter and highly motivated with strong prioritization skills managing multiple projects required
Creative problem solver that is able to look at an issue from a variety of angles required
Ability to work in fast-paced teams with a great work ethic and an enthusiasm for problem solving and data analysis required
Working experience in medical device, pharmaceutical, regulatory or other related industries highly preferred
Experience and capability using visualization software such as Tableau/ Power BI, and familiarity with SAP, CRM or other ERP system preferred
Knowledge of the business of medical device development and manufacture preferred
Excellent writing skills and documentation skills that enable analyses to be reliably reproduced using the same data years later preferred
We are an AA/EEO/Veterans/Disabled employer.
We will consider for employment qualified applicants with arrest and conviction records in accordance with fair chance laws.
Amick Brown is an Information Technology consulting company specializing in ERP, Data Analytics, Information Security, Application Development, Networking, and Cloud Computing. The company was founded in 2010 and is headquartered in San Ramon, California.
Regular full-time employees are eligible for the following Amick Brown-provided benefits:
Health
Vision
Dental
401k with company match
Paid time off
Sick Leave
Long-Term Disability
Life Insurance
Wellness & Discount Programs
Show more
Show less","BI Data Analyst, Tableau, SQL, Python, Microsoft SQL, Programming, Data Exploration, Data Presentation, Statistical Analysis, Data Visualization, Problem Solving, Data Analytics, Medical Device, Pharmaceutical, Regulatory, SAP, CRM, ERP, Medical Device Development, Data Documentation","bi data analyst, tableau, sql, python, microsoft sql, programming, data exploration, data presentation, statistical analysis, data visualization, problem solving, data analytics, medical device, pharmaceutical, regulatory, sap, crm, erp, medical device development, data documentation","bi data analyst, crm, data documentation, data exploration, data presentation, dataanalytics, erp, medical device, medical device development, microsoft sql, pharmaceutical, problem solving, programming, python, regulatory, sap, sql, statistical analysis, tableau, visualization"
Data Analyst,Rezilient Health,"St Louis, MO",https://www.linkedin.com/jobs/view/data-analyst-at-rezilient-health-3781946727,2023-12-17,Nome,United States,Associate,Onsite,"We're not telehealth and we're not a traditional doctor's office: we're the best parts of both.
Our mission at Rezilient is simple: to make access to primary care convenient, timely and seamless. Because we virtually beam our doctors into our CloudClinics, our members can choose their doctor based on their preferences, not their location, for a completely different primary care experience.
Our doctors can be anywhere, while our CloudClinics are conveniently placed close to where people live, work and shop. Each CloudClinic is staffed by an experienced clinic specialist who becomes the doctor’s hands. Our members can also interact with their Rezilient doctors through chat and video, providing a continuous relationship with their doctor no matter where they are.
Above all, our tech-forward approach streamlines the primary care experience so our doctors have the time to treat our members as a whole person, not just a collection of symptoms. And we’re continuing to add specialty services and breakthrough technology to offer the most comprehensive, convenient care possible.
We are looking for a Data Analyst to come in to support our growing multi-disciplinary operations and set the foundation for intelligent service delivery and rapid scaling. The ideal candidate excels in the analysis and manipulation of large data sets with the ability to create meaningful insights from the data. This candidate should have a passion for doing high-quality work, continuously learning and improving, effectively analyzing healthcare data, consistently developing and improving data and analytics methodologies, and regularly exceeding customer and internal stakeholder expectations. You will work closely with the product, data engineering, clinical, finance, and go-to-market teams in a cross-functional work environment to identify and implement process improvements, develop and maintain timely and accurate reporting and analytics, and support business planning and forecasting.
The selected candidate will have ample opportunity to refine sophisticated analytical skills, hone project management skills, and grow their career, skill sets, and expertise within the healthcare industry. This position is located at our St. Louis HQ in the beautiful DeBaliviere neighborhood, steps from Forest Park.
Key Responsibilities:
Work closely with the CTO, CMO, and CoS on all new data and analytics initiatives. You’ll have a seat at the table and the ability to learn on the fly
Collaborate with cross-functional team to understand business needs and identify areas for process improvement
Develop and maintain reporting and analytics to track key performance indicators and provide insights across the business
Assist with planning and forecasting by analyzing data and providing recommendations
Collaborate with cross-functional teams to identify and implement solutions to improve efficiency and effectiveness through a data-driven approach
Continuously learn, build, and improve across all things “data” at Rezilient, including:
Data Engineering – perform Extract, Transform, Load (ETL) processes to ensure accurate and timely movement of data from diverse sources into a centralized data repository, employing data cleansing and transformation techniques to maintain data quality and integrity
Data Analysis – conduct comprehensive and innovative analysis from claims and clinical data to extract and interpret relevant data, while extracting unique insights or trends from insurance claims and clinical data, such as comorbidities and common healthcare services utilized, and investigate discrepancies between clinical concepts and how they manifest in the data sets
Generating Insights – collaborate with the Product, Marketing, and Clinical teams to translate data insights into actionable strategies for engaging identified members through analysis of clinical data of engaged and enrolled members (e.g., identify correlations with prior healthcare utilization)
Clinical Operations Insight – analyze the performance of treatment protocols, achievement of milestones, and variance at different levels; work with cross-functional teams to generate and validate hypotheses regarding clinical operations
Client Reporting and Material Preparation – prepare and maintain dashboards that translate data insights into internal and external facing materials; develop predictive models for future results and forecasts of early indicators into long-term value; prepare performance year retrospective summaries
Requirements
Degree in Data Science, Computer Science, Statistics, Biostatistics, or related field
2-3 years in a similar role is preferred – with experience working with healthcare data and taxonomies (e.g. claims, eligibility, etc.)
Detail-oriented with a commitment to accuracy
Strong analytical and problem-solving skills – with an ability to translate complex data into understandable insights
Self-starter that is intellectually curious, able to work independently in a team environment, and excited to build from the ground-up without a predefined playbook
Proficiency in Excel, as well as SQL, Python, and other relevant programming languages
Familiarity with modern data stack technologies and cloud-based platforms (e.g., AWS, Snowflake, Dbt, ETL, etc.)
Proficiency in visualization preferably with PowerBI, Looker, etc
Familiarity with Github/Notebook documentation
Exceptional communication and organizational skills
Benefits
Unlimited Vacation & PTO
Medical, Dental, Vision and Life Insurance
Ergonomic Desk Setup
Show more
Show less","Data analysis, Manipulation of data sets, Data engineering, ETL processes, Data transformation, Data cleansing, SQL, Python, Programming languages, Data stack technologies, AWS, Snowflake, Dbt, PowerBI, Looker, Github, Notebook, Exceptional communication, Medical terminology, Healthcare services, Healthcare data, Taxonomies, Claims, Eligibility, Clinical concepts, Statistics, Biostatistics","data analysis, manipulation of data sets, data engineering, etl processes, data transformation, data cleansing, sql, python, programming languages, data stack technologies, aws, snowflake, dbt, powerbi, looker, github, notebook, exceptional communication, medical terminology, healthcare services, healthcare data, taxonomies, claims, eligibility, clinical concepts, statistics, biostatistics","aws, biostatistics, claims, clinical concepts, data engineering, data stack technologies, data transformation, dataanalytics, datacleaning, dbt, eligibility, etl, exceptional communication, github, healthcare data, healthcare services, looker, manipulation of data sets, medical terminology, notebook, powerbi, programming languages, python, snowflake, sql, statistics, taxonomies"
Data Analyst,VRK IT Vision Inc.,"Columbus, OH",https://www.linkedin.com/jobs/view/data-analyst-at-vrk-it-vision-inc-3741179036,2023-12-17,Nome,United States,Associate,Onsite,"Title: Data Analyst
Location: Columbus,OH (Onsite)
Job type: W2
Job Description
Drive the definition, design and delivery of new data products within the customer’s Data Platform
and related applications
Work with Data Engineers, Data Scientists and Data analysts as a single Agile scrum teams on
design, definition, documentation and implementation of features for new applications, business
infrastructure, data security, and data structures
Document and enter user stories in Jira and assist in story point analyses
Document monitor and revise the following artifacts in support of application development:
product requirements, process maps, use cases, user acceptance test plans, project plans, and release
notes.
Create product strategy documents/PRDs that describe business cases, high-level use cases,
technical requirements and ROI
Coordinate and participate in user acceptance testing of functional requirements.
Develop, update and implement standard operating procedures for deliverables including Reports,
Analysis, Presentations and Technical Deliverables.
Interpret and prioritize business needs across various stakeholders group including other Product
Managers, Business Analytics and CDM
Define and track key success metrics and SLAs on the key data products and services Ideal candidate
Will Have Following Background And Skills
Ability to handle ambiguity and fast-paced environment. Prior experience in managing Director+
level stakeholders a plus.
Hands-on experience with SQL and Python
Ability to formulate project strategy & roadmap through Metrics and Data-driven approach.
Ability to pick-up new technology trends and vocabulary of Big-data Tech stack in a short amount
of time.
Strong analytics and structured problem-solving skills.
Strong written and verbal communication skills
Show more
Show less","Data Analyst, SQL, Python, Jira, Agile, Scrum, User Stories, Story Points, Product Requirements, Process Maps, Use Cases, User Acceptance Test Plans, Project Plans, Release Notes, Product Strategy Documents, PRDs, Business Cases, Highlevel Use Cases, Technical Requirements, ROI, User Acceptance Testing, Functional Requirements, Standard Operating Procedures, Reports, Analysis, Presentations, Technical Deliverables, Business Analytics, CDM, Key Success Metrics, SLAs, Bigdata Tech Stack","data analyst, sql, python, jira, agile, scrum, user stories, story points, product requirements, process maps, use cases, user acceptance test plans, project plans, release notes, product strategy documents, prds, business cases, highlevel use cases, technical requirements, roi, user acceptance testing, functional requirements, standard operating procedures, reports, analysis, presentations, technical deliverables, business analytics, cdm, key success metrics, slas, bigdata tech stack","agile, analysis, bigdata tech stack, business analytics, business cases, cdm, dataanalytics, functional requirements, highlevel use cases, jira, key success metrics, prds, presentations, process maps, product requirements, product strategy documents, project plans, python, release notes, reports, roi, scrum, slas, sql, standard operating procedures, story points, technical deliverables, technical requirements, use cases, user acceptance test plans, user acceptance testing, user stories"
Principal Engineer Software (Prisma Access Data-Plane Applications),Palo Alto Networks,"Santa Clara, CA",https://www.linkedin.com/jobs/view/principal-engineer-software-prisma-access-data-plane-applications-at-palo-alto-networks-3784597648,2023-12-17,Nome,United States,Associate,Onsite,"Company Description
Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Our Approach to Work
We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your well-being support to your growth and development, and beyond!
At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together!
Job Description
Your Career
Prisma Access™ (formally GlobalProtect Cloud Service) provides protection straight from the cloud to make access to the cloud secure. It combines the connectivity and security you need and delivers it everywhere you need it. Using cutting-edge public and private cloud technologies extending the next-generation security protection to all cloud services, customers on-premise remote networks and mobile users.
We are seeking an experienced Software Engineer to design, develop and deliver next-generation technologies within our Prisma Access team. We want passionate engineers who love to code and build great products. Engineers who bring new ideas in all facets of software development. We are looking for leaders who take ownership of their areas of focus and who are driven to solve problems at every level. Collaboration and teamwork are at the foundation of our culture and we need engineers who can communicate at a high level and work well with others towards achieving a common goal.
Your Impact
Design, develop and implement highly scalable software features and infrastructure on our next-generation security platform ready for cloud native deployment from inception to completion
Work with different development and quality assurance groups to achieve the best quality - You accomplish this by being hands-on, creating tools, processes, and systems that produce transparency, alignment, and direction
Profile, optimize and tune systems software (management/control/dataplane) for efficient cloud operation
Work with DevOps and the Technical Support teams to troubleshoot customer issues
Work with other software development team to apply PanOS features on Prisma Access
Interview, mentor and coach new team members
Qualifications
Your Experience
10+ years of experience in developing data-plane applications
Required C/C++ Programming
Strong Data structures/Algorithms & debugging
Experience with building applications in the cloud
Nice to have hands-on programming experience in Python and Go
Working experience with Envoy is strongly desired
In-depth understanding of Operating System principles and OS like Linux/Unix
In-depth understanding of networking concepts and TCP/IP stack, TLS
Exposure to building Microservices
Enjoys working with many different teams with strong collaboration and communication skills
Solid foundation in design, data structures, and algorithms, and strong analytical and debugging skills
Education
M.S./B.S. degree in Computer Science or equivalent military experience required
Additional Information
The Team
Our engineering team is at the core of our products – connected directly to the mission of preventing cyberattacks. We are constantly innovating – challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before.
We define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $144,200/yr to $233,200/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.
Is role eligible for Immigration Sponsorship?: Yes
Show more
Show less","C/C++, Data structures, Algorithms, Python, Go, Linux/Unix, TCP/IP stack, TLS, Microservices, Design, Data structures, Algorithms, Computer Science","cc, data structures, algorithms, python, go, linuxunix, tcpip stack, tls, microservices, design, data structures, algorithms, computer science","algorithms, cc, computer science, data structures, design, go, linuxunix, microservices, python, tcpip stack, tls"
Sr. Data Analyst,eSense Incorporated,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-esense-incorporated-3772601277,2023-12-17,Nome,United States,Associate,Onsite,"The preferred candidate for Sr. Data Analyst will possess the skills needed to participate in data management improvement efforts. Candidates must have the ability to analyze and present operational data for work prioritization, work assignment and productivity analysis within a large public service department.
Responsibilities
Understand data source systems and downstream data applications
Identify data dependencies between development teams, despite different release cadences
Anticipate, document, and educate customers for data changes and impacts
Analyze business information requirements
Collaborate with development teams to ensure data requirements are met
Explore data to validate business and provide reports
Recommend appropriate scope of requirements and priorities
Work with architects to translate requirements into technical specifications
Help identify and assess potential data sources
Qualifications
5+ years of experience as a Data Analyst (Government industries strongly preferred)
Strong SQL skills a must
Proficiency in Micro Strategy (or similar reporting tools)
Ability to balance solid strategic thinking and flawless execution in an extremely fast-paced environment
Excellent organization and project management skills, solid written and oral communication skills
Proactive, quick learner, willing to take ownership of assignments and see through to successful completion
Motivated, career-oriented, team player who is organized, professional and friendly
Interact with the customer and SME(s) to identify the relationship between the data element and the business rules to map data
Experience in data mapping and translation on Financial vertical.
Required Qualifications
Experience using extremely large data sets
General understanding of data integrity
Good critical thinking skills
Expert in Microsoft Excel
Advanced Access skills
Show more
Show less","Data Analytics, Data Management, SQL, MicroStrategy, Data Visualization, Data Requirements Gathering, Data Validation, Data Mapping, Microsoft Excel, Microsoft Access, Data Quality, Data Integration","data analytics, data management, sql, microstrategy, data visualization, data requirements gathering, data validation, data mapping, microsoft excel, microsoft access, data quality, data integration","data integration, data management, data mapping, data quality, data requirements gathering, data validation, dataanalytics, microsoft access, microsoft excel, microstrategy, sql, visualization"
"DBA Data Analyst (Business Services Analyst, Associate)",City of Tacoma,"Tacoma, WA",https://www.linkedin.com/jobs/view/dba-data-analyst-business-services-analyst-associate-at-city-of-tacoma-3786315559,2023-12-17,Nome,United States,Associate,Onsite,"Salary:
$32.54 - $45.81 Hourly
Position Description
Tacoma Venues & Events is responsible for sports and entertainment venues within the City of Tacoma (including the Tacoma Dome, Greater Tacoma Convention Center, historic theaters, and Cheney Stadium), and the Special Events program.
TVE is seeking a Data Analyst (Business Services Analyst, Associate classification) capable of assisting our management team to drive strategy with data and actionable insights. This position is responsible for understanding overall TVE operations, generating and analyzing internal and external data, and making recommendations to improve TVE’s decision making in support of our goals. To accomplish this task, the Data Analyst works with all departments to understand, define and document their overarching business objectives. The successful candidate for this position will be the organizational champion for data-driven decision making and foster an environment that promotes innovation and implementation of best industry practices to improve service delivery and efficiency.
Key
responsibilities
Develop data insights and reporting capabilities, and continuously monitor performance and quality control plans to identify improvements to the overall data eco-system.
Combines data as needed from disparate data sources to complete analysis and builds processes to advise the business of Tacoma Venues & Events including analyzing the competitive marketplace, economic impact, and otherwise influence leadership data driven decisions.
With minimal oversight, effectively communicate difficult and complex analysis to colleagues possessing a wide range of backgrounds and perspectives
Complete ad hoc data queries and effectively present analysis using written reports and data visualizations.
Understand legal obligations related to data collection and privacy and advise digital staff.
Develop budget projections for potential events including expenses, projected revenues, ticket scaling and ancillary revenue sources.
Serve as primary point of contact, administrator, and subject matter expert for core TVE internal databases and programs, including ticketing, booking/event planning, and CRM. Develop cross functional data sharing between programs.
Create and develop reports and dashboards for data visualizations to support the day-to day needs of TVE, including customer demographics, marketing metrics, event data, economic impact, and City-wide initiatives (Racial Equity Action Plan, Climate Action Plan, etc.).
Identify, analyze, and interpret trends or patterns in complex data sets.
Work closely with management to prioritize business and information needs.
Locate and define new process improvement opportunities.
Develop then pursue ongoing in-depth knowledge of the industry, sharing insights, trends, benchmarking and best practices
Use historical data to create predictive analysis necessary for large scale event pricing, attendance, and revenue
Work in tandem with the City of Tacoma Information Services team to develop best practices both within the department and in cooperation with the City’s SOPs
Ad hoc special projects as required or as necessary.
Additional information about the Tacoma Venues & Events venues can be found at www.tacomavenues.org
Benefits
The City of Tacoma provides excellent medical, dental and vision benefits for the whole family; paid holidays and personal time off; participation in Tacoma's Public Employees' Retirement System and a growing variety of City-sponsored health and wellness opportunities. For more information on the City of Tacoma's benefit package feel free to explore City of Tacoma Benefits.
City of Tacoma Commitment to Diversity and Inclusion
Tacoma's diversity is its greatest asset. Tacoma embraces its multi-cultural and multi-ethnic character. Communities of color and immigrant communities are fundamental to Tacoma's entrepreneurial spirit, workforce, and long-term success. In Tacoma, equity and empowerment are top priorities, meaning that all Tacoma residents must have equitable opportunities to reach their full potential and share in the benefits of community progress. One of our goals is for the City of Tacoma workforce to reflect the community it serves. We actively work to eliminate racial and other disparities and welcome candidates with diverse backgrounds and/or multicultural skill sets and experiences. Our goal is for Tacoma to be an inclusive and equitable place to live, work, and play.
Qualifications
Minimum Education*
Bachelor's degree public or business administration or directly related field
Minimum Experience*
0-1 years depending upon assignment
Equivalency: 1 year of experience = 1 year of education
Desired Qualifications
Graduation from an accredited four-year college or university with major course work in computer science, economics, mathematics, database administration, business analytics, sports/venue/music management program or completion of a certificate program in similar
Minimum one year experience data analytics, business intelligence, statistics, ideally in a stadium, arena, convention center, conference center, professional sports team or other similar experience.
Ability to work extended, irregular and flexible hours as related to the event schedule, including working evenings, weekends and holidays as required.
Knowledge & Skills
Understanding of Complex SQL and Statistical Methods.
Experience and comfort with multiple Business Intelligence tools.
A passion for finding creative & simple solutions for complex problems.
Experience building web applications and interactive data visualizations
Excellent communication skills with the ability to communicate information clearly to a wide range of stakeholders.
Highly organized to ensure all aspects of the work are carried out in an efficient and timely manner while delivering a superior work product
Solutions focused and a keen problem solver with the ability to think outside of the box; investigate situations and issues, gather and analyze data, and prepare detailed reports and recommendations.
Results-driven, possess an optimistic team-first attitude, and have excellent organizational and leadership skills.
Selection Process & Supplemental Information
Interested individuals should
apply online and attach a detailed resume and cover letter
that includes job experience, major responsibilities and accomplishments related to this position.
Applicants who meet the minimum qualifications will have their responses to the supplemental questions scored as the required Experience & Training (E&T) test. Applicants must receive a passing score (70% or better) to be placed on the eligible list for interview and hiring consideration. The eligibility list established for the recruitment is expected to be good for one year. Applicants selected for interviews may be required to complete a work problem or office skills assessment prior to their interviews.
Appointment is subject to successfully passing the background and reference checks. This position is covered by a Labor Agreement between the City of Tacoma and the Local 483. New employees must successfully complete a nine-month probationary period prior to obtaining permanent status in this classification.
Communication from the City of Tacoma
For questions regarding this specific recruitment contact Kat Flores, HR Analyst.
We primarily communicate via email during the application process. Emails from cityoftacoma.org and/or governmentjobs.com must be placed on your safe domain list to ensure that you receive notifications in a timely manner. As a precaution, you may also want to check your junk email folders.
The online application system requires you to enter a substantial amount of information. Be prepared to spend an hour or more entering the required information. In order for your application materials to be considered, all information must be submitted by the closing date and time listed on this job announcement.
For assistance with the NEOGOV application process, questions regarding this job announcement, or if you are experiencing complications while applying, please contact the Human Resources office at 253-591-5400 by 4:00 pm of the closing date of the job announcement. This will allow us to assist you before the job announcement closes. (For technical difficulties using the NeoGov system, call the applicant support line at 1-855-524-5627 between 6:00 am and 5:00 pm Pacific Time.)
The City of Tacoma provides excellent medical, dental and vision plans for the whole family; paid holidays and paid leave; participation in the Tacoma Public Employees' Retirement System (alternate plan for Police/Fire); continuing education and advancement opportunities and a growing variety of City-sponsored health and wellness opportunities and incentives.
Medical Coverage: For eligible employees and their families, including domestic partners and dependent children age 26 or younger.
Dental Coverage: For eligible employees and their families, including domestic partners and dependent children age 26 or younger.
Vision Coverage: For eligible employees and their eligible dependents.
Paid Leave: City employees are entitled to received paid holidays, sick/vacation leave or personal time off (PTO), depending upon union affiliation and appointment type.
Insurance Plans: Employees are covered by a long-term disability plan. Short-term and expanded long-term disability insurance plans are also available to employees. The State Industrial Insurance Act also covers employees.
Deferred Compensation: Income can be set aside on a pretax basis and invested for supplementation of normal retirement income.
Retirement: All employees of the City, except members of the Police and Fire services, Tacoma Rail and certain project employees, are included in the Tacoma Employees' Retirement System. Information on the Tacoma Employees' Retirement System can be found at www.cityoftacoma.org or by calling (253) 502-8200.
Other Employment Information
Direct Deposit: Employees are paid on a bi-weekly schedule by direct deposit.
Salary Increases: Based on satisfactory job performance, the City provides for a regular progression of salary increases for most classifications according to the salary schedule.
Union Affiliation: Many job classifications are covered by union security provisions which require union membership, dues, or payment of equivalent service fees.
Note: The provisions of this job announcement do not constitute an expressed or implied contract. Any provision contained herein may be modified and/or revoked without notice.
Closing Date/Time: 12/28/2023 5:00 PM Pacific
Show more
Show less","SQL, Statistics, Business Intelligence tools, Web applications, Data visualizations, Communication skills, Problem solving, Microsoft Power BI, Tableau, Data analytics, Data mining, Predictive analysis, Machine learning, Data warehousing, Business intelligence, Data visualization, Data governance, Data engineering, Data management, Data science, Data architecture, Agile development, Scrum, Kanban, PRINCE2","sql, statistics, business intelligence tools, web applications, data visualizations, communication skills, problem solving, microsoft power bi, tableau, data analytics, data mining, predictive analysis, machine learning, data warehousing, business intelligence, data visualization, data governance, data engineering, data management, data science, data architecture, agile development, scrum, kanban, prince2","agile development, business intelligence, business intelligence tools, communication skills, data architecture, data engineering, data governance, data management, data mining, data science, data visualizations, dataanalytics, datawarehouse, kanban, machine learning, microsoft power bi, predictive analysis, prince2, problem solving, scrum, sql, statistics, tableau, visualization, web applications"
"Data Administrator (System Reliability Engineer, Azure))",Abbott,"Lake Forest, IL",https://www.linkedin.com/jobs/view/data-administrator-system-reliability-engineer-azure-at-abbott-3730038625,2023-12-17,Nome,United States,Associate,Onsite,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.
Working at Abbott
At Abbott, You Can Have a Rewarding Career, You Can Grow And Learn, Care For Yourself And Your Family And Have a Healthy Work Life Balance. You’ll Have Access To
At Abbott, you can do work that matters, grow, and learn, care for yourself and family, be your true self and live a full life. You’ll have access to:
Career development with an international company where you can grow the career you dream of.
Free medical coverage for employees* via the Health Investment Plan (HIP) PPO
An excellent retirement savings plan with high employer contribution
Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor’s degree.
A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune.
A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.
The Opportunity
This position works out of either our
Lake Forest, IL or
St. Paul Minnesota
offices in the Rapid Diagnostics division. The resource will be responsible for supporting BI systems and processes such as data pipelines from corporate systems using a combination of BW, Azure Data Factory, Azure Data Lake, and Azure Databricks.
The Data Processing Admin provides support to the global business intelligence team and works across a cross functional team. The person hired will apply the knowledge to ensure that daily processes execute successfully, data is accurate and is fully aligned to the requirements. The admin ensures proper documentation of operational systems and provides level of support with day-to-day operations.
Other Responsibilities Are
Works with the teams to maintain data pipelines and schedules across multiple technologies ensuring timely/trusted/targeted data to data value consumers.
Creates and/or maintains clear, well-constructed strategic technical documents for large or complex system architectures and pipelines using Spark, Python, Scala and other.
Analyzes and troubleshoots BI processes; elicits, and documents technical requirements; identifies solutions, assesses feasibility, and provides best recommendations.
Works directly with the development teams to ensure operational gateways and requirements to enter production systems are completed accurately.
Collaborates with cross functional managers and supports department level initiatives.
Ensures systems are compliant with applicable Corporate and Divisional Policies and procedures.
Uses a solid understanding and application of business concepts, procedures and practices.
Performs work using existing standards, methodologies and processes and understands other systems/business processes.
Required Qualifications
Bachelor’s degree in a related field or relevant work experience.
Minimum 3 years of experience using Azure Data Lake, Data Factory and/or Data Bricks.
Minimum 3 years of experience working in an IT environment independently and in teams.
Required to travel up to 10%, including internationally.
Preferred Qualifications
Experience with DevOps Version Control and Storyboarding.
Experience with regulatory tools for document archiving and document control.
Ability to perform in a regulated and/or validated environment.
General knowledge of a variety of alternatives and their impact on business.
Experience with other reporting tools such as Snowflake, BOBJ, Tableau
Minimum 3 years of experience working with SAP technologies such as Hana or BW.
Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.
Learn more about our health and wellness benefits, which provide the security to help you and your family live full lives:
www.abbottbenefits.com
Follow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.
Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.
The base pay for this position is $80,700.00 – $161,300.00. In specific locations, the pay range may vary from the range posted.
Show more
Show less","Azure Data Factory, Azure Data Lake, Azure Databricks, SAP, SAP Hana, SAP BW, Snowflake, BOBJ, Tableau, Spark, Python, Scala, DevOps, Version Control, Storyboarding, BI, Data Pipelines","azure data factory, azure data lake, azure databricks, sap, sap hana, sap bw, snowflake, bobj, tableau, spark, python, scala, devops, version control, storyboarding, bi, data pipelines","azure data factory, azure data lake, azure databricks, bi, bobj, datapipeline, devops, python, sap, sap bw, sap hana, scala, snowflake, spark, storyboarding, tableau, version control"
Data Analyst,Accroid Inc,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-analyst-at-accroid-inc-3763534389,2023-12-17,Nome,United States,Associate,Onsite,"Able to understand and assess the quality of source data; Gain knowledge of the system to validate the data being reported by providers and prepared for use by stakeholders. Collect metrics for trends in data reported into IBHRS. Review data reported for outliers.
Provide technical assistance and guidance to providers to clean data and prepare it for analysis.
Able to use a relational database, reporting software, and data analytic software to create simple to complex ad hoc queries, write reports and create dashboards against the data reported from IBHRS.
Uses data visualization tools such as Microsoft Excel and Power BI or Tableau. Set up and maintain executive dashboards so all stakeholders (internal and external) view the same data in the same way.
Show more
Show less","Data quality assessment, Data validation, Data collection, Data analysis, Data visualization, Data reporting, Relational database, Reporting software, Data analytic software, Microsoft Excel, Power BI, Tableau, Executive dashboards","data quality assessment, data validation, data collection, data analysis, data visualization, data reporting, relational database, reporting software, data analytic software, microsoft excel, power bi, tableau, executive dashboards","data analytic software, data collection, data quality assessment, data reporting, data validation, dataanalytics, executive dashboards, microsoft excel, powerbi, relational database, reporting software, tableau, visualization"
Senior Data Analyst/Developer,Workcog Inc,"Dallas, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-developer-at-workcog-inc-3747709670,2023-12-17,Nome,United States,Associate,Onsite,"Healthcare Client
100% Remote | CST or EST Hours
12+ Month Contract
MUST Have
10+ years’ experience
Experience with Identity and Access Management in a large organization.
Well versed with MySQL database queries and creation of database views.
Development experience with REST, SOAP, LDAP, MySQL
Development experience with Java
Self-starter who is excellent with managing external relationships and communications (important: following up) with technical contacts
Nice To Have:
IAM Tools/Systems:
Radiant Logic HDAP (High-Availability Directory Access Protocol)
ICS (Industrial Control System)
FID (Federated Identity)... Formally, known as VDS (Virtual Desk Service).
Show more
Show less","Identity and Access Management, MySQL, REST, SOAP, LDAP, Java, Database Queries, Database Views, Radiant Logic HDAP, ICS, FID","identity and access management, mysql, rest, soap, ldap, java, database queries, database views, radiant logic hdap, ics, fid","database queries, database views, fid, ics, identity and access management, java, ldap, mysql, radiant logic hdap, rest, soap"
Salesforce Data Analyst,Software Technology Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-software-technology-inc-3778536234,2023-12-17,Nome,United States,Associate,Onsite,"Hi ,
Hope you are doing safe and healthy!!
We're #hiring. Know anyone who might be interested?
We have an urgent requirement for a
""
Salesforce Data Analyst "".
If you are available and interested in this position, then please #share your updated resume at
Raghava.P@stiorg.com
or you can #call me at
609-416-8027 x 127.
Job Title: Salesforce Data Analyst
Location: New York, NY
Duration: Long Term Contract
Job Description
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes.
This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience bachelor’s degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Work Location: On-site in NYC (40%) and remote.
Thanks,
Raghava Sharma
Sr. US IT Recruiter
Direct:
609-416-8027 X 127
Email:
raghava.p@stiorg.com |
Web:
www.stiorg.com
Linkedin
:
https://www.linkedin.com/in/raghava-sharma-04641b5a/
100 Overlook Center, Suite 200
Princeton, NJ 08540.
Show more
Show less","Salesforce, SQL, Tableau, Data visualization tools, Data Analysis, Data Reporting, Data Integration, Data Quality Management, Data Modeling, ETL, SSIS, Salesforce API, Salesforce Platform, Salesforce OutoftheBox Functionality, Data Governance, Power BI","salesforce, sql, tableau, data visualization tools, data analysis, data reporting, data integration, data quality management, data modeling, etl, ssis, salesforce api, salesforce platform, salesforce outofthebox functionality, data governance, power bi","data governance, data integration, data quality management, data reporting, data visualization tools, dataanalytics, datamodeling, etl, powerbi, salesforce, salesforce api, salesforce outofthebox functionality, salesforce platform, sql, ssis, tableau"
Junior Data Analyst,Red Gate,"Arlington, VA",https://www.linkedin.com/jobs/view/junior-data-analyst-at-red-gate-3783116939,2023-12-17,Nome,United States,Associate,Onsite,"Company Description
At
RED GATE
we do everything we can to serve our clients:
Using the right technical skills, unique methodologies, best practices, and integrated technology, we help clients implement bold solutions. New approaches to emerging and evolving threats. Non-traditional ways to overcome entrenched obstacles. Advantage through opportunity. If you have a serious challenge or problem, we can help you solve it. The below job description provides details on how this role will help to serve our clients.
Job Description
The Red Gate Group is seeking a
Junior Data Analyst
to support the Human Capital Management Office (HCMO), Office of the Under Secretary of Defense for Intelligence & Security (OUSD (I&S)).
Elevate your career as a Junior Data Analyst with us! Dive into cutting-edge Human Capital IT initiatives within the DoD and the Intelligence Community (IC). Your role involves crafting insightful reports by analyzing and visualizing data from diverse databases, ensuring the smooth functioning of DCIPS workforce systems.
As part of your role, you’ll collaborate with a dynamic team of data analysts and SMEs, managing authorization and manning info, configuring databases, and generating tailored reports. You'll be at the forefront of shaping DCIPS data initiatives, attending demos, participating in testing, evaluating ease of use, and ensuring user-friendly interfaces.
If you're passionate about impactful data analysis, IT innovation, and contributing to dynamic decision-making, this is your chance! Join us in shaping the future of DCIPS human capital IT programs and fostering innovation within the DoD and IC. Apply now to be part of a dynamic team at the forefront of data-driven excellence.
Qualifications
Required Experience & Skills
Active TS/SCI
BA/BS Degree
1+ years of experience in database and business intelligence reporting tools
1+ years of experience working with intelligence information systems or architecture supporting the combatant commands, services, or the IC.
1+ years of experience of data-mining in databases such as CMIS, DCPDS, ADVANA; and visualizing analysis discovered from data-mining
Experience with SAS statistical software packages
Experience with basis statistics with demonstrated experience in inferential statistics
Experience with visualizing data in databases
Experience with microsoft excel: pivot tables, graphs, charts and formulas
Experience with visualization tools such as Tableau, Qlik.
Experience with coding, programing, and scripting languages in SQL, Java, Python, R.
Desired Experience & Skills
Program Management experience in a DoD environment
OUSD(I&S) experience is highly preferred
Additional Information
The Red Gate Group, Ltd. is an Equal Opportunity/Affirmative Action Employer. The Red Gate Group, Ltd. considers applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Know Your Rights
Show more
Show less","Data Analysis, Data Visualization, Database Management, Software Development, SQL, Java, Python, R, Tableau, Qlik, SAS, Microsoft Excel, Statistics, Inferential Statistics, Business Intelligence, Coding, Programming, Scripting, DCIPS, CMIS, DCPDS, ADVANA","data analysis, data visualization, database management, software development, sql, java, python, r, tableau, qlik, sas, microsoft excel, statistics, inferential statistics, business intelligence, coding, programming, scripting, dcips, cmis, dcpds, advana","advana, business intelligence, cmis, coding, dataanalytics, database management, dcips, dcpds, inferential statistics, java, microsoft excel, programming, python, qlik, r, sas, scripting, software development, sql, statistics, tableau, visualization"
Data Analyst,Infotree Global Solutions,"Seattle, WA",https://www.linkedin.com/jobs/view/data-analyst-at-infotree-global-solutions-3717278488,2023-12-17,Nome,United States,Associate,Onsite,"Manager is looking for someone that is very advanced in SQL, both creation and simplification along with skills in Job description.
Resource should have experience working with large data sets, analysing the data, visualization skills, data cleaning/data transformation experience.
Have reporting experience is a plus.
Please list 3-4 functional activities the resource should be capable of
Extract, load and transform data from multiple sources for global compliance reporting
Validate and verify data attributes via third party tools, internal tools, and via weblinks
Examine data quality issues for consistency and accuracy, identify resolutions
Investigate inconsistencies
Participate in testing for various test cases a set of reporting requirements by jurisdiction
What technical skills will successful candidates possess?
Understanding of common reporting and file transfer technologies
Proficiency in SQL, Tableau, Excel (e.g.vlookups, sumifs,pivots)
Experience with Power BI, a plus
Experience with the use of various data analytic tools
Show more
Show less","SQL, Tableau, Excel, Power BI, Data Analytics tools, Data Visualization, Data transformation, Data cleaning, Data validation, Data verification, Data quality, Data consistency, Data accuracy, Reporting technologies, File transfer technologies","sql, tableau, excel, power bi, data analytics tools, data visualization, data transformation, data cleaning, data validation, data verification, data quality, data consistency, data accuracy, reporting technologies, file transfer technologies","data accuracy, data analytics tools, data cleaning, data consistency, data quality, data transformation, data validation, data verification, excel, file transfer technologies, powerbi, reporting technologies, sql, tableau, visualization"
Data Analyst with Healthcare- REMOTE,PSRTEK,"New York, NY",https://www.linkedin.com/jobs/view/data-analyst-with-healthcare-remote-at-psrtek-3783931921,2023-12-17,Nome,United States,Associate,Onsite,"Position:- Data Analyst -
strong expertise in Python and SQL
Remote
Extract actionable insights by mining and transforming data from various sources
Apply advanced statistical techniques including distributions, and regression analysis to interpret data and solve complex business problems
Process, cleanse, and validate data to ensure its accuracy, completeness, and reliability
Analyze vast amounts of information to identify patterns, trends, and correlations, providing valuable insights to business stakeholders
Utilize data visualization tools, specifically Power BI, to create intuitive and interactive dashboards
Should have strong expertise in Python and SQL
Demonstrate expertise in data domain knowledge, enabling comprehensive analysis and exploration of healthcare-related data, especially in clinical and financial contexts
Collaborate closely with both business and IT teams
PSRTEK is a reputed technology recruitment and IT staffing brand with a global footprint and an admired client base. As an ideas and innovation powerhouse with a culture of excellence, we bring remarkable expertise and deliver powerfully transformative results.
Show more
Show less","Python, SQL, Data Mining, Data Visualization, Power BI, Statistical Analysis, Data Transformation, Data Cleansing, Data Validation, Clinical Data, Financial Data, Healthcare Data, Business Intelligence, Data Analysis, Data Interpretation, Data Modeling, Reporting, Communication, Collaboration","python, sql, data mining, data visualization, power bi, statistical analysis, data transformation, data cleansing, data validation, clinical data, financial data, healthcare data, business intelligence, data analysis, data interpretation, data modeling, reporting, communication, collaboration","business intelligence, clinical data, collaboration, communication, data interpretation, data mining, data transformation, data validation, dataanalytics, datacleaning, datamodeling, financial data, healthcare data, powerbi, python, reporting, sql, statistical analysis, visualization"
Data Analyst,eStaffing Inc.,"Sacramento, CA",https://www.linkedin.com/jobs/view/data-analyst-at-estaffing-inc-3732652896,2023-12-17,Nome,United States,Associate,Onsite,"Job Description
Researches, analyzes, consolidates and interprets data using statistical and data analytics methods to create information on business- relevant topics, e.g. market environment, operational process and equipment performance etc.
Acquires data from primary or secondary data sources and maintain databases/data systems.
Operates and optimizes pre-defined tools, applications and data bases/data management systems.
Creates reports and communicates results to various internal and/or external stakeholders (e.g. management, customers).
Impacts: Needs professional technological, economical or scientific know how, methods, tools and principles, and applies to differing and variable situations.
Needs to solve assigned, non-routine tasks, to contribute to projects or assignments, or to support development of guidelines, methods, tools or business processes.
Key Responsibilities
""Experienced Professional"" These positions are expected to contribute to and deliver business processes or detailed technology solutions, in a self-managed and target oriented manner, and thus contributes to team results.
Positions focusing on analyzing, developing, testing or implementing processes, technologies and systems in their specific field of expertise, from first principles.
Targets are short to medium term (best to be characterized as milestones), and achievements are monitored. They may provide functional advice to and integrate services of operational staff or semi-professional colleagues.
Typically these functions review to 2 to 5 years of relevant experience in their field of work and qualification. Experience: 2+ years or Master Entry.
Knowledge: Good knowledge in a technical field or business method including the basic theoretical background.
Develops basic business understanding.
Contributes to team effort, awareness of expected value add.
May improve processes, business methods or technical components.
Show more
Show less","Data analysis, Statistical methods, Data analytics, Data consolidation, Data interpretation, Primary data sources, Secondary data sources, Databases, Data systems, Predefined tools, Applications, Data management systems, Reporting, Communication, Stakeholders, Management, Customers, Economics, Scientific knowhow, Methods, Tools, Principles, Processes, Technologies, Systems, Expertise, Milestones, Functional advice, Integration, Services, Experience, Basic theoretical background, Business understanding, Team effort, Valueadd, Process improvement, Business methods, Technical components","data analysis, statistical methods, data analytics, data consolidation, data interpretation, primary data sources, secondary data sources, databases, data systems, predefined tools, applications, data management systems, reporting, communication, stakeholders, management, customers, economics, scientific knowhow, methods, tools, principles, processes, technologies, systems, expertise, milestones, functional advice, integration, services, experience, basic theoretical background, business understanding, team effort, valueadd, process improvement, business methods, technical components","applications, basic theoretical background, business methods, business understanding, communication, customers, data consolidation, data interpretation, data management systems, data systems, dataanalytics, databases, economics, experience, expertise, functional advice, integration, management, methods, milestones, predefined tools, primary data sources, principles, process improvement, processes, reporting, scientific knowhow, secondary data sources, services, stakeholders, statistical methods, systems, team effort, technical components, technologies, tools, valueadd"
Data Engineer,"Mitsubishi HC Capital America, Inc.","Norwalk, CT",https://www.linkedin.com/jobs/view/data-engineer-at-mitsubishi-hc-capital-america-inc-3761823739,2023-12-17,Nome,United States,Associate,Onsite,"For this role, we will consider candidates located near our Norwalk, CT office or Itasca, IL office and must be able to work 5 days per work week in office.
Position Overview
The Data Engineer holds the primary responsibility for skillfully designing, developing, implementing, and supporting Mitsubishi HC Capital America, Inc. (MHCCNA)'s enterprise Microsoft Azure Data Warehouse. This role assumes accountability for ensuring the dependable, efficient, and secure operation and advancement of MHCCNA's On-Premise and Azure Synapse Data Warehouses to effectively meet crucial business needs.
Commitment To Internal Control
The Data Engineer is required to possess a comprehensive understanding of and adhere to the system of internal controls associated with the fundamental duties and responsibilities of the role. This includes compliance with SOX and all other pertinent regulatory and compliance policies and requirements.
Essential Duties And Responsibilities
The responsibility of Data Engineer encompasses the entire lifecycle of the Data Warehouse environments that underpin the vital business requirements of MHCCNA. This includes the design, development, implementation, operation, and ongoing support of these critical systems.
The individual in this position is tasked with developing a flexible, enterprise-level environment that integrates multiple warehouses to guarantee precise, comprehensive, consistent, and timely data. Their primary goal is to create a cohesive system that fulfills these demands while catering to diverse business requirements.
The role necessitates the capacity to explore and grasp emerging technologies while collaborating closely with peer teams to establish strategic roadmaps and priorities. The ability to swiftly acquire and proficiently apply hands-on administration skills is essential. As a Subject Matter Expert in technical requirements, this position will play a crucial role in supporting and implementing data projects, as well as engaging effectively with users and other IT staff.
The Data Engineer Responsibilities Include
Provide support in designing and overseeing enterprise-grade data pipelines and data stores.
Implement automation and streamline processes to optimize the entire data and analytics platform, ensuring efficient throughput and high-performance outcomes.
Recognize, devise, and execute internal process enhancements, including automation of manual tasks, optimizing data delivery, and redesigning architecture or infrastructure to enhance scalability.
Collate large, intricate datasets that align with functional and non-functional business demands.
Develop processes that facilitate data transformation, manage data structures, metadata, dependencies, and workload management.
Collaborate with business users to understand functional and data requirements, contributing to the enhancement of data models and pipelines.
Apply analytical and problem-solving skills to diagnose and resolve intricate technical issues.
Create, maintain, and continuously enhance scalable data pipelines, while also developing new data source integrations to accommodate the growing volume and complexity of data.
Designing, implementing, and managing data extraction, transformation, and loading (ETL) processes.
Creating comprehensive technical specification documents and application interface designs.
Creating data processing and integration solutions for both batch and real-time scenarios, proficiently handling structured and unstructured data.
Participate in design discussions, code reviews, and project-related team meetings.
Ensuring data security and compliance with relevant regulations and best practices in all data operations.
Troubleshooting and resolving data and system issues, stepping in when necessary to address outages and challenges.
Other duties and responsibilities as assigned or needed.
KPI’s (Key Performance Indicators)
Deliver Business Intelligence solutions that are 95% defect-free providing that adequate written business requirements, development time, and business test review were afforded during the project. This standard does not apply to legacy remediation efforts or ready to serve emergency production response activities.
Effectively utilize consulting resources on all significant projects (over 40 hours) to allow for development power of scale. Consultants should do lower value work that is considered heavy lift, freeing up programmer analysts to spend more time in analysis and design while maintaining tight control over quality, code, and company intellectual property.
These are overarching KPI metrics that are applicable to all goals that are defined over the course of the business year.
Responsibility And Decision-Making Authority
Exercise independent judgment and decision-making while adhering to Company Policy.
Management/Supervisory Responsibilities
N/A
Qualifications/Competencies
Key Technical Knowledge, Skills, and Abilities:
Must have experience deploying modern data solutions leveraging components like Azure functions, Azure Synapse, Azure Data Factory, Data Flows, Azure Data Lake, Azure SQL.
Strong level of understanding on Azure Synapse, ADLS, and Azure DevOps.
Exhibit an understanding of Data Lake architectures, including raw, enriched, and curated layer concepts, and ETL/ELT operations.
Exhibit a solid understanding of database design, data warehousing concepts, big data platforms, and ETL operations.
Experience working with data integration techniques & self-service data preparation.
Experience in requirements analysis, design, and prototyping.
Experience with DevOps tools like Azure DevOps, Jenkins, Maven etc.
Experience in building/operating/maintaining fault tolerant and scalable data processing integrations.
Demonstrated experience of turning business use cases and requirements into technical solutions.
Ability to conduct data profiling, cataloging, and mappings for technical design and construction of data flows.
Strong collaboration and experience working with remote teams.
Strong problem-solving skills with emphasis on optimization data pipelines.
Showcase excellent communication and presentation skills for effective collaboration with technical and non-technical stakeholders.
Strong analytical skills and a drive to learn and master new technologies and techniques.
Experience working with third party providers and vendors for critical support requirements.
Competencies:
Possesses the professional or technical skills required to effectively assume job responsibilities and perform tasks.
Conscientiously attends to detail in order to produce precise and error-free work.
Able to identify and analyze a problem, evaluate possible solutions, and select the most suitable one.
Education and Experience:
Demonstrated expertise in Microsoft Azure development.
3-5 years of hands-on Data Warehouse architecture and development experience within the Microsoft Azure environment.
Bachelor’s degree with a minimum of 3-5 years of related work experience outside of educational studies.
Working Hours / Travel Requirements
Hours may vary and will require periodic overtime, including occasional evening and weekends, depending on business needs.
On call 24x7 for emergency support
Must be able to work in either Norwalk CT or Itasca IL office, 5 days per work week in office (no relocation assistance).
Occasional travel for business meetings, seminars or training may be required.
Physical Demands
Digital dexterity and hand/eye coordination in operation of office equipment.
Light lifting and carrying of supplies, files, etc.
Ability to speak to and hear customers and/or other employees via phone, in-person or virtually.
Body motor skills sufficient to enable incumbent to move from one office location to another.
The job description does not constitute an employment contract, implied or otherwise, other than an “at will” relationship and is subject to change by the employer as the needs of the employer and requirements of the job change.
Salary Range: ($97,400 to $111,700) per year, plus a discretionary Bonus.
The salary range is determined and based on internal equity, market data/ranges, applicant's skills, prior relevant experience, and education.
Additional Benefits
Medical, Dental and Vision Plans
401(k) and matching
Generous Paid Time Off
Company paid Life Insurance
Employee assistance program
Training and Development Opportunities
Employee discounts
Show more
Show less","Microsoft Azure, Azure functions, Azure Synapse, Azure Data Factory, Data Flows, Azure Data Lake, Azure SQL, ADLS, Azure DevOps, Data Lake architectures, Database design, Data warehousing, Big data platforms, ETL operations, Data integration, Selfservice data preparation, Requirements analysis, Design, Prototyping, DevOps tools, Fault tolerant data processing, Scalable data processing, Data profiling, Data cataloging, Data mappings, Communication, Presentation, Problemsolving, Optimization, Analytical skills, Third party providers, Digital dexterity, Hand/eye coordination","microsoft azure, azure functions, azure synapse, azure data factory, data flows, azure data lake, azure sql, adls, azure devops, data lake architectures, database design, data warehousing, big data platforms, etl operations, data integration, selfservice data preparation, requirements analysis, design, prototyping, devops tools, fault tolerant data processing, scalable data processing, data profiling, data cataloging, data mappings, communication, presentation, problemsolving, optimization, analytical skills, third party providers, digital dexterity, handeye coordination","adls, analytical skills, azure data factory, azure data lake, azure devops, azure functions, azure sql, azure synapse, big data platforms, communication, data cataloging, data flows, data integration, data lake architectures, data mappings, data profiling, database design, datawarehouse, design, devops tools, digital dexterity, etl operations, fault tolerant data processing, handeye coordination, microsoft azure, optimization, presentation, problemsolving, prototyping, requirements analysis, scalable data processing, selfservice data preparation, third party providers"
Data Engineer,Datavant,United States,https://www.linkedin.com/jobs/view/data-engineer-at-datavant-3785833825,2023-12-17,Arthur,United States,Mid senior,Remote,"Datavant protects, connects, and delivers the world’s health data to power better decisions and advance human health. We are a data logistics company for healthcare whose products and solutions enable organizations to move and connect data securely. Datavant has a network of networks consisting of thousands of organizations, more than 70,000 hospitals and clinics, 70% of the 100 largest health systems, and an ecosystem of 500+ real-world data partners.
By joining Datavant today, you’re stepping onto a highly collaborative, remote-first team that is passionate about creating transformative change in healthcare. We hire for three traits: we want people who are smart, nice, and get things done. We invest in our people and believe in hiring for high-potential and humble individuals who can rapidly grow their responsibilities as the company scales. Datavant is a distributed, remote-first team, and we empower Datavanters to shape their working environment in a way that suits their needs.
The Healthjump delivery data engineers are masters of data ETL. We create and maintain integrations for over 60+ EHR’s powering every patient use case you can think of. Our team is full of go getters that work in a matrixed 100% remote environment on agile teams that support customers from go live to support for the live of the integration.
You will:
Create and maintain ELT/ETL processes for existing and new systems.
Collaborate with development and business teams to understand requirements and define source system data flows
Develop and maintain ETL/ETL specifications for data integration development
Define and deliver consistent data modeling and data architecture standards, methodologies, guidelines and techniques
Document, implement and maintain the data pipeline architecture and related business processes
Serve as a source of knowledge of industry practices and processes.
Participate in the development of enterprise standards and guidelines for data model quality and accuracy
Audit project level data model quality deliverables to ensure that practices and standards are met
Analyze information and data requirements and understand effects of data inconsistencies
Identify inefficiencies in current architecture and processes and communicate solutions in a manner that gets support from the teams involved
Perform cost and sizing estimates for projects
Collaborate with the project coordinator and the rest of the agile team to identify epics, stories and estimate effort
Create and maintain data dictionary documents, table and data lineage models and produce artifacts to support project development and communicate project information to customers
Requirements:
Bachelors in Computer Science or other engineering degree equivalent
What you will bring to the table:
Master of SQL and Python Languages
Experience with using Airflow
Bonus points if Requirements:
Healthcare data experience
Knowledge of clinical systems (e.g. Cerner, Epic, Meditech, etc.) and standard Acute/Ambulatory workflows.
Preferred AWS Cloud Practitioner or above certification
We are committed to building a diverse team of Datavanters who are all responsible for stewarding a high-performance culture in which all Datavanters belong and thrive. We are proud to be an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status.
Our compensation philosophy is to be externally competitive, internally fair, and not win or lose on compensation. Salary ranges for this position are developed with the support of benchmarks and industry best practices.
We’re building a high-growth, high-autonomy culture. We rely less on job titles and more on cultivating an environment where anyone can contribute, the best ideas win, and personal growth is driven by expanding impact. The range posted is for a given job title, which can include multiple levels. Individual rates for the same job title may differ based on their level, responsibilities, skills, and experience for a specific job. The estimated salary range for this role is $90,000 - $130,000.
At the end of this application, you will find a set of voluntary demographic questions. If you choose to respond, your responses will be
anonymous and
used to help us identify areas of improvement in our recruitment process.
(
We can only see aggregate responses, not individual responses. In fact, we aren’t even able to see if you’ve responded or not
.)
Responding is your choice and it will not be used in any way in our hiring process
.
Show more
Show less","SQL, Python, Airflow, AWS Cloud, Acute/Ambulatory workflows, Cerner, Epic, Meditech, ETL/ELT, Data modeling, Data architecture, Healthcare data, Clinical systems","sql, python, airflow, aws cloud, acuteambulatory workflows, cerner, epic, meditech, etlelt, data modeling, data architecture, healthcare data, clinical systems","acuteambulatory workflows, airflow, aws cloud, cerner, clinical systems, data architecture, datamodeling, epic, etlelt, healthcare data, meditech, python, sql"
Data Engineer,Brooksource,United States,https://www.linkedin.com/jobs/view/data-engineer-at-brooksource-3773890689,2023-12-17,Arthur,United States,Mid senior,Remote,"Data Engineer
Fully Remote (EST)
6 Month Contract
We are looking for a skilled Snowflake Contractor to assist in migrating our data from Redshift and Google BigQuery to Snowflake. The ideal candidate will have a strong background in database management, ETL processes, and cloud computing services. Your role will be crucial in optimizing our data processes and ensuring efficient use of Snowflake's features and pricing structure. This position offers the opportunity to work with cutting-edge technology in a challenging yet rewarding environment, where your contributions will significantly impact the efficiency and effectiveness of our data management and analysis capabilities.
Responsibilities:
Lead the migration of data from Redshift and Google BigQuery to Snowflake, ensuring minimal disruption and data integrity
Optimize queries and data processes for efficiency and performance in the Snowflake environment
Possess a thorough understanding of Snowflake's capabilities and pricing structure to maximize its utility
Develop and manage robust ETL processes, integrating multiple databases and ensuring seamless data flow
Utilize cloud services like Lambda, EventBridge, Redshift, GCP, and Google Cloud Functions effectively in data processes. Understand how these services integrate with Snowflake
Identify and implement opportunities to enhance performance through database structure improvements and indexing methods
Advise on infrastructure changes to augment storage capacity and performance
Demonstrate strong proficiency in SQL and Python for database management. Create and maintain comprehensive design and code documentation
Work closely with cross-functional teams to understand data needs and communicate technical concepts effectively
Qualifications:
At least 5 years in an advanced analytics role, or 3 years with a related advanced degree
Proven experience in database management and migration, especially with Snowflake, Redshift, and Google BigQuery
Bachelor’s or Master’s Degree in Computer Science, Information Technology, or related field is preferred
Technical Expertise: Deep knowledge of ETL processes and experience with multiple databases. Familiarity with Lambda, EventBridge, GCP, and Google Cloud Functions
Analytical Skills: Strong ability in optimizing queries, improving database structures, and indexing methods for performance enhancement
Technical Proficiency: Strong coding skills in SQL and Python. Experience in creating and maintaining design and code documentation
In-depth understanding of Snowflake, including its features, capabilities, and pricing structure
Excellent communication abilities, capable of collaborating with diverse teams and explaining technical concepts clearly
Strong problem-solving skills, with the ability to recommend and implement effective solutions for data storage and performance
ABOUT EIGHT ELEVEN:
At Eight Eleven, our business is people. Relationships are at the center of what we do. A successful partnership is only as strong as the relationship built. We’re your trusted partner for IT hiring, recruiting and staffing needs.
For over 16 years, Eight Eleven has established and maintained relationships that are designed to meet your IT staffing needs. Whether it’s contract, contract-to-hire, or permanent placement work, we customize our search based upon your company's unique initiatives, culture and technologies. With our national team of recruiters placed at 21 major hubs around the nation, Eight Eleven finds the people best-suited for your business. When you work with us, we work with you. That’s the Eight Eleven promise.
Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","Snowflake, Redshift, Google BigQuery, ETL, Lambda, EventBridge, GCP, Google Cloud Functions, SQL, Python, Data migration, Database management, Cloud computing, Data analysis, Data storage, Performance optimization, Communication, Problemsolving","snowflake, redshift, google bigquery, etl, lambda, eventbridge, gcp, google cloud functions, sql, python, data migration, database management, cloud computing, data analysis, data storage, performance optimization, communication, problemsolving","cloud computing, communication, data migration, data storage, dataanalytics, database management, etl, eventbridge, gcp, google bigquery, google cloud functions, lambda, performance optimization, problemsolving, python, redshift, snowflake, sql"
Data Engineer,murmuration,United States,https://www.linkedin.com/jobs/view/data-engineer-at-murmuration-3782265057,2023-12-17,Arthur,United States,Mid senior,Remote,"Who We Are
Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact – and to put those decisions into action.
Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming.
About the Position
We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values.
The Data Engineer will report to our Senior Data Engineer.
What You’ll Do:
Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores;
Ensure pipelines are scalable, reliable, and fault-tolerant;
Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally;
Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity;
Transform and cleanse raw data into a structured and usable format;
Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines;
Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency;
Continuously optimize data pipelines for better performance and cost efficiency;
Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations;
Ensure documentation is up-to-date and accessible to team members;
Provide support for data-related issues, including investigating and resolving pipeline failures;
Respond to ad-hoc data requests and troubleshoot data-related problems;
Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and
Work closely with other data engineers to align data pipelines with overall data architecture strategies.
What You
Should
Have:
Education and/or experience in Computer Science, Computer Engineering, or relevant field;
A minimum of 3 years’ experience working with large scale databases/cloud databases using SQL and Python;
Strong organizational and analytical abilities;
Strong problem-solving skills;
Strong written and verbal communication skills;
Familiarity with Data Orchestration Tools (Dagster, Airflow);
Familiarity with Snowflake and AWS (primarily S3, EC2, ECS);
Experience working flexibly within smaller teams; and
Practical knowledge of software development lifecycle (SDLC).
What You
Could
Have:
Familiarity with Voter File Data;
Experience with or interest in political data; and
Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.)
Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!
Location and Compensation
The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is $100,000 - $130,000 and is commensurate with experience.
Our Culture of Care
We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:
Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members;
Retirement benefits with a 4% employer match;
A flexible unlimited PTO plan;
Generous paid parental leave;
Pre-tax commuter benefits;
A company laptop;
A flexible remote work environment;
A home office setup stipend for all new employees;
Monthly reimbursement for remote work expenses;
A yearly professional development fund;
Mental health and wellness benefits through Calm and Better Help; and
Yearly in-person staff retreats; and
A welcoming culture that celebrates diversity, equity, and inclusion.
An Equal-Opportunity Employer with a Commitment to Diversity
Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.
Show more
Show less","Dagster, Airflow, Snowflake, AWS, MongoDB, SQL, Python, Data Orchestration Tools, Voter File Data, Political Data, SDLC, ETL, Data Engineering, Data Pipelines, Data Infrastructure, Data Quality, Data Integrity, Data Transformation, Data Cleansing, Data Monitoring, Data Alerting, Data Architecture, Data Lineage, Data Dependencies, Data Configurations, Data Documentation, Data Analytics, Data Science, Data Management","dagster, airflow, snowflake, aws, mongodb, sql, python, data orchestration tools, voter file data, political data, sdlc, etl, data engineering, data pipelines, data infrastructure, data quality, data integrity, data transformation, data cleansing, data monitoring, data alerting, data architecture, data lineage, data dependencies, data configurations, data documentation, data analytics, data science, data management","airflow, aws, dagster, data alerting, data architecture, data configurations, data dependencies, data documentation, data engineering, data infrastructure, data integrity, data lineage, data management, data monitoring, data orchestration tools, data quality, data science, data transformation, dataanalytics, datacleaning, datapipeline, etl, mongodb, political data, python, sdlc, snowflake, sql, voter file data"
