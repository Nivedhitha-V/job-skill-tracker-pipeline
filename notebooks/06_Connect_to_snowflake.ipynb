{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folders = [\n",
        "    'data/feed_simulated',\n",
        "    'data/processed',\n",
        "    'logs'\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Folder structure ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_f8mCLr9cUy",
        "outputId": "9e0a5454-a8ae-4bb2-fdc6-7fd38f494ac2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Folder structure ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gVnTrEm8bft",
        "outputId": "43b14611-06c4-40b2-a868-ce7fd4aac89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Splitting 'full_jobs.csv' into 10 feed files...\n",
            "âœ… Done: Created day1.csv to day10.csv in data/feed_simulated/\n",
            "ğŸ“¥ Processing: day1.csv\n",
            "âœ… Saved: data/processed/skills_day1.csv\n",
            "ğŸ“ Logged: day1.csv\n",
            "ğŸ“¥ Processing: day10.csv\n",
            "âœ… Saved: data/processed/skills_day10.csv\n",
            "ğŸ“ Logged: day10.csv\n",
            "ğŸ“¥ Processing: day2.csv\n",
            "âœ… Saved: data/processed/skills_day2.csv\n",
            "ğŸ“ Logged: day2.csv\n",
            "ğŸ“¥ Processing: day3.csv\n",
            "âœ… Saved: data/processed/skills_day3.csv\n",
            "ğŸ“ Logged: day3.csv\n",
            "ğŸ“¥ Processing: day4.csv\n",
            "âœ… Saved: data/processed/skills_day4.csv\n",
            "ğŸ“ Logged: day4.csv\n",
            "ğŸ“¥ Processing: day5.csv\n",
            "âœ… Saved: data/processed/skills_day5.csv\n",
            "ğŸ“ Logged: day5.csv\n",
            "ğŸ“¥ Processing: day6.csv\n",
            "âœ… Saved: data/processed/skills_day6.csv\n",
            "ğŸ“ Logged: day6.csv\n",
            "ğŸ“¥ Processing: day7.csv\n",
            "âœ… Saved: data/processed/skills_day7.csv\n",
            "ğŸ“ Logged: day7.csv\n",
            "ğŸ“¥ Processing: day8.csv\n",
            "âœ… Saved: data/processed/skills_day8.csv\n",
            "ğŸ“ Logged: day8.csv\n",
            "ğŸ“¥ Processing: day9.csv\n",
            "âœ… Saved: data/processed/skills_day9.csv\n",
            "ğŸ“ Logged: day9.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# -------------------------\n",
        "# CONFIGURATION\n",
        "# -------------------------\n",
        "FULL_DATASET = '/content/postings_cleaned.csv'  # <-- Your full uploaded file\n",
        "FEED_FOLDER = 'data/feed_simulated'\n",
        "OUTPUT_FOLDER = 'data/processed'\n",
        "LOG_FILE = 'logs/ingestion.log'\n",
        "SKILLS_COLUMN = 'job_skills_cleaned'  # Adjust if needed\n",
        "START_DATE = datetime(2025, 7, 1)  # Simulated start date for day1\n",
        "\n",
        "# -------------------------\n",
        "# SETUP FOLDERS\n",
        "# -------------------------\n",
        "for folder in [FEED_FOLDER, OUTPUT_FOLDER, 'logs']:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# SPLIT FULL DATA INTO 10 FILES\n",
        "# -------------------------\n",
        "if not os.path.exists(os.path.join(FEED_FOLDER, 'day1.csv')):\n",
        "    print(\"ğŸ“¦ Splitting 'full_jobs.csv' into 10 feed files...\")\n",
        "    try:\n",
        "        df_full = pd.read_csv(FULL_DATASET)\n",
        "        chunk_size = len(df_full) // 10\n",
        "        for i in range(10):\n",
        "            start = i * chunk_size\n",
        "            end = (i + 1) * chunk_size if i < 9 else len(df_full)\n",
        "            chunk = df_full.iloc[start:end]\n",
        "            chunk.to_csv(os.path.join(FEED_FOLDER, f'day{i+1}.csv'), index=False)\n",
        "        print(\"âœ… Done: Created day1.csv to day10.csv in data/feed_simulated/\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading '{FULL_DATASET}': {e}\")\n",
        "        exit()\n",
        "\n",
        "# -------------------------\n",
        "# FUNCTIONS\n",
        "# -------------------------\n",
        "\n",
        "def get_next_file():\n",
        "    processed = set()\n",
        "    if os.path.exists(LOG_FILE):\n",
        "        with open(LOG_FILE, 'r') as f:\n",
        "            processed = set(line.strip() for line in f)\n",
        "    all_files = sorted(f for f in os.listdir(FEED_FOLDER) if f.endswith('.csv'))\n",
        "    for file in all_files:\n",
        "        if file not in processed:\n",
        "            return file\n",
        "    return None\n",
        "\n",
        "def extract_top_skills(df):\n",
        "    all_skills = df[SKILLS_COLUMN].dropna().str.split(',').explode().str.strip().str.lower()\n",
        "    top_skills = all_skills.value_counts().head(20).reset_index()\n",
        "    top_skills.columns = ['skill', 'count']\n",
        "    return top_skills\n",
        "\n",
        "def save_output(df, output_path):\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… Saved: {output_path}\")\n",
        "\n",
        "def log_file(file_name):\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(file_name + '\\n')\n",
        "\n",
        "def main():\n",
        "    next_file = get_next_file()\n",
        "    if not next_file:\n",
        "        print(\"ğŸ‰ All files processed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸ“¥ Processing: {next_file}\")\n",
        "    try:\n",
        "        df = pd.read_csv(os.path.join(FEED_FOLDER, next_file))\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading {next_file}: {e}\")\n",
        "        return\n",
        "\n",
        "    if SKILLS_COLUMN not in df.columns:\n",
        "        print(f\"âŒ Column '{SKILLS_COLUMN}' not found in data.\")\n",
        "        return\n",
        "\n",
        "    top_skills = extract_top_skills(df)\n",
        "\n",
        "    day_id = next_file.replace('.csv', '')        # e.g., 'day1'\n",
        "    day_num = int(day_id[3:])                     # e.g., 1\n",
        "    simulated_date = START_DATE + timedelta(days=day_num - 1)\n",
        "\n",
        "    top_skills['date'] = simulated_date.date()\n",
        "\n",
        "    output_file = os.path.join(OUTPUT_FOLDER, f'skills_{day_id}.csv')\n",
        "    save_output(top_skills, output_file)\n",
        "    log_file(next_file)\n",
        "    print(f\"ğŸ“ Logged: {next_file}\")\n",
        "\n",
        "# -------------------------\n",
        "# RUN LOOP FOR ALL 10 DAYS\n",
        "# -------------------------\n",
        "if __name__ == '__main__':\n",
        "    for _ in range(10):\n",
        "        main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkN0CkFY9oqD",
        "outputId": "4dbb1783-e627-4d11-a080-7b6df4460e60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ All files processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "feed_files = sorted(os.listdir('data/feed_simulated/'))\n",
        "print(\"ğŸ“ Feed Files Found:\", feed_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vBbNL4M9pcU",
        "outputId": "98b6c8c5-ffb1-43ee-8171-ad8e609c410a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Feed Files Found: ['day1.csv', 'day10.csv', 'day2.csv', 'day3.csv', 'day4.csv', 'day5.csv', 'day6.csv', 'day7.csv', 'day8.csv', 'day9.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to Snowflake\n"
      ],
      "metadata": {
        "id": "wE7q9rVXKHqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-connector-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQzwP_DRiFM",
        "outputId": "d863a281-545c-4d83-8ab9-942fcfc1fe07"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.11/dist-packages (3.16.0)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: boto3>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.39.4)\n",
            "Requirement already satisfied: botocore>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.39.4)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snowflake.connector\n",
        "import getpass\n",
        "\n",
        "# Connect to Snowflake\n",
        "conn = snowflake.connector.connect(\n",
        "    user=getpass.getpass(\"Enter Snowflake username: \"),\n",
        "    password=getpass.getpass(\"Enter Snowflake password: \"),\n",
        "    account=getpass.getpass(\"Enter Snowflake account ID (e.g. xy12345.ap-south-1): \"),\n",
        "    warehouse=getpass.getpass(\"Enter warehouse name: \"),\n",
        "    database=getpass.getpass(\"Enter database name: \"),\n",
        "    schema=getpass.getpass(\"Enter schema name: \")\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "CFcqB2zWKO2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "PROCESSED_FOLDER = 'data/processed'\n",
        "LOG_FILE = 'logs/uploaded_files.log'\n",
        "\n",
        "# Create log folder if missing\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "# Load already uploaded files (if any)\n",
        "if os.path.exists(LOG_FILE):\n",
        "    with open(LOG_FILE, 'r') as f:\n",
        "        uploaded_files = set(line.strip() for line in f)\n",
        "else:\n",
        "    uploaded_files = set()\n",
        "\n",
        "# Loop through all processed skill files\n",
        "for file in sorted(os.listdir(PROCESSED_FOLDER)):\n",
        "    if file.endswith('.csv') and file not in uploaded_files:\n",
        "        file_path = os.path.join(PROCESSED_FOLDER, file)\n",
        "        print(f\"ğŸ“¤ Loading: {file}\")\n",
        "\n",
        "        # Read file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Insert each row into Snowflake\n",
        "        for _, row in df.iterrows():\n",
        "            skill = row['skill']\n",
        "            count = int(row['count'])\n",
        "            date = row['date']\n",
        "\n",
        "            query = \"\"\"\n",
        "                INSERT INTO top_skills_daily (skill, count, date)\n",
        "                VALUES (%s, %s, %s)\n",
        "            \"\"\"\n",
        "            conn.cursor().execute(query, (skill, count, date))\n",
        "\n",
        "        # Mark file as uploaded\n",
        "        with open(LOG_FILE, 'a') as f:\n",
        "            f.write(file + '\\n')\n",
        "\n",
        "        print(f\"âœ… Inserted & Logged: {file}\")\n",
        "\n",
        "    elif file in uploaded_files:\n",
        "        print(f\"â© Skipped (already uploaded): {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4hu-8GqSbWV",
        "outputId": "e9069c5f-30e4-4646-b2a7-fcd16aaf3f3b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Loading: skills_day1.csv\n",
            "âœ… Inserted & Logged: skills_day1.csv\n",
            "ğŸ“¤ Loading: skills_day10.csv\n",
            "âœ… Inserted & Logged: skills_day10.csv\n",
            "ğŸ“¤ Loading: skills_day2.csv\n",
            "âœ… Inserted & Logged: skills_day2.csv\n",
            "ğŸ“¤ Loading: skills_day3.csv\n",
            "âœ… Inserted & Logged: skills_day3.csv\n",
            "ğŸ“¤ Loading: skills_day4.csv\n",
            "âœ… Inserted & Logged: skills_day4.csv\n",
            "ğŸ“¤ Loading: skills_day5.csv\n",
            "âœ… Inserted & Logged: skills_day5.csv\n",
            "ğŸ“¤ Loading: skills_day6.csv\n",
            "âœ… Inserted & Logged: skills_day6.csv\n",
            "ğŸ“¤ Loading: skills_day7.csv\n",
            "âœ… Inserted & Logged: skills_day7.csv\n",
            "ğŸ“¤ Loading: skills_day8.csv\n",
            "âœ… Inserted & Logged: skills_day8.csv\n",
            "ğŸ“¤ Loading: skills_day9.csv\n",
            "âœ… Inserted & Logged: skills_day9.csv\n"
          ]
        }
      ]
    }
  ]
}
