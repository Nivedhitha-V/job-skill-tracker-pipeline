{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vn-xmxb5ZOa",
        "outputId": "4e6abea4-cdab-43de-eb5b-91907ec51d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2214155171.py:10: ParserWarning: Skipping line 219: expected 13 fields, saw 15\n",
            "Skipping line 1727: expected 13 fields, saw 16\n",
            "Skipping line 2379: expected 13 fields, saw 14\n",
            "Skipping line 3260: expected 13 fields, saw 15\n",
            "Skipping line 3954: expected 13 fields, saw 21\n",
            "Skipping line 4368: expected 13 fields, saw 14\n",
            "\n",
            "  df = pd.read_csv(input_file, on_bad_lines='warn')  # Skip rows with column mismatch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Total rows after cleanup: 5194\n",
            "ğŸ“¦ Splitting into 10 files (~519 rows each)\n",
            "âœ… Created data/feed_simulated/day1.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day2.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day3.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day4.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day5.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day6.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day7.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day8.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day9.csv with 519 rows\n",
            "âœ… Created data/feed_simulated/day10.csv with 523 rows\n",
            "\n",
            "ğŸ‰ Dataset splitting complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def split_dataset(input_file, output_folder, num_files=10):\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Read the full dataset with error handling\n",
        "    try:\n",
        "        df = pd.read_csv(input_file, on_bad_lines='warn')  # Skip rows with column mismatch\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ File not found: {input_file}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading file: {e}\")\n",
        "        return\n",
        "\n",
        "    total_rows = len(df)\n",
        "    chunk_size = total_rows // num_files\n",
        "\n",
        "    print(f\"ğŸ“„ Total rows after cleanup: {total_rows}\")\n",
        "    print(f\"ğŸ“¦ Splitting into {num_files} files (~{chunk_size} rows each)\")\n",
        "\n",
        "    # Split and save\n",
        "    for i in range(num_files):\n",
        "        start = i * chunk_size\n",
        "        end = (i + 1) * chunk_size if i < num_files - 1 else total_rows\n",
        "        chunk = df.iloc[start:end]\n",
        "        output_path = os.path.join(output_folder, f'day{i+1}.csv')\n",
        "        chunk.to_csv(output_path, index=False)\n",
        "        print(f\"âœ… Created {output_path} with {len(chunk)} rows\")\n",
        "\n",
        "    print(\"\\nğŸ‰ Dataset splitting complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Modify these paths if needed\n",
        "    input_file = '/content/postings_cleaned.csv'\n",
        "    output_folder = 'data/feed_simulated/'\n",
        "\n",
        "    split_dataset(input_file, output_folder)\n"
      ]
    }
  ]
}